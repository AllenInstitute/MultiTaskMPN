{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374fb8f2-9b83-44ce-821b-8917a114c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# PyTorch Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Data Handling and Image Processing\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Style for Matplotlib\n",
    "import scienceplots\n",
    "plt.style.use('science')\n",
    "plt.style.use(['no-latex'])\n",
    "\n",
    "# Scientific Computing and Machine Learning\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.linalg import subspace_angles\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Custom Modules and Extensions\n",
    "sys.path.append('/content/drive/MyDrive/neuro_research/mpns_more/')\n",
    "sys.path.append(\"../netrep/\")\n",
    "sys.path.append(\"../svcca/\")\n",
    "import cca_core\n",
    "from netrep.metrics import LinearMetric\n",
    "import networks as nets  # Contains RNNs\n",
    "import net_helpers\n",
    "import tasks\n",
    "import helper\n",
    "# import mpn\n",
    "\n",
    "# Memory Optimization\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1556c8-a4b6-434b-a60f-37035980bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 Red, 1 blue, 2 green, 3 purple, 4 orange, 5 teal, 6 gray, 7 pink, 8 yellow\n",
    "c_vals = ['#e53e3e', '#3182ce', '#38a169', '#805ad5','#dd6b20', '#319795', '#718096', '#d53f8c', '#d69e2e',]\n",
    "c_vals_l = ['#feb2b2', '#90cdf4', '#9ae6b4', '#d6bcfa', '#fbd38d', '#81e6d9', '#e2e8f0', '#fbb6ce', '#faf089',]\n",
    "c_vals_d = ['#9b2c2c', '#2c5282', '#276749', '#553c9a', '#9c4221', '#285e61', '#2d3748', '#97266d', '#975a16',]\n",
    "l_vals = ['solid', 'dashed', 'dotted', 'dashdot', '-', '--', '-.', ':', (0, (3, 1, 1, 1)), (0, (5, 10))]\n",
    "markers_vals = ['.', 'o', 'v', '^', '<', '>', '1', '2', '3', '4', 's', 'p', '*', 'h', 'H', '+', 'x', 'D', 'd', '|', '_']\n",
    "hyp_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34978bf6-67b8-41bd-a022-a7b46a320686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules if changes have been made to them\n",
    "from importlib import reload\n",
    "\n",
    "reload(nets)\n",
    "reload(net_helpers)\n",
    "# reload(analysis)\n",
    "\n",
    "seed = 6\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "hyp_dict['task_type'] = 'multitask' # int, NeuroGym, multitask\n",
    "hyp_dict['mode_for_all'] = \"random_batch\"\n",
    "hyp_dict['ruleset'] = 'delaygo' # low_dim, all, test\n",
    "\n",
    "accept_rules = ('fdgo', 'fdanti', 'delaygo', 'delayanti', 'reactgo', 'reactanti', \n",
    "                'delaydm1', 'delaydm2', 'dmsgo', 'dmcgo', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm')\n",
    "\n",
    "\n",
    "rules_dict = \\\n",
    "    {'all' : ['fdgo', 'reactgo', 'delaygo', 'fdanti', 'reactanti', 'delayanti',\n",
    "              'dm1', 'dm2', 'contextdm1', 'contextdm2', 'multidm',\n",
    "              'delaydm1', 'delaydm2', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm',\n",
    "              'dmsgo', 'dmsnogo', 'dmcgo', 'dmcnogo'],\n",
    "     'low_dim' : ['fdgo', 'reactgo', 'delaygo', 'fdanti', 'reactanti', 'delayanti',\n",
    "                 'delaydm1', 'delaydm2', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm',\n",
    "                 'dmsgo', 'dmsnogo', 'dmcgo', 'dmcnogo'],\n",
    "\n",
    "     'gofamily': ['fdgo', 'fdanti', 'reactgo', 'reactanti', 'delaygo', 'delayanti'],\n",
    "\n",
    "     'delaygo': ['delaygo'],\n",
    "     'delaygofamily': ['delaygo', 'delayanti'],\n",
    "     'fdgo': ['fdgo'],\n",
    "     'fdfamily': ['fdgo', 'fdanti'],\n",
    "     'reactgo': ['reactgo'],\n",
    "     'reactfamily': ['reactgo', 'reactanti'],\n",
    "     \n",
    "     'delaydm1': ['delaydm1'],\n",
    "     'delaydmfamily': ['delaydm1', 'delaydm2'],\n",
    "     \n",
    "     'dmsgofamily': ['dmsgo', 'dmsnogo'],\n",
    "     'dmsgo': ['dmsgo'],\n",
    "     'dmcgo': ['dmcgo'],\n",
    "     'contextdelayfamily': ['contextdelaydm1', 'contextdelaydm2'],\n",
    "    }\n",
    "    \n",
    "\n",
    "# This can either be used to set parameters OR set parameters and train\n",
    "train = True # whether or not to train the network\n",
    "verbose = True\n",
    "hyp_dict['run_mode'] = 'minimal' # minimal, debug\n",
    "hyp_dict['chosen_network'] = \"dmpn\"\n",
    "\n",
    "# suffix for saving images\n",
    "hyp_dict['addon_name'] = \"\"\n",
    "\n",
    "mpn_depth = 1\n",
    "\n",
    "# for coding \n",
    "if hyp_dict['chosen_network'] in (\"gru\", \"vanilla\"):\n",
    "    mpn_depth = 1\n",
    "\n",
    "def current_basic_params():\n",
    "    task_params = {\n",
    "        'task_type': hyp_dict['task_type'],\n",
    "        'rules': rules_dict[hyp_dict['ruleset']],\n",
    "        'dt': 40, # ms, directly influence sequence lengths,\n",
    "        'ruleset': hyp_dict['ruleset'],\n",
    "        'n_eachring': 8, # Number of distinct possible inputs on each ring\n",
    "        'in_out_mode': 'low_dim',  # high_dim or low_dim or low_dim_pos (Robert vs. Laura's paper, resp)\n",
    "        'sigma_x': 0.00, # Laura raised to 0.1 to prevent overfitting (Robert uses 0.01)\n",
    "        'mask_type': 'cost', # 'cost', None\n",
    "        'fixate_off': True, # Second fixation signal goes on when first is off\n",
    "        'randomize_inputs': False,\n",
    "        'n_input': 20, # Only used if inputs are randomized\n",
    "    }\n",
    "\n",
    "    train_params = {\n",
    "        'lr': 1e-3,\n",
    "        'n_batches': 64,\n",
    "        'batch_size': 64,\n",
    "        'gradient_clip': 10,\n",
    "        'valid_n_batch': 200,\n",
    "        'n_datasets': 100, # Number of distinct batches\n",
    "        'n_epochs_per_set': 10, # longer/shorter training\n",
    "        'task_mask': None, # None, task\n",
    "        # 'weight_reg': 'L2',\n",
    "        # 'reg_lambda': 1e-4,\n",
    "    }\n",
    "\n",
    "    if not train: # some \n",
    "        assert train_params['n_epochs_per_set'] == 0\n",
    "\n",
    "    n_hidden = 100\n",
    "\n",
    "    net_params = {\n",
    "        'net_type': hyp_dict['chosen_network'], # mpn1, dmpn, vanilla\n",
    "        'n_neurons': [1] + [n_hidden] * mpn_depth + [1],\n",
    "        'output_bias': False, # Turn off biases for easier interpretation\n",
    "        'loss_type': 'MSE', # XE, MSE\n",
    "        'activation': 'tanh', # linear, ReLU, sigmoid, tanh, tanh_re, tukey, heaviside\n",
    "        'cuda': True,\n",
    "        'monitor_freq': 100,\n",
    "        'monitor_valid_out': True, # Whether or not to save validation output throughout training\n",
    "        \n",
    "        # for one-layer MPN, GRU or Vanilla\n",
    "\n",
    "        'ml_params': {\n",
    "            'bias': True, # Bias of layer\n",
    "            'mp_type': 'mult',\n",
    "            'm_update_type': 'hebb_assoc', # hebb_assoc, hebb_pre\n",
    "            'eta_type': 'scalar', # scalar, pre_vector, post_vector, matrix\n",
    "            'eta_train': True,\n",
    "            # 'eta_init': 'mirror_gaussian', #0.0,\n",
    "            'lam_type': 'scalar', # scalar, pre_vector, post_vector, matrix\n",
    "            'm_time_scale': 400, # ms, sets lambda\n",
    "            'lam_train': True,\n",
    "        },\n",
    "\n",
    "        # Vanilla RNN params\n",
    "        'leaky': True,\n",
    "        'alpha': 0.2,\n",
    "    }\n",
    "\n",
    "    # for multiple MPN layers, assert \n",
    "    if mpn_depth > 1:\n",
    "        for mpl_idx in range(mpn_depth - 1):\n",
    "            assert f'ml_params{mpl_idx}' in net_params.keys()\n",
    "\n",
    "    # actually I don't think it is needed\n",
    "    # putting here to warn the parameter checking every time \n",
    "    # when switching network\n",
    "    if hyp_dict['chosen_network'] in (\"gru\", \"vanilla\"):\n",
    "        assert f'ml_params' in net_params.keys()\n",
    "\n",
    "    return task_params, train_params, net_params\n",
    "\n",
    "task_params, train_params, net_params = current_basic_params()\n",
    "\n",
    "lag = \"trained\" if train_params[\"n_epochs_per_set\"] > 10 and train else \"untrained\"\n",
    "\n",
    "if hyp_dict['task_type'] in ('multitask',):\n",
    "    task_params, train_params, net_params = tasks.convert_and_init_multitask_params(\n",
    "        (task_params, train_params, net_params)\n",
    "    )\n",
    "\n",
    "    net_params['prefs'] = tasks.get_prefs(task_params['hp'])\n",
    "\n",
    "    print('Rules: {}'.format(task_params['rules']))\n",
    "    print('  Input size {}, Output size {}'.format(\n",
    "        task_params['n_input'], task_params['n_output'],\n",
    "    ))\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "if net_params['cuda']:\n",
    "    print('Using CUDA...')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('Using CPU...')\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "######\n",
    "hyp_dict[\"mess_with_training\"] = False\n",
    "\n",
    "if hyp_dict['mess_with_training']:\n",
    "    hyp_dict['addon_name'] += \"messwithtraining\"\n",
    "\n",
    "params = task_params, train_params, net_params\n",
    "\n",
    "net, _ = net_helpers.train_network(params, device=device, verbose=verbose, train=train, hyp_dict=hyp_dict)\n",
    "\n",
    "if train:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    ax.plot(net.hist['iters_monitor'], net.hist['train_loss'], color=c_vals[0], label='Full train loss')\n",
    "    ax.plot(net.hist['iters_monitor'], net.hist['valid_loss'], color=c_vals[1], label='Full valid loss')\n",
    "    if net.weight_reg is not None:\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['train_loss_output_label'], color=c_vals_l[0], zorder=-1, label='Output label')\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['train_loss_reg_term'], color=c_vals_l[0], zorder=-1, label='Reg term', linestyle='dashed')\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['valid_loss_output_label'], color=c_vals_l[1], zorder=-1, label='Output valid label')\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['valid_loss_reg_term'], color=c_vals_l[1], zorder=-1, label='Reg valid term', linestyle='dashed')\n",
    "    \n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.set_ylabel('Loss ({})'.format(net.loss_type))\n",
    "    ax.set_xlabel('# Batches')\n",
    "    plt.savefig(f\"./results/loss_{hyp_dict['ruleset']}_{hyp_dict['chosen_network']}_{hyp_dict['addon_name']}.png\")\n",
    "    \n",
    "    if net_params['loss_type'] in ('MSE',):\n",
    "        helper.plot_some_ouputs(params, net, hyp_dict['mode_for_all'], nameadd=f\"{hyp_dict['ruleset']}_{chosen_network}_{hyp_dict['addon_name']}\")\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e544aca8-271f-49d3-9ed3-ab7023f23600",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    net_helpers.net_eta_lambda_analysis(net, net_params, hyp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c10ab17-bab6-4228-9096-af042e9ac385",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_n_batch = train_params[\"valid_n_batch\"]\n",
    "\n",
    "if task_params['task_type'] in ('multitask',): # Test batch consists of all the rules\n",
    "    task_params['hp']['batch_size_train'] = test_n_batch\n",
    "    # using homogeneous cutting off\n",
    "    test_mode_for_all = \"random\"\n",
    "    # ZIHAN\n",
    "    # generate test data using \"random\"\n",
    "    test_data, test_trials_extra = tasks.generate_trials_wrap(task_params, test_n_batch, \\\n",
    "                rules=task_params['rules'], mode_input=test_mode_for_all)\n",
    "    _, test_trials, test_rule_idxs = test_trials_extra\n",
    "\n",
    "    task_params['dataset_name'] = 'multitask'\n",
    "\n",
    "    if task_params['in_out_mode'] in ('low_dim_pos',):\n",
    "        output_dim_labels = ('Fixate', 'Cos', '-Cos', 'Sin', '-Sin')\n",
    "    elif task_params['in_out_mode'] in ('low_dim',):\n",
    "        output_dim_labels = ('Fixate', 'Cos', 'Sin')\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    labels = []\n",
    "    for rule_idx, rule in enumerate(task_params['rules']):\n",
    "        print(rule)\n",
    "        if rule in accept_rules:\n",
    "            if hyp_dict['ruleset'] in ('dmsgo', 'dmcgo'):\n",
    "                labels.append(test_trials[rule_idx].meta['matches'])\n",
    "            else:\n",
    "                labels.append(test_trials[rule_idx].meta['resp1'])\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "test_input, test_output, test_mask = test_data\n",
    "print(f\"test_input: {test_input.shape}\")\n",
    "\n",
    "test_input_np = test_input.detach().cpu().numpy()\n",
    "test_output_np = test_output.detach().cpu().numpy()\n",
    "\n",
    "n_batch_all = test_input_np.shape[0] # Total number of batches, might be different than test_n_batch\n",
    "# print(n_batch_all)\n",
    "max_seq_len = test_input_np.shape[1]\n",
    "\n",
    "# plotting output in the validation set\n",
    "net_out, db = net.iterate_sequence_batch(test_input, run_mode='track_states')\n",
    "\n",
    "W_output = net.W_output.detach().cpu().numpy()\n",
    "W_all_ = []\n",
    "for i in range(len(net.mp_layers)):\n",
    "    W_ = net.mp_layers[i].W.detach().cpu().numpy()\n",
    "    W_all_.append(W_)\n",
    "\n",
    "net_out = net_out.detach().cpu().numpy()\n",
    "\n",
    "if net_params['loss_type'] in ('MSE',):\n",
    "    test_out_np = test_output.cpu().numpy()\n",
    "\n",
    "    fig, axs = plt.subplots(4, 1, figsize=(8, 8))\n",
    "\n",
    "    if test_out_np.shape[-1] == 1:\n",
    "        for batch_idx, ax in enumerate(axs):\n",
    "            ax.plot(net_out[batch_idx, :, 0], color=c_vals[batch_idx])\n",
    "            ax.plot(test_out_np[batch_idx, :, 0], color=c_vals_l[batch_idx], zorder=-1)\n",
    "\n",
    "            # ax.set_ylim((0, 2.25))\n",
    "    else:\n",
    "        for batch_idx, ax in enumerate(axs):\n",
    "            for out_idx in range(test_out_np.shape[-1]):\n",
    "                ax.plot(net_out[batch_idx, :, out_idx], color=c_vals[out_idx])\n",
    "                ax.plot(test_out_np[batch_idx, :, out_idx], color=c_vals_l[out_idx], zorder=-1)\n",
    "\n",
    "            # ax.set_ylim((-3, 2.25))\n",
    "\n",
    "    fig.suptitle(\"Validation Set Output Comparison\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"./results/lowD_{lag}_{hyp_dict['ruleset']}_{chosen_network}_{hyp_dict['addon_name']}.png\")\n",
    "\n",
    "\n",
    "#######\n",
    "#######\n",
    "#######\n",
    "#######\n",
    "\n",
    "\n",
    "if net_params[\"net_type\"] in (\"dmpn\", ):\n",
    "    if mpn_depth == 1:\n",
    "        Ms = np.concatenate((\n",
    "            db['M0'].detach().cpu().numpy().reshape(n_batch_all, max_seq_len, -1),\n",
    "            # db['M1'].detach().cpu().numpy().reshape(n_batch_all, max_seq_len, -1)\n",
    "        ), axis=-1)\n",
    "\n",
    "        Ms_orig = np.concatenate((\n",
    "            db['M0'].detach().cpu().numpy(),\n",
    "            # db['M1'].detach().cpu().numpy().reshape(n_batch_all, max_seq_len, -1)\n",
    "        ), axis=-1)\n",
    "\n",
    "        hs = np.concatenate((\n",
    "            db['hidden0'].detach().cpu().numpy().reshape(n_batch_all, max_seq_len, -1),\n",
    "        ), axis=-1)\n",
    "\n",
    "    else:\n",
    "        # raise NotImplementedError() # Let's focusing on one layer first\n",
    "        modulations, hiddens = [], []\n",
    "        for i in range(mpn_depth):\n",
    "            modulations.append(db[f'M{i}'].detach().cpu().numpy().reshape(n_batch_all, max_seq_len, -1))\n",
    "            hiddens.append(db[f'hidden{i}'].detach().cpu().numpy().reshape(n_batch_all, max_seq_len, -1),)\n",
    "\n",
    "        # Ms = np.concatenate(modulations, axis=-1)\n",
    "        Ms = modulations[0]\n",
    "        hs = hiddens[0]\n",
    "        \n",
    "elif net_params[\"net_type\"] in (\"vanilla\", \"gru\"):\n",
    "    hs = db['hidden'].detach().cpu().numpy()\n",
    "\n",
    "# print(f\"Ms.shape: {Ms.shape}\")\n",
    "# print(f\"hs.shape: {hs.shape}\")\n",
    "\n",
    "pca_type = 'full' # full, cell_types\n",
    "pca_target_lst = ['hs', 'Ms'] # hs, 'Ms' \n",
    "if net_params[\"net_type\"] in (\"vanilla\", \"gru\"):\n",
    "    pca_target_lst = ['hs'] # if not dmpn, no M information effectively\n",
    "\n",
    "\n",
    "# using recorded information\n",
    "recordkyle_all, recordkyle_nameall = [], []\n",
    "for test_subtrial in test_trials:\n",
    "    metaepoch = test_subtrial.epochs\n",
    "    periodname = list(metaepoch.keys())\n",
    "    recordkyle, recordkyle_name = [], []\n",
    "    # print(metaepoch)\n",
    "    for keyiter in range(len(periodname)):\n",
    "        try:\n",
    "            recordkyle_name.append(periodname[keyiter])\n",
    "            if test_mode_for_all == \"random\":\n",
    "                recordkyle.append(metaepoch[periodname[keyiter]][1])\n",
    "            elif test_mode_for_all == \"random_batch\":\n",
    "                recordkyle.append(list(metaepoch[periodname[keyiter]][1]))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    if test_mode_for_all in (\"random\",):\n",
    "        fillrecordkyle = []\n",
    "        for timestamp in recordkyle:\n",
    "            fillrecordkyle.append([timestamp for _ in range(hs.shape[0])])\n",
    "        recordkyle = fillrecordkyle\n",
    "\n",
    "    recordkyle.insert(0, [0 for _ in range(len(recordkyle[1]))])\n",
    "    recordkyle = np.array(recordkyle).T.tolist()\n",
    "    recordkyle_all.extend(recordkyle)\n",
    "    recordkyle_nameall.append(recordkyle_name)\n",
    "\n",
    "# Sep 30th\n",
    "# This part of code should be adaptive for multitask, which may have different breaks and periods for each task\n",
    "\n",
    "unique_lists = set(tuple(lst) for lst in recordkyle_all)\n",
    "# here select task specific information\n",
    "# which maximally should have length of number of tasks\n",
    "unique_recordkyle_all = [list(lst) for lst in unique_lists]\n",
    "assert len(unique_recordkyle_all) >= len(rules_dict[hyp_dict['ruleset']])\n",
    "\n",
    "all_session_breakdown = []\n",
    "for task_specific_time in unique_recordkyle_all:\n",
    "    session_breakdown = []\n",
    "    for sindex in range(0,len(task_specific_time)-1):\n",
    "        # all sessions should be the same for each task\n",
    "        # but different across tasks\n",
    "        # though the time of when response period starts might be similar across\n",
    "        session_breakdown.append([task_specific_time[sindex], task_specific_time[sindex+1]]) \n",
    "    session_breakdown.append([task_specific_time[0], task_specific_time[-1]])\n",
    "    all_session_breakdown.append(session_breakdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f41fdc5-88a2-459a-8ea5-8177aa1a9393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# break down time\n",
    "all_breaks = []\n",
    "for session_breakdown in all_session_breakdown:\n",
    "    breaks = [cut[1] for cut in session_breakdown[:-1]]\n",
    "    print(f\"Task {all_session_breakdown.index(session_breakdown)}; breaks: {breaks}\")\n",
    "    all_breaks.append(breaks)\n",
    "\n",
    "# Sanity check from Equation 2-7\n",
    "figexh1, axsexh1 = plt.subplots(3,3,figsize=(4*3,4*3))  \n",
    "figexh2, axsexh2 = plt.subplots(4,3,figsize=(4*3,4*4))  \n",
    "\n",
    "for batch_iter in range(test_input.shape[0]):\n",
    "    res_eq26, res_eq8, res_eq11 = [], [], []\n",
    "    res_meta = []\n",
    "\n",
    "    # # analyze the change of M\n",
    "    # M_beforestim = Ms_orig[batch_iter, breaks[0], :, :]\n",
    "    # M_afterstim = Ms_orig[batch_iter, breaks[0]+1, :, :]\n",
    "    # h_s_beforesstim = hs[batch_iter, breaks[0], :].reshape(-1,1)\n",
    "\n",
    "    saver_shape1 = (3,3)\n",
    "    saver1 = np.empty((saver_shape1[0], saver_shape1[1]), dtype=object)\n",
    "    saver_shape2 = (4,3)\n",
    "    saver2 = np.empty((saver_shape2[0], saver_shape2[1]), dtype=object)\n",
    "\n",
    "    for i in range(saver_shape1[0]):\n",
    "        for j in range(saver_shape1[1]):\n",
    "            saver1[i, j] = np.array([])\n",
    "\n",
    "    for i in range(saver_shape2[0]):\n",
    "        for j in range(saver_shape2[1]):\n",
    "            saver2[i, j] = np.array([])\n",
    "    \n",
    "\n",
    "    for time_iter in range(test_input.shape[1]):\n",
    "        x = test_input[batch_iter, time_iter, :].cpu().numpy().reshape(-1,1)\n",
    "        input_length = len(x)\n",
    "\n",
    "        x_fixon, x_fixoff, x_stimulus, x_task = [np.zeros((input_length, 1)) for _ in range(4)]\n",
    "        # one-hot encoded vector for fixation\n",
    "        x_fixon[0,0] = x[0,0] \n",
    "        # one-hot encoded vector for fixation off\n",
    "        x_fixoff[1,0] = x[1,0]\n",
    "        # one-hot encoded vector for stimulus\n",
    "        x_stimulus[2:6,0] = x[2:6,0]\n",
    "        # one-hot encoded vector for task\n",
    "        # task (dynamically setting for all element after the 6th elements)\n",
    "        x_task[6:,0] = x[6:,0]\n",
    "        \n",
    "        Mt = Ms_orig[batch_iter, time_iter, :, :]\n",
    "        \n",
    "        middle =  W_ + W_ * Mt\n",
    "        \n",
    "        y_fix = W_output[0,:].reshape(1,-1)\n",
    "        Y_resp1 = W_output[1,:].reshape(1,-1)\n",
    "        Y_resp2 = W_output[2,:].reshape(1,-1)\n",
    "\n",
    "        allX1 = [x_fixon+x_task, x_fixoff+x_task, x_stimulus+x_fixon+x_task]\n",
    "        allX1name = [\"x_fixon+x_task\", \"x_fixoff+x_task\", \"x_stimulus+x_fixon+x_task\"]\n",
    "        allX2 = [x_fixon, x_fixoff, x_stimulus, x_task]\n",
    "        allX2name = [\"x_fixon\", \"x_fixoff\", \"x_stimulus\", \"x_task\"]\n",
    "        allY = [y_fix, Y_resp1, Y_resp2]\n",
    "        allYname = [\"y_fix\", \"Y_resp1\", \"Y_resp2\"]\n",
    "\n",
    "        for yiter in range(len(allY)):\n",
    "            for xiter in range(len(allX1)):\n",
    "                res1 = helper.to_unit_vector(allY[yiter]) @ helper.to_unit_vector(middle @ allX1[xiter])\n",
    "                saver1[xiter, yiter] = np.append(saver1[xiter, yiter], res1[0,0])\n",
    "\n",
    "        for yiter in range(len(allY)):\n",
    "            for xiter in range(len(allX2)):\n",
    "                res2 = helper.to_unit_vector(allY[yiter]) @ helper.to_unit_vector(middle @ allX2[xiter])\n",
    "                saver2[xiter, yiter] = np.append(saver2[xiter, yiter], res2[0,0])\n",
    "\n",
    "    for i in range(saver_shape1[0]):\n",
    "        for j in range(saver_shape1[1]):\n",
    "            axsexh1[i,j].plot(saver1[i,j], color=c_vals[labels[batch_iter]])\n",
    "\n",
    "    for i in range(saver_shape2[0]):\n",
    "        for j in range(saver_shape2[1]):            \n",
    "            axsexh2[i,j].plot(saver2[i,j], color=c_vals[labels[batch_iter]])\n",
    "\n",
    "for i in range(saver_shape1[0]):\n",
    "    for j in range(saver_shape1[1]):\n",
    "        axsexh1[i,j].set_title(f\"{allX1name[i]} & {allYname[j]}\")\n",
    "\n",
    "for i in range(saver_shape2[0]):\n",
    "    for j in range(saver_shape2[1]):\n",
    "        axsexh2[i,j].set_title(f\"{allX2name[i]} & {allYname[j]}\")\n",
    "\n",
    "for ax in np.concatenate((axsexh1.flatten(), axsexh2.flatten())):\n",
    "    for breaks in all_breaks:\n",
    "        for bb in breaks:\n",
    "            ax.axvline(bb, linestyle=\"--\", c=c_vals[all_breaks.index(breaks)])\n",
    "            ax.set_ylim([-1,1])\n",
    "\n",
    "figexh1.suptitle(\"Exhaustive Search 1\")\n",
    "figexh1.tight_layout()\n",
    "figexh2.suptitle(\"Exhaustive Search 2\")\n",
    "figexh2.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec30e08f-8431-4054-abb3-ddd2c28aedc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for session_iter in range(0,len(session_breakdown)):\n",
    "#     session_part = session_breakdown[session_iter]\n",
    "\n",
    "#     for pca_target in pca_target_lst:\n",
    "#         print(f\"session_part: {session_part}; pca_target: {pca_target}\")\n",
    "\n",
    "#         if net.hidden_cell_types is not None:\n",
    "#             cell_types = net.hidden_cell_types.detach().cpu().numpy()[0] # Remove batch idx\n",
    "#         cell_type_names = ('Inh.', 'Exc.')\n",
    "#         pca_sort_type = 'ratios' # How to sort cell type PCA; vars, ratios\n",
    "\n",
    "#         if pca_target in ('hs',):\n",
    "#             n_activity = hs.shape[-1]\n",
    "#             # Truncate into session specifically\n",
    "#             hs_cut = hs[:,session_part[0]:session_part[1],:]\n",
    "#             as_shape = hs_cut.shape \n",
    "#             as_flat = hs_cut.reshape((-1, n_activity,))\n",
    "#         elif pca_target in ('Ms',):\n",
    "#             n_activity = Ms.shape[-1]\n",
    "#             # Truncate into session specifically\n",
    "#             Ms_cut = Ms[:,session_part[0]:session_part[1],:]\n",
    "#             # as_shape = (Ms.shape[0], Ms.shape[1], n_activity) \n",
    "#             as_shape = Ms_cut.shape\n",
    "#             as_flat = Ms_cut.reshape((-1, n_activity))\n",
    "\n",
    "#         print(f\"as_shape: {as_shape}\")\n",
    "#         as_shape_saver = as_shape\n",
    "\n",
    "#         total_vars = np.var(as_flat, axis=0) # Var per activity dimension\n",
    "#         activity_zero = np.zeros((1, n_activity,))\n",
    "\n",
    "#         if pca_type in ('full',):\n",
    "#             activity_pca = PCA()\n",
    "\n",
    "#             activity_pca.fit(as_flat)\n",
    "            \n",
    "#             # Corrects signs of components so that their mean/average is positive\n",
    "#             activity_pca.components_ = np.sign(np.sum(activity_pca.components_, axis=-1))[:, np.newaxis] * activity_pca.components_\n",
    "            \n",
    "#             as_pca = activity_pca.transform(as_flat)\n",
    "#             # as_pca = as_pca.reshape(as_shape) # Separates back into batches and sequences\n",
    "#             as_pca = as_pca.reshape(as_shape_saver[0],as_shape_saver[1],-1)\n",
    "\n",
    "#             zeros_pca = activity_pca.transform(activity_zero)\n",
    "#             if pca_target in ('hs',): # Some transformations only make sense for certain activities\n",
    "#                 W_output_pca = activity_pca.transform(W_output)\n",
    "\n",
    "#             # ZIHAN: only do for all period trial (last)\n",
    "#             #\n",
    "#             # Should we use PCA result or the original result?\n",
    "#             #\n",
    "#             if session_iter == len(session_breakdown)-1 and pca_target in ('hs', ):\n",
    "#                 output_num = W_output.shape[0]\n",
    "#                 assert output_num == 3 # low_dim\n",
    "#                 figtest, axstest = plt.subplots(1,2,figsize=(5*2,5))\n",
    "#                 # ? Original or PCA\n",
    "#                 fixation_pca = W_output[0,:].reshape(1,-1)\n",
    "#                 # fixation_pca = fixation_pca / np.linalg.norm(fixation_pca, axis=1, keepdims=True)\n",
    "#                 stimulus_pca = W_output[1:,:]\n",
    "#                 # stimulus_pca = stimulus_pca / np.linalg.norm(stimulus_pca, axis=1, keepdims=True)\n",
    "#                 batch_record_fixate, batch_record_stimulus = [], []\n",
    "                \n",
    "#                 for batchiter in range(hs_cut.shape[0]):\n",
    "#                     # ? Original or PCA\n",
    "#                     batch_activity_pca = hs_cut[batchiter,:,:]\n",
    "                    \n",
    "#                     batch_record1, batch_record2 = [], []\n",
    "#                     for timestamp in range(batch_activity_pca.shape[0]):\n",
    "#                         batch_time_activity_pca = batch_activity_pca[timestamp,:].reshape(1,-1)\n",
    "#                         #\n",
    "#                         # batch_time_activity_pca = batch_time_activity_pca / np.linalg.norm(batch_time_activity_pca, axis=1, keepdims=True)\n",
    "\n",
    "#                         projection_magnitude_fix = helper.magnitude_of_projection(batch_time_activity_pca, fixation_pca)\n",
    "#                         batch_record1.append(projection_magnitude_fix)\n",
    "#                         projection_magnitude_stimulus = helper.magnitude_of_projection(batch_time_activity_pca, stimulus_pca)\n",
    "#                         batch_record2.append(projection_magnitude_stimulus)\n",
    "#                         # print(f\"projection_magnitude_fix: {projection_magnitude_fix}; projection_magnitude_stimulus: {projection_magnitude_stimulus}\")\n",
    "#                         # time.sleep(10000)\n",
    "#                     batch_record_fixate.append(batch_record1)\n",
    "#                     batch_record_stimulus.append(batch_record2)\n",
    "\n",
    "#                 batch_record_fixate, batch_record_stimulus = np.array(batch_record_fixate), np.array(batch_record_stimulus)\n",
    "#                 mean_fix, std_fix = np.mean(batch_record_fixate, axis=0), np.std(batch_record_fixate, axis=0)\n",
    "#                 mean_stimulus, std_stimulus = np.mean(batch_record_stimulus, axis=0), np.std(batch_record_stimulus, axis=0)\n",
    "#                 xxx = [i for i in range(mean_fix.shape[0])]\n",
    "#                 axstest[0].plot(xxx, mean_fix)\n",
    "#                 axstest[0].fill_between(xxx, mean_fix-std_fix, mean_fix+std_fix, alpha=0.7, color=\"red\")\n",
    "#                 axstest[0].set_title(\"Projection Magnitude on 1D Subspace of Fixation Period\")\n",
    "#                 axstest[1].plot(xxx, mean_stimulus)\n",
    "#                 axstest[1].fill_between(xxx, mean_stimulus-std_stimulus, mean_stimulus+std_stimulus, alpha=0.7, color=\"red\")\n",
    "#                 axstest[1].set_title(\"Projection Magnitude on 2D Subspace of Fixation Period\")\n",
    "#                 for ax in axstest:\n",
    "#                     for spp in session_breakdown[:-1]:\n",
    "#                         ax.axvline(spp[1], linestyle=\"--\")\n",
    "#                 figtest.tight_layout()\n",
    "#                 # figtest.savefig(f\"./results/zz_test_{pca_target}_{hyp_dict['ruleset']}_{chosen_network}_{hyp_dict['addon_name']}_test.png\")\n",
    "                \n",
    "\n",
    "#             print('PR: {:.2f}'.format(\n",
    "#                 helper.participation_ratio_vector(activity_pca.explained_variance_ratio_)\n",
    "#             ))\n",
    "#             print('PCA component PRs - PC1: {:.1f}, PC2: {:.1f}, PC3: {:.1f}'.format(\n",
    "#                     helper.participation_ratio_vector(np.abs(activity_pca.components_[0, :])),\n",
    "#                     helper.participation_ratio_vector(np.abs(activity_pca.components_[1, :])),\n",
    "#                     helper.participation_ratio_vector(np.abs(activity_pca.components_[2, :])),\n",
    "#                 ))\n",
    "\n",
    "#         elif pca_type in ('cell_types',):\n",
    "#             raise NotImplementedError('Need to correct this for Ms activity')\n",
    "#             cell_type_vals = np.unique(cell_types) # Gets unique cell type idxs\n",
    "\n",
    "#             n_cell_types = cell_type_vals.shape[0]\n",
    "#             pcas = [PCA() for _ in range(n_cell_types)]\n",
    "\n",
    "#             cell_types_pca = [] # This needs to be diferent from cell_types because may do cell types in a different order\n",
    "#             hs_pca = []\n",
    "#             explained_vars = []\n",
    "#             explained_var_ratios = []\n",
    "\n",
    "#             zeros_pca = []\n",
    "#             W_output_pca = []\n",
    "#             # Fit each PCA individually\n",
    "#             for cell_type_idx, (cell_type_val, cell_type_name) in enumerate(zip(\n",
    "#                 cell_type_vals, cell_type_names\n",
    "#             )):\n",
    "#                 print('Cell type: {}'.format(cell_type_name))\n",
    "#                 cell_type_filter = (cell_types == cell_type_val)\n",
    "#                 n_cells_type = np.sum(cell_type_filter.astype(np.int32))\n",
    "\n",
    "#                 print(' Ratio of population: {:.2f}, variance: {:.2f}'.format(\n",
    "#                     n_cells_type / n_cells,\n",
    "#                     np.sum(total_vars[cell_type_filter]) / np.sum(total_vars)\n",
    "#                 ))\n",
    "\n",
    "#                 pcas[cell_type_idx].fit(hs_flat[:, cell_type_filter])\n",
    "#                 # Corrects signs of components so that their mean/average is positive\n",
    "#                 pcas[cell_type_idx].components_ = np.sign(np.sum(pcas[cell_type_idx].components_, axis=-1))[:, np.newaxis] * pcas[cell_type_idx].components_\n",
    "\n",
    "#                 hs_pca_type = pcas[cell_type_idx].transform(hs_flat[:, cell_type_filter])\n",
    "\n",
    "#                 print(' PR: {:.2f}'.format(\n",
    "#                     helper.participation_ratio_vector(pcas[cell_type_idx].explained_variance_ratio_)\n",
    "#                 ))\n",
    "#                 print(' PCA component PRs - PC1: {:.1f}, PC2: {:.1f}, PC3: {:.1f}'.format(\n",
    "#                     helper.participation_ratio_vector(np.abs(pcas[cell_type_idx].components_[0, :])),\n",
    "#                     helper.participation_ratio_vector(np.abs(pcas[cell_type_idx].components_[1, :])),\n",
    "#                     helepr.participation_ratio_vector(np.abs(pcas[cell_type_idx].components_[2, :])),\n",
    "#                 ))\n",
    "\n",
    "#                 cell_types_pca.append([cell_type_val for _ in range(n_cells_type)])\n",
    "#                 hs_pca.append(hs_pca_type)\n",
    "#                 explained_vars.append(pcas[cell_type_idx].explained_variance_)\n",
    "#                 explained_var_ratios.append(pcas[cell_type_idx].explained_variance_ratio_)\n",
    "\n",
    "#                 zeros_pca.append(pcas[cell_type_idx].transform(hidden_zero[:, cell_type_filter]))\n",
    "#                 W_output_pca.append(pcas[cell_type_idx].transform(W_output[:, cell_type_filter]))\n",
    "\n",
    "#             explained_vars = np.concatenate(explained_vars, axis=-1)\n",
    "#             explained_var_ratios = np.concatenate(explained_var_ratios, axis=-1)\n",
    "#             # Now sort based on explained variances/explained variance ratio\n",
    "#             if pca_sort_type in ('vars',):\n",
    "#                 pca_type_sort = np.argsort(explained_vars)[::-1] # largest to smallest\n",
    "#             elif pca_sort_type in ('ratios',):\n",
    "#                 pca_type_sort = np.argsort(explained_var_ratios)[::-1] # largest to smallest\n",
    "\n",
    "#             cell_types_pca =  np.concatenate(cell_types_pca, axis=-1)[pca_type_sort]\n",
    "#             hs_pca = np.concatenate(hs_pca, axis=-1)[:, pca_type_sort]\n",
    "#             explained_vars = explained_vars[pca_type_sort]\n",
    "#             explained_var_ratios = explained_var_ratios[pca_type_sort]\n",
    "#             zeros_pca = np.concatenate(zeros_pca, axis=-1)[:, pca_type_sort]\n",
    "#             W_output_pca = np.concatenate(W_output_pca, axis=-1)[:, pca_type_sort]\n",
    "#             print('Overall:')\n",
    "#             print(' PR: {:.2f}'.format(\n",
    "#                 participation_ratio_vector(explained_var_ratios)\n",
    "#             ))\n",
    "#             hs_pca = hs_pca.reshape(hs.shape)\n",
    "\n",
    "#         if session_iter == len(session_breakdown)-1:\n",
    "#             fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4))\n",
    "#             n_pcs_plot = 3\n",
    "\n",
    "#             if pca_type in ('full',):\n",
    "#                 ax1.scatter(np.arange(n_activity), activity_pca.explained_variance_ratio_, color=c_vals[2])\n",
    "#                 cutoff = np.sum(activity_pca.explained_variance_ratio_ > 0.1) # PC with > 0.1 \n",
    "\n",
    "#                 for pc_idx in range(n_pcs_plot):\n",
    "#                     ax2.plot(activity_pca.components_[pc_idx, :], color=c_vals[pc_idx], label='PC{}'.format(pc_idx+1),\n",
    "#                             zorder=5-pc_idx)\n",
    "\n",
    "#             ax1.set_xlabel('PC')\n",
    "#             ax1.set_ylabel('Explained var ratio')\n",
    "#             ax1.set_title(f\"Good PC: {cutoff}\")\n",
    "\n",
    "#             for ax in (ax2, ax3):\n",
    "#                 ax.legend()\n",
    "#                 ax.set_xlabel('Neuron idx')\n",
    "#                 ax.set_ylabel('PC weight')\n",
    "\n",
    "#             fig.savefig(f\"./results/PC_{pca_target}_{hyp_dict['ruleset']}_{chosen_network}_{hyp_dict['addon_name']}.png\")\n",
    "\n",
    "#         #######\n",
    "#         #######\n",
    "#         #######\n",
    "#         #######\n",
    "\n",
    "#         fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 8))\n",
    "#         figdomain, axdomain = plt.subplots(1, 3, figsize=(24, 8))\n",
    "#         figmatch, axmatch = plt.subplots(1, 3, figsize=(24, 8))\n",
    "\n",
    "\n",
    "#         # single_rule: color by labels\n",
    "#         # all_rules: color by rule\n",
    "\n",
    "#         # if multiple testrules are detected, automatically plot based on the rule type\n",
    "#         plot_mode = 'single_rule' if len(task_params[\"rules\"]) == 1 else 'all_rules'\n",
    "\n",
    "#         if plot_mode == 'single_rule':\n",
    "#             rule_idx = 0 # Only used for individual labels\n",
    "#             rule = task_params['rules'][rule_idx]\n",
    "#             rule_batch_idxs = np.arange(n_batch_all)[test_rule_idxs == rule_idx]\n",
    "#         elif plot_mode == 'all_rules':\n",
    "#             rule_batch_idxs = np.arange(n_batch_all)\n",
    "\n",
    "#         if pca_target in ('hs'):\n",
    "#             pc1_idx, pc2_idx, pc3_idx = 0,1,2\n",
    "#         elif pca_target in ('Ms'):\n",
    "#             pc1_idx, pc2_idx, pc3_idx = 0,1,2\n",
    "\n",
    "#         # batch_plot = (0, 1, 2,)\n",
    "#         if task_params['dataset_name'] in ('DelayMatchSample-v0',):\n",
    "#             batch_plot = [False, False, False, False] # 4 distinct paths\n",
    "#         else:\n",
    "#             # batch_plot = (0, 1, 3,)\n",
    "#             batch_plot = (0,1,2)\n",
    "        \n",
    "#         # ZIHAN\n",
    "#         # additional analysis\n",
    "#         if session_iter == len(session_breakdown)-1 and pca_target in ('hs', 'Ms'):\n",
    "#             all_normal_vectors = []\n",
    "#             # minus the zero activity pca\n",
    "#             pc_indices_consider = [pc1_idx, pc2_idx, pc3_idx]\n",
    "\n",
    "#             cosine_pc = np.hstack([W_output_pca[1, idx] - zeros_pca[:, idx] for idx in pc_indices_consider])\n",
    "#             # cosine_pc = np.vstack((W_output_pca[1,pc1_idx]-zeros_pca[:,pc1_idx], W_output_pca[1,pc2_idx]-zeros_pca[:,pc2_idx], W_output_pca[1,pc3_idx]-zeros_pca[:,pc3_idx])).reshape(-1)\n",
    "#             cosine_pc = cosine_pc / np.linalg.norm(cosine_pc)\n",
    "\n",
    "#             sine_pc = np.hstack([W_output_pca[2, idx] - zeros_pca[:, idx] for idx in pc_indices_consider])\n",
    "#             # sine_pc = np.vstack((W_output_pca[2,pc1_idx]-zeros_pca[:,pc1_idx], W_output_pca[2,pc2_idx]-zeros_pca[:,pc2_idx], W_output_pca[2,pc3_idx]-zeros_pca[:,pc3_idx])).reshape(-1)\n",
    "#             sine_pc = sine_pc / np.linalg.norm(sine_pc)\n",
    "\n",
    "#             normal_cosine_sine = np.cross(cosine_pc, sine_pc)\n",
    "#             # normalization\n",
    "#             normal_cosine_sine = normal_cosine_sine / np.linalg.norm(normal_cosine_sine)\n",
    "#             all_normal_vectors.append(normal_cosine_sine)\n",
    "\n",
    "#             figscatter = plt.figure(figsize=(21, 7))\n",
    "#             ax1_3d = figscatter.add_subplot(131, projection='3d')\n",
    "#             ax2_3d = figscatter.add_subplot(132, projection='3d')\n",
    "#             ax3 = figscatter.add_subplot(133)\n",
    "\n",
    "#             init_point_3d = np.stack((zeros_pca[:, pc1_idx], zeros_pca[:, pc2_idx], zeros_pca[:, pc3_idx]), axis=1)\n",
    "#             ax1_3d.scatter(init_point_3d[0,0], init_point_3d[0,1], init_point_3d[0,2], marker=\"s\", color=\"black\")\n",
    "\n",
    "#             ax1_3d.plot([init_point_3d[0,0], init_point_3d[0,0] + cosine_pc[0]],\n",
    "#                         [init_point_3d[0,1], init_point_3d[0,1] + cosine_pc[1]],\n",
    "#                         [init_point_3d[0,2], init_point_3d[0,2] + cosine_pc[2]],\n",
    "#                         color='red', label=\"cosine\")\n",
    "\n",
    "#             ax1_3d.plot([init_point_3d[0,0], init_point_3d[0,0] + sine_pc[0]],\n",
    "#                         [init_point_3d[0,1], init_point_3d[0,1] + sine_pc[1]],\n",
    "#                         [init_point_3d[0,2], init_point_3d[0,2] + sine_pc[2]],\n",
    "#                         color='blue', label=\"sine\")\n",
    "\n",
    "#             ax1_3d.plot([init_point_3d[0,0], init_point_3d[0,0] + normal_cosine_sine[0]],\n",
    "#                         [init_point_3d[0,1], init_point_3d[0,1] + normal_cosine_sine[1]],\n",
    "#                         [init_point_3d[0,2], init_point_3d[0,2] + normal_cosine_sine[2]],\n",
    "#                         color='green', label=\"cosine * sine\")\n",
    "            \n",
    "#             ax1_3d.legend()\n",
    "\n",
    "#             for ii in range(len(session_breakdown[:-1])):\n",
    "#                 subsession = session_breakdown[ii]\n",
    "#                 clustered_points = as_pca[:, subsession[0]:subsession[1], :]\n",
    "                \n",
    "#                 # Extract the principal components for the scatter plot\n",
    "#                 pc1_points = clustered_points[:, :, pc1_idx].flatten()\n",
    "#                 pc2_points = clustered_points[:, :, pc2_idx].flatten()\n",
    "#                 pc3_points = clustered_points[:, :, pc3_idx].flatten()\n",
    "                \n",
    "#                 ax1_3d.scatter(pc1_points, pc2_points, pc3_points, c=c_vals[ii], alpha=0.15)\n",
    "\n",
    "#                 points_3d = np.vstack((pc1_points, pc2_points, pc3_points)).T\n",
    "#                 # points_3d_centered = points_3d - np.mean(points_3d, axis=0)\n",
    "#                 scaler = StandardScaler()\n",
    "#                 points_3d_standardized = scaler.fit_transform(points_3d)\n",
    "#                 pca = PCA(n_components=3)\n",
    "#                 pca.fit(points_3d_standardized)\n",
    "#                 normal_vector = pca.components_[-1]\n",
    "#                 normalized_normal_vector = normal_vector / np.linalg.norm(normal_vector)\n",
    "#                 all_normal_vectors.append(normalized_normal_vector)\n",
    "\n",
    "#                 point = np.mean(points_3d, axis=0)\n",
    "#                 d = -point.dot(normal_vector)\n",
    "#                 xx, yy = np.meshgrid(np.linspace(np.min(points_3d[:, 0]), np.max(points_3d[:, 0]), 10), \n",
    "#                                     np.linspace(np.min(points_3d[:, 1]), np.max(points_3d[:, 1]), 10))\n",
    "#                 zz = (-normal_vector[0] * xx - normal_vector[1] * yy - d) * 1. / normal_vector[2]\n",
    "                \n",
    "#                 ax2_3d.plot_surface(xx, yy, zz, alpha=0.5, color=c_vals_d[ii])\n",
    "\n",
    "#             for thisax in [ax1_3d, ax2_3d]:\n",
    "#                 thisax.set_xlabel(f\"PCA {pc1_idx+1}\")\n",
    "#                 thisax.set_ylabel(f\"PCA {pc2_idx+1}\")\n",
    "#                 thisax.set_zlabel(f\"PCA {pc3_idx+1}\")\n",
    "\n",
    "#             nnn = len(all_normal_vectors)\n",
    "#             normal_align = np.zeros((nnn, nnn))\n",
    "#             for n1 in range(nnn):\n",
    "#                 for n2 in range(nnn):\n",
    "#                     dot_product = np.dot(all_normal_vectors[n1], all_normal_vectors[n2])\n",
    "#                     angle_deg = np.degrees(np.arccos(dot_product))\n",
    "#                     # consider both alignment and anti-alignment\n",
    "#                     angle_deg = min(angle_deg, 180.0-angle_deg)\n",
    "#                     normal_align[n1,n2] = angle_deg\n",
    "\n",
    "#             np.fill_diagonal(normal_align, np.nan)\n",
    "#             # mark_labels = [\"response\"] + [f\"period {i+1}\" for i in range(nnn-1)]\n",
    "#             mark_labels = [\"response\"] + recordkyle_nameall[0]\n",
    "\n",
    "#             sns.heatmap(normal_align, cbar=True, annot=True, ax=ax3, cmap='coolwarm', xticklabels=mark_labels, yticklabels=mark_labels)\n",
    "#             ax3.set_xticklabels(ax3.get_xticklabels(), rotation=45)\n",
    "#             ax3.set_yticklabels(ax3.get_yticklabels(), rotation=45)\n",
    "\n",
    "#             figscatter.tight_layout()\n",
    "#             # figscatter.savefig(f\"./results/zz_test_{pca_target}_{hyp_dict['ruleset']}_{chosen_network}_{hyp_dict['addon_name']}_scatter.png\")\n",
    "\n",
    "#             # what if not based on PC -> Aug 3nd\n",
    "            \n",
    "#             # middle of each session\n",
    "#             breaks_1afterstart = [int((cut[0]+cut[1])/2) for cut in session_breakdown[:-1]]\n",
    "\n",
    "#             # plot more frames \n",
    "#             considerall = 0\n",
    "#             if considerall:\n",
    "#                 for cut in session_breakdown[:-1]:\n",
    "#                     breaks_1afterstart.append(cut[0])\n",
    "#                     breaks_1afterstart.append(cut[1]-1)\n",
    "#             breaks_1afterstart = list(np.sort(breaks_1afterstart))\n",
    "#             print(session_breakdown)\n",
    "\n",
    "#             if pca_target in ('hs',): # in random\n",
    "#                 figall, axsall = plt.subplots(3,2,figsize=(2*4,3*4))\n",
    "#                 figavg, axsavg = plt.subplots(3,2,figsize=(2*4,3*4))\n",
    "\n",
    "#                 assert mpn_depth == 1 # for now\n",
    "#                 W_ = W_all_[0]\n",
    "\n",
    "#                 # plot two trials\n",
    "#                 figW, axsW = plt.subplots(len(breaks_1afterstart)+1,4,figsize=(12*4,4*(len(breaks_1afterstart)+1)))\n",
    "#                 select_batch = [29,49,69,89]\n",
    "#                 for pp in select_batch:\n",
    "#                     sns.heatmap(W_.T, cbar=True, cmap=\"coolwarm\", ax=axsW[0,select_batch.index(pp)])\n",
    "#                     axsW[0,select_batch.index(pp)].set_title(\"W\")\n",
    "#                     # take average across batches\n",
    "#                     # Ms_samplebatch = np.mean(Ms_orig, axis=0) \n",
    "#                     Ms_samplebatch = Ms_orig[pp,:,:,:]\n",
    "#                     print(Ms_samplebatch.shape)\n",
    "\n",
    "#                     for bb in breaks_1afterstart:\n",
    "#                         sns.heatmap(Ms_samplebatch[bb,:,:].T, cbar=True, cmap=\"coolwarm\", center=0, \\\n",
    "#                                             vmin=-1, vmax=1, ax=axsW[breaks_1afterstart.index(bb)+1,select_batch.index(pp)])\n",
    "\n",
    "#                     for bb in breaks_1afterstart:\n",
    "#                         axsW[breaks_1afterstart.index(bb)+1,select_batch.index(pp)].set_title(f\"M: Time: {bb}\")\n",
    "#                         axsW[breaks_1afterstart.index(bb)+1,select_batch.index(pp)].set_xlabel(f\"Neuron\")\n",
    "#                         axsW[breaks_1afterstart.index(bb)+1,select_batch.index(pp)].set_ylabel(f\"Input Index\")\n",
    "\n",
    "#                 figW.tight_layout()\n",
    "#                 figW.savefig(f\"./results/zz_test_{pca_target}_{hyp_dict['ruleset']}_{chosen_network}_{hyp_dict['addon_name']}_W.png\")\n",
    "\n",
    "#                 results = {\n",
    "#                     'fix_hs_Woutput_all': [],\n",
    "#                     'response_hs_Woutput_all': [],\n",
    "#                     'fix_Ms_Woutput_all': [],\n",
    "#                     'response_Ms_Woutput_all': [],\n",
    "#                     'fix_MsWs_Woutput_all': [],\n",
    "#                     'response_MsWs_Woutput_all': [],\n",
    "#                 }\n",
    "\n",
    "#                 pltout, axsout = plt.subplots(figsize=(6,6))\n",
    "#                 sns.heatmap(W_output, ax=axsout, cmap=\"coolwarm\", center=0, cbar=True, square=True)\n",
    "#                 pltout.savefig(f\"./results/zz_test_{pca_target}_{hyp_dict['ruleset']}_{chosen_network}_{hyp_dict['addon_name']}_Woutput.png\")\n",
    "\n",
    "#                 W_output_fixation = W_output[0,:].reshape(-1,1) # (N,1)\n",
    "#                 W_output_response = W_output[1:3,:].T # (N,2)\n",
    "\n",
    "\n",
    "#                 for batch_iter in range(hs.shape[0]):\n",
    "#                     hs_per_batch = hs[batch_iter,:,:]\n",
    "#                     Ms_per_batch = Ms_orig[batch_iter,:,:,:]\n",
    "\n",
    "#                     fix_hs_Woutput, response_hs_Woutput = [], []\n",
    "#                     fix_Ms_Woutput, response_Ms_Woutput = [], []\n",
    "#                     fix_MsWs_Woutput, response_MsWs_Woutput = [], []\n",
    "                    \n",
    "#                     for time_cut in range(hs_per_batch.shape[0]):\n",
    "#                         hs_per_batch_per_time = hs_per_batch[time_cut,:].reshape(-1,1) # (N,1)\n",
    "#                         Ms_per_batch_per_time = Ms_per_batch[time_cut,:,:] # (N, n_input)\n",
    "\n",
    "#                         ang_fix_hs_Ws = subspace_angles(hs_per_batch_per_time, W_output_fixation)[0]\n",
    "#                         fix_hs_Woutput.append(np.degrees(ang_fix_hs_Ws))\n",
    "\n",
    "#                         ang_response_hs_Ws = subspace_angles(hs_per_batch_per_time, W_output_response)[0]\n",
    "#                         response_hs_Woutput.append(np.degrees(ang_response_hs_Ws))\n",
    "\n",
    "#                         try:\n",
    "#                             ang_fix_Ms_Ws = subspace_angles(Ms_per_batch_per_time, W_output_fixation)[0]\n",
    "#                             fix_Ms_Woutput.append(np.degrees(ang_fix_Ms_Ws))\n",
    "#                         except:\n",
    "#                             fix_Ms_Woutput.append(np.nan)\n",
    "                        \n",
    "#                         try:\n",
    "#                             ang_response_Ms_Ws = subspace_angles(Ms_per_batch_per_time, W_output_response)[0]\n",
    "#                             response_Ms_Woutput.append(np.degrees(ang_response_Ms_Ws))\n",
    "#                         except:\n",
    "#                             response_Ms_Woutput.append(np.nan)\n",
    "\n",
    "\n",
    "#                         try:\n",
    "#                             ang_fix_MsWs_Ws = subspace_angles(Ms_per_batch_per_time * W_ + W_, W_output_fixation)[0]\n",
    "#                             fix_MsWs_Woutput.append(np.degrees(ang_fix_MsWs_Ws))\n",
    "#                         except:\n",
    "#                             fix_MsWs_Woutput.append(np.nan)\n",
    "                        \n",
    "#                         try:\n",
    "#                             ang_response_MsWs_Ws = subspace_angles(Ms_per_batch_per_time * W_ + W_, W_output_response)[0]\n",
    "#                             response_MsWs_Woutput.append(np.degrees(ang_response_MsWs_Ws))\n",
    "#                         except:\n",
    "#                             response_MsWs_Woutput.append(np.nan)\n",
    "\n",
    "\n",
    "#                     results['fix_hs_Woutput_all'].append(fix_hs_Woutput)\n",
    "#                     results['response_hs_Woutput_all'].append(response_hs_Woutput)\n",
    "#                     results['fix_Ms_Woutput_all'].append(fix_Ms_Woutput)\n",
    "#                     results['response_Ms_Woutput_all'].append(response_Ms_Woutput)\n",
    "#                     results['fix_MsWs_Woutput_all'].append(fix_MsWs_Woutput)\n",
    "#                     results['response_MsWs_Woutput_all'].append(response_MsWs_Woutput)\n",
    "\n",
    "#                 for key in results:\n",
    "#                     results[key] = np.array(results[key])\n",
    "                \n",
    "#                 result_key = list(results.keys())\n",
    "#                 axsall = axsall.flatten()\n",
    "#                 for batch_iter in range(hs.shape[0]):\n",
    "#                     for key in result_key:\n",
    "#                         axsall[result_key.index(key)].plot([x for x in range(results[key].shape[1])], results[key][batch_iter,:], color=c_vals[labels[batch_iter]])             \n",
    "#                         axsall[result_key.index(key)].set_title(key)\n",
    "\n",
    "#                 for ax in axsall.flatten():\n",
    "#                     for bb in breaks:\n",
    "#                         ax.axvline(bb, color='r', linestyle=\"--\")\n",
    "\n",
    "#                 breaks = [0] + breaks\n",
    "#                 axsavg = axsavg.flatten()\n",
    "#                 for key in result_key:\n",
    "#                     means, stds = [], []\n",
    "#                     for bb in range(len(breaks)-1):\n",
    "#                         meanp = np.nanmean(results[key][:,breaks[bb]:breaks[bb+1]])\n",
    "#                         stdp = np.nanstd(results[key][:,breaks[bb]:breaks[bb+1]])\n",
    "#                         means.append(meanp)\n",
    "#                         stds.append(stdp)\n",
    "#                     means, stds = np.array(means), np.array(stds)\n",
    "#                     axsavg[result_key.index(key)].plot([x for x in range(len(means))], means, \"-o\")\n",
    "#                     axsavg[result_key.index(key)].fill_between([x for x in range(len(means))], means-stds, means+stds, alpha=0.3, color=\"red\")\n",
    "#                     axsavg[result_key.index(key)].set_title(key)\n",
    "\n",
    "                    \n",
    "#                 figall.savefig(f\"./results/zz_test_{hyp_dict['ruleset']}_{chosen_network}_{hyp_dict['addon_name']}_all2all.png\")\n",
    "#                 figavg.savefig(f\"./results/zz_test_{hyp_dict['ruleset']}_{chosen_network}_{hyp_dict['addon_name']}_all2allavg.png\")\n",
    "\n",
    "#         for ax, pc_idxs in zip((ax1, ax2, ax3),((pc1_idx, pc2_idx), (pc1_idx, pc3_idx), (pc2_idx, pc3_idx))):\n",
    "#             domain_save = []\n",
    "#             for batch_idx_idx, batch_idx in enumerate(rule_batch_idxs):\n",
    "#                 if plot_mode == 'single_rule':\n",
    "#                     batch_color_idx = labels[batch_idx]\n",
    "#                 elif plot_mode == 'all_rules':\n",
    "#                     batch_color_idx = int(test_rule_idxs[batch_idx])\n",
    "\n",
    "#                 # MODIFY BY ZIHAN\n",
    "#                 # only plot the categorization at the whole final period\n",
    "#                 # where all sections are included in plotting\n",
    "#                 if session_iter == len(session_breakdown)-1:\n",
    "#                     cutoff_evidence = recordkyle_all\n",
    "                    \n",
    "#                     temp = []\n",
    "#                     for jj in range(len(cutoff_evidence[batch_idx])-1):\n",
    "#                         xx = as_pca[batch_idx, cutoff_evidence[batch_idx][jj]:cutoff_evidence[batch_idx][jj+1], pc_idxs[0]]\n",
    "#                         yy = as_pca[batch_idx, cutoff_evidence[batch_idx][jj]:cutoff_evidence[batch_idx][jj+1], pc_idxs[1]]\n",
    "#                         temp.append([xx, yy])\n",
    "#                         ax.scatter(xx, yy, color=c_vals_l[batch_color_idx], alpha=0.7, marker=markers_vals[jj], s=8)\n",
    "#                     domain_save.append(temp)\n",
    "\n",
    "#                     if batch_idx_idx in batch_plot:\n",
    "#                         for jj in range(len(cutoff_evidence[batch_idx])-1):\n",
    "#                             ax.plot(as_pca[batch_idx, cutoff_evidence[batch_idx][jj]:cutoff_evidence[batch_idx][jj+1], pc_idxs[0]], \n",
    "#                                     as_pca[batch_idx, cutoff_evidence[batch_idx][jj]:cutoff_evidence[batch_idx][jj+1], pc_idxs[1]], \n",
    "#                                     color=c_vals[batch_color_idx], \n",
    "#                                     linestyle=l_vals[jj], alpha=1.0, zorder=10\n",
    "#                             )\n",
    "\n",
    "#                 else:\n",
    "#                     ax.scatter(as_pca[batch_idx, :, pc_idxs[0]], as_pca[batch_idx, :, pc_idxs[1]], color=c_vals_l[batch_color_idx], alpha=0.7, marker=\"o\", s=8)\n",
    "\n",
    "#             if pca_type in ('full',):\n",
    "#                 ax.set_xlabel('PC {}'.format(pc_idxs[0]+1))\n",
    "#                 ax.set_ylabel('PC {}'.format(pc_idxs[1]+1))\n",
    "#             elif pca_type in ('cell_types',):\n",
    "#                 ax.set_xlabel('PC {} (cell type: {})'.format(pc_idxs[0]+1, cell_types_pca[pc_idxs[0]]))\n",
    "#                 ax.set_ylabel('PC {} (cell type: {})'.format(pc_idxs[1]+1, cell_types_pca[pc_idxs[1]]))\n",
    "#             # Plot zero point\n",
    "#             ax.scatter(zeros_pca[:, pc_idxs[0]], zeros_pca[:, pc_idxs[1]], color='k',\n",
    "#                     marker='s')\n",
    "\n",
    "#             if ax == ax1:\n",
    "#                 ax.legend()\n",
    "\n",
    "#             # Plot readouts\n",
    "#             ro_vector_dir_all = []\n",
    "#             init_points = []\n",
    "#             # if pca_target in ('Ms',):\n",
    "#             #     continue\n",
    "#             for out_idx, output_dim_label in enumerate(output_dim_labels):\n",
    "#                 if pca_target in ('Ms',):\n",
    "#                     RO_SCALE = 5\n",
    "#                 elif pca_target in ('hs',):\n",
    "#                     RO_SCALE = 1\n",
    "\n",
    "#                 ro_vector_dir = np.array((\n",
    "#                     W_output_pca[out_idx, pc_idxs[0]] - zeros_pca[:, pc_idxs[0]],\n",
    "#                     W_output_pca[out_idx, pc_idxs[1]] - zeros_pca[:, pc_idxs[1]],\n",
    "#                 ))\n",
    "\n",
    "#                 norm = np.linalg.norm(ro_vector_dir)\n",
    "\n",
    "#                 if norm != 0:\n",
    "#                     ro_vector_dir = ro_vector_dir / norm\n",
    "#                 else:\n",
    "#                     ro_vector_dir = ro_vector_dir\n",
    "\n",
    "#                 ro_vector_dir = RO_SCALE * ro_vector_dir\n",
    "#                 ro_vector_dir_all.append(ro_vector_dir)\n",
    "\n",
    "#                 init_point = [zeros_pca[:, pc_idxs[0]], zeros_pca[:, pc_idxs[1]]]\n",
    "\n",
    "#                 ax.plot((zeros_pca[:, pc_idxs[0]], zeros_pca[:, pc_idxs[0]] + ro_vector_dir[0]),\n",
    "#                         (zeros_pca[:, pc_idxs[1]], zeros_pca[:, pc_idxs[1]] + ro_vector_dir[1]),\n",
    "#                         color=c_vals[out_idx], label=output_dim_label, zorder=9)\n",
    "\n",
    "#                 init_points.append(init_point)\n",
    "                \n",
    "#                 ax.legend()\n",
    "\n",
    "#             # reload and regenerate the PCA projections separately\n",
    "\n",
    "#             # direction_vector_all = []\n",
    "#             # if session_iter == len(session_breakdown)-1 and pca_target in ('hs',):\n",
    "#             #     for evi in range(len(cutoff_evidence[batch_idx])-1):\n",
    "#             #         domain = [temp[evi] for temp in domain_save]\n",
    "#             #         domain_x = [batch_domain[0] for batch_domain in domain]\n",
    "#             #         domain_y = [batch_domain[1] for batch_domain in domain]\n",
    "#             #         flatten_domain_x = [item for sublist in domain_x for item in sublist]\n",
    "#             #         flatten_domain_y = [item for sublist in domain_y for item in sublist]\n",
    "#             #         axdomain[count].scatter(flatten_domain_x, flatten_domain_y, marker=markers_vals[evi], c=c_vals[evi])\n",
    "\n",
    "#             #         # directional analysis\n",
    "#             #         pts = [[flatten_domain_x[i], flatten_domain_y[i]] for i in range(len(flatten_domain_x))]\n",
    "#             #         points = np.array(pts)\n",
    "#             #         mean = np.mean(points, axis=0)\n",
    "#             #         centered_points = points - mean\n",
    "#             #         covariance_matrix = np.cov(centered_points, rowvar=False)\n",
    "#             #         eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
    "#             #         direction_vector = eigenvectors[:, np.argmax(eigenvalues)]\n",
    "#             #         direction_vector = direction_vector / np.linalg.norm(direction_vector)\n",
    "#             #         direction_vector_all.append(direction_vector)\n",
    "\n",
    "#             #         start = [1,1] # dummy initial point\n",
    "#             #         axdomain[count].plot((start[0], start[0] + direction_vector[0]), \\\n",
    "#             #                             (start[1], start[1] + direction_vector[1]), \n",
    "#             #                             c=c_vals[evi], linestyle=\"--\")\n",
    "\n",
    "#             #         if evi == 0:\n",
    "#             #             for jjj in range(len(output_dim_labels)):\n",
    "#             #                 init_xx, init_yy = init_points[jjj][0], init_points[evi][1]\n",
    "#             #                 ro_vector_dir = ro_vector_dir_all[jjj]\n",
    "\n",
    "#             #                 axdomain[count].plot((init_xx, init_xx + ro_vector_dir[0]), \\\n",
    "#             #                                     (init_yy, init_yy + ro_vector_dir[1]), \\\n",
    "#             #                                     color=c_vals[jjj], label=output_dim_labels[jjj], zorder=9)\n",
    "\n",
    "#             # if session_iter == len(session_breakdown)-1 and pca_target in ('hs',):\n",
    "#             #     matching_angle = np.zeros((len(direction_vector_all), len(ro_vector_dir_all)))\n",
    "#             #     for v1 in range(len(direction_vector_all)):\n",
    "#             #         for v2 in range(len(ro_vector_dir_all)):\n",
    "#             #             vec1, vec2 = direction_vector_all[v1].flatten(), ro_vector_dir_all[v2].flatten()\n",
    "#             #             dot_product = np.dot(vec1, vec2)\n",
    "#             #             norm1 = np.linalg.norm(vec1)\n",
    "#             #             norm2 = np.linalg.norm(vec2)\n",
    "#             #             cosine_similarity = dot_product / (norm1 * norm2)\n",
    "#             #             matching_angle[v1,v2] = np.abs(cosine_similarity)\n",
    "\n",
    "#             #     sns.heatmap(matching_angle, annot=True, cmap='coolwarm', cbar=True, ax=axmatch[count])\n",
    "#             #     figmatch.savefig(f\"./results/zz_test_{pca_target}_{hyp_dict['ruleset']}_{chosen_network}_{hyp_dict['addon_name']}_match.png\")\n",
    "\n",
    "                    \n",
    "#             # count += 1\n",
    "\n",
    "#         # if pca_type in ('cell_types',): # Some additional PC plots of only one cell type\n",
    "#         #     print('Yes')\n",
    "\n",
    "#         # if session_iter == len(session_breakdown)-1 and pca_target in ('hs',):\n",
    "#         #     for ax in axdomain:\n",
    "#         #         ax.legend()\n",
    "                \n",
    "#         fig.savefig(f\"./results/trajectory_{pca_target}_{hyp_dict['ruleset']}_{chosen_network}_{hyp_dict['addon_name']}_period{session_iter}.png\")\n",
    "#         # figdomain.savefig(f\"./results/zz_test_{pca_target}_{hyp_dict['ruleset']}_{chosen_network}_{hyp_dict['addon_name']}.png\")\n",
    "\n",
    "\n",
    "# #######\n",
    "# #######\n",
    "# #######\n",
    "# #######\n",
    "\n",
    "# print(db.keys())\n",
    "\n",
    "# if len(rules_dict[hyp_dict['ruleset']]) > 1:\n",
    "#     hs_all = []\n",
    "#     if net_params['net_type'] in ('dmpn',):\n",
    "#         layer_idx_lst = [i for i in range(mpn_depth)]\n",
    "#         for layer_idx in layer_idx_lst:\n",
    "#             if layer_idx == 0:\n",
    "#                 hs = db['hidden0'].detach().cpu().numpy()\n",
    "#             elif layer_idx == 1:\n",
    "#                 hs = db['hidden1'].detach().cpu().numpy()\n",
    "#             hs_all.append(hs)\n",
    "#     elif net_params['net_type'] in ('gru',):\n",
    "#         layer_idx_lst = [0]\n",
    "#         hs_all = [db['hidden'].detach().cpu().numpy()]\n",
    "\n",
    "#     for hsiter in range(len(hs_all)):\n",
    "#         hs = hs_all[hsiter]\n",
    "#         layer_name = layer_idx_lst[hsiter]\n",
    "\n",
    "#         cell_vars_tot = np.var(hs, axis=(0, 1)) # Var over batch and sequence\n",
    "#         n_rules = len(task_params['rules'])\n",
    "#         n_cells = hs.shape[-1]\n",
    "\n",
    "#         # cell_vars_dtypes = [('rule{}'.format(rule_idx), np.float) for rule_idx in range(n_rules)]# Useful for sorting later\n",
    "#         cell_vars_rules = np.zeros((n_rules, n_cells,))\n",
    "#         cell_vars_rules_norm = np.zeros_like(cell_vars_rules)\n",
    "\n",
    "#         for rule_idx, rule in enumerate(task_params['rules']):\n",
    "#             print('Rule {} (idx {})'.format(rule, rule_idx))\n",
    "#             rule_hs = hs[test_rule_idxs == rule_idx, :, :]\n",
    "#             cell_vars_rules[rule_idx] = np.var(rule_hs, axis=(0, 1)) # Var over batch and sequence\n",
    "\n",
    "#         # Now normalize everything\n",
    "#         cell_max_var = np.max(cell_vars_rules, axis=0) # Across rules\n",
    "#         for rule_idx, rule in enumerate(task_params['rules']):\n",
    "#             cell_vars_rules_norm[rule_idx] = np.where(\n",
    "#                 cell_max_var > 0., cell_vars_rules[rule_idx] / cell_max_var, 0.\n",
    "#             )\n",
    "\n",
    "#         # Now sort\n",
    "#         if n_rules > 1:\n",
    "#             rule0_vals = cell_vars_rules_norm[0].tolist()\n",
    "#             rule1_vals = cell_vars_rules_norm[1].tolist()\n",
    "\n",
    "#         rule01_vals = np.array(list(zip(rule0_vals, rule1_vals)), dtype=[('rule0', float), ('rule1', float)])\n",
    "#         sort_idxs = np.argsort(rule01_vals, order=['rule0', 'rule1'])[::-1]\n",
    "\n",
    "#         # sort_idxs = np.argsort(cell_vars_rules_norm[0])[::-1]\n",
    "#         cell_vars_rules_sorted_norm = cell_vars_rules_norm[:, sort_idxs]\n",
    "\n",
    "#         fig, ax = plt.subplots(2, 1, figsize=(12, 4*2))\n",
    "\n",
    "#         for rule_idx, rule in enumerate(task_params['rules']):\n",
    "#             ax[0].plot(cell_vars_rules_sorted_norm[rule_idx], color=c_vals[rule_idx],\n",
    "#                     label=task_params['rules'][rule_idx])\n",
    "\n",
    "#         ax[0].legend()\n",
    "\n",
    "#         ax[0].set_xlabel('Cell_idx')\n",
    "#         ax[0].set_ylabel('Norm. task var.')\n",
    "\n",
    "\n",
    "#         ax[1].matshow(cell_vars_rules_sorted_norm, aspect='auto', vmin=0.0, vmax=1.0,)\n",
    "#         ax[1].set_yticks(np.arange(n_rules))\n",
    "#         ax[1].set_yticklabels(task_params['rules'])\n",
    "#         ax[1].set_xlabel('Cell idx')\n",
    "#         fig.savefig(f\"./results/categorization_{hyp_dict['ruleset']}_{chosen_network}_layer_{layer_name}_{hyp_dict['addon_name']}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
