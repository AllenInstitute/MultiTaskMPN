{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc844e6c-421e-4e41-9761-cab6d0de524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import gc \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import mpn \n",
    "\n",
    "class SequentialMNIST(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, root, train=True, download=True, normalize=True):\n",
    "        tfms = [transforms.ToTensor()]\n",
    "        self.ds = datasets.MNIST(root=root, train=train, download=download, transform=transforms.Compose(tfms))\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.ds[idx]             \n",
    "        x = x.view(-1)                  \n",
    "        if self.normalize:\n",
    "            x = (x - 0.1307) / 0.3081\n",
    "        x_seq = x.unsqueeze(-1)        \n",
    "        return x_seq, y\n",
    "\n",
    "\n",
    "def collate_seq(batch):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    xs, ys = zip(*batch)\n",
    "    x = torch.stack(xs, dim=0)\n",
    "    y = torch.tensor(ys, dtype=torch.long)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c6d0c86-cbc9-430f-8372-55d2b936b289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cuda_tensor_shapes(limit=None, sort_by_numel=True, include_nonleaf=True):\n",
    "    \"\"\"\n",
    "    Prints shapes (and a bit more) for all live torch tensors on CUDA.\n",
    "\n",
    "    Notes:\n",
    "    - This lists tensors that are still referenced by Python (reachable by GC).\n",
    "    - It may include duplicates (views). We de-duplicate by storage data_ptr.\n",
    "    \"\"\"\n",
    "    cuda_tensors = []\n",
    "    seen = set()\n",
    "\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj):\n",
    "                t = obj\n",
    "            elif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\n",
    "                # Parameters and some wrappers\n",
    "                t = obj.data\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if t.is_cuda:\n",
    "                # de-dup by underlying storage pointer (works for views)\n",
    "                try:\n",
    "                    key = (t.untyped_storage().data_ptr(), t.storage_offset(), tuple(t.size()), str(t.dtype))\n",
    "                except Exception:\n",
    "                    key = (t.data_ptr(), tuple(t.size()), str(t.dtype))\n",
    "\n",
    "                if key in seen:\n",
    "                    continue\n",
    "                seen.add(key)\n",
    "\n",
    "                if (not include_nonleaf) and (t.grad_fn is not None):\n",
    "                    continue\n",
    "\n",
    "                cuda_tensors.append(t)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if sort_by_numel:\n",
    "        cuda_tensors.sort(key=lambda x: x.numel(), reverse=True)\n",
    "\n",
    "    if limit is not None:\n",
    "        cuda_tensors = cuda_tensors[:limit]\n",
    "\n",
    "    total_bytes = 0\n",
    "    for i, t in enumerate(cuda_tensors, 1):\n",
    "        nbytes = t.numel() * t.element_size()\n",
    "        total_bytes += nbytes\n",
    "        print(\n",
    "            f\"[{i:04d}] shape={tuple(t.shape)} dtype={t.dtype} \"\n",
    "            f\"device={t.device} requires_grad={t.requires_grad} \"\n",
    "            f\"bytes={nbytes/1024**2:.2f}MB\"\n",
    "        )\n",
    "\n",
    "    print(f\"\\nCount: {len(cuda_tensors)} tensors\")\n",
    "    print(f\"Estimated total (sum of listed tensor sizes): {total_bytes/1024**2:.2f}MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6797e325-ca35-435b-be90-cea710e8490f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@torch.no_grad()\n",
    "def evaluate(net, loader, device):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y in loader:\n",
    "        x = x.to(device) \n",
    "        y = y.to(device)\n",
    "\n",
    "        B, T, D = x.shape\n",
    "        net.reset_state(B=B)\n",
    "\n",
    "        out = None\n",
    "        for t in range(T):\n",
    "            out, _, _ = net.network_step(x[:, t, :], run_mode=\"minimal\", seq_idx=t)\n",
    "\n",
    "        pred = out.argmax(dim=-1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.numel()\n",
    "\n",
    "    return correct / max(total, 1)\n",
    "\n",
    "def count_parameter(net):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    trainable = [(n, p) for n, p in net.named_parameters() if p.requires_grad]\n",
    "    \n",
    "    # Print a readable summary\n",
    "    total = 0\n",
    "    for n, p in trainable:\n",
    "        num = p.numel()\n",
    "        total += num\n",
    "        print(f\"{n:50s}  shape={tuple(p.shape)}  numel={num}\")\n",
    "    \n",
    "    print(f\"\\nTotal trainable parameters: {total}\")\n",
    "\n",
    "# Example usage:\n",
    "# print_cuda_params(net)\n",
    "\n",
    "\n",
    "def train_sequential_mnist(\n",
    "    device=\"cuda\",\n",
    "    data_root=\"./data\",\n",
    "    hidden_dim=256,\n",
    "    batch_size=64,\n",
    "    lr=1e-3,\n",
    "    epochs=5,\n",
    "    mpn_depth=5\n",
    "):\n",
    "    device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"device: {device}\")\n",
    "\n",
    "    # Data\n",
    "    train_ds = SequentialMNIST(root=data_root, train=True, download=True, normalize=True)\n",
    "    test_ds  = SequentialMNIST(root=data_root, train=False, download=True, normalize=True)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2,\n",
    "                              pin_memory=True, collate_fn=collate_seq)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=batch_size*10, shuffle=False, num_workers=2,\n",
    "                              pin_memory=True, collate_fn=collate_seq)\n",
    "\n",
    "    net_params = {\n",
    "        \"n_neurons\": [1] + [hidden_dim] * mpn_depth + [10],\n",
    "        \"linear_embed\": 100, \n",
    "        \"dt\": 1.0,\n",
    "        \"activation\": \"tanh\",            \n",
    "        \"output_bias\": True,\n",
    "        \"W_output_init\": \"xavier\",\n",
    "        \"input_layer_add\": True, \n",
    "        'input_layer_add_trainable': True, \n",
    "        'input_layer_bias': False, \n",
    "        \"output_matrix\": \"\", \n",
    "\n",
    "        'ml_params': {\n",
    "            'bias': True, # Bias of layer\n",
    "            'mp_type': 'mult',\n",
    "            'm_update_type': 'hebb_assoc', # hebb_assoc, hebb_pre\n",
    "            'eta_type': 'scalar', # scalar, pre_vector, post_vector, matrix\n",
    "            'eta_train': False,\n",
    "            'lam_type': 'scalar', # scalar, pre_vector, post_vector, matrix\n",
    "            'm_time_scale': 100, \n",
    "            'lam_train': False,\n",
    "            'W_freeze': False, # different combination with [input_layer_add_trainable]\n",
    "        },\n",
    "    }\n",
    "\n",
    "    net = mpn.DeepMultiPlasticNet(net_params, verbose=True, forzihan=False).to(device)\n",
    "    \n",
    "    count_parameter(net)\n",
    "    # print_cuda_tensor_shapes()\n",
    "\n",
    "    opt = torch.optim.Adam([p for p in net.parameters() if p.requires_grad], lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        opt,\n",
    "        mode=\"max\",        # because we maximize accuracy\n",
    "        factor=0.5,        # lr <- lr * 0.5\n",
    "        patience=2,        # wait 2 epochs without improvement\n",
    "        threshold=1e-3,\n",
    "        min_lr=1e-6,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    stats = {\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        net.train()\n",
    "        t0 = time.time()\n",
    "        running_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device, non_blocking=True)   # (B,T,1)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "\n",
    "            B, T, D = x.shape\n",
    "            net.reset_state(B=B)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            out = None\n",
    "            \n",
    "            for t in range(T):\n",
    "                out, _, _ = net.network_step(x[:, t, :], run_mode=\"minimal\", seq_idx=t)\n",
    "\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "\n",
    "            if (n_batches % 100) == 0:\n",
    "                mpl = net.mp_layers[0]\n",
    "                with torch.no_grad():\n",
    "                    M_abs_mean = mpl.M.abs().mean().item()\n",
    "                    M_abs_max  = mpl.M.abs().max().item()\n",
    "                    W_abs_mean = mpl.W.abs().mean().item()\n",
    "                print(f\"[ep {ep} | batch {n_batches}] loss={loss.item():.4f} \"\n",
    "                      f\"|M| mean={M_abs_mean:.3e} max={M_abs_max:.3e} |W| mean={W_abs_mean:.3e}\")\n",
    "\n",
    "            opt.step()\n",
    "            net.param_clamp()\n",
    "\n",
    "            running_loss += float(loss.item())\n",
    "            n_batches += 1\n",
    "\n",
    "        train_loss = running_loss / max(n_batches, 1)\n",
    "        test_acc = evaluate(net, test_loader, device=device)\n",
    "        scheduler.step(test_acc)\n",
    "\n",
    "        dt = time.time() - t0\n",
    "\n",
    "        current_lr = opt.param_groups[0][\"lr\"]\n",
    "        print(f\"Epoch {ep:02d}/{epochs} | lr={current_lr:.2e} | loss={train_loss:.4f} | test_acc={test_acc*100:.2f}%\")\n",
    "\n",
    "        stats[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    return net, net_params, stats\n",
    "'''\n",
    "@torch.no_grad()\n",
    "def evaluate(net, loader, device, chunk_size=64):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y in loader:\n",
    "        x = x.to(device, non_blocking=True)  # (B,T,D)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        # Convert to (T,B,D)\n",
    "        x_TBD = x.transpose(0, 1).contiguous()\n",
    "\n",
    "        logits_seq, _ = net.forward_sequence_checkpointed(\n",
    "            x_TBD,\n",
    "            chunk_size=chunk_size,\n",
    "            Ms0=None,\n",
    "            run_mode=\"minimal\",\n",
    "        )\n",
    "\n",
    "        out = logits_seq[-1]  # last step logits: (B,10)\n",
    "        pred = out.argmax(dim=-1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.numel()\n",
    "\n",
    "    return correct / max(total, 1)\n",
    "\n",
    "def count_parameter(net):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    trainable = [(n, p) for n, p in net.named_parameters() if p.requires_grad]\n",
    "    \n",
    "    # Print a readable summary\n",
    "    total = 0\n",
    "    for n, p in trainable:\n",
    "        num = p.numel()\n",
    "        total += num\n",
    "        print(f\"{n:50s}  shape={tuple(p.shape)}  numel={num}\")\n",
    "    \n",
    "    print(f\"\\nTotal trainable parameters: {total}\")\n",
    "\n",
    "# Example usage:\n",
    "# print_cuda_params(net)\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def train_sequential_mnist(\n",
    "    device=\"cuda\",\n",
    "    data_root=\"./data\",\n",
    "    hidden_dim=256,\n",
    "    batch_size=64,\n",
    "    lr=1e-3,\n",
    "    epochs=5,\n",
    "    mpn_depth=5,\n",
    "    chunk_size=32,\n",
    "    use_amp=False,\n",
    "):\n",
    "    device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"device: {device}\")\n",
    "\n",
    "    # Data\n",
    "    train_ds = SequentialMNIST(root=data_root, train=True, download=True, normalize=True)\n",
    "    test_ds  = SequentialMNIST(root=data_root, train=False, download=True, normalize=True)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2,\n",
    "                              pin_memory=True, collate_fn=collate_seq)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=batch_size*10, shuffle=False, num_workers=2,\n",
    "                              pin_memory=True, collate_fn=collate_seq)\n",
    "\n",
    "    net_params = {\n",
    "        \"n_neurons\": [1] + [hidden_dim] * mpn_depth + [10],\n",
    "        \"linear_embed\": 100,\n",
    "        \"dt\": 1.0,\n",
    "        \"activation\": \"tanh\",\n",
    "        \"output_bias\": True,\n",
    "        \"W_output_init\": \"xavier\",\n",
    "        \"input_layer_add\": True,\n",
    "        'input_layer_add_trainable': True,\n",
    "        'input_layer_bias': False,\n",
    "        \"output_matrix\": \"\",\n",
    "\n",
    "        'ml_params': {\n",
    "            'bias': True,\n",
    "            'mp_type': 'mult',\n",
    "            'm_update_type': 'hebb_assoc',\n",
    "            'eta_type': 'scalar',\n",
    "            'eta_train': False,\n",
    "            'lam_type': 'scalar',\n",
    "            'm_time_scale': 100,\n",
    "            'lam_train': False,\n",
    "            'W_freeze': False,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    net = mpn.DeepMultiPlasticNet(net_params, verbose=True, forzihan=False).to(device)\n",
    "\n",
    "    count_parameter(net)\n",
    "\n",
    "    opt = torch.optim.Adam([p for p in net.parameters() if p.requires_grad], lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        opt, mode=\"max\", factor=0.5, patience=2, threshold=1e-3, min_lr=1e-6, verbose=True\n",
    "    )\n",
    "\n",
    "    scaler = GradScaler(enabled=(use_amp and device.type == \"cuda\"))\n",
    "\n",
    "    stats = {\"test_acc\": []}\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        net.train()\n",
    "        t0 = time.time()\n",
    "        running_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device, non_blocking=True)   # (B,T,1)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "\n",
    "            # Convert to (T,B,D) for forward_sequence_checkpointed\n",
    "            x_TBD = x.transpose(0, 1).contiguous()\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Forward with time checkpointing\n",
    "            with autocast(enabled=True):#autocast(device_type=\"cuda\", enabled=(use_amp and device.type == \"cuda\")):\n",
    "                logits_seq, Ms_T = net.forward_sequence_checkpointed(\n",
    "                    x_TBD,\n",
    "                    chunk_size=chunk_size,\n",
    "                    Ms0=None,\n",
    "                    run_mode=\"minimal\",\n",
    "                )\n",
    "                out = logits_seq[-1]  # (B,10)\n",
    "                loss = criterion(out, y)\n",
    "\n",
    "            # Backward\n",
    "            if scaler.is_enabled():\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "            net.param_clamp()\n",
    "\n",
    "            # Logging: use Ms_T (returned) instead of net.mp_layers[0].M\n",
    "            if (n_batches % 100) == 0:\n",
    "                with torch.no_grad():\n",
    "                    if Ms_T is not None and len(Ms_T) > 0:\n",
    "                        M0 = Ms_T[0]  # (B, out, in)\n",
    "                        M_abs_mean = M0.abs().mean().item()\n",
    "                        M_abs_max  = M0.abs().max().item()\n",
    "                    else:\n",
    "                        M_abs_mean, M_abs_max = float(\"nan\"), float(\"nan\")\n",
    "\n",
    "                    W0 = net.mp_layers[0].W\n",
    "                    W_abs_mean = W0.abs().mean().item()\n",
    "\n",
    "                print(f\"[ep {ep} | batch {n_batches}] loss={loss.item():.4f} \"\n",
    "                      f\"|M| mean={M_abs_mean:.3e} max={M_abs_max:.3e} |W| mean={W_abs_mean:.3e}\")\n",
    "                torch.cuda.synchronize()\n",
    "                alloc = torch.cuda.memory_allocated() / 1024**3\n",
    "                reserv = torch.cuda.memory_reserved() / 1024**3\n",
    "                print(f\"ep {ep} batch {n_batches} | loss={loss.item():.4f} | alloc={alloc:.2f}GB reserv={reserv:.2f}GB\", flush=True)\n",
    "\n",
    "            running_loss += float(loss.item())\n",
    "            n_batches += 1\n",
    "\n",
    "        train_loss = running_loss / max(n_batches, 1)\n",
    "        test_acc = evaluate(net, test_loader, device=device, chunk_size=max(chunk_size, 64))\n",
    "        scheduler.step(test_acc)\n",
    "\n",
    "        dt = time.time() - t0\n",
    "        current_lr = opt.param_groups[0][\"lr\"]\n",
    "        print(f\"Epoch {ep:02d}/{epochs} | lr={current_lr:.2e} | loss={train_loss:.4f} | test_acc={test_acc*100:.2f}% | dt={dt:.1f}s\")\n",
    "\n",
    "        stats[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    return net, net_params, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea7e23-c307-4bce-b19c-5b4d7fdfd480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "MultiPlastic Net:\n",
      "  output neurons: 10\n",
      "  Act: tanh\n",
      "\n",
      "1.0\n",
      "  MP Layer1 parameters:\n",
      "    n_neurons - input: 100, output: 128\n",
      "    M matrix parameters:    update bounds - Max mult: 1.0, Min mult: -1.0\n",
      "      type: mult // Update - type: hebb_assoc // Act fn: linear\n",
      "      Eta: scalar (fixed) // Lambda: scalar (fixed) // Lambda_max: 0.99 (tau: 1.0e+02)\n",
      "  MP Layer2 parameters:\n",
      "    n_neurons - input: 128, output: 128\n",
      "    M matrix parameters:    update bounds - Max mult: 1.0, Min mult: -1.0\n",
      "      type: mult // Update - type: hebb_assoc // Act fn: linear\n",
      "      Eta: scalar (fixed) // Lambda: scalar (fixed) // Lambda_max: 0.99 (tau: 1.0e+02)\n",
      "  MP Layer3 parameters:\n",
      "    n_neurons - input: 128, output: 128\n",
      "    M matrix parameters:    update bounds - Max mult: 1.0, Min mult: -1.0\n",
      "      type: mult // Update - type: hebb_assoc // Act fn: linear\n",
      "      Eta: scalar (fixed) // Lambda: scalar (fixed) // Lambda_max: 0.99 (tau: 1.0e+02)\n",
      "  No Hidden Recurrency.\n",
      "W_output                                            shape=(10, 128)  numel=1280\n",
      "b_output                                            shape=(10,)  numel=10\n",
      "W_initial_linear.weight                             shape=(100, 1)  numel=100\n",
      "mp_layer1.W                                         shape=(128, 100)  numel=12800\n",
      "mp_layer1.b                                         shape=(128,)  numel=128\n",
      "mp_layer2.W                                         shape=(128, 128)  numel=16384\n",
      "mp_layer2.b                                         shape=(128,)  numel=128\n",
      "mp_layer3.W                                         shape=(128, 128)  numel=16384\n",
      "mp_layer3.b                                         shape=(128,)  numel=128\n",
      "\n",
      "Total trainable parameters: 47342\n",
      "[ep 1 | batch 0] loss=2.4398 |M| mean=3.868e-01 max=1.000e+00 |W| mean=8.082e-02\n"
     ]
    }
   ],
   "source": [
    "trained_net, params = train_sequential_mnist(\n",
    "                device=\"cuda\",\n",
    "                hidden_dim=128,\n",
    "                batch_size=64,\n",
    "                lr=1e-3,\n",
    "                epochs=20,\n",
    "                mpn_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731c36a7-94af-4b00-a201-1ca08bb2822b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ckpt_path = \"./mnist/mpn_seqmnist_ckpt.pt\"\n",
    "\n",
    "checkpoint = {\n",
    "    \"model_class\": \"DeepMultiPlasticNet\",\n",
    "    \"net_params\": net_params,  # the same dict used to build the net\n",
    "    \"state_dict\": trained_net.state_dict(),\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, ckpt_path)\n",
    "print(f\"Saved checkpoint to: {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c147060-1175-4f2f-aaee-0710ead1dd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55ce6b3-f52b-4740-b034-1ea923bbaa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, W2, W3 = trained_net.mp_layer1.W.cpu().detach(), trained_net.mp_layer2.W.cpu().detach(), trained_net.mp_layer3.W.cpu().detach()\n",
    "Ws = [W1, W2, W3]\n",
    "fig, axs = plt.subplots(1,3,figsize=(4*3,4))\n",
    "for i in range(len(Ws)): \n",
    "    sns.heatmap(Ws[i], ax=axs[i], square=True, cmap=\"coolwarm\", center=0, vmax=1, vmin=-1)\n",
    "    axs[i].set_title(f\"Mean: {torch.mean(Ws[i]):1f}\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf66705-16c7-4fe4-b15a-96eb30fb2e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, W2, W3 = trained_net.mp_layer1.eta.cpu().detach(), trained_net.mp_layer2.eta.cpu().detach(), trained_net.mp_layer3.eta.cpu().detach()\n",
    "Ws = [W1, W2, W3]\n",
    "fig, axs = plt.subplots(1,3,figsize=(4*3,4))\n",
    "for i in range(len(Ws)): \n",
    "    sns.heatmap(Ws[i], ax=axs[i], square=True, cmap=\"coolwarm\", center=0, vmax=2, vmin=-2)\n",
    "    axs[i].set_title(f\"Mean: {torch.mean(Ws[i]):1f}\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133f4090-6d28-439e-ba76-e92491930111",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, W2, W3 = trained_net.mp_layer1.lam.cpu().detach(), trained_net.mp_layer2.lam.cpu().detach(), trained_net.mp_layer3.lam.cpu().detach()\n",
    "Ws = [W1, W2, W3]\n",
    "fig, axs = plt.subplots(1,3,figsize=(4*3,4))\n",
    "for i in range(len(Ws)): \n",
    "    sns.heatmap(Ws[i], ax=axs[i], square=True, cmap=\"coolwarm\", center=0.5, vmax=1, vmin=0)\n",
    "    axs[i].set_title(f\"Mean: {torch.mean(Ws[i]):1f}\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b218977-de07-4f3c-93f2-c15a063a379b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TorchOnGPUs",
   "language": "python",
   "name": "torchongpus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
