{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc844e6c-421e-4e41-9761-cab6d0de524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import gc \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import mpn \n",
    "\n",
    "class SequentialMNIST(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, root, train=True, download=True, normalize=True):\n",
    "        tfms = [transforms.ToTensor()]\n",
    "        self.ds = datasets.MNIST(root=root, train=train, download=download, transform=transforms.Compose(tfms))\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.ds[idx]             \n",
    "        x = x.view(-1)                  \n",
    "        if self.normalize:\n",
    "            x = (x - 0.1307) / 0.3081\n",
    "        x_seq = x.unsqueeze(-1)        \n",
    "        return x_seq, y\n",
    "\n",
    "\n",
    "def collate_seq(batch):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    xs, ys = zip(*batch)\n",
    "    x = torch.stack(xs, dim=0)\n",
    "    y = torch.tensor(ys, dtype=torch.long)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c6d0c86-cbc9-430f-8372-55d2b936b289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cuda_tensor_shapes(limit=None, sort_by_numel=True, include_nonleaf=True):\n",
    "    \"\"\"\n",
    "    Prints shapes (and a bit more) for all live torch tensors on CUDA.\n",
    "\n",
    "    Notes:\n",
    "    - This lists tensors that are still referenced by Python (reachable by GC).\n",
    "    - It may include duplicates (views). We de-duplicate by storage data_ptr.\n",
    "    \"\"\"\n",
    "    cuda_tensors = []\n",
    "    seen = set()\n",
    "\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj):\n",
    "                t = obj\n",
    "            elif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\n",
    "                # Parameters and some wrappers\n",
    "                t = obj.data\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if t.is_cuda:\n",
    "                # de-dup by underlying storage pointer (works for views)\n",
    "                try:\n",
    "                    key = (t.untyped_storage().data_ptr(), t.storage_offset(), tuple(t.size()), str(t.dtype))\n",
    "                except Exception:\n",
    "                    key = (t.data_ptr(), tuple(t.size()), str(t.dtype))\n",
    "\n",
    "                if key in seen:\n",
    "                    continue\n",
    "                seen.add(key)\n",
    "\n",
    "                if (not include_nonleaf) and (t.grad_fn is not None):\n",
    "                    continue\n",
    "\n",
    "                cuda_tensors.append(t)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if sort_by_numel:\n",
    "        cuda_tensors.sort(key=lambda x: x.numel(), reverse=True)\n",
    "\n",
    "    if limit is not None:\n",
    "        cuda_tensors = cuda_tensors[:limit]\n",
    "\n",
    "    total_bytes = 0\n",
    "    for i, t in enumerate(cuda_tensors, 1):\n",
    "        nbytes = t.numel() * t.element_size()\n",
    "        total_bytes += nbytes\n",
    "        print(\n",
    "            f\"[{i:04d}] shape={tuple(t.shape)} dtype={t.dtype} \"\n",
    "            f\"device={t.device} requires_grad={t.requires_grad} \"\n",
    "            f\"bytes={nbytes/1024**2:.2f}MB\"\n",
    "        )\n",
    "\n",
    "    print(f\"\\nCount: {len(cuda_tensors)} tensors\")\n",
    "    print(f\"Estimated total (sum of listed tensor sizes): {total_bytes/1024**2:.2f}MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6797e325-ca35-435b-be90-cea710e8490f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(net, loader, device, chunk_size=64):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y in loader:\n",
    "        x = x.to(device, non_blocking=True)  # (B,T,D)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        # Convert to (T,B,D)\n",
    "        x_TBD = x.transpose(0, 1).contiguous()\n",
    "\n",
    "        logits_last = net.forward_sequence_checkpointed(\n",
    "            x_TBD,\n",
    "            chunk_size=chunk_size,\n",
    "            Ms0=None,\n",
    "            run_mode=\"minimal\",\n",
    "        )\n",
    "\n",
    "        pred = logits_last.argmax(dim=-1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.numel()\n",
    "\n",
    "    return correct / max(total, 1)\n",
    "\n",
    "def count_parameter(net):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    trainable = [(n, p) for n, p in net.named_parameters() if p.requires_grad]\n",
    "    \n",
    "    # Print a readable summary\n",
    "    total = 0\n",
    "    for n, p in trainable:\n",
    "        num = p.numel()\n",
    "        total += num\n",
    "        print(f\"{n:50s}  shape={tuple(p.shape)}  numel={num}\")\n",
    "    \n",
    "    print(f\"\\nTotal trainable parameters: {total}\")\n",
    "\n",
    "# Example usage:\n",
    "# print_cuda_params(net)\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def train_sequential_mnist(\n",
    "    device=\"cuda\",\n",
    "    data_root=\"./data\",\n",
    "    hidden_dim=256,\n",
    "    batch_size=64,\n",
    "    lr=1e-3,\n",
    "    epochs=5,\n",
    "    mpn_depth=5,\n",
    "    chunk_size=32,\n",
    "    use_amp=True,\n",
    "):\n",
    "    device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"device: {device}\")\n",
    "\n",
    "    # Data\n",
    "    train_ds = SequentialMNIST(root=data_root, train=True, download=True, normalize=True)\n",
    "    test_ds  = SequentialMNIST(root=data_root, train=False, download=True, normalize=True)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2,\n",
    "                              pin_memory=True, collate_fn=collate_seq)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=batch_size*10, shuffle=False, num_workers=2,\n",
    "                              pin_memory=True, collate_fn=collate_seq)\n",
    "\n",
    "    net_params = {\n",
    "        \"n_neurons\": [1] + [hidden_dim] * mpn_depth + [10],\n",
    "        \"linear_embed\": 100,\n",
    "        \"dt\": 1.0,\n",
    "        \"activation\": \"tanh\",\n",
    "        \"output_bias\": True,\n",
    "        \"W_output_init\": \"xavier\",\n",
    "        \"input_layer_add\": True,\n",
    "        'input_layer_add_trainable': True,\n",
    "        'input_layer_bias': False,\n",
    "        \"output_matrix\": \"\",\n",
    "\n",
    "        'ml_params': {\n",
    "            'bias': True,\n",
    "            'mp_type': 'mult',\n",
    "            'm_update_type': 'hebb_assoc',\n",
    "            'eta_type': 'scalar',\n",
    "            'eta_train': False,\n",
    "            'lam_type': 'scalar',\n",
    "            'm_time_scale': 100,\n",
    "            'lam_train': False,\n",
    "            'W_freeze': False,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    net = mpn.DeepMultiPlasticNet(net_params, verbose=True, forzihan=False).to(device)\n",
    "\n",
    "    count_parameter(net)\n",
    "\n",
    "    opt = torch.optim.Adam([p for p in net.parameters() if p.requires_grad], lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        opt, mode=\"max\", factor=0.5, patience=2, threshold=1e-3, min_lr=1e-6, verbose=True\n",
    "    )\n",
    "\n",
    "    scaler = GradScaler(enabled=(use_amp and device.type == \"cuda\"))\n",
    "\n",
    "    stats = {\"test_acc\": []}\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        net.train()\n",
    "        t0 = time.time()\n",
    "        running_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device, non_blocking=True)   # (B,T,1)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "\n",
    "            # Convert to (T,B,D) for forward_sequence_checkpointed\n",
    "            x_TBD = x.transpose(0, 1).contiguous()\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Forward with time checkpointing\n",
    "            with autocast(enabled=(use_amp and device.type == \"cuda\")):#autocast(device_type=\"cuda\", enabled=(use_amp and device.type == \"cuda\")):\n",
    "                logits_last = net.forward_sequence_checkpointed(\n",
    "                    x_TBD,\n",
    "                    chunk_size=chunk_size,\n",
    "                    Ms0=None,\n",
    "                    run_mode=\"minimal\",\n",
    "                )\n",
    "                loss = criterion(logits_last, y)\n",
    "\n",
    "            # Backward\n",
    "            if scaler.is_enabled():\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "            net.param_clamp()\n",
    "\n",
    "            # Logging: \n",
    "            if (n_batches % 10) == 0:\n",
    "                with torch.no_grad():\n",
    "                    W0 = net.mp_layers[0].W\n",
    "                    W_abs_mean = W0.abs().mean().item()\n",
    "\n",
    "                print(f\"[ep {ep} | batch {n_batches}] loss={loss.item():.4f} \"\n",
    "                      f\"|M| |W| mean={W_abs_mean:.3e}\")\n",
    "                torch.cuda.synchronize()\n",
    "                alloc = torch.cuda.memory_allocated() / 1024**3\n",
    "                reserv = torch.cuda.memory_reserved() / 1024**3\n",
    "                print(f\"ep {ep} batch {n_batches} | loss={loss.item():.4f} | alloc={alloc:.2f}GB reserv={reserv:.2f}GB\", flush=True)\n",
    "\n",
    "            running_loss += float(loss.item())\n",
    "            n_batches += 1\n",
    "\n",
    "        train_loss = running_loss / max(n_batches, 1)\n",
    "        test_acc = evaluate(net, test_loader, device=device, chunk_size=max(chunk_size, 64))\n",
    "        scheduler.step(test_acc)\n",
    "\n",
    "        dt = time.time() - t0\n",
    "        current_lr = opt.param_groups[0][\"lr\"]\n",
    "        print(f\"Epoch {ep:02d}/{epochs} | lr={current_lr:.2e} | loss={train_loss:.4f} | test_acc={test_acc*100:.2f}% | dt={dt:.1f}s\")\n",
    "\n",
    "        stats[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    return net, net_params, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adba9e35-1bce-4f83-bb1c-179e42c4fddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "MultiPlastic Net:\n",
      "  output neurons: 10\n",
      "  Act: tanh\n",
      "\n",
      "1.0\n",
      "  MP Layer1 parameters:\n",
      "    n_neurons - input: 100, output: 128\n",
      "    M matrix parameters:    update bounds - Max mult: 1.0, Min mult: -1.0\n",
      "      type: mult // Update - type: hebb_assoc // Act fn: linear\n",
      "      Eta: scalar (fixed) // Lambda: scalar (fixed) // Lambda_max: 0.99 (tau: 1.0e+02)\n",
      "  MP Layer2 parameters:\n",
      "    n_neurons - input: 128, output: 128\n",
      "    M matrix parameters:    update bounds - Max mult: 1.0, Min mult: -1.0\n",
      "      type: mult // Update - type: hebb_assoc // Act fn: linear\n",
      "      Eta: scalar (fixed) // Lambda: scalar (fixed) // Lambda_max: 0.99 (tau: 1.0e+02)\n",
      "  MP Layer3 parameters:\n",
      "    n_neurons - input: 128, output: 128\n",
      "    M matrix parameters:    update bounds - Max mult: 1.0, Min mult: -1.0\n",
      "      type: mult // Update - type: hebb_assoc // Act fn: linear\n",
      "      Eta: scalar (fixed) // Lambda: scalar (fixed) // Lambda_max: 0.99 (tau: 1.0e+02)\n",
      "  No Hidden Recurrency.\n",
      "W_output                                            shape=(10, 128)  numel=1280\n",
      "b_output                                            shape=(10,)  numel=10\n",
      "W_initial_linear.weight                             shape=(100, 1)  numel=100\n",
      "mp_layer1.W                                         shape=(128, 100)  numel=12800\n",
      "mp_layer1.b                                         shape=(128,)  numel=128\n",
      "mp_layer2.W                                         shape=(128, 128)  numel=16384\n",
      "mp_layer2.b                                         shape=(128,)  numel=128\n",
      "mp_layer3.W                                         shape=(128, 128)  numel=16384\n",
      "mp_layer3.b                                         shape=(128,)  numel=128\n",
      "\n",
      "Total trainable parameters: 47342\n",
      "[ep 1 | batch 0] loss=2.3040 |M| |W| mean=8.024e-02\n",
      "ep 1 batch 0 | loss=2.3040 | alloc=0.02GB reserv=1.28GB\n",
      "[ep 1 | batch 10] loss=2.2986 |M| |W| mean=8.029e-02\n",
      "ep 1 batch 10 | loss=2.2986 | alloc=0.25GB reserv=1.50GB\n",
      "[ep 1 | batch 20] loss=2.2450 |M| |W| mean=8.033e-02\n",
      "ep 1 batch 20 | loss=2.2450 | alloc=0.48GB reserv=1.74GB\n",
      "[ep 1 | batch 30] loss=2.2385 |M| |W| mean=8.036e-02\n",
      "ep 1 batch 30 | loss=2.2385 | alloc=0.72GB reserv=1.96GB\n",
      "[ep 1 | batch 40] loss=2.2968 |M| |W| mean=8.040e-02\n",
      "ep 1 batch 40 | loss=2.2968 | alloc=0.95GB reserv=2.20GB\n",
      "[ep 1 | batch 50] loss=2.2741 |M| |W| mean=8.044e-02\n",
      "ep 1 batch 50 | loss=2.2741 | alloc=1.18GB reserv=2.44GB\n",
      "[ep 1 | batch 60] loss=2.2835 |M| |W| mean=8.046e-02\n",
      "ep 1 batch 60 | loss=2.2835 | alloc=1.41GB reserv=2.66GB\n",
      "[ep 1 | batch 70] loss=2.2117 |M| |W| mean=8.047e-02\n",
      "ep 1 batch 70 | loss=2.2117 | alloc=1.64GB reserv=2.90GB\n",
      "[ep 1 | batch 80] loss=2.1725 |M| |W| mean=8.049e-02\n",
      "ep 1 batch 80 | loss=2.1725 | alloc=1.87GB reserv=3.12GB\n",
      "[ep 1 | batch 90] loss=2.1906 |M| |W| mean=8.050e-02\n",
      "ep 1 batch 90 | loss=2.1906 | alloc=2.10GB reserv=3.34GB\n",
      "[ep 1 | batch 100] loss=2.1336 |M| |W| mean=8.051e-02\n",
      "ep 1 batch 100 | loss=2.1336 | alloc=2.34GB reserv=3.60GB\n",
      "[ep 1 | batch 110] loss=2.1550 |M| |W| mean=8.053e-02\n",
      "ep 1 batch 110 | loss=2.1550 | alloc=2.57GB reserv=3.82GB\n",
      "[ep 1 | batch 120] loss=2.1407 |M| |W| mean=8.055e-02\n",
      "ep 1 batch 120 | loss=2.1407 | alloc=2.80GB reserv=4.04GB\n",
      "[ep 1 | batch 130] loss=2.0452 |M| |W| mean=8.057e-02\n",
      "ep 1 batch 130 | loss=2.0452 | alloc=3.04GB reserv=4.30GB\n",
      "[ep 1 | batch 140] loss=2.0280 |M| |W| mean=8.060e-02\n",
      "ep 1 batch 140 | loss=2.0280 | alloc=3.28GB reserv=4.54GB\n",
      "[ep 1 | batch 150] loss=2.0922 |M| |W| mean=8.063e-02\n",
      "ep 1 batch 150 | loss=2.0922 | alloc=3.52GB reserv=4.79GB\n",
      "[ep 1 | batch 160] loss=2.0436 |M| |W| mean=8.068e-02\n",
      "ep 1 batch 160 | loss=2.0436 | alloc=3.76GB reserv=5.03GB\n",
      "[ep 1 | batch 170] loss=2.0431 |M| |W| mean=8.073e-02\n",
      "ep 1 batch 170 | loss=2.0431 | alloc=4.00GB reserv=5.27GB\n",
      "[ep 1 | batch 180] loss=1.9967 |M| |W| mean=8.074e-02\n",
      "ep 1 batch 180 | loss=1.9967 | alloc=4.24GB reserv=5.51GB\n",
      "[ep 1 | batch 190] loss=2.1326 |M| |W| mean=8.076e-02\n",
      "ep 1 batch 190 | loss=2.1326 | alloc=4.48GB reserv=5.75GB\n",
      "[ep 1 | batch 200] loss=2.0721 |M| |W| mean=8.078e-02\n",
      "ep 1 batch 200 | loss=2.0721 | alloc=4.71GB reserv=5.99GB\n",
      "[ep 1 | batch 210] loss=2.0040 |M| |W| mean=8.085e-02\n",
      "ep 1 batch 210 | loss=2.0040 | alloc=4.95GB reserv=6.23GB\n",
      "[ep 1 | batch 220] loss=2.1641 |M| |W| mean=8.097e-02\n",
      "ep 1 batch 220 | loss=2.1641 | alloc=5.19GB reserv=6.47GB\n",
      "[ep 1 | batch 230] loss=2.1466 |M| |W| mean=8.104e-02\n",
      "ep 1 batch 230 | loss=2.1466 | alloc=5.43GB reserv=6.71GB\n",
      "[ep 1 | batch 240] loss=1.9763 |M| |W| mean=8.107e-02\n",
      "ep 1 batch 240 | loss=1.9763 | alloc=5.67GB reserv=6.95GB\n",
      "[ep 1 | batch 250] loss=2.0003 |M| |W| mean=8.108e-02\n",
      "ep 1 batch 250 | loss=2.0003 | alloc=5.91GB reserv=7.19GB\n",
      "[ep 1 | batch 260] loss=1.9445 |M| |W| mean=8.109e-02\n",
      "ep 1 batch 260 | loss=1.9445 | alloc=6.14GB reserv=7.43GB\n",
      "[ep 1 | batch 270] loss=2.0945 |M| |W| mean=8.114e-02\n",
      "ep 1 batch 270 | loss=2.0945 | alloc=6.38GB reserv=7.67GB\n",
      "[ep 1 | batch 280] loss=2.1553 |M| |W| mean=8.120e-02\n",
      "ep 1 batch 280 | loss=2.1553 | alloc=6.62GB reserv=7.91GB\n",
      "[ep 1 | batch 290] loss=1.9322 |M| |W| mean=8.122e-02\n",
      "ep 1 batch 290 | loss=1.9322 | alloc=6.86GB reserv=8.15GB\n",
      "[ep 1 | batch 300] loss=1.9104 |M| |W| mean=8.123e-02\n",
      "ep 1 batch 300 | loss=1.9104 | alloc=7.10GB reserv=8.39GB\n",
      "[ep 1 | batch 310] loss=1.8498 |M| |W| mean=8.124e-02\n",
      "ep 1 batch 310 | loss=1.8498 | alloc=7.33GB reserv=8.63GB\n",
      "[ep 1 | batch 320] loss=1.9958 |M| |W| mean=8.127e-02\n",
      "ep 1 batch 320 | loss=1.9958 | alloc=7.57GB reserv=8.87GB\n",
      "[ep 1 | batch 330] loss=1.9754 |M| |W| mean=8.129e-02\n",
      "ep 1 batch 330 | loss=1.9754 | alloc=7.81GB reserv=9.11GB\n",
      "[ep 1 | batch 340] loss=1.8102 |M| |W| mean=8.130e-02\n",
      "ep 1 batch 340 | loss=1.8102 | alloc=8.05GB reserv=9.35GB\n",
      "[ep 1 | batch 350] loss=1.7826 |M| |W| mean=8.131e-02\n",
      "ep 1 batch 350 | loss=1.7826 | alloc=8.29GB reserv=9.59GB\n",
      "[ep 1 | batch 360] loss=1.8067 |M| |W| mean=8.130e-02\n",
      "ep 1 batch 360 | loss=1.8067 | alloc=8.53GB reserv=9.83GB\n",
      "[ep 1 | batch 370] loss=1.7364 |M| |W| mean=8.130e-02\n",
      "ep 1 batch 370 | loss=1.7364 | alloc=8.76GB reserv=10.07GB\n",
      "[ep 1 | batch 380] loss=1.8446 |M| |W| mean=8.131e-02\n",
      "ep 1 batch 380 | loss=1.8446 | alloc=9.00GB reserv=10.31GB\n",
      "[ep 1 | batch 390] loss=1.7798 |M| |W| mean=8.132e-02\n",
      "ep 1 batch 390 | loss=1.7798 | alloc=9.24GB reserv=10.55GB\n",
      "[ep 1 | batch 400] loss=1.7766 |M| |W| mean=8.133e-02\n",
      "ep 1 batch 400 | loss=1.7766 | alloc=9.48GB reserv=10.79GB\n",
      "[ep 1 | batch 410] loss=1.8024 |M| |W| mean=8.134e-02\n",
      "ep 1 batch 410 | loss=1.8024 | alloc=9.72GB reserv=11.03GB\n",
      "[ep 1 | batch 420] loss=1.6548 |M| |W| mean=8.135e-02\n",
      "ep 1 batch 420 | loss=1.6548 | alloc=9.95GB reserv=11.27GB\n",
      "[ep 1 | batch 430] loss=1.5649 |M| |W| mean=8.136e-02\n",
      "ep 1 batch 430 | loss=1.5649 | alloc=10.19GB reserv=11.51GB\n",
      "[ep 1 | batch 440] loss=1.5703 |M| |W| mean=8.137e-02\n",
      "ep 1 batch 440 | loss=1.5703 | alloc=10.43GB reserv=11.75GB\n",
      "[ep 1 | batch 450] loss=1.6386 |M| |W| mean=8.137e-02\n",
      "ep 1 batch 450 | loss=1.6386 | alloc=10.67GB reserv=11.99GB\n",
      "[ep 1 | batch 460] loss=1.7084 |M| |W| mean=8.138e-02\n",
      "ep 1 batch 460 | loss=1.7084 | alloc=10.91GB reserv=12.23GB\n",
      "[ep 1 | batch 470] loss=1.5657 |M| |W| mean=8.137e-02\n",
      "ep 1 batch 470 | loss=1.5657 | alloc=11.14GB reserv=12.47GB\n",
      "[ep 1 | batch 480] loss=1.7876 |M| |W| mean=8.138e-02\n",
      "ep 1 batch 480 | loss=1.7876 | alloc=11.38GB reserv=12.71GB\n",
      "[ep 1 | batch 490] loss=1.5883 |M| |W| mean=8.138e-02\n",
      "ep 1 batch 490 | loss=1.5883 | alloc=11.62GB reserv=12.95GB\n",
      "[ep 1 | batch 500] loss=1.6142 |M| |W| mean=8.138e-02\n",
      "ep 1 batch 500 | loss=1.6142 | alloc=11.86GB reserv=13.19GB\n",
      "[ep 1 | batch 510] loss=1.6597 |M| |W| mean=8.139e-02\n",
      "ep 1 batch 510 | loss=1.6597 | alloc=12.10GB reserv=13.43GB\n",
      "[ep 1 | batch 520] loss=1.6173 |M| |W| mean=8.140e-02\n",
      "ep 1 batch 520 | loss=1.6173 | alloc=12.34GB reserv=13.67GB\n",
      "[ep 1 | batch 530] loss=1.5003 |M| |W| mean=8.142e-02\n",
      "ep 1 batch 530 | loss=1.5003 | alloc=12.57GB reserv=13.91GB\n",
      "[ep 1 | batch 540] loss=1.4718 |M| |W| mean=8.143e-02\n",
      "ep 1 batch 540 | loss=1.4718 | alloc=12.81GB reserv=14.15GB\n",
      "[ep 1 | batch 550] loss=1.6995 |M| |W| mean=8.143e-02\n",
      "ep 1 batch 550 | loss=1.6995 | alloc=13.05GB reserv=14.39GB\n",
      "[ep 1 | batch 560] loss=1.6192 |M| |W| mean=8.144e-02\n",
      "ep 1 batch 560 | loss=1.6192 | alloc=13.29GB reserv=14.63GB\n",
      "[ep 1 | batch 570] loss=1.6438 |M| |W| mean=8.145e-02\n",
      "ep 1 batch 570 | loss=1.6438 | alloc=13.53GB reserv=14.88GB\n",
      "[ep 1 | batch 580] loss=1.7011 |M| |W| mean=8.147e-02\n",
      "ep 1 batch 580 | loss=1.7011 | alloc=13.76GB reserv=15.12GB\n",
      "[ep 1 | batch 590] loss=1.5535 |M| |W| mean=8.148e-02\n",
      "ep 1 batch 590 | loss=1.5535 | alloc=14.00GB reserv=15.36GB\n",
      "[ep 1 | batch 600] loss=1.6166 |M| |W| mean=8.147e-02\n",
      "ep 1 batch 600 | loss=1.6166 | alloc=14.24GB reserv=15.60GB\n",
      "[ep 1 | batch 610] loss=1.4303 |M| |W| mean=8.148e-02\n",
      "ep 1 batch 610 | loss=1.4303 | alloc=14.48GB reserv=15.84GB\n",
      "[ep 1 | batch 620] loss=1.4214 |M| |W| mean=8.148e-02\n",
      "ep 1 batch 620 | loss=1.4214 | alloc=14.72GB reserv=16.08GB\n",
      "[ep 1 | batch 630] loss=1.5797 |M| |W| mean=8.150e-02\n",
      "ep 1 batch 630 | loss=1.5797 | alloc=14.95GB reserv=16.32GB\n",
      "[ep 1 | batch 640] loss=1.5237 |M| |W| mean=8.151e-02\n",
      "ep 1 batch 640 | loss=1.5237 | alloc=15.19GB reserv=16.56GB\n",
      "[ep 1 | batch 650] loss=1.6191 |M| |W| mean=8.151e-02\n",
      "ep 1 batch 650 | loss=1.6191 | alloc=15.43GB reserv=16.80GB\n",
      "[ep 1 | batch 660] loss=1.4992 |M| |W| mean=8.149e-02\n",
      "ep 1 batch 660 | loss=1.4992 | alloc=15.67GB reserv=17.04GB\n",
      "[ep 1 | batch 670] loss=1.5274 |M| |W| mean=8.149e-02\n",
      "ep 1 batch 670 | loss=1.5274 | alloc=15.91GB reserv=17.28GB\n",
      "[ep 1 | batch 680] loss=1.4685 |M| |W| mean=8.153e-02\n",
      "ep 1 batch 680 | loss=1.4685 | alloc=16.15GB reserv=17.52GB\n",
      "[ep 1 | batch 690] loss=1.6185 |M| |W| mean=8.154e-02\n",
      "ep 1 batch 690 | loss=1.6185 | alloc=16.38GB reserv=17.76GB\n",
      "[ep 1 | batch 700] loss=1.3948 |M| |W| mean=8.154e-02\n",
      "ep 1 batch 700 | loss=1.3948 | alloc=16.62GB reserv=18.00GB\n",
      "[ep 1 | batch 710] loss=1.3753 |M| |W| mean=8.154e-02\n",
      "ep 1 batch 710 | loss=1.3753 | alloc=16.86GB reserv=18.24GB\n",
      "[ep 1 | batch 720] loss=1.5673 |M| |W| mean=8.156e-02\n",
      "ep 1 batch 720 | loss=1.5673 | alloc=17.10GB reserv=18.48GB\n",
      "[ep 1 | batch 730] loss=1.3759 |M| |W| mean=8.155e-02\n",
      "ep 1 batch 730 | loss=1.3759 | alloc=17.34GB reserv=18.72GB\n",
      "[ep 1 | batch 740] loss=1.4162 |M| |W| mean=8.155e-02\n",
      "ep 1 batch 740 | loss=1.4162 | alloc=17.57GB reserv=18.96GB\n",
      "[ep 1 | batch 750] loss=1.5350 |M| |W| mean=8.157e-02\n",
      "ep 1 batch 750 | loss=1.5350 | alloc=17.81GB reserv=19.20GB\n",
      "[ep 1 | batch 760] loss=1.7272 |M| |W| mean=8.161e-02\n",
      "ep 1 batch 760 | loss=1.7272 | alloc=18.05GB reserv=19.44GB\n",
      "[ep 1 | batch 770] loss=1.5937 |M| |W| mean=8.161e-02\n",
      "ep 1 batch 770 | loss=1.5937 | alloc=18.29GB reserv=19.68GB\n",
      "[ep 1 | batch 780] loss=1.6551 |M| |W| mean=8.161e-02\n",
      "ep 1 batch 780 | loss=1.6551 | alloc=18.53GB reserv=19.92GB\n",
      "[ep 1 | batch 790] loss=1.7087 |M| |W| mean=8.161e-02\n",
      "ep 1 batch 790 | loss=1.7087 | alloc=18.77GB reserv=20.16GB\n",
      "[ep 1 | batch 800] loss=1.7082 |M| |W| mean=8.161e-02\n",
      "ep 1 batch 800 | loss=1.7082 | alloc=19.00GB reserv=20.40GB\n",
      "[ep 1 | batch 810] loss=1.4830 |M| |W| mean=8.162e-02\n",
      "ep 1 batch 810 | loss=1.4830 | alloc=19.24GB reserv=20.64GB\n",
      "[ep 1 | batch 820] loss=1.7528 |M| |W| mean=8.163e-02\n",
      "ep 1 batch 820 | loss=1.7528 | alloc=19.48GB reserv=20.88GB\n",
      "[ep 1 | batch 830] loss=1.6935 |M| |W| mean=8.165e-02\n",
      "ep 1 batch 830 | loss=1.6935 | alloc=19.72GB reserv=21.12GB\n",
      "[ep 1 | batch 840] loss=1.5498 |M| |W| mean=8.166e-02\n",
      "ep 1 batch 840 | loss=1.5498 | alloc=19.96GB reserv=21.36GB\n",
      "[ep 1 | batch 850] loss=1.6450 |M| |W| mean=8.166e-02\n",
      "ep 1 batch 850 | loss=1.6450 | alloc=20.19GB reserv=21.60GB\n",
      "[ep 1 | batch 860] loss=1.5820 |M| |W| mean=8.166e-02\n",
      "ep 1 batch 860 | loss=1.5820 | alloc=20.43GB reserv=21.84GB\n",
      "[ep 1 | batch 870] loss=1.4938 |M| |W| mean=8.165e-02\n",
      "ep 1 batch 870 | loss=1.4938 | alloc=20.67GB reserv=22.08GB\n",
      "[ep 1 | batch 880] loss=1.3937 |M| |W| mean=8.164e-02\n",
      "ep 1 batch 880 | loss=1.3937 | alloc=20.91GB reserv=22.32GB\n",
      "[ep 1 | batch 890] loss=1.7200 |M| |W| mean=8.162e-02\n",
      "ep 1 batch 890 | loss=1.7200 | alloc=21.15GB reserv=22.56GB\n",
      "[ep 1 | batch 900] loss=1.4863 |M| |W| mean=8.164e-02\n",
      "ep 1 batch 900 | loss=1.4863 | alloc=21.38GB reserv=22.80GB\n",
      "[ep 1 | batch 910] loss=1.9303 |M| |W| mean=8.165e-02\n",
      "ep 1 batch 910 | loss=1.9303 | alloc=21.62GB reserv=23.04GB\n",
      "[ep 1 | batch 920] loss=1.7070 |M| |W| mean=8.192e-02\n",
      "ep 1 batch 920 | loss=1.7070 | alloc=21.86GB reserv=23.29GB\n",
      "[ep 1 | batch 930] loss=1.9647 |M| |W| mean=8.219e-02\n",
      "ep 1 batch 930 | loss=1.9647 | alloc=22.10GB reserv=23.53GB\n",
      "Epoch 01/20 | lr=1.00e-03 | loss=1.8103 | test_acc=27.93% | dt=2366.2s\n",
      "[ep 2 | batch 0] loss=1.8704 |M| |W| mean=8.232e-02\n",
      "ep 2 batch 0 | loss=1.8704 | alloc=22.28GB reserv=24.44GB\n",
      "[ep 2 | batch 10] loss=1.7630 |M| |W| mean=8.239e-02\n",
      "ep 2 batch 10 | loss=1.7630 | alloc=22.51GB reserv=24.48GB\n",
      "[ep 2 | batch 20] loss=1.8517 |M| |W| mean=8.243e-02\n",
      "ep 2 batch 20 | loss=1.8517 | alloc=22.75GB reserv=24.61GB\n",
      "[ep 2 | batch 30] loss=1.7615 |M| |W| mean=8.246e-02\n",
      "ep 2 batch 30 | loss=1.7615 | alloc=22.99GB reserv=24.73GB\n",
      "[ep 2 | batch 40] loss=1.7386 |M| |W| mean=8.247e-02\n",
      "ep 2 batch 40 | loss=1.7386 | alloc=23.23GB reserv=24.85GB\n",
      "[ep 2 | batch 50] loss=1.6946 |M| |W| mean=8.249e-02\n",
      "ep 2 batch 50 | loss=1.6946 | alloc=23.46GB reserv=24.98GB\n",
      "[ep 2 | batch 60] loss=1.7483 |M| |W| mean=8.251e-02\n",
      "ep 2 batch 60 | loss=1.7483 | alloc=23.70GB reserv=25.12GB\n",
      "[ep 2 | batch 70] loss=1.7899 |M| |W| mean=8.254e-02\n",
      "ep 2 batch 70 | loss=1.7899 | alloc=23.94GB reserv=25.36GB\n",
      "[ep 2 | batch 80] loss=1.6734 |M| |W| mean=8.254e-02\n",
      "ep 2 batch 80 | loss=1.6734 | alloc=24.17GB reserv=25.60GB\n",
      "[ep 2 | batch 90] loss=1.6026 |M| |W| mean=8.255e-02\n",
      "ep 2 batch 90 | loss=1.6026 | alloc=24.41GB reserv=25.84GB\n",
      "[ep 2 | batch 100] loss=1.5929 |M| |W| mean=8.255e-02\n",
      "ep 2 batch 100 | loss=1.5929 | alloc=24.65GB reserv=26.08GB\n",
      "[ep 2 | batch 110] loss=1.7111 |M| |W| mean=8.256e-02\n",
      "ep 2 batch 110 | loss=1.7111 | alloc=24.89GB reserv=26.32GB\n",
      "[ep 2 | batch 120] loss=1.5934 |M| |W| mean=8.257e-02\n",
      "ep 2 batch 120 | loss=1.5934 | alloc=25.13GB reserv=26.56GB\n",
      "[ep 2 | batch 130] loss=1.6082 |M| |W| mean=8.256e-02\n",
      "ep 2 batch 130 | loss=1.6082 | alloc=25.36GB reserv=26.80GB\n",
      "[ep 2 | batch 140] loss=1.7103 |M| |W| mean=8.257e-02\n",
      "ep 2 batch 140 | loss=1.7103 | alloc=25.60GB reserv=27.04GB\n",
      "[ep 2 | batch 150] loss=1.4620 |M| |W| mean=8.257e-02\n",
      "ep 2 batch 150 | loss=1.4620 | alloc=25.84GB reserv=27.28GB\n",
      "[ep 2 | batch 160] loss=1.6553 |M| |W| mean=8.259e-02\n",
      "ep 2 batch 160 | loss=1.6553 | alloc=26.08GB reserv=27.52GB\n",
      "[ep 2 | batch 170] loss=1.5080 |M| |W| mean=8.259e-02\n",
      "ep 2 batch 170 | loss=1.5080 | alloc=26.32GB reserv=27.76GB\n",
      "[ep 2 | batch 180] loss=1.5458 |M| |W| mean=8.259e-02\n",
      "ep 2 batch 180 | loss=1.5458 | alloc=26.56GB reserv=28.00GB\n",
      "[ep 2 | batch 190] loss=1.6319 |M| |W| mean=8.259e-02\n",
      "ep 2 batch 190 | loss=1.6319 | alloc=26.79GB reserv=28.24GB\n",
      "[ep 2 | batch 200] loss=1.6194 |M| |W| mean=8.261e-02\n",
      "ep 2 batch 200 | loss=1.6194 | alloc=27.03GB reserv=28.48GB\n",
      "[ep 2 | batch 210] loss=1.6089 |M| |W| mean=8.260e-02\n",
      "ep 2 batch 210 | loss=1.6089 | alloc=27.27GB reserv=28.72GB\n",
      "[ep 2 | batch 220] loss=1.6503 |M| |W| mean=8.262e-02\n",
      "ep 2 batch 220 | loss=1.6503 | alloc=27.51GB reserv=28.96GB\n",
      "[ep 2 | batch 230] loss=1.4380 |M| |W| mean=8.263e-02\n",
      "ep 2 batch 230 | loss=1.4380 | alloc=27.75GB reserv=29.21GB\n",
      "[ep 2 | batch 240] loss=1.6971 |M| |W| mean=8.264e-02\n",
      "ep 2 batch 240 | loss=1.6971 | alloc=27.98GB reserv=29.45GB\n",
      "[ep 2 | batch 250] loss=1.6597 |M| |W| mean=8.263e-02\n",
      "ep 2 batch 250 | loss=1.6597 | alloc=28.22GB reserv=29.69GB\n",
      "[ep 2 | batch 260] loss=1.7989 |M| |W| mean=8.263e-02\n",
      "ep 2 batch 260 | loss=1.7989 | alloc=28.46GB reserv=29.93GB\n",
      "[ep 2 | batch 270] loss=1.5503 |M| |W| mean=8.265e-02\n",
      "ep 2 batch 270 | loss=1.5503 | alloc=28.70GB reserv=30.17GB\n",
      "[ep 2 | batch 280] loss=1.5111 |M| |W| mean=8.265e-02\n",
      "ep 2 batch 280 | loss=1.5111 | alloc=28.94GB reserv=30.41GB\n",
      "[ep 2 | batch 290] loss=1.5036 |M| |W| mean=8.266e-02\n",
      "ep 2 batch 290 | loss=1.5036 | alloc=29.17GB reserv=30.65GB\n",
      "[ep 2 | batch 300] loss=1.4762 |M| |W| mean=8.268e-02\n",
      "ep 2 batch 300 | loss=1.4762 | alloc=29.41GB reserv=30.89GB\n",
      "[ep 2 | batch 310] loss=1.6540 |M| |W| mean=8.268e-02\n",
      "ep 2 batch 310 | loss=1.6540 | alloc=29.65GB reserv=31.13GB\n",
      "[ep 2 | batch 320] loss=1.6382 |M| |W| mean=8.268e-02\n",
      "ep 2 batch 320 | loss=1.6382 | alloc=29.89GB reserv=31.37GB\n",
      "[ep 2 | batch 330] loss=1.6946 |M| |W| mean=8.268e-02\n",
      "ep 2 batch 330 | loss=1.6946 | alloc=30.13GB reserv=31.61GB\n",
      "[ep 2 | batch 340] loss=1.8959 |M| |W| mean=8.269e-02\n",
      "ep 2 batch 340 | loss=1.8959 | alloc=30.37GB reserv=31.85GB\n",
      "[ep 2 | batch 350] loss=1.5819 |M| |W| mean=8.268e-02\n",
      "ep 2 batch 350 | loss=1.5819 | alloc=30.60GB reserv=32.09GB\n",
      "[ep 2 | batch 360] loss=1.4902 |M| |W| mean=8.268e-02\n",
      "ep 2 batch 360 | loss=1.4902 | alloc=30.84GB reserv=32.33GB\n",
      "[ep 2 | batch 370] loss=1.7745 |M| |W| mean=8.269e-02\n",
      "ep 2 batch 370 | loss=1.7745 | alloc=31.08GB reserv=32.57GB\n",
      "[ep 2 | batch 380] loss=1.5911 |M| |W| mean=8.271e-02\n",
      "ep 2 batch 380 | loss=1.5911 | alloc=31.32GB reserv=32.81GB\n",
      "[ep 2 | batch 390] loss=1.5350 |M| |W| mean=8.271e-02\n",
      "ep 2 batch 390 | loss=1.5350 | alloc=31.56GB reserv=33.05GB\n",
      "[ep 2 | batch 400] loss=1.6067 |M| |W| mean=8.272e-02\n",
      "ep 2 batch 400 | loss=1.6067 | alloc=31.79GB reserv=33.29GB\n",
      "[ep 2 | batch 410] loss=1.5053 |M| |W| mean=8.273e-02\n",
      "ep 2 batch 410 | loss=1.5053 | alloc=32.03GB reserv=33.54GB\n",
      "[ep 2 | batch 420] loss=1.5251 |M| |W| mean=8.272e-02\n",
      "ep 2 batch 420 | loss=1.5251 | alloc=32.27GB reserv=33.78GB\n",
      "[ep 2 | batch 430] loss=1.5789 |M| |W| mean=8.274e-02\n",
      "ep 2 batch 430 | loss=1.5789 | alloc=32.51GB reserv=34.02GB\n",
      "[ep 2 | batch 440] loss=1.6405 |M| |W| mean=8.275e-02\n",
      "ep 2 batch 440 | loss=1.6405 | alloc=32.75GB reserv=34.26GB\n",
      "[ep 2 | batch 450] loss=1.5254 |M| |W| mean=8.276e-02\n",
      "ep 2 batch 450 | loss=1.5254 | alloc=32.99GB reserv=34.50GB\n",
      "[ep 2 | batch 460] loss=1.5317 |M| |W| mean=8.277e-02\n",
      "ep 2 batch 460 | loss=1.5317 | alloc=33.22GB reserv=34.74GB\n",
      "[ep 2 | batch 470] loss=1.4692 |M| |W| mean=8.278e-02\n",
      "ep 2 batch 470 | loss=1.4692 | alloc=33.46GB reserv=34.98GB\n",
      "[ep 2 | batch 480] loss=1.4108 |M| |W| mean=8.278e-02\n",
      "ep 2 batch 480 | loss=1.4108 | alloc=33.70GB reserv=35.22GB\n",
      "[ep 2 | batch 490] loss=1.6252 |M| |W| mean=8.278e-02\n",
      "ep 2 batch 490 | loss=1.6252 | alloc=33.94GB reserv=35.46GB\n",
      "[ep 2 | batch 500] loss=1.7257 |M| |W| mean=8.280e-02\n",
      "ep 2 batch 500 | loss=1.7257 | alloc=34.18GB reserv=35.70GB\n",
      "[ep 2 | batch 510] loss=1.4653 |M| |W| mean=8.282e-02\n",
      "ep 2 batch 510 | loss=1.4653 | alloc=34.41GB reserv=35.94GB\n",
      "[ep 2 | batch 520] loss=1.3941 |M| |W| mean=8.282e-02\n",
      "ep 2 batch 520 | loss=1.3941 | alloc=34.65GB reserv=36.18GB\n",
      "[ep 2 | batch 530] loss=1.4501 |M| |W| mean=8.282e-02\n",
      "ep 2 batch 530 | loss=1.4501 | alloc=34.89GB reserv=36.42GB\n",
      "[ep 2 | batch 540] loss=1.5280 |M| |W| mean=8.287e-02\n",
      "ep 2 batch 540 | loss=1.5280 | alloc=35.13GB reserv=36.66GB\n",
      "[ep 2 | batch 550] loss=1.5397 |M| |W| mean=8.290e-02\n",
      "ep 2 batch 550 | loss=1.5397 | alloc=35.37GB reserv=36.90GB\n",
      "[ep 2 | batch 560] loss=1.4674 |M| |W| mean=8.291e-02\n",
      "ep 2 batch 560 | loss=1.4674 | alloc=35.60GB reserv=37.14GB\n",
      "[ep 2 | batch 570] loss=1.7197 |M| |W| mean=8.291e-02\n",
      "ep 2 batch 570 | loss=1.7197 | alloc=35.84GB reserv=37.38GB\n",
      "[ep 2 | batch 580] loss=1.4480 |M| |W| mean=8.291e-02\n",
      "ep 2 batch 580 | loss=1.4480 | alloc=36.08GB reserv=37.62GB\n",
      "[ep 2 | batch 590] loss=1.3319 |M| |W| mean=8.291e-02\n",
      "ep 2 batch 590 | loss=1.3319 | alloc=36.32GB reserv=37.86GB\n",
      "[ep 2 | batch 600] loss=1.7311 |M| |W| mean=8.293e-02\n",
      "ep 2 batch 600 | loss=1.7311 | alloc=36.56GB reserv=38.10GB\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 39.49 GiB total capacity; 37.86 GiB already allocated; 8.56 MiB free; 38.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\nAt:\n  /allen/programs/mindscope/workgroups/auto-model/zhixin.lu/MultiTaskMPN/mpn.py(472): update_M_matrix_functional\n  /allen/programs/mindscope/workgroups/auto-model/zhixin.lu/MultiTaskMPN/mpn.py(919): step_functional\n  /allen/programs/mindscope/workgroups/auto-model/zhixin.lu/MultiTaskMPN/mpn.py(989): chunk_fn\n  /home/zhixin.lu/.conda/envs/TorchOnGPUs/lib/python3.8/site-packages/torch/utils/checkpoint.py(371): unpack\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trained_net, params \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_sequential_mnist\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmpn_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 134\u001b[0m, in \u001b[0;36mtrain_sequential_mnist\u001b[0;34m(device, data_root, hidden_dim, batch_size, lr, epochs, mpn_depth, chunk_size, use_amp)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scaler\u001b[38;5;241m.\u001b[39mis_enabled():\n\u001b[0;32m--> 134\u001b[0m     \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mstep(opt)\n\u001b[1;32m    136\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[0;32m~/.conda/envs/TorchOnGPUs/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/TorchOnGPUs/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 39.49 GiB total capacity; 37.86 GiB already allocated; 8.56 MiB free; 38.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\nAt:\n  /allen/programs/mindscope/workgroups/auto-model/zhixin.lu/MultiTaskMPN/mpn.py(472): update_M_matrix_functional\n  /allen/programs/mindscope/workgroups/auto-model/zhixin.lu/MultiTaskMPN/mpn.py(919): step_functional\n  /allen/programs/mindscope/workgroups/auto-model/zhixin.lu/MultiTaskMPN/mpn.py(989): chunk_fn\n  /home/zhixin.lu/.conda/envs/TorchOnGPUs/lib/python3.8/site-packages/torch/utils/checkpoint.py(371): unpack\n"
     ]
    }
   ],
   "source": [
    "trained_net, params = train_sequential_mnist(\n",
    "                device=\"cuda\",\n",
    "                hidden_dim=128,\n",
    "                batch_size=64,\n",
    "                lr=5e-3,\n",
    "                epochs=20,\n",
    "                mpn_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff34612a-ae43-422b-a747-29708739eeb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf315cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27780ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa98487a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731c36a7-94af-4b00-a201-1ca08bb2822b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ckpt_path = \"./mnist/mpn_seqmnist_ckpt.pt\"\n",
    "\n",
    "checkpoint = {\n",
    "    \"model_class\": \"DeepMultiPlasticNet\",\n",
    "    \"net_params\": net_params,  # the same dict used to build the net\n",
    "    \"state_dict\": trained_net.state_dict(),\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, ckpt_path)\n",
    "print(f\"Saved checkpoint to: {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c147060-1175-4f2f-aaee-0710ead1dd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55ce6b3-f52b-4740-b034-1ea923bbaa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, W2, W3 = trained_net.mp_layer1.W.cpu().detach(), trained_net.mp_layer2.W.cpu().detach(), trained_net.mp_layer3.W.cpu().detach()\n",
    "Ws = [W1, W2, W3]\n",
    "fig, axs = plt.subplots(1,3,figsize=(4*3,4))\n",
    "for i in range(len(Ws)): \n",
    "    sns.heatmap(Ws[i], ax=axs[i], square=True, cmap=\"coolwarm\", center=0, vmax=1, vmin=-1)\n",
    "    axs[i].set_title(f\"Mean: {torch.mean(Ws[i]):1f}\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf66705-16c7-4fe4-b15a-96eb30fb2e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, W2, W3 = trained_net.mp_layer1.eta.cpu().detach(), trained_net.mp_layer2.eta.cpu().detach(), trained_net.mp_layer3.eta.cpu().detach()\n",
    "Ws = [W1, W2, W3]\n",
    "fig, axs = plt.subplots(1,3,figsize=(4*3,4))\n",
    "for i in range(len(Ws)): \n",
    "    sns.heatmap(Ws[i], ax=axs[i], square=True, cmap=\"coolwarm\", center=0, vmax=2, vmin=-2)\n",
    "    axs[i].set_title(f\"Mean: {torch.mean(Ws[i]):1f}\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133f4090-6d28-439e-ba76-e92491930111",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, W2, W3 = trained_net.mp_layer1.lam.cpu().detach(), trained_net.mp_layer2.lam.cpu().detach(), trained_net.mp_layer3.lam.cpu().detach()\n",
    "Ws = [W1, W2, W3]\n",
    "fig, axs = plt.subplots(1,3,figsize=(4*3,4))\n",
    "for i in range(len(Ws)): \n",
    "    sns.heatmap(Ws[i], ax=axs[i], square=True, cmap=\"coolwarm\", center=0.5, vmax=1, vmin=0)\n",
    "    axs[i].set_title(f\"Mean: {torch.mean(Ws[i]):1f}\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b218977-de07-4f3c-93f2-c15a063a379b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TorchOnGPUs",
   "language": "python",
   "name": "torchongpus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
