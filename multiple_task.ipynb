{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "374fb8f2-9b83-44ce-821b-8917a114c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "import copy \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# PyTorch Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Data Handling and Image Processing\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Style for Matplotlib\n",
    "import scienceplots\n",
    "plt.style.use('science')\n",
    "plt.style.use(['no-latex'])\n",
    "\n",
    "# Scientific Computing and Machine Learning\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.linalg import subspace_angles\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import networks as nets  # Contains RNNs\n",
    "import net_helpers\n",
    "import mpn_tasks\n",
    "import helper\n",
    "import mpn\n",
    "\n",
    "import scienceplots\n",
    "plt.style.use('science')\n",
    "plt.style.use(['no-latex'])\n",
    "\n",
    "# Memory Optimization\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd1556c8-a4b6-434b-a60f-37035980bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 Red, 1 blue, 2 green, 3 purple, 4 orange, 5 teal, 6 gray, 7 pink, 8 yellow\n",
    "c_vals = ['#e53e3e', '#3182ce', '#38a169', '#805ad5','#dd6b20', '#319795', '#718096', '#d53f8c', '#d69e2e',] * 10\n",
    "c_vals_l = ['#feb2b2', '#90cdf4', '#9ae6b4', '#d6bcfa', '#fbd38d', '#81e6d9', '#e2e8f0', '#fbb6ce', '#faf089',] * 10\n",
    "c_vals_d = ['#9b2c2c', '#2c5282', '#276749', '#553c9a', '#9c4221', '#285e61', '#2d3748', '#97266d', '#975a16',] * 10 \n",
    "l_vals = ['solid', 'dashed', 'dotted', 'dashdot', '-', '--', '-.', ':', (0, (3, 1, 1, 1)), (0, (5, 10))]\n",
    "markers_vals = ['o', 'v', '*', '+', '>', '1', '2', '3', '4', 's', 'p', '*', 'h', 'H', '+', 'x', 'D', 'd', '|', '_']\n",
    "linestyles = [\"-\", \"--\", \"-.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58d36397-6fe4-4124-bf57-307f571e9309",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34978bf6-67b8-41bd-a022-a7b46a320686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set seed 741\n",
      "Fixation_off: True; Task_info: True\n",
      "Rules: ['dmsgo']\n",
      "  Input size 7, Output size 3\n",
      "Using CUDA...\n"
     ]
    }
   ],
   "source": [
    "# Reload modules if changes have been made to them\n",
    "from importlib import reload\n",
    "\n",
    "reload(nets)\n",
    "reload(net_helpers)\n",
    "\n",
    "fixseed = False # randomize setting the seed may lead to not perfectly solved results\n",
    "seed = random.randint(1,1000) if not fixseed else 8 # random set the seed to test robustness by default\n",
    "print(f\"Set seed {seed}\")\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "hyp_dict['task_type'] = 'multitask' # int, NeuroGym, multitask\n",
    "hyp_dict['mode_for_all'] = \"random_batch\"\n",
    "hyp_dict['ruleset'] = 'dmsgo' # low_dim, all, test\n",
    "\n",
    "accept_rules = ('fdgo', 'fdanti', 'delaygo', 'delayanti', 'reactgo', 'reactanti', \n",
    "                'delaydm1', 'delaydm2', 'dmsgo', 'dmcgo', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm', 'dmsnogo', 'dmcnogo')\n",
    "\n",
    "\n",
    "rules_dict = \\\n",
    "    {'all' : ['fdgo', 'reactgo', 'delaygo', 'fdanti', 'reactanti', 'delayanti',\n",
    "              'dm1', 'dm2', 'contextdm1', 'contextdm2', 'multidm',\n",
    "              'delaydm1', 'delaydm2', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm',\n",
    "              'dmsgo', 'dmsnogo', 'dmcgo', 'dmcnogo'],\n",
    "     'low_dim' : ['fdgo', 'reactgo', 'delaygo', 'fdanti', 'reactanti', 'delayanti',\n",
    "                 'delaydm1', 'delaydm2', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm',\n",
    "                 'dmsgo', 'dmsnogo', 'dmcgo', 'dmcnogo'],\n",
    "     'delayfamily': ['delaygo', 'delayanti'], \n",
    "     'dmsgo': ['dmsgo'],\n",
    "     'delaydm1': ['delaydm1'], \n",
    "     'simplegofamily': ['fdgo', 'fdanti', 'reactgo', 'reactanti'],\n",
    "     'gofamily': ['fdgo', 'fdanti', 'reactgo', 'reactanti', 'delaygo', 'delayanti'],\n",
    "     'gofamily_delaydm': ['fdgo', 'fdanti', 'reactgo', 'reactanti', 'delaygo', 'delayanti', 'delaydm1', 'delaydm2'],\n",
    "     'dm_family': ['delaydm1', 'delaydm2', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm'],\n",
    "     'go_dm_family': ['fdgo', 'fdanti', 'reactgo', 'reactanti', 'delaygo', 'delayanti', \\\n",
    "                      'delaydm1', 'delaydm2', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm'],\n",
    "     'everything': ['fdgo', 'fdanti', 'reactgo', 'reactanti', 'delaygo', 'delayanti', \\\n",
    "                      'delaydm1', 'delaydm2', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm', \n",
    "                       'dmsgo', 'dmsnogo', 'dmcgo', 'dmcnogo'],\n",
    "    }\n",
    "\n",
    "rules_dict_frequency = {\n",
    "    'dmsgo': np.array([1]), \n",
    "    'delaydm1': np.array([1]), \n",
    "    'go_dm_family': np.array([1, 1, 1, 1, 1, 1, \n",
    "                              3, 3, 3, 3, 3,\n",
    "    ]), \n",
    "    'everything': np.array([1, 1, 1, 1, 1, 1, \n",
    "                              2, 2, 2, 2, 2,\n",
    "                              2, 2, 2, 2\n",
    "    ])\n",
    "}\n",
    "\n",
    "    \n",
    "\n",
    "# This can either be used to set parameters OR set parameters and train\n",
    "train = True # whether or not to train the network\n",
    "verbose = True\n",
    "hyp_dict['run_mode'] = 'minimal' # minimal, debug\n",
    "hyp_dict['chosen_network'] = \"dmpn\"\n",
    "\n",
    "# suffix for saving images\n",
    "# inputadd, Wfix, WL2, hL2\n",
    "# inputrandom, Wtrain\n",
    "# noise001\n",
    "# largeregularization\n",
    "# trainetalambda\n",
    "\n",
    "mpn_depth = 1\n",
    "n_hidden = 150\n",
    "\n",
    "hyp_dict['addon_name'] = \"inputtrain+Wtrain+yesoversample\"\n",
    "hyp_dict['addon_name'] += f\"+hidden{n_hidden}\"\n",
    "\n",
    "# for coding \n",
    "if hyp_dict['chosen_network'] in (\"gru\", \"vanilla\"):\n",
    "    mpn_depth = 1\n",
    "\n",
    "def current_basic_params():\n",
    "    task_params = {\n",
    "        'task_type': hyp_dict['task_type'],\n",
    "        'rules': rules_dict[hyp_dict['ruleset']],\n",
    "        'rules_probs': rules_dict_frequency[hyp_dict['ruleset']], \n",
    "        'dt': 40, # ms, directly influence sequence lengths,\n",
    "        'ruleset': hyp_dict['ruleset'],\n",
    "        'n_eachring': 8, # Number of distinct possible inputs on each ring\n",
    "        'in_out_mode': 'low_dim',  # high_dim or low_dim or low_dim_pos (Robert vs. Laura's paper, resp)\n",
    "        'sigma_x': 0.00, # Laura raised to 0.1 to prevent overfitting (Robert uses 0.01)\n",
    "        'mask_type': 'cost', # 'cost', None\n",
    "        'fixate_off': True, # Second fixation signal goes on when first is off\n",
    "        'task_info': True, \n",
    "        'randomize_inputs': False,\n",
    "        'n_input': 20, # Only used if inputs are randomized,\n",
    "        'modality_diff': True,\n",
    "        'label_strength': True, \n",
    "        'long_delay': 'normal' \n",
    "    }\n",
    "\n",
    "    print(f\"Fixation_off: {task_params['fixate_off']}; Task_info: {task_params['task_info']}\")\n",
    "\n",
    "    train_params = {\n",
    "        'lr': 1e-3,\n",
    "        'n_batches': 640,\n",
    "        'batch_size': 640,\n",
    "        'gradient_clip': 10,\n",
    "        'valid_n_batch': 50,\n",
    "        'n_datasets': 10, \n",
    "        'n_epochs_per_set': 200, \n",
    "        # 'weight_reg': 'L2',\n",
    "        # 'activity_reg': 'L2', \n",
    "        # 'reg_lambda': 1e-4,\n",
    "        \n",
    "        'scheduler': {\n",
    "            'type': 'ReduceLROnPlateau',  # or 'StepLR'\n",
    "            'mode': 'min',                # for ReduceLROnPlateau\n",
    "            'factor': 0.5,                # factor to reduce LR\n",
    "            'patience': 10,                # epochs to wait before reducing LR\n",
    "            'min_lr': 1e-6,\n",
    "            'step_size': 30,              # for StepLR (step every 30 datasets)\n",
    "            'gamma': 0.1                  # for StepLR (multiply LR by 0.1)\n",
    "        },\n",
    "    }\n",
    "\n",
    "    if not train: # some \n",
    "        assert train_params['n_epochs_per_set'] == 0\n",
    "\n",
    "    net_params = {\n",
    "        'net_type': hyp_dict['chosen_network'], # mpn1, dmpn, vanilla\n",
    "        'n_neurons': [1] + [n_hidden] * mpn_depth + [1],\n",
    "        'output_bias': False, # Turn off biases for easier interpretation\n",
    "        'loss_type': 'MSE', # XE, MSE\n",
    "        'activation': 'tanh', # linear, ReLU, sigmoid, tanh, tanh_re, tukey, heaviside\n",
    "        'cuda': True,\n",
    "        'monitor_freq': 100,\n",
    "        'monitor_valid_out': True, # Whether or not to save validation output throughout training\n",
    "        'output_matrix': '',# \"\" (default); \"untrained\", or \"orthogonal\"\n",
    "        'input_layer_add': True, \n",
    "        'input_layer_add_trainable': True, # revise this is effectively to [randomize_inputs], tune this\n",
    "        'input_layer_bias': False, \n",
    "        'input_layer': \"trainable\", # for RNN only\n",
    "        \n",
    "        # for one-layer MPN, GRU or Vanilla\n",
    "        'ml_params': {\n",
    "            'bias': True, # Bias of layer\n",
    "            'mp_type': 'mult',\n",
    "            'm_update_type': 'hebb_assoc', # hebb_assoc, hebb_pre\n",
    "            'eta_type': 'scalar', # scalar, pre_vector, post_vector, matrix\n",
    "            'eta_train': True,\n",
    "            # 'eta_init': 'mirror_gaussian', #0.0,\n",
    "            'lam_type': 'scalar', # scalar, pre_vector, post_vector, matrix\n",
    "            'm_time_scale': 4000, # ms, sets lambda\n",
    "            'lam_train': False,\n",
    "            'W_freeze': False, # different combination with [input_layer_add_trainable]\n",
    "        },\n",
    "\n",
    "        # Vanilla RNN params\n",
    "        'leaky': True,\n",
    "        'alpha': 0.2,\n",
    "    }\n",
    "\n",
    "    # Ensure the two options are *not* activated at the same time\n",
    "    assert not (task_params[\"randomize_inputs\"] and net_params[\"input_layer_add\"]), (\n",
    "        \"task_params['randomize_inputs'] and net_params['input_layer_add'] cannot both be True.\"\n",
    "    )\n",
    "\n",
    "    # for multiple MPN layers, assert \n",
    "    if mpn_depth > 1:\n",
    "        for mpl_idx in range(mpn_depth - 1):\n",
    "            assert f'ml_params{mpl_idx}' in net_params.keys()\n",
    "\n",
    "    # actually I don't think it is needed\n",
    "    # putting here to warn the parameter checking every time \n",
    "    # when switching network\n",
    "    if hyp_dict['chosen_network'] in (\"gru\", \"vanilla\"):\n",
    "        assert f'ml_params' in net_params.keys()\n",
    "\n",
    "    return task_params, train_params, net_params\n",
    "\n",
    "task_params, train_params, net_params = current_basic_params()\n",
    "hyp_dict['addon_name'] += f\"+batch{train_params['n_batches']}\"\n",
    "\n",
    "shift_index = 1 if not task_params['fixate_off'] else 0\n",
    "\n",
    "if hyp_dict['task_type'] in ('multitask',):\n",
    "    task_params, train_params, net_params = mpn_tasks.convert_and_init_multitask_params(\n",
    "        (task_params, train_params, net_params)\n",
    "    )\n",
    "\n",
    "    net_params['prefs'] = mpn_tasks.get_prefs(task_params['hp'])\n",
    "\n",
    "    print('Rules: {}'.format(task_params['rules']))\n",
    "    print('  Input size {}, Output size {}'.format(\n",
    "        task_params['n_input'], task_params['n_output'],\n",
    "    ))\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "if net_params['cuda']:\n",
    "    print('Using CUDA...')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('Using CPU...')\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# how many epoch each dataset will be trained on\n",
    "epoch_multiply = train_params[\"n_epochs_per_set\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a341b36-dc7a-42b0-bffb-cb17d380041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = task_params, train_params, net_params\n",
    "\n",
    "if net_params['net_type'] == 'mpn1':\n",
    "    netFunction = mpn.MultiPlasticNet\n",
    "elif net_params['net_type'] == 'dmpn':\n",
    "    netFunction = mpn.DeepMultiPlasticNet\n",
    "elif net_params['net_type'] == 'vanilla':\n",
    "    netFunction = nets.VanillaRNN\n",
    "elif net_params['net_type'] == 'gru':\n",
    "    netFunction = nets.GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07e4fe48-2af6-4741-8d42-01803ba0abf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Align ['dmsgo'] With Same Time\n",
      "dmsgo\n",
      "{'dmsgo': {'fix1': (None, array([15, 10,  5, 10, 10, 15, 10, 15,  5,  5,  5,  5, 10, 15, 15, 15, 10,\n",
      "       15, 15,  5, 15, 15,  5,  5, 15,  5, 10, 10, 10,  5,  5,  5,  5, 15,\n",
      "       10, 10, 10, 15, 15,  5,  5, 10, 10,  5, 15, 15, 15, 10,  5,  5],\n",
      "      dtype=int32)), 'stim1': (array([15, 10,  5, 10, 10, 15, 10, 15,  5,  5,  5,  5, 10, 15, 15, 15, 10,\n",
      "       15, 15,  5, 15, 15,  5,  5, 15,  5, 10, 10, 10,  5,  5,  5,  5, 15,\n",
      "       10, 10, 10, 15, 15,  5,  5, 10, 10,  5, 15, 15, 15, 10,  5,  5],\n",
      "      dtype=int32), array([25, 15, 20, 15, 25, 25, 15, 20, 20, 20, 10, 15, 25, 25, 25, 25, 15,\n",
      "       20, 20, 15, 25, 25, 15, 15, 20, 20, 15, 15, 15, 15, 15, 15, 15, 20,\n",
      "       15, 25, 15, 25, 30, 15, 10, 20, 20, 20, 20, 20, 25, 20, 15, 10],\n",
      "      dtype=int32)), 'delay1': (array([25, 15, 20, 15, 25, 25, 15, 20, 20, 20, 10, 15, 25, 25, 25, 25, 15,\n",
      "       20, 20, 15, 25, 25, 15, 15, 20, 20, 15, 15, 15, 15, 15, 15, 15, 20,\n",
      "       15, 25, 15, 25, 30, 15, 10, 20, 20, 20, 20, 20, 25, 20, 15, 10],\n",
      "      dtype=int32), array([30, 55, 60, 55, 45, 35, 20, 60, 40, 40, 50, 55, 65, 35, 30, 65, 35,\n",
      "       40, 40, 20, 65, 45, 35, 55, 30, 60, 35, 20, 25, 20, 20, 20, 25, 30,\n",
      "       55, 30, 20, 45, 40, 35, 15, 25, 30, 60, 25, 25, 35, 40, 35, 30],\n",
      "      dtype=int32)), 'go1': (array([30, 55, 60, 55, 45, 35, 20, 60, 40, 40, 50, 55, 65, 35, 30, 65, 35,\n",
      "       40, 40, 20, 65, 45, 35, 55, 30, 60, 35, 20, 25, 20, 20, 20, 25, 30,\n",
      "       55, 30, 20, 45, 40, 35, 15, 25, 30, 60, 25, 25, 35, 40, 35, 30],\n",
      "      dtype=int32), array([42, 67, 72, 67, 57, 47, 32, 72, 52, 52, 62, 67, 77, 47, 42, 77, 47,\n",
      "       52, 52, 32, 77, 57, 47, 67, 42, 72, 47, 32, 37, 32, 32, 32, 37, 42,\n",
      "       67, 42, 32, 57, 52, 47, 27, 37, 42, 72, 37, 37, 47, 52, 47, 42],\n",
      "      dtype=int32))}}\n"
     ]
    }
   ],
   "source": [
    "test_n_batch = train_params[\"valid_n_batch\"]\n",
    "color_by = \"stim\" # or \"resp\" \n",
    "\n",
    "task_random_fix = True\n",
    "if task_random_fix:\n",
    "    print(f\"Align {task_params['rules']} With Same Time\")\n",
    "\n",
    "if task_params['task_type'] in ('multitask',): # Test batch consists of all the rules\n",
    "    task_params['hp']['batch_size_train'] = test_n_batch\n",
    "    # using homogeneous cutting off if multiple tasks are presented in the pool\n",
    "    # if single task, using inhomogeneous cutoff to show diversity & robustness\n",
    "    test_mode_for_all = \"random\" if len(rules_dict[hyp_dict['ruleset']]) > 1 else \"random_batch\"\n",
    "    # ZIHAN\n",
    "    # generate test data using \"random\"\n",
    "    test_data, test_trials_extra = mpn_tasks.generate_trials_wrap(task_params, test_n_batch, \\\n",
    "                rules=task_params['rules'], mode_input=test_mode_for_all, fix=task_random_fix\n",
    "    )\n",
    "    _, test_trials, test_rule_idxs = test_trials_extra\n",
    "    task_params['dataset_name'] = 'multitask'\n",
    "\n",
    "    if task_params['in_out_mode'] in ('low_dim_pos',):\n",
    "        output_dim_labels = ('Fixate', 'Cos', '-Cos', 'Sin', '-Sin')\n",
    "    elif task_params['in_out_mode'] in ('low_dim',):\n",
    "        output_dim_labels = ('Fixate', 'Cos', 'Sin')\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def generate_response_stimulus(task_params, test_trials): \n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        labels_resp, labels_stim = [], []\n",
    "        rules_epochs = {} \n",
    "        for rule_idx, rule in enumerate(task_params['rules']):\n",
    "            print(rule)\n",
    "            if rule in accept_rules:\n",
    "                rules_epochs[rule] = test_trials[rule_idx].epochs\n",
    "                if hyp_dict['ruleset'] in ('dmsgo','dmcgo',):\n",
    "                    labels_resp.append(test_trials[rule_idx].meta['matches'])\n",
    "                    labels_stim.append(test_trials[rule_idx].meta['stim1']) \n",
    "                else:\n",
    "                    try: \n",
    "                        labels_resp.append(test_trials[rule_idx].meta['resp1'])\n",
    "                    except Exception as e:\n",
    "                        labels_resp.append(test_trials[rule_idx].meta['matches'])\n",
    "                    labels_stim.append(test_trials[rule_idx].meta['stim1']) \n",
    "    \n",
    "            else:\n",
    "                raise NotImplementedError()\n",
    "\n",
    "        print(rules_epochs)\n",
    "        \n",
    "        labels_resp = np.concatenate(labels_resp, axis=0).reshape(-1,1)\n",
    "        labels_stim = np.concatenate(labels_stim, axis=0).reshape(-1,1)\n",
    "\n",
    "        return labels_resp, labels_stim, rules_epochs\n",
    "\n",
    "    labels_resp, labels_stim, rules_epochs = generate_response_stimulus(task_params, test_trials)\n",
    "\n",
    "\n",
    "labels = labels_stim if color_by == \"stim\" else labels_resp\n",
    "    \n",
    "test_input, test_output, test_mask = test_data\n",
    "\n",
    "permutation = np.random.permutation(test_input.shape[0])\n",
    "test_input = test_input[permutation]\n",
    "test_output = test_output[permutation]\n",
    "test_mask = test_mask[permutation]\n",
    "labels = labels[permutation]\n",
    "\n",
    "test_input_np = test_input.detach().cpu().numpy()\n",
    "test_output_np = test_output.detach().cpu().numpy()\n",
    "\n",
    "# Total number of batches, might be different than test_n_batch\n",
    "# this should be the same regardless of variety of test_input\n",
    "n_batch_all = test_input_np.shape[0] \n",
    "\n",
    "def find_task(task_params, test_input_np, shift_index):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    test_task = [] # which task\n",
    "    for batch_idx in range(test_input_np.shape[0]):\n",
    "        \n",
    "        if task_params[\"randomize_inputs\"]: \n",
    "            test_input_np_ = test_input_np @ np.linalg.pinv(task_params[\"randomize_matrix\"])\n",
    "        else: \n",
    "            test_input_np_ = test_input_np\n",
    "            \n",
    "        task_label = test_input_np_[batch_idx, 0, 6-shift_index:]\n",
    "        \n",
    "        task_label = np.asarray(task_label)       \n",
    "        dist = np.abs(task_label - 1)     \n",
    "        mask = dist == dist.min() \n",
    "        \n",
    "        indices = np.where(mask)[0]\n",
    "        \n",
    "        if indices.size:                \n",
    "            task_label_index = indices[0]   \n",
    "        else:\n",
    "            raise ValueError(\"No entry close enough to 1 found\")\n",
    "            \n",
    "        test_task.append(task_label_index)\n",
    "\n",
    "    return test_task  \n",
    "\n",
    "test_task = find_task(task_params, test_input_np, shift_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78b44e9-b6e3-4c1c-a5c7-5a608a6d090b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 150, 150, 3]\n",
      "MultiPlastic Net:\n",
      "  output neurons: 3\n",
      "  Act: tanh\n",
      "\n",
      "=== Layer Universal Setup ===\n",
      "150\n",
      "  MP Layer1 parameters:\n",
      "    n_neurons - input: 150, output: 150\n",
      "    M matrix parameters:    update bounds - Max mult: 1.0, Min mult: -1.0\n",
      "      type: mult // Update - type: hebb_assoc // Act fn: linear\n",
      "      Eta: scalar (train) // Lambda: scalar (fixed) // Lambda_max: 0.99 (tau: 4.0e+03)\n",
      "Rule: dmsgo\n",
      "Rule dmsgo seq_len 82, max_seq_len 82\n",
      "inputs_all: torch.Size([640, 82, 7])\n",
      "How about Test Data at dataset 0\n",
      "Train parameters:\n",
      "  Loss: MSE // LR: 1.00e-03 // Optim: adam\n",
      "  Grad type: backprop // Gradient clip: 1.0e+01\n",
      "Weight reg: None\n",
      "Activity reg: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zihan.zhang/.conda/envs/mpn/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, LR: 1.000e-03 - train_loss:2.457e-01, rounded train_acc:0.046, valid_loss:2.345e-01, rounded valid_acc:0.060\n",
      "Iter: 100, LR: 1.000e-03 - train_loss:4.671e-02, rounded train_acc:0.560, valid_loss:4.398e-02, rounded valid_acc:0.440\n",
      "Iter: 200, LR: 1.000e-03 - train_loss:2.825e-02, rounded train_acc:0.528, valid_loss:2.978e-02, rounded valid_acc:0.465\n",
      "Rule: dmsgo\n",
      "Rule dmsgo seq_len 82, max_seq_len 82\n",
      "inputs_all: torch.Size([640, 82, 7])\n",
      "How about Test Data at dataset 1\n",
      "Iter: 300, LR: 1.000e-03 - train_loss:1.470e-02, rounded train_acc:0.578, valid_loss:2.358e-02, rounded valid_acc:0.440\n"
     ]
    }
   ],
   "source": [
    "# we use net at different training stage on the same test_input\n",
    "net, _, (counter_lst, netout_lst, db_lst, Winput_lst, Winputbias_lst,\\\n",
    "         Woutput_lst, Wall_lst, marker_lst, loss_lst, acc_lst) = net_helpers.train_network(params, device=device, verbose=verbose,\\\n",
    "                                                                                           train=train, hyp_dict=hyp_dict,\\\n",
    "                                                                                           netFunction=netFunction,\\\n",
    "                                                                                           test_input=[test_input])\n",
    "counter_lst = [x * epoch_multiply + 1 for x in counter_lst] # avoid log plot issue    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07eee03-d879-44bc-89ef-60f11bdf721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyp_dict['chosen_network'] == \"dmpn\":\n",
    "    if net_params[\"input_layer_add\"]:\n",
    "        fignorm, axsnorm = plt.subplots(1,1,figsize=(4,4))\n",
    "        axsnorm.plot(counter_lst, [np.linalg.norm(Winput_matrix) for Winput_matrix in Winput_lst], \"-o\")\n",
    "        axsnorm.set_xscale(\"log\")\n",
    "        axsnorm.set_ylabel(\"Frobenius Norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f0805a-0938-48e5-976d-3682d45b4448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check, if W_freeze, then the recorded W matrix for the modulation layer should not be changed\n",
    "if net_params[\"ml_params\"][\"W_freeze\"]: \n",
    "    assert np.allclose(Wall_lst[-1][0], Wall_lst[0][0])\n",
    "\n",
    "if net_params[\"input_layer_bias\"]: \n",
    "    assert net_params[\"input_layer_add\"] is True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47256480-5b47-496e-887e-b8de35dcc8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    fig, ax = plt.subplots(1,1,figsize=(3,3))\n",
    "    ax.plot(net.hist['iters_monitor'][1:], net.hist['train_acc'][1:], color=c_vals[0], label='Full train accuracy')\n",
    "    ax.plot(net.hist['iters_monitor'][1:], net.hist['valid_acc'][1:], color=c_vals[1], label='Full valid accuracy')\n",
    "    if net.weight_reg is not None:\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['train_loss_output_label'], color=c_vals_l[0], zorder=-1, label='Output label')\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['train_loss_reg_term'], color=c_vals_l[0], zorder=-1, label='Reg term', linestyle='dashed')\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['valid_loss_output_label'], color=c_vals_l[1], zorder=-1, label='Output valid label')\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['valid_loss_reg_term'], color=c_vals_l[1], zorder=-1, label='Reg valid term', linestyle='dashed')\n",
    "    \n",
    "    # ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.set_ylim([0.5, 1.05])\n",
    "    # ax.set_ylabel('Loss ({})'.format(net.loss_type))\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_xlabel('# Batches')\n",
    "    plt.savefig(f\"./multiple_tasks/loss_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=1000)\n",
    "    \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e544aca8-271f-49d3-9ed3-ab7023f23600",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    net_helpers.net_eta_lambda_analysis(net, net_params, hyp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c10ab17-bab6-4228-9096-af042e9ac385",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_finalstage = False\n",
    "if use_finalstage:\n",
    "    # plotting output in the validation set\n",
    "    net_out, db = net.iterate_sequence_batch(test_input, run_mode='track_states')\n",
    "    W_output = net.W_output.detach().cpu().numpy()\n",
    "\n",
    "    W_all_ = []\n",
    "    for i in range(len(net.mp_layers)):\n",
    "        W_all_.append(net.mp_layers[i].W.detach().cpu().numpy())\n",
    "    W_ = W_all_[0]\n",
    "    \n",
    "else:\n",
    "    ind = len(marker_lst)-1 \n",
    "    # ind = 0\n",
    "    network_at_percent = (marker_lst[ind]+1)/train_params['n_datasets']*100\n",
    "    print(f\"Using network at {network_at_percent}%\")\n",
    "    # by default using the first test_input \n",
    "    net_out = netout_lst[0][ind]\n",
    "    db = db_lst[0][ind]\n",
    "    W_output = Woutput_lst[ind]\n",
    "    W_ = Wall_lst[ind][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8feb8c0-4029-49b1-b6e9-a5fd01ab5efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_input_output(test_input_np, net_out, test_output_np, test_task=None, tag=\"\", batch_num=5):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    test_input_np = helper.to_ndarray(test_input_np)\n",
    "    net_out = helper.to_ndarray(net_out)\n",
    "    test_output_np = helper.to_ndarray(test_output_np)\n",
    "    \n",
    "    fig_all, axs_all = plt.subplots(batch_num,2,figsize=(4*2,batch_num*2))\n",
    "    \n",
    "    if test_output_np.shape[-1] == 1:\n",
    "        for batch_idx, ax in enumerate(axs):\n",
    "            ax.plot(net_out[batch_idx, :, 0], color=c_vals[batch_idx])\n",
    "            ax.plot(test_output_np[batch_idx, :, 0], color=c_vals_l[batch_idx])\n",
    "    \n",
    "    else:\n",
    "        for batch_idx in range(batch_num):\n",
    "            for out_idx in range(test_output_np.shape[-1]):\n",
    "                axs_all[batch_idx,0].plot(net_out[batch_idx, :, out_idx], color=c_vals[out_idx], label=out_idx)\n",
    "                axs_all[batch_idx,0].plot(test_output_np[batch_idx, :, out_idx], color=c_vals_l[out_idx], linewidth=5, alpha=0.5)\n",
    "                if test_task is not None: \n",
    "                    axs_all[batch_idx,0].set_title(f\"{task_params['rules'][test_task[batch_idx]]}\")\n",
    "                # axs_all[batch_idx,0].legend()\n",
    "    \n",
    "            input_batch = test_input_np[batch_idx,:,:]\n",
    "            if task_params[\"randomize_inputs\"]: \n",
    "                input_batch = input_batch @ np.linalg.pinv(task_params[\"randomize_matrix\"])\n",
    "            for inp_idx in range(input_batch.shape[-1]):\n",
    "                axs_all[batch_idx,1].plot(input_batch[:,inp_idx], color=c_vals[inp_idx], label=inp_idx)\n",
    "                if test_task is not None: \n",
    "                    axs_all[batch_idx,1].set_title(f\"{task_params['rules'][test_task[batch_idx]]}\")\n",
    "                # axs_all[batch_idx,1].legend()\n",
    "\n",
    "    for ax in axs_all.flatten(): \n",
    "        ax.set_ylim([-2, 2])\n",
    "    fig_all.tight_layout()\n",
    "    fig_all.savefig(f\"./multiple_tasks/lowD_{hyp_dict['ruleset']}_{hyp_dict['chosen_network']}_seed{seed}_{hyp_dict['addon_name']}_{tag}.png\", dpi=1000)\n",
    "\n",
    "plot_input_output(test_input_np, net_out, test_output_np, test_task, tag=\"\", batch_num=20 if len(rules_dict[hyp_dict['ruleset']]) > 1 else 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1f92a0-c04f-4a7a-8b08-e753efb1b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here db is selected based on learning stage selection \n",
    "\n",
    "layer_index = 0 # 1 layer MPN \n",
    "if net_params[\"input_layer_add\"]:\n",
    "    layer_index += 1 \n",
    "    \n",
    "def modulation_extraction(test_input, db, layer_index):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    max_seq_len = test_input.shape[1] \n",
    "    n_batch_all_ = test_input.shape[0]\n",
    "    \n",
    "    Ms = np.concatenate((\n",
    "        db[f'M{layer_index}'].detach().cpu().numpy().reshape(n_batch_all_, max_seq_len, -1),\n",
    "    ), axis=-1)\n",
    "\n",
    "    Ms_orig = np.concatenate((\n",
    "        db[f'M{layer_index}'].detach().cpu().numpy(),\n",
    "    ), axis=-1)\n",
    "\n",
    "    bs = np.concatenate((\n",
    "        db[f'b{layer_index}'].detach().cpu().numpy(),\n",
    "    ), axis=-1) \n",
    "\n",
    "    hs = np.concatenate((\n",
    "        db[f'hidden{layer_index}'].detach().cpu().numpy().reshape(n_batch_all_, max_seq_len, -1),\n",
    "    ), axis=-1)\n",
    "\n",
    "    return Ms, Ms_orig, hs, bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dbf3e8-cd24-4cca-9ff3-31140e83e7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rules_epochs)\n",
    "all_rules = task_params[\"rules\"]\n",
    "print(all_rules)\n",
    "test_task = np.array(test_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b170e5bb-a866-40cd-8426-f62f6f364b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ms, Ms_orig, hs, bs = modulation_extraction(test_input, db_lst[0][-1], layer_index)\n",
    "print(hs.shape)\n",
    "\n",
    "# in order of appearance\n",
    "# phase_to_indices = [\n",
    "#     (\"stim1\",  [0, 1, 4, 5, 6, 7]),\n",
    "#     (\"stim2\",  [6, 7]),\n",
    "#     (\"delay1\", [4, 5, 6, 7]),\n",
    "#     (\"delay2\", [6, 7]),\n",
    "#     (\"go1\",    [0, 1, 2, 3, 4, 5, 6, 7])\n",
    "# ]\n",
    "\n",
    "phase_to_indices = [\n",
    "    (\"stim1\",  [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]),\n",
    "    (\"stim2\",  [6, 7, 8, 9, 10]),\n",
    "    (\"delay1\", [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]),\n",
    "    (\"delay2\", [6, 7, 8, 9, 10]),\n",
    "    (\"go1\",    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])\n",
    "]\n",
    "\n",
    "tb_break = [\n",
    "    [idx, rules_epochs[all_rules[idx]][phase]]\n",
    "    for phase, indices in phase_to_indices\n",
    "    for idx in indices\n",
    "]\n",
    "\n",
    "tb_break_name = [\n",
    "    f\"{all_rules[idx]}-{phase}\"\n",
    "    for phase, indices in phase_to_indices\n",
    "    for idx in indices\n",
    "]\n",
    "\n",
    "tb_break_name = np.array(tb_break_name)\n",
    "\n",
    "fig, ax = plt.subplots(2,1,figsize=(24,8*2))\n",
    "\n",
    "cell_vars_rules = [] \n",
    "\n",
    "for el in range(len(tb_break)):\n",
    "    n_rules = len(task_params['rules'])\n",
    "    n_cells = hs.shape[-1]\n",
    "    \n",
    "    # cell_vars_rules = np.zeros((n_rules, n_cells,)) \n",
    "    \n",
    "    rule_idx, period_time = tb_break[el][0], tb_break[el][1]\n",
    "    \n",
    "    print('Rule {} (idx {})'.format(all_rules[rule_idx], rule_idx))\n",
    "    rule_hs = hs[test_task == rule_idx, period_time[0]:period_time[1], :]\n",
    "    print(np.max(np.var(rule_hs, axis=(0, 1))))\n",
    "    cell_vars_rules.append(np.var(rule_hs, axis=(0, 1))) \n",
    "\n",
    "cell_vars_rules = np.array(cell_vars_rules)\n",
    "print(cell_vars_rules.shape)\n",
    "\n",
    "cell_vars_rules_norm = np.zeros_like(cell_vars_rules)\n",
    "\n",
    "# normalize\n",
    "cell_max_var = np.max(cell_vars_rules, axis=0) # Across rules\n",
    "for period_idx in range(len(tb_break)):\n",
    "    cell_vars_rules_norm[period_idx] = np.where(\n",
    "        cell_max_var > 0., cell_vars_rules[period_idx] / cell_max_var, 0.\n",
    "    )\n",
    "\n",
    "# build rule-wise value lists and corresponding field names dynamically\n",
    "rule_vals  = [cell_vars_rules_norm[i].tolist() for i in range(n_rules)]\n",
    "rule_names = [f\"rule{i}\" for i in range(n_rules)]\n",
    "\n",
    "# structured array whose fields are rule0, rule1, …, rule{n_rules-1}\n",
    "dtype = np.dtype([(name, float) for name in rule_names])\n",
    "rules_struct = np.array(list(zip(*rule_vals)), dtype=dtype)\n",
    "\n",
    "# descending lexicographic sort across all rule columns\n",
    "sort_idxs = np.argsort(rules_struct, order=rule_names)[::-1]\n",
    "\n",
    "# sort it \n",
    "cell_vars_rules_sorted_norm = cell_vars_rules_norm[:, sort_idxs]\n",
    "\n",
    "for period_idx in range(cell_vars_rules_sorted_norm.shape[0]): \n",
    "    ax[0].plot(cell_vars_rules_sorted_norm[period_idx], color=c_vals[period_idx],\n",
    "            label=tb_break_name[period_idx])\n",
    "# ax[0].legend()\n",
    "ax[0].set_xlabel('Cell_idx')\n",
    "ax[0].set_ylabel('Normalized task variance')\n",
    "\n",
    "sns.heatmap(cell_vars_rules_sorted_norm, ax=ax[1], cmap=\"coolwarm\", cbar=True, vmin=0, vmax=1)\n",
    "ax[1].set_xlabel('Cell idx')\n",
    "ax[1].set_ylabel('Rule / Break-name', fontsize=12, labelpad=12)\n",
    "ax[1].set_yticks(np.arange(len(tb_break_name)))\n",
    "ax[1].set_yticklabels(\n",
    "    tb_break_name,\n",
    "    rotation=0,              # keep text horizontal (often easiest to read)\n",
    "    ha='right',              # right-align so long names don't collide with the heatmap\n",
    "    va='center',\n",
    "    fontsize=9,              # shrink a bit if you have many rows\n",
    ")\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"./multiple_tasks/hidden_variance_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e470b72c-1ea5-4082-9ab9-bc952dcf2981",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram, optimal_leaf_ordering\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def _hierarchical_clustering(data, k_min=3, k_max=40, metric=\"euclidean\"):\n",
    "    \"\"\"\n",
    "    Hierarchical Ward clustering on `data` (observations × features).\n",
    "    \"\"\"\n",
    "    n_obs = data.shape[0]\n",
    "\n",
    "    pairwise_dists = pdist(data, metric=metric)\n",
    "    Z = linkage(pairwise_dists, method=\"ward\")\n",
    "    Z = optimal_leaf_ordering(Z, pairwise_dists)\n",
    "\n",
    "    best_k, best_score, best_labels = None, -np.inf, None\n",
    "    k_range = range(max(k_min, 2), min(k_max, n_obs - 1) + 1)\n",
    "    D_square = squareform(pairwise_dists)\n",
    "\n",
    "    for k in k_range:\n",
    "        labels = fcluster(Z, k, criterion=\"maxclust\")\n",
    "        score = silhouette_score(D_square, labels, metric=\"precomputed\")\n",
    "        if score > best_score:\n",
    "            best_k, best_score, best_labels = k, score, labels\n",
    "\n",
    "    leaf_order = dendrogram(Z, no_plot=True)[\"leaves\"]\n",
    "\n",
    "    return dict(\n",
    "        linkage=Z,\n",
    "        leaf_order=leaf_order,\n",
    "        labels=best_labels,\n",
    "        k=best_k,\n",
    "        silhouette=best_score,\n",
    "    )\n",
    "\n",
    "\n",
    "def cluster_variance_matrix(V, k_min=3, k_max=40):\n",
    "    \"\"\"\n",
    "    Cluster a variance matrix V (shape: N features × M neurons)\n",
    "    for both rows and columns.\n",
    "    \"\"\"\n",
    "    V = np.asarray(V)\n",
    "\n",
    "    row_res = _hierarchical_clustering(V,  k_min, k_max)\n",
    "    col_res = _hierarchical_clustering(V.T, k_min, k_max)\n",
    "\n",
    "    return dict(\n",
    "        row_order=row_res[\"leaf_order\"],\n",
    "        col_order=col_res[\"leaf_order\"],\n",
    "        row_labels=row_res[\"labels\"],\n",
    "        col_labels=col_res[\"labels\"],\n",
    "        row_k=row_res[\"k\"],\n",
    "        col_k=col_res[\"k\"],\n",
    "        row_linkage=row_res[\"linkage\"],   # full row hierarchy\n",
    "        col_linkage=col_res[\"linkage\"],   # full column hierarchy\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47aacc0-d4d3-4b17-aeec-1ea0ff31c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cluster_variance_matrix(cell_vars_rules_sorted_norm)\n",
    "cell_vars_rules_sorted_norm_ordered = cell_vars_rules_sorted_norm[np.ix_(result[\"row_order\"], result[\"col_order\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9771191-9cb1-4ad2-a670-a230e5614cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1,figsize=(24,8*2))\n",
    "sns.heatmap(cell_vars_rules_sorted_norm, ax=ax[0], cmap=\"coolwarm\", cbar=True, vmin=0, vmax=1)\n",
    "sns.heatmap(cell_vars_rules_sorted_norm_ordered, ax=ax[1], cmap=\"coolwarm\", cbar=True, vmin=0, vmax=1)\n",
    "ax[0].set_ylabel('Rule / Break-name', fontsize=12, labelpad=12)\n",
    "ax[0].set_yticks(np.arange(len(tb_break_name)))\n",
    "ax[0].set_yticklabels(\n",
    "    tb_break_name,\n",
    "    rotation=0,              \n",
    "    ha='right',              \n",
    "    va='center',\n",
    "    fontsize=9,              \n",
    ")\n",
    "ax[1].set_ylabel('Rule / Break-name', fontsize=12, labelpad=12)\n",
    "ax[1].set_yticks(np.arange(len(tb_break_name)))\n",
    "ax[1].set_yticklabels(\n",
    "    tb_break_name[result[\"row_order\"]],\n",
    "    rotation=0,              \n",
    "    ha='right',              \n",
    "    va='center',\n",
    "    fontsize=9,              \n",
    ")    \n",
    "fig.savefig(f\"./multiple_tasks/hidden_variance_cluster_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b189f0-c14e-4527-af3b-1a57534fdcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"row_linkage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b8843b-e5f8-4524-9735-e05092b9f0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_break_name\n",
    "print(len(result[\"row_linkage\"]))\n",
    "print(len(result[\"col_linkage\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fc1bb2-f9df-4e97-88c0-8214c5deba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(15*2,4))\n",
    "dendrogram(result[\"row_linkage\"], ax=axs[0], labels=tb_break_name, leaf_rotation=45)\n",
    "axs[0].set_title(f\"Row hierarchy (k = {result['row_k']})\")\n",
    "\n",
    "dendrogram(result[\"col_linkage\"], ax=axs[1], labels=np.array([i for i in range(cell_vars_rules_sorted_norm_ordered.shape[1])]), leaf_rotation=45)\n",
    "axs[1].set_title(f\"Col hierarchy (k = {result['col_k']})\")\n",
    "fig.savefig(f\"./multiple_tasks/hidden_variance_hierarchy_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad6b702-b4e0-4177-9233-de7d99462256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2192dc0e-0021-446d-8e39-8da925909b68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mpn)",
   "language": "python",
   "name": "mpn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
