{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "374fb8f2-9b83-44ce-821b-8917a114c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "import copy \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# PyTorch Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Data Handling and Image Processing\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Style for Matplotlib\n",
    "import scienceplots\n",
    "plt.style.use('science')\n",
    "plt.style.use(['no-latex'])\n",
    "\n",
    "# Scientific Computing and Machine Learning\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.linalg import subspace_angles\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Custom Modules and Extensions\n",
    "sys.path.append(\"../netrep/\")\n",
    "sys.path.append(\"../svcca/\")\n",
    "import cca_core\n",
    "from netrep.metrics import LinearMetric\n",
    "import networks as nets  # Contains RNNs\n",
    "import net_helpers\n",
    "import mpn_tasks\n",
    "import helper\n",
    "import mpn\n",
    "\n",
    "import scienceplots\n",
    "plt.style.use('science')\n",
    "plt.style.use(['no-latex'])\n",
    "\n",
    "# Memory Optimization\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd1556c8-a4b6-434b-a60f-37035980bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 Red, 1 blue, 2 green, 3 purple, 4 orange, 5 teal, 6 gray, 7 pink, 8 yellow\n",
    "c_vals = ['#e53e3e', '#3182ce', '#38a169', '#805ad5','#dd6b20', '#319795', '#718096', '#d53f8c', '#d69e2e',] * 10\n",
    "c_vals_l = ['#feb2b2', '#90cdf4', '#9ae6b4', '#d6bcfa', '#fbd38d', '#81e6d9', '#e2e8f0', '#fbb6ce', '#faf089',] * 10\n",
    "c_vals_d = ['#9b2c2c', '#2c5282', '#276749', '#553c9a', '#9c4221', '#285e61', '#2d3748', '#97266d', '#975a16',] * 10 \n",
    "l_vals = ['solid', 'dashed', 'dotted', 'dashdot', '-', '--', '-.', ':', (0, (3, 1, 1, 1)), (0, (5, 10))]\n",
    "markers_vals = ['o', 'v', '*', '+', '>', '1', '2', '3', '4', 's', 'p', '*', 'h', 'H', '+', 'x', 'D', 'd', '|', '_']\n",
    "linestyles = [\"-\", \"--\", \"-.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58d36397-6fe4-4124-bf57-307f571e9309",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34978bf6-67b8-41bd-a022-a7b46a320686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set seed 343\n",
      "Fixation_off: True; Task_info: True\n",
      "Rules: ['fdgo', 'fdanti', 'reactgo', 'reactanti', 'delaygo', 'delayanti', 'delaydm1', 'delaydm2']\n",
      "  Input size 14, Output size 3\n",
      "Using CUDA...\n"
     ]
    }
   ],
   "source": [
    "# Reload modules if changes have been made to them\n",
    "from importlib import reload\n",
    "\n",
    "reload(nets)\n",
    "reload(net_helpers)\n",
    "\n",
    "fixseed = False # randomize setting the seed may lead to not perfectly solved results\n",
    "seed = random.randint(1,1000) if not fixseed else 8 # random set the seed to test robustness by default\n",
    "print(f\"Set seed {seed}\")\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "hyp_dict['task_type'] = 'multitask' # int, NeuroGym, multitask\n",
    "hyp_dict['mode_for_all'] = \"random_batch\"\n",
    "hyp_dict['ruleset'] = 'gofamily_delaydm' # low_dim, all, test\n",
    "\n",
    "accept_rules = ('fdgo', 'fdanti', 'delaygo', 'delayanti', 'reactgo', 'reactanti', \n",
    "                'delaydm1', 'delaydm2', 'dmsgo', 'dmcgo', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm')\n",
    "\n",
    "\n",
    "rules_dict = \\\n",
    "    {'all' : ['fdgo', 'reactgo', 'delaygo', 'fdanti', 'reactanti', 'delayanti',\n",
    "              'dm1', 'dm2', 'contextdm1', 'contextdm2', 'multidm',\n",
    "              'delaydm1', 'delaydm2', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm',\n",
    "              'dmsgo', 'dmsnogo', 'dmcgo', 'dmcnogo'],\n",
    "     'low_dim' : ['fdgo', 'reactgo', 'delaygo', 'fdanti', 'reactanti', 'delayanti',\n",
    "                 'delaydm1', 'delaydm2', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm',\n",
    "                 'dmsgo', 'dmsnogo', 'dmcgo', 'dmcnogo'],\n",
    "     'gofamily': ['fdgo', 'fdanti', 'reactgo', 'reactanti', 'delaygo', 'delayanti'],\n",
    "     'gofamily_delaydm': ['fdgo', 'fdanti', 'reactgo', 'reactanti', 'delaygo', 'delayanti', 'delaydm1', 'delaydm2'],\n",
    "    }\n",
    "    \n",
    "\n",
    "# This can either be used to set parameters OR set parameters and train\n",
    "train = True # whether or not to train the network\n",
    "verbose = True\n",
    "hyp_dict['run_mode'] = 'minimal' # minimal, debug\n",
    "hyp_dict['chosen_network'] = \"dmpn\"\n",
    "\n",
    "# suffix for saving images\n",
    "# inputadd, Wfix, WL2, hL2\n",
    "# inputrandom, Wtrain\n",
    "# noise001\n",
    "# largeregularization\n",
    "# trainetalambda\n",
    "\n",
    "mpn_depth = 1\n",
    "n_hidden = 100\n",
    "\n",
    "hyp_dict['addon_name'] = \"inputrandom+Wtrain+WL2+hL2\"\n",
    "hyp_dict['addon_name'] += f\"+hidden{n_hidden}\"\n",
    "\n",
    "# for coding \n",
    "if hyp_dict['chosen_network'] in (\"gru\", \"vanilla\"):\n",
    "    mpn_depth = 1\n",
    "\n",
    "def current_basic_params():\n",
    "    task_params = {\n",
    "        'task_type': hyp_dict['task_type'],\n",
    "        'rules': rules_dict[hyp_dict['ruleset']],\n",
    "        'dt': 40, # ms, directly influence sequence lengths,\n",
    "        'ruleset': hyp_dict['ruleset'],\n",
    "        'n_eachring': 8, # Number of distinct possible inputs on each ring\n",
    "        'in_out_mode': 'low_dim',  # high_dim or low_dim or low_dim_pos (Robert vs. Laura's paper, resp)\n",
    "        'sigma_x': 0.00, # Laura raised to 0.1 to prevent overfitting (Robert uses 0.01)\n",
    "        'mask_type': 'cost', # 'cost', None\n",
    "        'fixate_off': True, # Second fixation signal goes on when first is off\n",
    "        'task_info': True, \n",
    "        'randomize_inputs': False,\n",
    "        'n_input': 20, # Only used if inputs are randomized,\n",
    "        'modality_diff': True,\n",
    "        'label_strength': True, \n",
    "        'long_delay': 'normal' \n",
    "    }\n",
    "\n",
    "    print(f\"Fixation_off: {task_params['fixate_off']}; Task_info: {task_params['task_info']}\")\n",
    "\n",
    "    train_params = {\n",
    "        'lr': 1e-3,\n",
    "        'n_batches': 128,\n",
    "        'batch_size': 128,\n",
    "        'gradient_clip': 10,\n",
    "        'valid_n_batch': 30,\n",
    "        'n_datasets': 100, # Number of distinct batches\n",
    "        'n_epochs_per_set': 100, # longer/shorter training\n",
    "        'weight_reg': 'L2',\n",
    "        'activity_reg': 'L2', \n",
    "        'reg_lambda': 1e-4,\n",
    "        \n",
    "    }\n",
    "\n",
    "    if not train: # some \n",
    "        assert train_params['n_epochs_per_set'] == 0\n",
    "\n",
    "    net_params = {\n",
    "        'net_type': hyp_dict['chosen_network'], # mpn1, dmpn, vanilla\n",
    "        'n_neurons': [1] + [n_hidden] * mpn_depth + [1],\n",
    "        'output_bias': False, # Turn off biases for easier interpretation\n",
    "        'loss_type': 'MSE', # XE, MSE\n",
    "        'activation': 'tanh', # linear, ReLU, sigmoid, tanh, tanh_re, tukey, heaviside\n",
    "        'cuda': True,\n",
    "        'monitor_freq': 100,\n",
    "        'monitor_valid_out': True, # Whether or not to save validation output throughout training\n",
    "        'output_matrix': '',# \"\" (default); \"untrained\", or \"orthogonal\"\n",
    "        'input_layer_add': True, \n",
    "        'input_layer_add_trainable': False, # revise this is effectively to [randomize_inputs], tune this\n",
    "        'input_layer_bias': False, \n",
    "        'input_layer': \"trainable\", # for RNN only\n",
    "        \n",
    "        # for one-layer MPN, GRU or Vanilla\n",
    "        'ml_params': {\n",
    "            'bias': True, # Bias of layer\n",
    "            'mp_type': 'mult',\n",
    "            'm_update_type': 'hebb_assoc', # hebb_assoc, hebb_pre\n",
    "            'eta_type': 'scalar', # scalar, pre_vector, post_vector, matrix\n",
    "            'eta_train': True,\n",
    "            # 'eta_init': 'mirror_gaussian', #0.0,\n",
    "            'lam_type': 'scalar', # scalar, pre_vector, post_vector, matrix\n",
    "            'm_time_scale': 4000, # ms, sets lambda\n",
    "            'lam_train': False,\n",
    "            'W_freeze': False, # different combination with [input_layer_add_trainable]\n",
    "        },\n",
    "\n",
    "        # Vanilla RNN params\n",
    "        'leaky': True,\n",
    "        'alpha': 0.2,\n",
    "    }\n",
    "\n",
    "    # Ensure the two options are *not* activated at the same time\n",
    "    assert not (task_params[\"randomize_inputs\"] and net_params[\"input_layer_add\"]), (\n",
    "        \"task_params['randomize_inputs'] and net_params['input_layer_add'] cannot both be True.\"\n",
    "    )\n",
    "\n",
    "    # for multiple MPN layers, assert \n",
    "    if mpn_depth > 1:\n",
    "        for mpl_idx in range(mpn_depth - 1):\n",
    "            assert f'ml_params{mpl_idx}' in net_params.keys()\n",
    "\n",
    "    # actually I don't think it is needed\n",
    "    # putting here to warn the parameter checking every time \n",
    "    # when switching network\n",
    "    if hyp_dict['chosen_network'] in (\"gru\", \"vanilla\"):\n",
    "        assert f'ml_params' in net_params.keys()\n",
    "\n",
    "    return task_params, train_params, net_params\n",
    "\n",
    "task_params, train_params, net_params = current_basic_params()\n",
    "\n",
    "shift_index = 1 if not task_params['fixate_off'] else 0\n",
    "\n",
    "if hyp_dict['task_type'] in ('multitask',):\n",
    "    task_params, train_params, net_params = mpn_tasks.convert_and_init_multitask_params(\n",
    "        (task_params, train_params, net_params)\n",
    "    )\n",
    "\n",
    "    net_params['prefs'] = mpn_tasks.get_prefs(task_params['hp'])\n",
    "\n",
    "    print('Rules: {}'.format(task_params['rules']))\n",
    "    print('  Input size {}, Output size {}'.format(\n",
    "        task_params['n_input'], task_params['n_output'],\n",
    "    ))\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "if net_params['cuda']:\n",
    "    print('Using CUDA...')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('Using CPU...')\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# how many epoch each dataset will be trained on\n",
    "epoch_multiply = train_params[\"n_epochs_per_set\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a341b36-dc7a-42b0-bffb-cb17d380041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = task_params, train_params, net_params\n",
    "\n",
    "if net_params['net_type'] == 'mpn1':\n",
    "    netFunction = mpn.MultiPlasticNet\n",
    "elif net_params['net_type'] == 'dmpn':\n",
    "    netFunction = mpn.DeepMultiPlasticNet\n",
    "elif net_params['net_type'] == 'vanilla':\n",
    "    netFunction = nets.VanillaRNN\n",
    "elif net_params['net_type'] == 'gru':\n",
    "    netFunction = nets.GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07e4fe48-2af6-4741-8d42-01803ba0abf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Align ['fdgo', 'fdanti', 'reactgo', 'reactanti', 'delaygo', 'delayanti', 'delaydm1', 'delaydm2'] With Same Time\n",
      "rng reset with seed 9215\n",
      "rng reset with seed 9215\n",
      "rng reset with seed 9215\n",
      "rng reset with seed 9215\n",
      "rng reset with seed 9215\n",
      "rng reset with seed 9215\n",
      "fdgo\n",
      "fdanti\n",
      "reactgo\n",
      "reactanti\n",
      "delaygo\n",
      "delayanti\n",
      "delaydm1\n",
      "delaydm2\n",
      "{'fdgo': {'fix1': (None, 12), 'stim1': (12, 30), 'go1': (30, 42)}, 'fdanti': {'fix1': (None, 12), 'stim1': (12, 30), 'go1': (30, 42)}, 'reactgo': {'fix1': (None, 48), 'go1': (48, 60)}, 'reactanti': {'fix1': (None, 48), 'go1': (48, 60)}, 'delaygo': {'fix1': (None, 17), 'stim1': (17, 27), 'delay1': (27, 67), 'go1': (67, 79)}, 'delayanti': {'fix1': (None, 17), 'stim1': (17, 27), 'delay1': (27, 67), 'go1': (67, 79)}, 'delaydm1': {'fix1': (None, 5), 'stim1': (5, 15), 'delay1': (15, 35), 'stim2': (35, 50), 'delay2': (50, 56), 'go1': (56, 68)}, 'delaydm2': {'fix1': (None, 10), 'stim1': (10, 20), 'delay1': (20, 30), 'stim2': (30, 40), 'delay2': (40, 43), 'go1': (43, 55)}}\n"
     ]
    }
   ],
   "source": [
    "test_n_batch = train_params[\"valid_n_batch\"]\n",
    "color_by = \"stim\" # or \"resp\" \n",
    "\n",
    "task_random_fix = True\n",
    "if task_random_fix:\n",
    "    print(f\"Align {task_params['rules']} With Same Time\")\n",
    "\n",
    "if task_params['task_type'] in ('multitask',): # Test batch consists of all the rules\n",
    "    task_params['hp']['batch_size_train'] = test_n_batch\n",
    "    # using homogeneous cutting off\n",
    "    test_mode_for_all = \"random\"\n",
    "    # ZIHAN\n",
    "    # generate test data using \"random\"\n",
    "    test_data, test_trials_extra = mpn_tasks.generate_trials_wrap(task_params, test_n_batch, \\\n",
    "                rules=task_params['rules'], mode_input=test_mode_for_all, fix=task_random_fix\n",
    "    )\n",
    "    _, test_trials, test_rule_idxs = test_trials_extra\n",
    "    task_params['dataset_name'] = 'multitask'\n",
    "\n",
    "    if task_params['in_out_mode'] in ('low_dim_pos',):\n",
    "        output_dim_labels = ('Fixate', 'Cos', '-Cos', 'Sin', '-Sin')\n",
    "    elif task_params['in_out_mode'] in ('low_dim',):\n",
    "        output_dim_labels = ('Fixate', 'Cos', 'Sin')\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def generate_response_stimulus(task_params, test_trials): \n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        labels_resp, labels_stim = [], []\n",
    "        rules_epochs = {} \n",
    "        for rule_idx, rule in enumerate(task_params['rules']):\n",
    "            print(rule)\n",
    "            if rule in accept_rules:\n",
    "                rules_epochs[rule] = test_trials[rule_idx].epochs\n",
    "                if hyp_dict['ruleset'] in ('dmsgo', 'dmcgo'):\n",
    "                    labels.append(test_trials[rule_idx].meta['matches'])\n",
    "                else:\n",
    "                    labels_resp.append(test_trials[rule_idx].meta['resp1'])\n",
    "                    labels_stim.append(test_trials[rule_idx].meta['stim1']) \n",
    "    \n",
    "            else:\n",
    "                raise NotImplementedError()\n",
    "\n",
    "        print(rules_epochs)\n",
    "        \n",
    "        labels_resp = np.concatenate(labels_resp, axis=0).reshape(-1,1)\n",
    "        labels_stim = np.concatenate(labels_stim, axis=0).reshape(-1,1)\n",
    "\n",
    "        return labels_resp, labels_stim, rules_epochs\n",
    "\n",
    "    labels_resp, labels_stim, rules_epochs = generate_response_stimulus(task_params, test_trials)\n",
    "\n",
    "\n",
    "labels = labels_stim if color_by == \"stim\" else labels_resp\n",
    "    \n",
    "test_input, test_output, test_mask = test_data\n",
    "\n",
    "permutation = np.random.permutation(test_input.shape[0])\n",
    "test_input = test_input[permutation]\n",
    "test_output = test_output[permutation]\n",
    "test_mask = test_mask[permutation]\n",
    "labels = labels[permutation]\n",
    "\n",
    "test_input_np = test_input.detach().cpu().numpy()\n",
    "test_output_np = test_output.detach().cpu().numpy()\n",
    "\n",
    "# Total number of batches, might be different than test_n_batch\n",
    "# this should be the same regardless of variety of test_input\n",
    "n_batch_all = test_input_np.shape[0] \n",
    "\n",
    "def find_task(task_params, test_input_np, shift_index):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    test_task = [] # which task\n",
    "    for batch_idx in range(test_input_np.shape[0]):\n",
    "        \n",
    "        if task_params[\"randomize_inputs\"]: \n",
    "            test_input_np_ = test_input_np @ np.linalg.pinv(task_params[\"randomize_matrix\"])\n",
    "        else: \n",
    "            test_input_np_ = test_input_np\n",
    "            \n",
    "        task_label = test_input_np_[batch_idx, 0, 6-shift_index:]\n",
    "        \n",
    "        task_label = np.asarray(task_label)       \n",
    "        dist = np.abs(task_label - 1)     \n",
    "        mask = dist == dist.min() \n",
    "        \n",
    "        indices = np.where(mask)[0]\n",
    "        \n",
    "        if indices.size:                \n",
    "            task_label_index = indices[0]   \n",
    "        else:\n",
    "            raise ValueError(\"No entry close enough to 1 found\")\n",
    "            \n",
    "        test_task.append(task_label_index)\n",
    "\n",
    "    return test_task  \n",
    "\n",
    "test_task = find_task(task_params, test_input_np, shift_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e78b44e9-b6e3-4c1c-a5c7-5a608a6d090b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 100, 100, 3]\n",
      "MultiPlastic Net:\n",
      "  output neurons: 3\n",
      "  Act: tanh\n",
      "\n",
      "Input Layer Frozen\n",
      "=== Layer Universal Setup ===\n",
      "  MP Layer1 parameters:\n",
      "    n_neurons - input: 100, output: 100\n",
      "    M matrix parameters:    update bounds - Max mult: 1.0, Min mult: -1.0\n",
      "      type: mult // Update - type: hebb_assoc // Act fn: linear\n",
      "      Eta: scalar (train) // Lambda: scalar (fixed) // Lambda_max: 0.99 (tau: 4.0e+03)\n",
      "How about Test Data at dataset 0\n",
      "Train parameters:\n",
      "  Loss: MSE // LR: 1.00e-03 // Optim: adam\n",
      "  Grad type: backprop // Gradient clip: 1.0e+01\n",
      "Weight reg: L2, coef: 1.0e-04\n",
      "Activity reg: L2, coef: 1.0e-04\n",
      "Iter: 0, LR: 1.000e-03 - train_loss:4.326e-01, rounded train_acc:0.133, valid_loss:4.540e-01, rounded valid_acc:0.111\n",
      "Iter: 100, LR: 1.000e-03 - train_loss:1.473e-02, rounded train_acc:1.000, valid_loss:3.946e-01, rounded valid_acc:0.225\n",
      "How about Test Data at dataset 1\n",
      "Iter: 200, LR: 1.000e-03 - train_loss:1.226e-01, rounded train_acc:0.466, valid_loss:3.648e-01, rounded valid_acc:0.210\n",
      "Iter: 300, LR: 1.000e-03 - train_loss:1.026e-02, rounded train_acc:1.000, valid_loss:3.354e-01, rounded valid_acc:0.273\n",
      "Iter: 400, LR: 1.000e-03 - train_loss:2.611e-02, rounded train_acc:1.000, valid_loss:3.724e-01, rounded valid_acc:0.281\n",
      "How about Test Data at dataset 4\n",
      "Iter: 500, LR: 1.000e-03 - train_loss:2.762e-02, rounded train_acc:1.000, valid_loss:3.379e-01, rounded valid_acc:0.296\n",
      "Iter: 600, LR: 1.000e-03 - train_loss:2.942e-02, rounded train_acc:0.944, valid_loss:2.538e-01, rounded valid_acc:0.313\n",
      "Iter: 700, LR: 1.000e-03 - train_loss:1.531e-02, rounded train_acc:1.000, valid_loss:2.678e-01, rounded valid_acc:0.311\n",
      "Iter: 800, LR: 1.000e-03 - train_loss:1.372e-02, rounded train_acc:1.000, valid_loss:2.855e-01, rounded valid_acc:0.325\n",
      "Iter: 900, LR: 1.000e-03 - train_loss:2.334e-02, rounded train_acc:0.960, valid_loss:2.325e-01, rounded valid_acc:0.358\n",
      "Iter: 1000, LR: 1.000e-03 - train_loss:1.029e-02, rounded train_acc:1.000, valid_loss:1.897e-01, rounded valid_acc:0.427\n",
      "Iter: 1100, LR: 1.000e-03 - train_loss:1.475e-01, rounded train_acc:0.417, valid_loss:1.759e-01, rounded valid_acc:0.457\n",
      "Iter: 1200, LR: 1.000e-03 - train_loss:2.153e-02, rounded train_acc:0.982, valid_loss:1.772e-01, rounded valid_acc:0.498\n",
      "Iter: 1300, LR: 1.000e-03 - train_loss:1.707e-02, rounded train_acc:0.990, valid_loss:1.678e-01, rounded valid_acc:0.495\n",
      "Iter: 1400, LR: 1.000e-03 - train_loss:1.950e-02, rounded train_acc:0.989, valid_loss:1.895e-01, rounded valid_acc:0.491\n",
      "Iter: 1500, LR: 1.000e-03 - train_loss:1.633e-02, rounded train_acc:0.990, valid_loss:1.563e-01, rounded valid_acc:0.565\n",
      "Iter: 1600, LR: 1.000e-03 - train_loss:1.909e-02, rounded train_acc:1.000, valid_loss:1.737e-01, rounded valid_acc:0.470\n",
      "How about Test Data at dataset 16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# we use net at different training stage on the same test_input\u001b[39;00m\n\u001b[1;32m      2\u001b[0m net, _, (counter_lst, netout_lst, db_lst, Winput_lst, Winputbias_lst,\\\n\u001b[0;32m----> 3\u001b[0m          Woutput_lst, Wall_lst, marker_lst, loss_lst, acc_lst) \u001b[38;5;241m=\u001b[39m \u001b[43mnet_helpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                                                                           \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyp_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyp_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                                                                           \u001b[49m\u001b[43mnetFunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetFunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                                                                           \u001b[49m\u001b[43mtest_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m counter_lst \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;241m*\u001b[39m epoch_multiply \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m counter_lst]\n",
      "File \u001b[0;32m/mmfs1/gscratch/amath/zihan-zhang/mpn/net_helpers.py:121\u001b[0m, in \u001b[0;36mtrain_network\u001b[0;34m(params, net, device, verbose, train, hyp_dict, netFunction, test_input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     Wall_lst\u001b[38;5;241m.\u001b[39mappend(W_all_)\n\u001b[1;32m    119\u001b[0m     marker_lst\u001b[38;5;241m.\u001b[39mappend(dataset_idx)\n\u001b[0;32m--> 121\u001b[0m _, monitor_loss, monitor_acc \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_trails\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_trails\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_trails\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_thresh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyp_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrun_mode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (is_power_of_4_or_zero(dataset_idx) \u001b[38;5;129;01mor\u001b[39;00m dataset_idx \u001b[38;5;241m==\u001b[39m train_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_datasets\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;66;03m# print(f\"monitor_loss: {monitor_loss}\")\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     loss_lst\u001b[38;5;241m.\u001b[39mappend(monitor_loss)\n",
      "File \u001b[0;32m/mmfs1/gscratch/amath/zihan-zhang/mpn/net_helpers.py:658\u001b[0m, in \u001b[0;36mBaseNetwork.fit\u001b[0;34m(self, train_params, train_data, train_trails, valid_batch, valid_trails, new_thresh, run_mode)\u001b[0m\n\u001b[1;32m    655\u001b[0m \t\t\u001b[38;5;28mprint\u001b[39m(init_string) \n\u001b[1;32m    657\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain() \u001b[38;5;66;03m# put in train mode (doesn't really do anything unless we are using dropout/batch norm)\u001b[39;00m\n\u001b[0;32m--> 658\u001b[0m db, monitor_loss, monitor_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_trails\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_trails\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_trails\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m\t\t\t  \u001b[49m\u001b[43mnew_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_thresh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval() \u001b[38;5;66;03m# return to eval mode\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m db, monitor_loss, monitor_acc\n",
      "File \u001b[0;32m/mmfs1/gscratch/amath/zihan-zhang/mpn/net_helpers.py:768\u001b[0m, in \u001b[0;36mBaseNetwork.train_epochs\u001b[0;34m(self, train_params, train_data, train_trails, valid_batch, valid_trails, new_thresh, monitor, run_mode)\u001b[0m\n\u001b[1;32m    765\u001b[0m \tt0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;66;03m# Backward start\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_type \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackprop\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_debug\u001b[39m\u001b[38;5;124m'\u001b[39m,): \u001b[38;5;66;03m# Standard supervised training procedures to compute gradient\u001b[39;00m\n\u001b[0;32m--> 768\u001b[0m \t\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_clip \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    770\u001b[0m \t\ttorch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_clip)\n",
      "File \u001b[0;32m/gscratch/deepthought/zihan/miniconda3/envs/playground/lib/python3.8/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gscratch/deepthought/zihan/miniconda3/envs/playground/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# we use net at different training stage on the same test_input\n",
    "net, _, (counter_lst, netout_lst, db_lst, Winput_lst, Winputbias_lst,\\\n",
    "         Woutput_lst, Wall_lst, marker_lst, loss_lst, acc_lst) = net_helpers.train_network(params, device=device, verbose=verbose,\\\n",
    "                                                                                           train=train, hyp_dict=hyp_dict,\\\n",
    "                                                                                           netFunction=netFunction,\\\n",
    "                                                                                           test_input=[test_input])\n",
    "counter_lst = [x * epoch_multiply + 1 for x in counter_lst] # avoid log plot issue    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07eee03-d879-44bc-89ef-60f11bdf721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyp_dict['chosen_network'] == \"dmpn\":\n",
    "    if net_params[\"input_layer_add\"]:\n",
    "        fignorm, axsnorm = plt.subplots(1,1,figsize=(4,4))\n",
    "        axsnorm.plot(counter_lst, [np.linalg.norm(Winput_matrix) for Winput_matrix in Winput_lst], \"-o\")\n",
    "        axsnorm.set_xscale(\"log\")\n",
    "        axsnorm.set_ylabel(\"Frobenius Norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f0805a-0938-48e5-976d-3682d45b4448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check, if W_freeze, then the recorded W matrix for the modulation layer should not be changed\n",
    "if net_params[\"ml_params\"][\"W_freeze\"]: \n",
    "    assert np.allclose(Wall_lst[-1][0], Wall_lst[0][0])\n",
    "\n",
    "if net_params[\"input_layer_bias\"]: \n",
    "    assert net_params[\"input_layer_add\"] is True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47256480-5b47-496e-887e-b8de35dcc8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    fig, ax = plt.subplots(1,1,figsize=(3,3))\n",
    "    ax.plot(net.hist['iters_monitor'][1:], net.hist['train_acc'][1:], color=c_vals[0], label='Full train accuracy')\n",
    "    ax.plot(net.hist['iters_monitor'][1:], net.hist['valid_acc'][1:], color=c_vals[1], label='Full valid accuracy')\n",
    "    if net.weight_reg is not None:\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['train_loss_output_label'], color=c_vals_l[0], zorder=-1, label='Output label')\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['train_loss_reg_term'], color=c_vals_l[0], zorder=-1, label='Reg term', linestyle='dashed')\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['valid_loss_output_label'], color=c_vals_l[1], zorder=-1, label='Output valid label')\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['valid_loss_reg_term'], color=c_vals_l[1], zorder=-1, label='Reg valid term', linestyle='dashed')\n",
    "    \n",
    "    # ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.set_ylim([0.9, 1.05])\n",
    "    # ax.set_ylabel('Loss ({})'.format(net.loss_type))\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_xlabel('# Batches')\n",
    "    plt.savefig(f\"./multiple_tasks/loss_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=1000)\n",
    "    \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e544aca8-271f-49d3-9ed3-ab7023f23600",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    net_helpers.net_eta_lambda_analysis(net, net_params, hyp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c10ab17-bab6-4228-9096-af042e9ac385",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_finalstage = False\n",
    "if use_finalstage:\n",
    "    # plotting output in the validation set\n",
    "    net_out, db = net.iterate_sequence_batch(test_input, run_mode='track_states')\n",
    "    W_output = net.W_output.detach().cpu().numpy()\n",
    "\n",
    "    W_all_ = []\n",
    "    for i in range(len(net.mp_layers)):\n",
    "        W_all_.append(net.mp_layers[i].W.detach().cpu().numpy())\n",
    "    W_ = W_all_[0]\n",
    "    \n",
    "else:\n",
    "    ind = len(marker_lst)-1 \n",
    "    # ind = 0\n",
    "    network_at_percent = (marker_lst[ind]+1)/train_params['n_datasets']*100\n",
    "    print(f\"Using network at {network_at_percent}%\")\n",
    "    # by default using the first test_input \n",
    "    net_out = netout_lst[0][ind]\n",
    "    db = db_lst[0][ind]\n",
    "    W_output = Woutput_lst[ind]\n",
    "    W_ = Wall_lst[ind][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8feb8c0-4029-49b1-b6e9-a5fd01ab5efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_input_output(test_input_np, net_out, test_output_np, test_task=None, tag=\"\", batch_num=5):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    test_input_np = helper.to_ndarray(test_input_np)\n",
    "    net_out = helper.to_ndarray(net_out)\n",
    "    test_output_np = helper.to_ndarray(test_output_np)\n",
    "    \n",
    "    fig_all, axs_all = plt.subplots(batch_num,2,figsize=(4*2,batch_num*2))\n",
    "    \n",
    "    if test_output_np.shape[-1] == 1:\n",
    "        for batch_idx, ax in enumerate(axs):\n",
    "            ax.plot(net_out[batch_idx, :, 0], color=c_vals[batch_idx])\n",
    "            ax.plot(test_output_np[batch_idx, :, 0], color=c_vals_l[batch_idx])\n",
    "    \n",
    "    else:\n",
    "        for batch_idx in range(batch_num):\n",
    "            for out_idx in range(test_output_np.shape[-1]):\n",
    "                axs_all[batch_idx,0].plot(net_out[batch_idx, :, out_idx], color=c_vals[out_idx], label=out_idx)\n",
    "                axs_all[batch_idx,0].plot(test_output_np[batch_idx, :, out_idx], color=c_vals_l[out_idx], linewidth=5, alpha=0.5)\n",
    "                if test_task is not None: \n",
    "                    axs_all[batch_idx,0].set_title(f\"{task_params['rules'][test_task[batch_idx]]}\")\n",
    "                # axs_all[batch_idx,0].legend()\n",
    "    \n",
    "            input_batch = test_input_np[batch_idx,:,:]\n",
    "            if task_params[\"randomize_inputs\"]: \n",
    "                input_batch = input_batch @ np.linalg.pinv(task_params[\"randomize_matrix\"])\n",
    "            for inp_idx in range(input_batch.shape[-1]):\n",
    "                axs_all[batch_idx,1].plot(input_batch[:,inp_idx], color=c_vals[inp_idx], label=inp_idx)\n",
    "                if test_task is not None: \n",
    "                    axs_all[batch_idx,1].set_title(f\"{task_params['rules'][test_task[batch_idx]]}\")\n",
    "                # axs_all[batch_idx,1].legend()\n",
    "\n",
    "    for ax in axs_all.flatten(): \n",
    "        ax.set_ylim([-2, 2])\n",
    "    fig_all.tight_layout()\n",
    "    fig_all.savefig(f\"./multiple_tasks/lowD_{hyp_dict['ruleset']}_{hyp_dict['chosen_network']}_seed{seed}_{hyp_dict['addon_name']}_{tag}.png\", dpi=1000)\n",
    "\n",
    "plot_input_output(test_input_np, net_out, test_output_np, test_task, tag=\"\", batch_num=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1f92a0-c04f-4a7a-8b08-e753efb1b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here db is selected based on learning stage selection \n",
    "\n",
    "layer_index = 0 # 1 layer MPN \n",
    "if net_params[\"input_layer_add\"]:\n",
    "    layer_index += 1 \n",
    "    \n",
    "def modulation_extraction(test_input, db, layer_index):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    max_seq_len = test_input.shape[1] \n",
    "    n_batch_all_ = test_input.shape[0]\n",
    "    \n",
    "    Ms = np.concatenate((\n",
    "        db[f'M{layer_index}'].detach().cpu().numpy().reshape(n_batch_all_, max_seq_len, -1),\n",
    "    ), axis=-1)\n",
    "\n",
    "    Ms_orig = np.concatenate((\n",
    "        db[f'M{layer_index}'].detach().cpu().numpy(),\n",
    "    ), axis=-1)\n",
    "\n",
    "    bs = np.concatenate((\n",
    "        db[f'b{layer_index}'].detach().cpu().numpy(),\n",
    "    ), axis=-1) \n",
    "\n",
    "    hs = np.concatenate((\n",
    "        db[f'hidden{layer_index}'].detach().cpu().numpy().reshape(n_batch_all_, max_seq_len, -1),\n",
    "    ), axis=-1)\n",
    "\n",
    "    return Ms, Ms_orig, hs, bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dbf3e8-cd24-4cca-9ff3-31140e83e7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rules_epochs)\n",
    "all_rules = task_params[\"rules\"]\n",
    "print(all_rules)\n",
    "test_task = np.array(test_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b170e5bb-a866-40cd-8426-f62f6f364b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ms, Ms_orig, hs, bs = modulation_extraction(test_input, db_lst[0][-1], layer_index)\n",
    "print(hs.shape)\n",
    "\n",
    "tb_break = [[0, rules_epochs[all_rules[0]]['stim1']], \n",
    "            [1, rules_epochs[all_rules[1]]['stim1']],\n",
    "            [4, rules_epochs[all_rules[4]]['stim1']], \n",
    "            [5, rules_epochs[all_rules[5]]['stim1']],\n",
    "            [6, rules_epochs[all_rules[6]]['stim1']], \n",
    "            [7, rules_epochs[all_rules[7]]['stim1']],\n",
    "            [6, rules_epochs[all_rules[6]]['stim2']], \n",
    "            [7, rules_epochs[all_rules[7]]['stim2']],\n",
    "            \n",
    "            [4, rules_epochs[all_rules[4]]['delay1']], \n",
    "            [5, rules_epochs[all_rules[5]]['delay1']],\n",
    "            [6, rules_epochs[all_rules[6]]['delay1']], \n",
    "            [7, rules_epochs[all_rules[7]]['delay1']],\n",
    "            [6, rules_epochs[all_rules[6]]['delay2']], \n",
    "            [7, rules_epochs[all_rules[7]]['delay2']],\n",
    "            \n",
    "            [0, rules_epochs[all_rules[0]]['go1']],\n",
    "            [1, rules_epochs[all_rules[1]]['go1']],\n",
    "            [2, rules_epochs[all_rules[2]]['go1']],\n",
    "            [3, rules_epochs[all_rules[3]]['go1']],\n",
    "            [4, rules_epochs[all_rules[4]]['go1']],\n",
    "            [5, rules_epochs[all_rules[5]]['go1']],\n",
    "            [6, rules_epochs[all_rules[6]]['go1']],\n",
    "            [7, rules_epochs[all_rules[7]]['go1']],\n",
    "]\n",
    "\n",
    "tb_break_name = [all_rules[0]+\"-\"+\"stim1\", all_rules[1]+\"-\"+\"stim1\", all_rules[4]+\"-\"+\"stim1\", all_rules[5]+\"-\"+\"stim1\", \\\n",
    "                 all_rules[4]+\"-\"+\"delay1\", all_rules[5]+\"-\"+\"delay1\", \\\n",
    "                 all_rules[0]+\"-\"+\"go1\", all_rules[1]+\"-\"+\"go1\", all_rules[2]+\"-\"+\"go1\", all_rules[3]+\"-\"+\"go1\",all_rules[4]+\"-\"+\"go1\",all_rules[5]+\"-\"+\"go1\"\n",
    "]\n",
    "tb_break_name = np.array(tb_break_name)\n",
    "\n",
    "fig, ax = plt.subplots(2,1,figsize=(12,4*2))\n",
    "\n",
    "cell_vars_rules = [] \n",
    "\n",
    "for el in range(len(tb_break)):\n",
    "    n_rules = len(task_params['rules'])\n",
    "    n_cells = hs.shape[-1]\n",
    "    \n",
    "    # cell_vars_rules = np.zeros((n_rules, n_cells,)) \n",
    "    \n",
    "    rule_idx, period_time = tb_break[el][0], tb_break[el][1]\n",
    "    \n",
    "    print('Rule {} (idx {})'.format(all_rules[rule_idx], rule_idx))\n",
    "    rule_hs = hs[test_task == rule_idx, period_time[0]:period_time[1], :]\n",
    "    print(np.max(np.var(rule_hs, axis=(0, 1))))\n",
    "    cell_vars_rules.append(np.var(rule_hs, axis=(0, 1))) \n",
    "\n",
    "cell_vars_rules = np.array(cell_vars_rules)\n",
    "print(cell_vars_rules.shape)\n",
    "\n",
    "cell_vars_rules_norm = np.zeros_like(cell_vars_rules)\n",
    "\n",
    "# normalize\n",
    "cell_max_var = np.max(cell_vars_rules, axis=0) # Across rules\n",
    "for period_idx in range(len(tb_break)):\n",
    "    cell_vars_rules_norm[period_idx] = np.where(\n",
    "        cell_max_var > 0., cell_vars_rules[period_idx] / cell_max_var, 0.\n",
    "    )\n",
    "\n",
    "# build rule-wise value lists and corresponding field names dynamically\n",
    "rule_vals  = [cell_vars_rules_norm[i].tolist() for i in range(n_rules)]\n",
    "rule_names = [f\"rule{i}\" for i in range(n_rules)]\n",
    "\n",
    "# structured array whose fields are rule0, rule1, …, rule{n_rules-1}\n",
    "dtype = np.dtype([(name, float) for name in rule_names])\n",
    "rules_struct = np.array(list(zip(*rule_vals)), dtype=dtype)\n",
    "\n",
    "# descending lexicographic sort across all rule columns\n",
    "sort_idxs = np.argsort(rules_struct, order=rule_names)[::-1]\n",
    "\n",
    "# sort it \n",
    "cell_vars_rules_sorted_norm = cell_vars_rules_norm[:, sort_idxs]\n",
    "\n",
    "for period_idx in range(cell_vars_rules_sorted_norm.shape[0]): \n",
    "    ax[0].plot(cell_vars_rules_sorted_norm[period_idx], color=c_vals[period_idx],\n",
    "            label=tb_break_name[period_idx])\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('Cell_idx')\n",
    "ax[0].set_ylabel('Normalized task variance')\n",
    "\n",
    "sns.heatmap(cell_vars_rules_sorted_norm, ax=ax[1], cmap=\"coolwarm\", cbar=True, vmin=0, vmax=1)\n",
    "ax[1].set_yticks(np.arange(len(tb_break_name)) + 0.5)\n",
    "ax[1].set_yticklabels(tb_break_name, rotation=45)\n",
    "ax[1].set_xlabel('Cell idx')\n",
    "\n",
    "fig.savefig(f\"./multiple_tasks/hidden_variance_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e470b72c-1ea5-4082-9ab9-bc952dcf2981",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram, optimal_leaf_ordering\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def _hierarchical_clustering(data, k_min=3, k_max=40, metric=\"euclidean\"):\n",
    "    \"\"\"\n",
    "    Hierarchical Ward clustering on `data` (observations × features).\n",
    "    \"\"\"\n",
    "    n_obs = data.shape[0]\n",
    "\n",
    "    # Ward linkage\n",
    "    pairwise_dists = pdist(data, metric=metric)\n",
    "    Z = linkage(pairwise_dists, method=\"ward\")\n",
    "    Z = optimal_leaf_ordering(Z, pairwise_dists)\n",
    "\n",
    "    # choose k that maximises silhouette\n",
    "    best_k, best_score, best_labels = None, -np.inf, None\n",
    "    k_range = range(max(k_min, 2), min(k_max, n_obs - 1) + 1)\n",
    "    D_square = squareform(pairwise_dists)\n",
    "\n",
    "    for k in k_range:\n",
    "        labels = fcluster(Z, k, criterion=\"maxclust\")\n",
    "        score = silhouette_score(D_square, labels, metric=\"precomputed\")\n",
    "        if score > best_score:\n",
    "            best_k, best_score, best_labels = k, score, labels\n",
    "\n",
    "    leaf_order = dendrogram(Z, no_plot=True)[\"leaves\"]\n",
    "\n",
    "    return dict(\n",
    "        linkage=Z,\n",
    "        leaf_order=leaf_order,\n",
    "        labels=best_labels,\n",
    "        k=best_k,\n",
    "        silhouette=best_score,\n",
    "    )\n",
    "\n",
    "\n",
    "def cluster_variance_matrix(V, k_min=3, k_max=40):\n",
    "    \"\"\"\n",
    "    Cluster a variance matrix V (shape: N features × M neurons)\n",
    "    for both rows and columns.\n",
    "    \"\"\"\n",
    "    V = np.asarray(V)\n",
    "\n",
    "    row_res = _hierarchical_clustering(V,  k_min, k_max)\n",
    "    col_res = _hierarchical_clustering(V.T, k_min, k_max)\n",
    "\n",
    "    return dict(\n",
    "        row_order=row_res[\"leaf_order\"],\n",
    "        col_order=col_res[\"leaf_order\"],\n",
    "        row_labels=row_res[\"labels\"],\n",
    "        col_labels=col_res[\"labels\"],\n",
    "        row_k=row_res[\"k\"],\n",
    "        col_k=col_res[\"k\"],\n",
    "        row_linkage=row_res[\"linkage\"],   # full row hierarchy\n",
    "        col_linkage=col_res[\"linkage\"],   # full column hierarchy\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47aacc0-d4d3-4b17-aeec-1ea0ff31c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cluster_variance_matrix(cell_vars_rules_sorted_norm)\n",
    "cell_vars_rules_sorted_norm_ordered = cell_vars_rules_sorted_norm[np.ix_(result[\"row_order\"], result[\"col_order\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9771191-9cb1-4ad2-a670-a230e5614cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1,figsize=(12,4*2))\n",
    "sns.heatmap(cell_vars_rules_sorted_norm, ax=ax[0], cmap=\"coolwarm\", cbar=True, vmin=0, vmax=1)\n",
    "ax[0].set_yticks(np.arange(len(tb_break_name)) + 0.5)\n",
    "ax[0].set_yticklabels(tb_break_name, rotation=45)\n",
    "sns.heatmap(cell_vars_rules_sorted_norm_ordered, ax=ax[1], cmap=\"coolwarm\", cbar=True, vmin=0, vmax=1)\n",
    "ax[1].set_yticks(np.arange(len(tb_break_name)) + 0.5)\n",
    "ax[1].set_yticklabels(tb_break_name[result[\"row_order\"]], rotation=45)\n",
    "fig.savefig(f\"./multiple_tasks/hidden_variance_cluster_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b189f0-c14e-4527-af3b-1a57534fdcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"row_linkage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b8843b-e5f8-4524-9735-e05092b9f0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_break_name\n",
    "print(len(result[\"row_linkage\"]))\n",
    "print(len(result[\"col_linkage\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fc1bb2-f9df-4e97-88c0-8214c5deba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(15*2,4))\n",
    "dendrogram(result[\"row_linkage\"], ax=axs[0], labels=tb_break_name, leaf_rotation=45)\n",
    "axs[0].set_title(f\"Row hierarchy (k = {result['row_k']})\")\n",
    "\n",
    "dendrogram(result[\"col_linkage\"], ax=axs[1], labels=np.array([i for i in range(cell_vars_rules_sorted_norm_ordered.shape[1])]), leaf_rotation=45)\n",
    "axs[1].set_title(f\"Col hierarchy (k = {result['col_k']})\")\n",
    "fig.savefig(f\"./multiple_tasks/hidden_variance_hierarchy_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad6b702-b4e0-4177-9233-de7d99462256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
