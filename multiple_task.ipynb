{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "374fb8f2-9b83-44ce-821b-8917a114c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "import copy \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# PyTorch Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Data Handling and Image Processing\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Style for Matplotlib\n",
    "import scienceplots\n",
    "plt.style.use('science')\n",
    "plt.style.use(['no-latex'])\n",
    "\n",
    "# Scientific Computing and Machine Learning\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.linalg import subspace_angles\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import networks as nets  # Contains RNNs\n",
    "import net_helpers\n",
    "import mpn_tasks\n",
    "import helper\n",
    "import mpn\n",
    "\n",
    "import scienceplots\n",
    "plt.style.use('science')\n",
    "plt.style.use(['no-latex'])\n",
    "\n",
    "# Memory Optimization\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd1556c8-a4b6-434b-a60f-37035980bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 Red, 1 blue, 2 green, 3 purple, 4 orange, 5 teal, 6 gray, 7 pink, 8 yellow\n",
    "c_vals = ['#e53e3e', '#3182ce', '#38a169', '#805ad5','#dd6b20', '#319795', '#718096', '#d53f8c', '#d69e2e',] * 10\n",
    "c_vals_l = ['#feb2b2', '#90cdf4', '#9ae6b4', '#d6bcfa', '#fbd38d', '#81e6d9', '#e2e8f0', '#fbb6ce', '#faf089',] * 10\n",
    "c_vals_d = ['#9b2c2c', '#2c5282', '#276749', '#553c9a', '#9c4221', '#285e61', '#2d3748', '#97266d', '#975a16',] * 10 \n",
    "l_vals = ['solid', 'dashed', 'dotted', 'dashdot', '-', '--', '-.', ':', (0, (3, 1, 1, 1)), (0, (5, 10))]\n",
    "markers_vals = ['o', 'v', '*', '+', '>', '1', '2', '3', '4', 's', 'p', '*', 'h', 'H', '+', 'x', 'D', 'd', '|', '_']\n",
    "linestyles = [\"-\", \"--\", \"-.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58d36397-6fe4-4124-bf57-307f571e9309",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34978bf6-67b8-41bd-a022-a7b46a320686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set seed 822\n",
      "Fixation_off: True; Task_info: True\n",
      "Rules: ['contextdelaydm1']\n",
      "  Input size 7, Output size 3\n",
      "Using CUDA...\n"
     ]
    }
   ],
   "source": [
    "# Reload modules if changes have been made to them\n",
    "from importlib import reload\n",
    "\n",
    "reload(nets)\n",
    "reload(net_helpers)\n",
    "\n",
    "fixseed = False # randomize setting the seed may lead to not perfectly solved results\n",
    "seed = random.randint(1,1000) if not fixseed else 8 # random set the seed to test robustness by default\n",
    "print(f\"Set seed {seed}\")\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "hyp_dict['task_type'] = 'multitask' # int, NeuroGym, multitask\n",
    "hyp_dict['mode_for_all'] = \"random_batch\"\n",
    "hyp_dict['ruleset'] = 'contextdelaydm1' # low_dim, all, test\n",
    "\n",
    "accept_rules = ('fdgo', 'fdanti', 'delaygo', 'delayanti', 'reactgo', 'reactanti', \n",
    "                'delaydm1', 'delaydm2', 'dmsgo', 'dmcgo', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm', 'dmsnogo', 'dmcnogo')\n",
    "\n",
    "\n",
    "rules_dict = \\\n",
    "    {'all' : ['fdgo', 'reactgo', 'delaygo', 'fdanti', 'reactanti', 'delayanti',\n",
    "              'dm1', 'dm2', 'contextdm1', 'contextdm2', 'multidm',\n",
    "              'delaydm1', 'delaydm2', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm',\n",
    "              'dmsgo', 'dmsnogo', 'dmcgo', 'dmcnogo'],\n",
    "     'low_dim' : ['fdgo', 'reactgo', 'delaygo', 'fdanti', 'reactanti', 'delayanti',\n",
    "                 'delaydm1', 'delaydm2', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm',\n",
    "                 'dmsgo', 'dmsnogo', 'dmcgo', 'dmcnogo'],\n",
    "     'delayfamily': ['delaygo', 'delayanti'], \n",
    "     'dmsgo': ['dmsgo'],\n",
    "     'dmcgo': ['dmcgo'], \n",
    "     'contextdelaydm1': ['contextdelaydm1'], \n",
    "     'delaygo': ['delaygo'],\n",
    "     'delaydm1': ['delaydm1'], \n",
    "     'simplegofamily': ['fdgo', 'fdanti', 'reactgo', 'reactanti'],\n",
    "     'gofamily': ['fdgo', 'fdanti', 'reactgo', 'reactanti', 'delaygo', 'delayanti'],\n",
    "     'gofamily_delaydm': ['fdgo', 'fdanti', 'reactgo', 'reactanti', 'delaygo', 'delayanti', 'delaydm1', 'delaydm2'],\n",
    "     'dm_family': ['delaydm1', 'delaydm2', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm'],\n",
    "     'go_dm_family': ['fdgo', 'fdanti', 'reactgo', 'reactanti', 'delaygo', 'delayanti', \\\n",
    "                      'delaydm1', 'delaydm2', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm'],\n",
    "     'everything': ['fdgo', 'fdanti', 'reactgo', 'reactanti', 'delaygo', 'delayanti', \\\n",
    "                      'delaydm1', 'delaydm2', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm', \n",
    "                       'dmsgo', 'dmsnogo', 'dmcgo', 'dmcnogo'],\n",
    "    }\n",
    "\n",
    "# only work if adjust_task_prop == False, otherwise will be overwritten\n",
    "rules_dict_frequency = {\n",
    "    'delaygo': np.array([1]),\n",
    "    'dmsgo': np.array([1]), \n",
    "    'dmcgo': np.array([1]),\n",
    "    'contextdelaydm1': np.array([1]), \n",
    "    'delaydm1': np.array([1]), \n",
    "    'go_dm_family': np.array([1, 1, 1, 1, 1, 1, \n",
    "                              3, 3, 3, 3, 3,\n",
    "    ]), \n",
    "    'everything': np.array([1, 1, 1, 1, 1, 1, \n",
    "                              3, 3, 3, 3, 3,\n",
    "                              1, 1, 1, 1\n",
    "    ])\n",
    "}\n",
    "\n",
    "    \n",
    "\n",
    "# This can either be used to set parameters OR set parameters and train\n",
    "train = True # whether or not to train the network\n",
    "verbose = True\n",
    "hyp_dict['run_mode'] = 'minimal' # minimal, debug\n",
    "hyp_dict['chosen_network'] = \"dmpn\"\n",
    "\n",
    "# suffix for saving images\n",
    "# inputadd, Wfix, WL2, hL2\n",
    "# inputrandom, Wtrain\n",
    "# noise001\n",
    "# largeregularization\n",
    "# trainetalambda\n",
    "\n",
    "mpn_depth = 1\n",
    "n_hidden = 150\n",
    "\n",
    "hyp_dict['addon_name'] = \"inputtrain+Wtrain+yesoversample\"\n",
    "hyp_dict['addon_name'] += f\"+hidden{n_hidden}\"\n",
    "\n",
    "# for coding \n",
    "if hyp_dict['chosen_network'] in (\"gru\", \"vanilla\"):\n",
    "    mpn_depth = 1\n",
    "\n",
    "def current_basic_params():\n",
    "    task_params = {\n",
    "        'task_type': hyp_dict['task_type'],\n",
    "        'rules': rules_dict[hyp_dict['ruleset']],\n",
    "        'rules_probs': rules_dict_frequency[hyp_dict['ruleset']], \n",
    "        'dt': 40, # ms, directly influence sequence lengths,\n",
    "        'ruleset': hyp_dict['ruleset'],\n",
    "        'n_eachring': 8, # Number of distinct possible inputs on each ring\n",
    "        'in_out_mode': 'low_dim',  # high_dim or low_dim or low_dim_pos (Robert vs. Laura's paper, resp)\n",
    "        'sigma_x': 0.00, # Laura raised to 0.1 to prevent overfitting (Robert uses 0.01)\n",
    "        'mask_type': 'cost', # 'cost', None\n",
    "        'fixate_off': True, # Second fixation signal goes on when first is off\n",
    "        'task_info': True, \n",
    "        'randomize_inputs': False,\n",
    "        'n_input': 20, # Only used if inputs are randomized,\n",
    "        'modality_diff': True,\n",
    "        'label_strength': True, \n",
    "        'long_delay': 'normal',\n",
    "        'adjust_task_prop': True,\n",
    "        'adjust_task_decay': 0.9, \n",
    "    }\n",
    "\n",
    "    assert task_params[\"fixate_off\"], \"Accuracy calculation is partially depended on that\"\n",
    "\n",
    "    print(f\"Fixation_off: {task_params['fixate_off']}; Task_info: {task_params['task_info']}\")\n",
    "\n",
    "    train_params = {\n",
    "        'lr': 1e-3,\n",
    "        'n_batches': 640,\n",
    "        'batch_size': 640,\n",
    "        'gradient_clip': 10,\n",
    "        'valid_n_batch': 20,\n",
    "        'n_datasets': 100, \n",
    "        'n_epochs_per_set': 500, \n",
    "        # 'weight_reg': 'L2',\n",
    "        # 'activity_reg': 'L2', \n",
    "        # 'reg_lambda': 1e-4,\n",
    "        \n",
    "        'scheduler': {\n",
    "            'type': 'ReduceLROnPlateau',  # or 'StepLR'\n",
    "            'mode': 'min',                # for ReduceLROnPlateau\n",
    "            'factor': 0.5,                # factor to reduce LR\n",
    "            'patience': 10,                # epochs to wait before reducing LR\n",
    "            'min_lr': 1e-6,\n",
    "            'step_size': 30,              # for StepLR (step every 30 datasets)\n",
    "            'gamma': 0.1                  # for StepLR (multiply LR by 0.1)\n",
    "        },\n",
    "    }\n",
    "\n",
    "    if not train: # some \n",
    "        assert train_params['n_epochs_per_set'] == 0\n",
    "\n",
    "    net_params = {\n",
    "        'net_type': hyp_dict['chosen_network'], # mpn1, dmpn, vanilla\n",
    "        'n_neurons': [1] + [n_hidden] * mpn_depth + [1],\n",
    "        'output_bias': False, # Turn off biases for easier interpretation\n",
    "        'loss_type': 'MSE', # XE, MSE\n",
    "        'activation': 'tanh', # linear, ReLU, sigmoid, tanh, tanh_re, tukey, heaviside\n",
    "        'cuda': True,\n",
    "        'monitor_freq': 100,\n",
    "        'monitor_valid_out': True, # Whether or not to save validation output throughout training\n",
    "        'output_matrix': '',# \"\" (default); \"untrained\", or \"orthogonal\"\n",
    "        'input_layer_add': True, \n",
    "        'input_layer_add_trainable': True, # revise this is effectively to [randomize_inputs], tune this\n",
    "        'input_layer_bias': False, \n",
    "        'input_layer': \"trainable\", # for RNN only\n",
    "        \n",
    "        # for one-layer MPN, GRU or Vanilla\n",
    "        'ml_params': {\n",
    "            'bias': True, # Bias of layer\n",
    "            'mp_type': 'mult',\n",
    "            'm_update_type': 'hebb_assoc', # hebb_assoc, hebb_pre\n",
    "            'eta_type': 'scalar', # scalar, pre_vector, post_vector, matrix\n",
    "            'eta_train': True,\n",
    "            # 'eta_init': 'mirror_gaussian', #0.0,\n",
    "            'lam_type': 'scalar', # scalar, pre_vector, post_vector, matrix\n",
    "            'm_time_scale': 4000, # ms, sets lambda\n",
    "            'lam_train': False,\n",
    "            'W_freeze': False, # different combination with [input_layer_add_trainable]\n",
    "        },\n",
    "\n",
    "        # Vanilla RNN params\n",
    "        'leaky': True,\n",
    "        'alpha': 0.2,\n",
    "    }\n",
    "\n",
    "    # Ensure the two options are *not* activated at the same time\n",
    "    assert not (task_params[\"randomize_inputs\"] and net_params[\"input_layer_add\"]), (\n",
    "        \"task_params['randomize_inputs'] and net_params['input_layer_add'] cannot both be True.\"\n",
    "    )\n",
    "\n",
    "    # for multiple MPN layers, assert \n",
    "    if mpn_depth > 1:\n",
    "        for mpl_idx in range(mpn_depth - 1):\n",
    "            assert f'ml_params{mpl_idx}' in net_params.keys()\n",
    "\n",
    "    # actually I don't think it is needed\n",
    "    # putting here to warn the parameter checking every time \n",
    "    # when switching network\n",
    "    if hyp_dict['chosen_network'] in (\"gru\", \"vanilla\"):\n",
    "        assert f'ml_params' in net_params.keys()\n",
    "\n",
    "    return task_params, train_params, net_params\n",
    "\n",
    "task_params, train_params, net_params = current_basic_params()\n",
    "hyp_dict['addon_name'] += f\"+batch{train_params['n_batches']}\"\n",
    "\n",
    "shift_index = 1 if not task_params['fixate_off'] else 0\n",
    "\n",
    "if hyp_dict['task_type'] in ('multitask',):\n",
    "    task_params, train_params, net_params = mpn_tasks.convert_and_init_multitask_params(\n",
    "        (task_params, train_params, net_params)\n",
    "    )\n",
    "\n",
    "    net_params['prefs'] = mpn_tasks.get_prefs(task_params['hp'])\n",
    "\n",
    "    print('Rules: {}'.format(task_params['rules']))\n",
    "    print('  Input size {}, Output size {}'.format(\n",
    "        task_params['n_input'], task_params['n_output'],\n",
    "    ))\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "if net_params['cuda']:\n",
    "    print('Using CUDA...')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('Using CPU...')\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# how many epoch each dataset will be trained on\n",
    "epoch_multiply = train_params[\"n_epochs_per_set\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a341b36-dc7a-42b0-bffb-cb17d380041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = task_params, train_params, net_params\n",
    "\n",
    "if net_params['net_type'] == 'mpn1':\n",
    "    netFunction = mpn.MultiPlasticNet\n",
    "elif net_params['net_type'] == 'dmpn':\n",
    "    netFunction = mpn.DeepMultiPlasticNet\n",
    "elif net_params['net_type'] == 'vanilla':\n",
    "    netFunction = nets.VanillaRNN\n",
    "elif net_params['net_type'] == 'gru':\n",
    "    netFunction = nets.GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07e4fe48-2af6-4741-8d42-01803ba0abf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Align ['contextdelaydm1'] With Same Time\n",
      "contextdelaydm1\n",
      "{'contextdelaydm1': {'fix1': (None, 10), 'stim1': (10, 22), 'delay1': (22, 31), 'stim2': (31, 38), 'delay2': (38, 42), 'go1': (42, None)}}\n"
     ]
    }
   ],
   "source": [
    "test_n_batch = train_params[\"valid_n_batch\"]\n",
    "color_by = \"stim\" # or \"resp\" \n",
    "\n",
    "task_random_fix = True\n",
    "if task_random_fix:\n",
    "    print(f\"Align {task_params['rules']} With Same Time\")\n",
    "\n",
    "if task_params['task_type'] in ('multitask',): # Test batch consists of all the rules\n",
    "    task_params['hp']['batch_size_train'] = test_n_batch\n",
    "    # using homogeneous cutting off if multiple tasks are presented in the pool\n",
    "    # if single task, using inhomogeneous cutoff to show diversity & robustness\n",
    "    # test_mode_for_all = \"random\" if len(rules_dict[hyp_dict['ruleset']]) > 1 else \"random_batch\"\n",
    "    test_mode_for_all = \"random\"\n",
    "    # ZIHAN\n",
    "    # generate test data using \"random\"\n",
    "    test_data, test_trials_extra = mpn_tasks.generate_trials_wrap(task_params, test_n_batch, \\\n",
    "                rules=task_params['rules'], mode_input=test_mode_for_all, fix=task_random_fix\n",
    "    )\n",
    "    _, test_trials, test_rule_idxs = test_trials_extra\n",
    "    task_params['dataset_name'] = 'multitask'\n",
    "\n",
    "    if task_params['in_out_mode'] in ('low_dim_pos',):\n",
    "        output_dim_labels = ('Fixate', 'Cos', '-Cos', 'Sin', '-Sin')\n",
    "    elif task_params['in_out_mode'] in ('low_dim',):\n",
    "        output_dim_labels = ('Fixate', 'Cos', 'Sin')\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def generate_response_stimulus(task_params, test_trials): \n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        labels_resp, labels_stim = [], []\n",
    "        rules_epochs = {} \n",
    "        for rule_idx, rule in enumerate(task_params['rules']):\n",
    "            print(rule)\n",
    "            if rule in accept_rules:\n",
    "                rules_epochs[rule] = test_trials[rule_idx].epochs\n",
    "                if hyp_dict['ruleset'] in ('dmsgo','dmcgo',):\n",
    "                    labels_resp.append(test_trials[rule_idx].meta['matches'])\n",
    "                    labels_stim.append(test_trials[rule_idx].meta['stim1']) \n",
    "                else:\n",
    "                    try: \n",
    "                        labels_resp.append(test_trials[rule_idx].meta['resp1'])\n",
    "                    except Exception as e:\n",
    "                        labels_resp.append(test_trials[rule_idx].meta['matches'])\n",
    "                    labels_stim.append(test_trials[rule_idx].meta['stim1']) \n",
    "    \n",
    "            else:\n",
    "                raise NotImplementedError()\n",
    "\n",
    "        print(rules_epochs)\n",
    "        \n",
    "        labels_resp = np.concatenate(labels_resp, axis=0).reshape(-1,1)\n",
    "        labels_stim = np.concatenate(labels_stim, axis=0).reshape(-1,1)\n",
    "\n",
    "        return labels_resp, labels_stim, rules_epochs\n",
    "\n",
    "    labels_resp, labels_stim, rules_epochs = generate_response_stimulus(task_params, test_trials)\n",
    "\n",
    "\n",
    "labels = labels_stim if color_by == \"stim\" else labels_resp\n",
    "    \n",
    "test_input, test_output, test_mask = test_data\n",
    "\n",
    "permutation = np.random.permutation(test_input.shape[0])\n",
    "test_input = test_input[permutation]\n",
    "test_output = test_output[permutation]\n",
    "test_mask = test_mask[permutation]\n",
    "labels = labels[permutation]\n",
    "\n",
    "test_input_np = test_input.detach().cpu().numpy()\n",
    "test_output_np = test_output.detach().cpu().numpy()\n",
    "\n",
    "# Total number of batches, might be different than test_n_batch\n",
    "# this should be the same regardless of variety of test_input\n",
    "n_batch_all = test_input_np.shape[0] \n",
    "\n",
    "def find_task(task_params, test_input_np, shift_index):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    test_task = [] # which task\n",
    "    for batch_idx in range(test_input_np.shape[0]):\n",
    "        \n",
    "        if task_params[\"randomize_inputs\"]: \n",
    "            test_input_np_ = test_input_np @ np.linalg.pinv(task_params[\"randomize_matrix\"])\n",
    "        else: \n",
    "            test_input_np_ = test_input_np\n",
    "            \n",
    "        task_label = test_input_np_[batch_idx, 0, 6-shift_index:]\n",
    "        \n",
    "        task_label = np.asarray(task_label)       \n",
    "        dist = np.abs(task_label - 1)     \n",
    "        mask = dist == dist.min() \n",
    "        \n",
    "        indices = np.where(mask)[0]\n",
    "        \n",
    "        if indices.size:                \n",
    "            task_label_index = indices[0]   \n",
    "        else:\n",
    "            raise ValueError(\"No entry close enough to 1 found\")\n",
    "            \n",
    "        test_task.append(task_label_index)\n",
    "\n",
    "    return test_task  \n",
    "\n",
    "test_task = find_task(task_params, test_input_np, shift_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e78b44e9-b6e3-4c1c-a5c7-5a608a6d090b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 150, 150, 3]\n",
      "MultiPlastic Net:\n",
      "  output neurons: 3\n",
      "  Act: tanh\n",
      "\n",
      "=== Layer Universal Setup ===\n",
      "150\n",
      "  MP Layer1 parameters:\n",
      "    n_neurons - input: 150, output: 150\n",
      "    M matrix parameters:    update bounds - Max mult: 1.0, Min mult: -1.0\n",
      "      type: mult // Update - type: hebb_assoc // Act fn: linear\n",
      "      Eta: scalar (train) // Lambda: scalar (fixed) // Lambda_max: 0.99 (tau: 4.0e+03)\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: contextdelaydm1\n",
      "Rule contextdelaydm1 seq_len 136, max_seq_len 136\n",
      "inputs_all: torch.Size([640, 136, 7])\n",
      "How about Test Data at dataset 0\n",
      "Train parameters:\n",
      "  Loss: MSE // LR: 1.00e-03 // Optim: adam\n",
      "  Grad type: backprop // Gradient clip: 1.0e+01\n",
      "Weight reg: None\n",
      "Activity reg: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zihan.zhang/.conda/envs/mpn/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, LR: 1.000e-03 - train_loss:9.784e-01, rounded train_acc:0.092, valid_loss:8.975e-01, rounded valid_acc:0.138\n",
      "Iter: 100, LR: 1.000e-03 - train_loss:2.671e-01, rounded train_acc:0.607, valid_loss:2.199e-01, rounded valid_acc:0.691\n",
      "Iter: 200, LR: 1.000e-03 - train_loss:1.847e-01, rounded train_acc:0.735, valid_loss:2.044e-01, rounded valid_acc:0.676\n",
      "Iter: 300, LR: 1.000e-03 - train_loss:1.952e-01, rounded train_acc:0.677, valid_loss:1.926e-01, rounded valid_acc:0.611\n",
      "Iter: 400, LR: 1.000e-03 - train_loss:2.043e-01, rounded train_acc:0.683, valid_loss:1.962e-01, rounded valid_acc:0.596\n",
      "Iter: 500, LR: 1.000e-03 - train_loss:1.814e-01, rounded train_acc:0.714, valid_loss:1.612e-01, rounded valid_acc:0.792\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: contextdelaydm1\n",
      "Rule contextdelaydm1 seq_len 138, max_seq_len 138\n",
      "inputs_all: torch.Size([640, 138, 7])\n",
      "How about Test Data at dataset 1\n",
      "Iter: 600, LR: 1.000e-03 - train_loss:2.384e-01, rounded train_acc:0.616, valid_loss:1.921e-01, rounded valid_acc:0.615\n",
      "Iter: 700, LR: 1.000e-03 - train_loss:2.120e-01, rounded train_acc:0.673, valid_loss:1.562e-01, rounded valid_acc:0.787\n",
      "Iter: 800, LR: 1.000e-03 - train_loss:2.209e-01, rounded train_acc:0.667, valid_loss:1.576e-01, rounded valid_acc:0.698\n",
      "Iter: 900, LR: 1.000e-03 - train_loss:2.498e-01, rounded train_acc:0.596, valid_loss:2.629e-01, rounded valid_acc:0.539\n",
      "Iter: 1000, LR: 1.000e-03 - train_loss:2.026e-01, rounded train_acc:0.663, valid_loss:1.936e-01, rounded valid_acc:0.549\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: contextdelaydm1\n",
      "Rule contextdelaydm1 seq_len 139, max_seq_len 139\n",
      "inputs_all: torch.Size([640, 139, 7])\n",
      "Iter: 1100, LR: 1.000e-03 - train_loss:2.682e-01, rounded train_acc:0.591, valid_loss:2.231e-01, rounded valid_acc:0.645\n",
      "Iter: 1200, LR: 1.000e-03 - train_loss:2.278e-01, rounded train_acc:0.644, valid_loss:2.327e-01, rounded valid_acc:0.524\n",
      "Iter: 1300, LR: 1.000e-03 - train_loss:2.336e-01, rounded train_acc:0.614, valid_loss:2.376e-01, rounded valid_acc:0.567\n",
      "Iter: 1400, LR: 1.000e-03 - train_loss:2.353e-01, rounded train_acc:0.629, valid_loss:2.624e-01, rounded valid_acc:0.475\n",
      "Iter: 1500, LR: 1.000e-03 - train_loss:2.512e-01, rounded train_acc:0.606, valid_loss:2.312e-01, rounded valid_acc:0.463\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: contextdelaydm1\n",
      "Rule contextdelaydm1 seq_len 139, max_seq_len 139\n",
      "inputs_all: torch.Size([640, 139, 7])\n",
      "Iter: 1600, LR: 1.000e-03 - train_loss:2.494e-01, rounded train_acc:0.608, valid_loss:2.466e-01, rounded valid_acc:0.632\n",
      "Iter: 1700, LR: 1.000e-03 - train_loss:2.763e-01, rounded train_acc:0.498, valid_loss:2.782e-01, rounded valid_acc:0.525\n",
      "Iter: 1800, LR: 1.000e-03 - train_loss:2.872e-01, rounded train_acc:0.508, valid_loss:2.498e-01, rounded valid_acc:0.552\n",
      "Iter: 1900, LR: 1.000e-03 - train_loss:2.398e-01, rounded train_acc:0.602, valid_loss:3.263e-01, rounded valid_acc:0.350\n",
      "Iter: 2000, LR: 1.000e-03 - train_loss:2.360e-01, rounded train_acc:0.622, valid_loss:2.419e-01, rounded valid_acc:0.365\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: contextdelaydm1\n",
      "Rule contextdelaydm1 seq_len 143, max_seq_len 143\n",
      "inputs_all: torch.Size([640, 143, 7])\n",
      "How about Test Data at dataset 4\n",
      "Iter: 2100, LR: 1.000e-03 - train_loss:2.756e-01, rounded train_acc:0.554, valid_loss:2.331e-01, rounded valid_acc:0.584\n",
      "Iter: 2200, LR: 1.000e-03 - train_loss:2.244e-01, rounded train_acc:0.653, valid_loss:2.298e-01, rounded valid_acc:0.642\n",
      "Iter: 2300, LR: 1.000e-03 - train_loss:1.894e-01, rounded train_acc:0.720, valid_loss:1.989e-01, rounded valid_acc:0.688\n",
      "Iter: 2400, LR: 1.000e-03 - train_loss:1.911e-01, rounded train_acc:0.701, valid_loss:1.751e-01, rounded valid_acc:0.757\n",
      "Iter: 2500, LR: 1.000e-03 - train_loss:2.168e-01, rounded train_acc:0.620, valid_loss:2.008e-01, rounded valid_acc:0.587\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: contextdelaydm1\n",
      "Rule contextdelaydm1 seq_len 138, max_seq_len 138\n",
      "inputs_all: torch.Size([640, 138, 7])\n",
      "Iter: 2600, LR: 1.000e-03 - train_loss:2.299e-01, rounded train_acc:0.675, valid_loss:2.276e-01, rounded valid_acc:0.701\n",
      "Iter: 2700, LR: 1.000e-03 - train_loss:1.713e-01, rounded train_acc:0.732, valid_loss:2.138e-01, rounded valid_acc:0.657\n",
      "Iter: 2800, LR: 1.000e-03 - train_loss:1.953e-01, rounded train_acc:0.675, valid_loss:2.155e-01, rounded valid_acc:0.532\n",
      "Iter: 2900, LR: 1.000e-03 - train_loss:1.920e-01, rounded train_acc:0.692, valid_loss:2.035e-01, rounded valid_acc:0.628\n",
      "Iter: 3000, LR: 1.000e-03 - train_loss:1.534e-01, rounded train_acc:0.767, valid_loss:2.019e-01, rounded valid_acc:0.567\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: contextdelaydm1\n",
      "Rule contextdelaydm1 seq_len 140, max_seq_len 140\n",
      "inputs_all: torch.Size([640, 140, 7])\n",
      "Iter: 3100, LR: 1.000e-03 - train_loss:2.027e-01, rounded train_acc:0.722, valid_loss:1.767e-01, rounded valid_acc:0.779\n",
      "Iter: 3200, LR: 1.000e-03 - train_loss:1.704e-01, rounded train_acc:0.743, valid_loss:1.746e-01, rounded valid_acc:0.593\n",
      "Iter: 3300, LR: 1.000e-03 - train_loss:1.617e-01, rounded train_acc:0.741, valid_loss:1.571e-01, rounded valid_acc:0.650\n",
      "Iter: 3400, LR: 1.000e-03 - train_loss:1.824e-01, rounded train_acc:0.719, valid_loss:1.484e-01, rounded valid_acc:0.716\n",
      "Iter: 3500, LR: 1.000e-03 - train_loss:1.914e-01, rounded train_acc:0.705, valid_loss:1.794e-01, rounded valid_acc:0.709\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: contextdelaydm1\n",
      "Rule contextdelaydm1 seq_len 135, max_seq_len 135\n",
      "inputs_all: torch.Size([640, 135, 7])\n",
      "Iter: 3600, LR: 1.000e-03 - train_loss:1.552e-01, rounded train_acc:0.729, valid_loss:1.755e-01, rounded valid_acc:0.621\n",
      "Iter: 3700, LR: 1.000e-03 - train_loss:1.463e-01, rounded train_acc:0.768, valid_loss:1.952e-01, rounded valid_acc:0.624\n",
      "Iter: 3800, LR: 1.000e-03 - train_loss:1.523e-01, rounded train_acc:0.750, valid_loss:1.324e-01, rounded valid_acc:0.820\n",
      "Iter: 3900, LR: 1.000e-03 - train_loss:1.472e-01, rounded train_acc:0.759, valid_loss:1.934e-01, rounded valid_acc:0.617\n",
      "Iter: 4000, LR: 1.000e-03 - train_loss:1.599e-01, rounded train_acc:0.732, valid_loss:1.646e-01, rounded valid_acc:0.695\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: contextdelaydm1\n",
      "Rule contextdelaydm1 seq_len 137, max_seq_len 137\n",
      "inputs_all: torch.Size([640, 137, 7])\n",
      "Iter: 4100, LR: 1.000e-03 - train_loss:2.006e-01, rounded train_acc:0.688, valid_loss:2.426e-01, rounded valid_acc:0.692\n",
      "Iter: 4200, LR: 1.000e-03 - train_loss:1.536e-01, rounded train_acc:0.744, valid_loss:2.324e-01, rounded valid_acc:0.603\n",
      "Iter: 4300, LR: 1.000e-03 - train_loss:1.740e-01, rounded train_acc:0.707, valid_loss:2.694e-01, rounded valid_acc:0.674\n",
      "Iter: 4400, LR: 1.000e-03 - train_loss:1.679e-01, rounded train_acc:0.708, valid_loss:1.704e-01, rounded valid_acc:0.801\n",
      "Iter: 4500, LR: 1.000e-03 - train_loss:1.375e-01, rounded train_acc:0.784, valid_loss:1.995e-01, rounded valid_acc:0.642\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: contextdelaydm1\n",
      "Rule contextdelaydm1 seq_len 140, max_seq_len 140\n",
      "inputs_all: torch.Size([640, 140, 7])\n",
      "Iter: 4600, LR: 1.000e-03 - train_loss:1.839e-01, rounded train_acc:0.671, valid_loss:1.552e-01, rounded valid_acc:0.684\n",
      "Iter: 4700, LR: 1.000e-03 - train_loss:1.928e-01, rounded train_acc:0.665, valid_loss:1.875e-01, rounded valid_acc:0.472\n",
      "Iter: 4800, LR: 1.000e-03 - train_loss:1.556e-01, rounded train_acc:0.720, valid_loss:1.562e-01, rounded valid_acc:0.695\n",
      "Iter: 4900, LR: 1.000e-03 - train_loss:1.688e-01, rounded train_acc:0.694, valid_loss:1.762e-01, rounded valid_acc:0.654\n",
      "Iter: 5000, LR: 1.000e-03 - train_loss:1.753e-01, rounded train_acc:0.726, valid_loss:1.728e-01, rounded valid_acc:0.673\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: contextdelaydm1\n",
      "Rule contextdelaydm1 seq_len 139, max_seq_len 139\n",
      "inputs_all: torch.Size([640, 139, 7])\n",
      "Iter: 5100, LR: 1.000e-03 - train_loss:1.752e-01, rounded train_acc:0.713, valid_loss:1.300e-01, rounded valid_acc:0.742\n",
      "Iter: 5200, LR: 1.000e-03 - train_loss:1.141e-01, rounded train_acc:0.811, valid_loss:1.697e-01, rounded valid_acc:0.721\n",
      "Iter: 5300, LR: 1.000e-03 - train_loss:1.085e-01, rounded train_acc:0.836, valid_loss:2.122e-01, rounded valid_acc:0.663\n",
      "Iter: 5400, LR: 1.000e-03 - train_loss:1.228e-01, rounded train_acc:0.794, valid_loss:1.655e-01, rounded valid_acc:0.737\n",
      "Iter: 5500, LR: 1.000e-03 - train_loss:1.133e-01, rounded train_acc:0.817, valid_loss:1.923e-01, rounded valid_acc:0.737\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: contextdelaydm1\n",
      "Rule contextdelaydm1 seq_len 142, max_seq_len 142\n",
      "inputs_all: torch.Size([640, 142, 7])\n",
      "Iter: 5600, LR: 1.000e-03 - train_loss:1.788e-01, rounded train_acc:0.733, valid_loss:1.477e-01, rounded valid_acc:0.782\n",
      "Iter: 5700, LR: 1.000e-03 - train_loss:1.858e-01, rounded train_acc:0.696, valid_loss:1.883e-01, rounded valid_acc:0.736\n",
      "Iter: 5800, LR: 1.000e-03 - train_loss:1.955e-01, rounded train_acc:0.727, valid_loss:2.099e-01, rounded valid_acc:0.642\n",
      "Iter: 5900, LR: 1.000e-03 - train_loss:1.764e-01, rounded train_acc:0.711, valid_loss:2.003e-01, rounded valid_acc:0.570\n",
      "Iter: 6000, LR: 1.000e-03 - train_loss:1.892e-01, rounded train_acc:0.709, valid_loss:2.487e-01, rounded valid_acc:0.632\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: contextdelaydm1\n",
      "Rule contextdelaydm1 seq_len 136, max_seq_len 136\n",
      "inputs_all: torch.Size([640, 136, 7])\n",
      "Iter: 6100, LR: 5.000e-04 - train_loss:1.749e-01, rounded train_acc:0.722, valid_loss:1.948e-01, rounded valid_acc:0.645\n",
      "Iter: 6200, LR: 5.000e-04 - train_loss:1.506e-01, rounded train_acc:0.756, valid_loss:1.610e-01, rounded valid_acc:0.749\n",
      "Iter: 6300, LR: 5.000e-04 - train_loss:1.460e-01, rounded train_acc:0.738, valid_loss:1.627e-01, rounded valid_acc:0.709\n",
      "Iter: 6400, LR: 5.000e-04 - train_loss:1.365e-01, rounded train_acc:0.782, valid_loss:1.683e-01, rounded valid_acc:0.735\n",
      "Iter: 6500, LR: 5.000e-04 - train_loss:1.315e-01, rounded train_acc:0.765, valid_loss:1.403e-01, rounded valid_acc:0.750\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: contextdelaydm1\n",
      "Rule contextdelaydm1 seq_len 140, max_seq_len 140\n",
      "inputs_all: torch.Size([640, 140, 7])\n",
      "Iter: 6600, LR: 5.000e-04 - train_loss:1.580e-01, rounded train_acc:0.743, valid_loss:1.689e-01, rounded valid_acc:0.699\n",
      "Iter: 6700, LR: 5.000e-04 - train_loss:1.364e-01, rounded train_acc:0.768, valid_loss:1.336e-01, rounded valid_acc:0.760\n",
      "Iter: 6800, LR: 5.000e-04 - train_loss:1.239e-01, rounded train_acc:0.790, valid_loss:1.146e-01, rounded valid_acc:0.772\n",
      "Iter: 6900, LR: 5.000e-04 - train_loss:1.048e-01, rounded train_acc:0.838, valid_loss:1.138e-01, rounded valid_acc:0.774\n",
      "Iter: 7000, LR: 5.000e-04 - train_loss:1.170e-01, rounded train_acc:0.829, valid_loss:1.691e-01, rounded valid_acc:0.662\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: contextdelaydm1\n",
      "Rule contextdelaydm1 seq_len 137, max_seq_len 137\n",
      "inputs_all: torch.Size([640, 137, 7])\n",
      "Iter: 7100, LR: 5.000e-04 - train_loss:1.405e-01, rounded train_acc:0.779, valid_loss:1.058e-01, rounded valid_acc:0.775\n",
      "Iter: 7200, LR: 5.000e-04 - train_loss:1.128e-01, rounded train_acc:0.815, valid_loss:9.375e-02, rounded valid_acc:0.795\n",
      "Iter: 7300, LR: 5.000e-04 - train_loss:1.076e-01, rounded train_acc:0.819, valid_loss:1.113e-01, rounded valid_acc:0.767\n",
      "Iter: 7400, LR: 5.000e-04 - train_loss:9.815e-02, rounded train_acc:0.832, valid_loss:1.084e-01, rounded valid_acc:0.829\n",
      "Iter: 7500, LR: 5.000e-04 - train_loss:9.479e-02, rounded train_acc:0.850, valid_loss:1.429e-01, rounded valid_acc:0.816\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: contextdelaydm1\n",
      "Rule contextdelaydm1 seq_len 139, max_seq_len 139\n",
      "inputs_all: torch.Size([640, 139, 7])\n",
      "Iter: 7600, LR: 5.000e-04 - train_loss:1.375e-01, rounded train_acc:0.773, valid_loss:1.466e-01, rounded valid_acc:0.673\n",
      "Iter: 7700, LR: 5.000e-04 - train_loss:1.163e-01, rounded train_acc:0.812, valid_loss:1.112e-01, rounded valid_acc:0.831\n",
      "Iter: 7800, LR: 5.000e-04 - train_loss:1.078e-01, rounded train_acc:0.829, valid_loss:1.012e-01, rounded valid_acc:0.698\n",
      "Iter: 7900, LR: 5.000e-04 - train_loss:1.080e-01, rounded train_acc:0.826, valid_loss:1.740e-01, rounded valid_acc:0.728\n",
      "Iter: 8000, LR: 5.000e-04 - train_loss:1.159e-01, rounded train_acc:0.818, valid_loss:1.592e-01, rounded valid_acc:0.677\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: contextdelaydm1\n",
      "Rule contextdelaydm1 seq_len 145, max_seq_len 145\n",
      "inputs_all: torch.Size([640, 145, 7])\n",
      "How about Test Data at dataset 16\n",
      "Iter: 8100, LR: 5.000e-04 - train_loss:1.255e-01, rounded train_acc:0.815, valid_loss:7.405e-02, rounded valid_acc:0.829\n",
      "Iter: 8200, LR: 5.000e-04 - train_loss:1.377e-01, rounded train_acc:0.799, valid_loss:1.126e-01, rounded valid_acc:0.754\n",
      "Iter: 8300, LR: 5.000e-04 - train_loss:1.247e-01, rounded train_acc:0.830, valid_loss:9.784e-02, rounded valid_acc:0.737\n",
      "Iter: 8400, LR: 5.000e-04 - train_loss:1.254e-01, rounded train_acc:0.819, valid_loss:1.075e-01, rounded valid_acc:0.695\n",
      "Iter: 8500, LR: 5.000e-04 - train_loss:1.080e-01, rounded train_acc:0.851, valid_loss:9.419e-02, rounded valid_acc:0.725\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: contextdelaydm1\n",
      "Rule contextdelaydm1 seq_len 139, max_seq_len 139\n",
      "inputs_all: torch.Size([640, 139, 7])\n",
      "Iter: 8600, LR: 5.000e-04 - train_loss:1.297e-01, rounded train_acc:0.789, valid_loss:8.986e-02, rounded valid_acc:0.722\n",
      "Iter: 8700, LR: 5.000e-04 - train_loss:1.347e-01, rounded train_acc:0.774, valid_loss:1.327e-01, rounded valid_acc:0.688\n",
      "Iter: 8800, LR: 5.000e-04 - train_loss:1.201e-01, rounded train_acc:0.788, valid_loss:1.096e-01, rounded valid_acc:0.795\n",
      "Iter: 8900, LR: 5.000e-04 - train_loss:1.090e-01, rounded train_acc:0.816, valid_loss:1.211e-01, rounded valid_acc:0.836\n",
      "Iter: 9000, LR: 5.000e-04 - train_loss:1.215e-01, rounded train_acc:0.787, valid_loss:1.333e-01, rounded valid_acc:0.701\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: contextdelaydm1\n",
      "Rule contextdelaydm1 seq_len 140, max_seq_len 140\n",
      "inputs_all: torch.Size([640, 140, 7])\n",
      "Iter: 9100, LR: 5.000e-04 - train_loss:1.113e-01, rounded train_acc:0.818, valid_loss:1.182e-01, rounded valid_acc:0.754\n",
      "Iter: 9200, LR: 5.000e-04 - train_loss:9.729e-02, rounded train_acc:0.837, valid_loss:1.026e-01, rounded valid_acc:0.781\n",
      "Iter: 9300, LR: 5.000e-04 - train_loss:9.847e-02, rounded train_acc:0.827, valid_loss:8.944e-02, rounded valid_acc:0.789\n",
      "Iter: 9400, LR: 5.000e-04 - train_loss:1.061e-01, rounded train_acc:0.833, valid_loss:7.986e-02, rounded valid_acc:0.844\n",
      "Iter: 9500, LR: 5.000e-04 - train_loss:9.070e-02, rounded train_acc:0.862, valid_loss:8.597e-02, rounded valid_acc:0.833\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: contextdelaydm1\n",
      "Rule contextdelaydm1 seq_len 144, max_seq_len 144\n",
      "inputs_all: torch.Size([640, 144, 7])\n",
      "Iter: 9600, LR: 5.000e-04 - train_loss:1.251e-01, rounded train_acc:0.802, valid_loss:1.145e-01, rounded valid_acc:0.685\n",
      "Iter: 9700, LR: 5.000e-04 - train_loss:1.182e-01, rounded train_acc:0.797, valid_loss:1.190e-01, rounded valid_acc:0.611\n",
      "Iter: 9800, LR: 5.000e-04 - train_loss:1.217e-01, rounded train_acc:0.792, valid_loss:1.160e-01, rounded valid_acc:0.736\n",
      "Iter: 9900, LR: 5.000e-04 - train_loss:1.139e-01, rounded train_acc:0.826, valid_loss:1.022e-01, rounded valid_acc:0.763\n",
      "Iter: 10000, LR: 5.000e-04 - train_loss:1.144e-01, rounded train_acc:0.818, valid_loss:7.918e-02, rounded valid_acc:0.893\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: contextdelaydm1\n",
      "Rule contextdelaydm1 seq_len 142, max_seq_len 142\n",
      "inputs_all: torch.Size([640, 142, 7])\n",
      "Iter: 10100, LR: 5.000e-04 - train_loss:1.253e-01, rounded train_acc:0.826, valid_loss:1.204e-01, rounded valid_acc:0.761\n",
      "Iter: 10200, LR: 5.000e-04 - train_loss:1.158e-01, rounded train_acc:0.819, valid_loss:9.719e-02, rounded valid_acc:0.735\n",
      "Iter: 10300, LR: 5.000e-04 - train_loss:1.062e-01, rounded train_acc:0.850, valid_loss:1.372e-01, rounded valid_acc:0.667\n",
      "Iter: 10400, LR: 5.000e-04 - train_loss:1.091e-01, rounded train_acc:0.823, valid_loss:1.127e-01, rounded valid_acc:0.781\n",
      "Iter: 10500, LR: 5.000e-04 - train_loss:1.044e-01, rounded train_acc:0.845, valid_loss:1.016e-01, rounded valid_acc:0.806\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: contextdelaydm1\n",
      "Rule contextdelaydm1 seq_len 147, max_seq_len 147\n",
      "inputs_all: torch.Size([640, 147, 7])\n",
      "Iter: 10600, LR: 5.000e-04 - train_loss:1.195e-01, rounded train_acc:0.808, valid_loss:1.006e-01, rounded valid_acc:0.820\n",
      "Iter: 10700, LR: 5.000e-04 - train_loss:1.147e-01, rounded train_acc:0.810, valid_loss:1.002e-01, rounded valid_acc:0.841\n",
      "Iter: 10800, LR: 5.000e-04 - train_loss:1.067e-01, rounded train_acc:0.845, valid_loss:1.417e-01, rounded valid_acc:0.740\n",
      "Iter: 10900, LR: 5.000e-04 - train_loss:1.491e-01, rounded train_acc:0.776, valid_loss:1.250e-01, rounded valid_acc:0.833\n",
      "Iter: 11000, LR: 5.000e-04 - train_loss:1.517e-01, rounded train_acc:0.773, valid_loss:1.884e-01, rounded valid_acc:0.805\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: contextdelaydm1\n",
      "Rule contextdelaydm1 seq_len 140, max_seq_len 140\n",
      "inputs_all: torch.Size([640, 140, 7])\n",
      "Iter: 11100, LR: 5.000e-04 - train_loss:1.093e-01, rounded train_acc:0.834, valid_loss:1.217e-01, rounded valid_acc:0.764\n",
      "Iter: 11200, LR: 5.000e-04 - train_loss:8.954e-02, rounded train_acc:0.859, valid_loss:9.049e-02, rounded valid_acc:0.791\n",
      "Iter: 11300, LR: 5.000e-04 - train_loss:1.096e-01, rounded train_acc:0.800, valid_loss:8.548e-02, rounded valid_acc:0.749\n",
      "Iter: 11400, LR: 5.000e-04 - train_loss:1.357e-01, rounded train_acc:0.758, valid_loss:1.708e-01, rounded valid_acc:0.844\n",
      "Iter: 11500, LR: 5.000e-04 - train_loss:1.208e-01, rounded train_acc:0.784, valid_loss:8.512e-02, rounded valid_acc:0.809\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: contextdelaydm1\n",
      "Rule contextdelaydm1 seq_len 137, max_seq_len 137\n",
      "inputs_all: torch.Size([640, 137, 7])\n",
      "Iter: 11600, LR: 5.000e-04 - train_loss:1.158e-01, rounded train_acc:0.798, valid_loss:9.219e-02, rounded valid_acc:0.785\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# we use net at different training stage on the same test_input\u001b[39;00m\n\u001b[1;32m      2\u001b[0m net, _, (counter_lst, netout_lst, db_lst, Winput_lst, Winputbias_lst,\\\n\u001b[0;32m----> 3\u001b[0m          Woutput_lst, Wall_lst, marker_lst, loss_lst, acc_lst) \u001b[38;5;241m=\u001b[39m \u001b[43mnet_helpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                                                                           \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyp_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyp_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                                                                           \u001b[49m\u001b[43mnetFunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetFunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                                                                           \u001b[49m\u001b[43mtest_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m counter_lst \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;241m*\u001b[39m epoch_multiply \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m counter_lst] \u001b[38;5;66;03m# avoid log plot issue    \u001b[39;00m\n",
      "File \u001b[0;32m/allen/programs/mindscope/workgroups/auto-model/zihan.zhang/MultiTaskMPN/net_helpers.py:120\u001b[0m, in \u001b[0;36mtrain_network\u001b[0;34m(params, net, device, verbose, train, hyp_dict, netFunction, test_input)\u001b[0m\n\u001b[1;32m    116\u001b[0m     Wall_lst\u001b[38;5;241m.\u001b[39mappend(W_all_)\n\u001b[1;32m    118\u001b[0m     marker_lst\u001b[38;5;241m.\u001b[39mappend(dataset_idx)\n\u001b[0;32m--> 120\u001b[0m _, monitor_loss, monitor_acc, goodness_history \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_trails\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[43mvalid_trails\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_trails\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_thresh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[43mrun_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyp_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrun_mode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# calculate the change of task sampling proportion \u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# aim for multi-task training (but clearly work for single-task)\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# --- inputs ---------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    127\u001b[0m df   \u001b[38;5;241m=\u001b[39m task_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madjust_task_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m]          \u001b[38;5;66;03m# scalar decay factor\u001b[39;00m\n",
      "File \u001b[0;32m/allen/programs/mindscope/workgroups/auto-model/zihan.zhang/MultiTaskMPN/net_helpers.py:775\u001b[0m, in \u001b[0;36mBaseNetwork.fit\u001b[0;34m(self, train_params, train_data, train_trails, valid_batch, valid_trails, new_thresh, run_mode)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain() \u001b[38;5;66;03m# put in train mode (doesn't really do anything unless we are using dropout/batch norm)\u001b[39;00m\n\u001b[0;32m--> 775\u001b[0m db, monitor_loss, monitor_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_trails\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_trails\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_trails\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m              \u001b[49m\u001b[43mnew_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_thresh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    778\u001b[0m last_group_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhist[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup_valid_acc\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    779\u001b[0m \u001b[38;5;66;03m# print(f\"last_group_acc: {last_group_acc}\")\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;66;03m# print(len(last_group_acc))\u001b[39;00m\n",
      "File \u001b[0;32m/allen/programs/mindscope/workgroups/auto-model/zihan.zhang/MultiTaskMPN/net_helpers.py:894\u001b[0m, in \u001b[0;36mBaseNetwork.train_epochs\u001b[0;34m(self, train_params, train_data, train_trails, valid_batch, valid_trails, new_thresh, monitor, run_mode)\u001b[0m\n\u001b[1;32m    891\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;66;03m# Backward start\u001b[39;00m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_type \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackprop\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_debug\u001b[39m\u001b[38;5;124m'\u001b[39m,): \u001b[38;5;66;03m# Standard supervised training procedures to compute gradient\u001b[39;00m\n\u001b[0;32m--> 894\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_clip \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    896\u001b[0m         torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_clip)\n",
      "File \u001b[0;32m~/.conda/envs/mpn/lib/python3.9/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mpn/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mpn/lib/python3.9/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# we use net at different training stage on the same test_input\n",
    "net, _, (counter_lst, netout_lst, db_lst, Winput_lst, Winputbias_lst,\\\n",
    "         Woutput_lst, Wall_lst, marker_lst, loss_lst, acc_lst) = net_helpers.train_network(params, device=device, verbose=verbose,\\\n",
    "                                                                                           train=train, hyp_dict=hyp_dict,\\\n",
    "                                                                                           netFunction=netFunction,\\\n",
    "                                                                                           test_input=[test_input])\n",
    "counter_lst = [x * epoch_multiply + 1 for x in counter_lst] # avoid log plot issue    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07eee03-d879-44bc-89ef-60f11bdf721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyp_dict['chosen_network'] == \"dmpn\":\n",
    "    if net_params[\"input_layer_add\"]:\n",
    "        fignorm, axsnorm = plt.subplots(1,1,figsize=(4,4))\n",
    "        axsnorm.plot(counter_lst, [np.linalg.norm(Winput_matrix) for Winput_matrix in Winput_lst], \"-o\")\n",
    "        axsnorm.set_xscale(\"log\")\n",
    "        axsnorm.set_ylabel(\"Frobenius Norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f0805a-0938-48e5-976d-3682d45b4448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check, if W_freeze, then the recorded W matrix for the modulation layer should not be changed\n",
    "if net_params[\"ml_params\"][\"W_freeze\"]: \n",
    "    assert np.allclose(Wall_lst[-1][0], Wall_lst[0][0])\n",
    "\n",
    "if net_params[\"input_layer_bias\"]: \n",
    "    assert net_params[\"input_layer_add\"] is True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47256480-5b47-496e-887e-b8de35dcc8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    fig, ax = plt.subplots(1,1,figsize=(3,3))\n",
    "    ax.plot(net.hist['iters_monitor'][1:], net.hist['train_acc'][1:], color=c_vals[0], label='Full train accuracy')\n",
    "    ax.plot(net.hist['iters_monitor'][1:], net.hist['valid_acc'][1:], color=c_vals[1], label='Full valid accuracy')\n",
    "    if net.weight_reg is not None:\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['train_loss_output_label'], color=c_vals_l[0], zorder=-1, label='Output label')\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['train_loss_reg_term'], color=c_vals_l[0], zorder=-1, label='Reg term', linestyle='dashed')\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['valid_loss_output_label'], color=c_vals_l[1], zorder=-1, label='Output valid label')\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['valid_loss_reg_term'], color=c_vals_l[1], zorder=-1, label='Reg valid term', linestyle='dashed')\n",
    "    \n",
    "    # ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.set_ylim([0.5, 1.05])\n",
    "    # ax.set_ylabel('Loss ({})'.format(net.loss_type))\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_xlabel('# Batches')\n",
    "    plt.savefig(f\"./multiple_tasks/loss_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=1000)\n",
    "    \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e544aca8-271f-49d3-9ed3-ab7023f23600",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    net_helpers.net_eta_lambda_analysis(net, net_params, hyp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c10ab17-bab6-4228-9096-af042e9ac385",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_finalstage = False\n",
    "if use_finalstage:\n",
    "    # plotting output in the validation set\n",
    "    net_out, db = net.iterate_sequence_batch(test_input, run_mode='track_states')\n",
    "    W_output = net.W_output.detach().cpu().numpy()\n",
    "\n",
    "    W_all_ = []\n",
    "    for i in range(len(net.mp_layers)):\n",
    "        W_all_.append(net.mp_layers[i].W.detach().cpu().numpy())\n",
    "    W_ = W_all_[0]\n",
    "    \n",
    "else:\n",
    "    ind = len(marker_lst)-1 \n",
    "    # ind = 0\n",
    "    network_at_percent = (marker_lst[ind]+1)/train_params['n_datasets']*100\n",
    "    print(f\"Using network at {network_at_percent}%\")\n",
    "    # by default using the first test_input \n",
    "    net_out = netout_lst[0][ind]\n",
    "    db = db_lst[0][ind]\n",
    "    W_output = Woutput_lst[ind]\n",
    "    W_ = Wall_lst[ind][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8feb8c0-4029-49b1-b6e9-a5fd01ab5efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_input_output(test_input_np, net_out, test_output_np, test_task=None, tag=\"\", batch_num=5):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    test_input_np = helper.to_ndarray(test_input_np)\n",
    "    net_out = helper.to_ndarray(net_out)\n",
    "    test_output_np = helper.to_ndarray(test_output_np)\n",
    "    \n",
    "    fig_all, axs_all = plt.subplots(batch_num,2,figsize=(4*2,batch_num*2))\n",
    "    \n",
    "    if test_output_np.shape[-1] == 1:\n",
    "        for batch_idx, ax in enumerate(axs):\n",
    "            ax.plot(net_out[batch_idx, :, 0], color=c_vals[batch_idx])\n",
    "            ax.plot(test_output_np[batch_idx, :, 0], color=c_vals_l[batch_idx])\n",
    "    \n",
    "    else:\n",
    "        for batch_idx in range(batch_num):\n",
    "            for out_idx in range(test_output_np.shape[-1]):\n",
    "                axs_all[batch_idx,0].plot(net_out[batch_idx, :, out_idx], color=c_vals[out_idx], label=out_idx)\n",
    "                axs_all[batch_idx,0].plot(test_output_np[batch_idx, :, out_idx], color=c_vals_l[out_idx], linewidth=5, alpha=0.3)\n",
    "                if test_task is not None: \n",
    "                    axs_all[batch_idx,0].set_title(f\"{task_params['rules'][test_task[batch_idx]]}\")\n",
    "                # axs_all[batch_idx,0].legend()\n",
    "    \n",
    "            input_batch = test_input_np[batch_idx,:,:]\n",
    "            if task_params[\"randomize_inputs\"]: \n",
    "                input_batch = input_batch @ np.linalg.pinv(task_params[\"randomize_matrix\"])\n",
    "            for inp_idx in range(input_batch.shape[-1]):\n",
    "                axs_all[batch_idx,1].plot(input_batch[:,inp_idx], color=c_vals[inp_idx], label=inp_idx, alpha=1.0)\n",
    "                if test_task is not None: \n",
    "                    axs_all[batch_idx,1].set_title(f\"{task_params['rules'][test_task[batch_idx]]}\")\n",
    "                # axs_all[batch_idx,1].legend()\n",
    "\n",
    "    for ax in axs_all.flatten(): \n",
    "        ax.set_ylim([-2, 2])\n",
    "    fig_all.tight_layout()\n",
    "    fig_all.savefig(f\"./multiple_tasks/lowD_{hyp_dict['ruleset']}_{hyp_dict['chosen_network']}_seed{seed}_{hyp_dict['addon_name']}_{tag}.png\", dpi=1000)\n",
    "\n",
    "plot_input_output(test_input_np, net_out, test_output_np, test_task, tag=\"\", batch_num=20 if len(rules_dict[hyp_dict['ruleset']]) > 1 else 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1f92a0-c04f-4a7a-8b08-e753efb1b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here db is selected based on learning stage selection \n",
    "\n",
    "layer_index = 0 # 1 layer MPN \n",
    "if net_params[\"input_layer_add\"]:\n",
    "    layer_index += 1 \n",
    "    \n",
    "def modulation_extraction(test_input, db, layer_index):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    max_seq_len = test_input.shape[1] \n",
    "    n_batch_all_ = test_input.shape[0]\n",
    "    \n",
    "    Ms = np.concatenate((\n",
    "        db[f'M{layer_index}'].detach().cpu().numpy().reshape(n_batch_all_, max_seq_len, -1),\n",
    "    ), axis=-1)\n",
    "\n",
    "    Ms_orig = np.concatenate((\n",
    "        db[f'M{layer_index}'].detach().cpu().numpy(),\n",
    "    ), axis=-1)\n",
    "\n",
    "    bs = np.concatenate((\n",
    "        db[f'b{layer_index}'].detach().cpu().numpy(),\n",
    "    ), axis=-1) \n",
    "\n",
    "    hs = np.concatenate((\n",
    "        db[f'hidden{layer_index}'].detach().cpu().numpy().reshape(n_batch_all_, max_seq_len, -1),\n",
    "    ), axis=-1)\n",
    "\n",
    "    return Ms, Ms_orig, hs, bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dbf3e8-cd24-4cca-9ff3-31140e83e7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rules_epochs)\n",
    "all_rules = task_params[\"rules\"]\n",
    "print(all_rules)\n",
    "test_task = np.array(test_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b170e5bb-a866-40cd-8426-f62f6f364b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ms, Ms_orig, hs, bs = modulation_extraction(test_input, db_lst[0][-1], layer_index)\n",
    "print(hs.shape)\n",
    "\n",
    "# in order of appearance\n",
    "# phase_to_indices = [\n",
    "#     (\"stim1\",  [0, 1, 4, 5, 6, 7]),\n",
    "#     (\"stim2\",  [6, 7]),\n",
    "#     (\"delay1\", [4, 5, 6, 7]),\n",
    "#     (\"delay2\", [6, 7]),\n",
    "#     (\"go1\",    [0, 1, 2, 3, 4, 5, 6, 7])\n",
    "# ]\n",
    "\n",
    "phase_to_indices = [\n",
    "    (\"stim1\",  [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]),\n",
    "    (\"stim2\",  [6, 7, 8, 9, 10]),\n",
    "    (\"delay1\", [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]),\n",
    "    (\"delay2\", [6, 7, 8, 9, 10]),\n",
    "    (\"go1\",    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])\n",
    "]\n",
    "\n",
    "tb_break = [\n",
    "    [idx, rules_epochs[all_rules[idx]][phase]]\n",
    "    for phase, indices in phase_to_indices\n",
    "    for idx in indices\n",
    "]\n",
    "\n",
    "tb_break_name = [\n",
    "    f\"{all_rules[idx]}-{phase}\"\n",
    "    for phase, indices in phase_to_indices\n",
    "    for idx in indices\n",
    "]\n",
    "\n",
    "tb_break_name = np.array(tb_break_name)\n",
    "\n",
    "fig, ax = plt.subplots(2,1,figsize=(24,8*2))\n",
    "\n",
    "cell_vars_rules = [] \n",
    "\n",
    "for el in range(len(tb_break)):\n",
    "    n_rules = len(task_params['rules'])\n",
    "    n_cells = hs.shape[-1]\n",
    "    \n",
    "    # cell_vars_rules = np.zeros((n_rules, n_cells,)) \n",
    "    \n",
    "    rule_idx, period_time = tb_break[el][0], tb_break[el][1]\n",
    "    \n",
    "    print('Rule {} (idx {})'.format(all_rules[rule_idx], rule_idx))\n",
    "    rule_hs = hs[test_task == rule_idx, period_time[0]:period_time[1], :]\n",
    "    print(np.max(np.var(rule_hs, axis=(0, 1))))\n",
    "    cell_vars_rules.append(np.var(rule_hs, axis=(0, 1))) \n",
    "\n",
    "cell_vars_rules = np.array(cell_vars_rules)\n",
    "print(cell_vars_rules.shape)\n",
    "\n",
    "cell_vars_rules_norm = np.zeros_like(cell_vars_rules)\n",
    "\n",
    "# normalize\n",
    "cell_max_var = np.max(cell_vars_rules, axis=0) # Across rules\n",
    "for period_idx in range(len(tb_break)):\n",
    "    cell_vars_rules_norm[period_idx] = np.where(\n",
    "        cell_max_var > 0., cell_vars_rules[period_idx] / cell_max_var, 0.\n",
    "    )\n",
    "\n",
    "# build rule-wise value lists and corresponding field names dynamically\n",
    "rule_vals  = [cell_vars_rules_norm[i].tolist() for i in range(n_rules)]\n",
    "rule_names = [f\"rule{i}\" for i in range(n_rules)]\n",
    "\n",
    "# structured array whose fields are rule0, rule1, , rule{n_rules-1}\n",
    "dtype = np.dtype([(name, float) for name in rule_names])\n",
    "rules_struct = np.array(list(zip(*rule_vals)), dtype=dtype)\n",
    "\n",
    "# descending lexicographic sort across all rule columns\n",
    "sort_idxs = np.argsort(rules_struct, order=rule_names)[::-1]\n",
    "\n",
    "# sort it \n",
    "cell_vars_rules_sorted_norm = cell_vars_rules_norm[:, sort_idxs]\n",
    "\n",
    "for period_idx in range(cell_vars_rules_sorted_norm.shape[0]): \n",
    "    ax[0].plot(cell_vars_rules_sorted_norm[period_idx], color=c_vals[period_idx],\n",
    "            label=tb_break_name[period_idx])\n",
    "# ax[0].legend()\n",
    "ax[0].set_xlabel('Cell_idx')\n",
    "ax[0].set_ylabel('Normalized task variance')\n",
    "\n",
    "sns.heatmap(cell_vars_rules_sorted_norm, ax=ax[1], cmap=\"coolwarm\", cbar=True, vmin=0, vmax=1)\n",
    "ax[1].set_xlabel('Cell idx')\n",
    "ax[1].set_ylabel('Rule / Break-name', fontsize=12, labelpad=12)\n",
    "ax[1].set_yticks(np.arange(len(tb_break_name)))\n",
    "ax[1].set_yticklabels(\n",
    "    tb_break_name,\n",
    "    rotation=0,              # keep text horizontal (often easiest to read)\n",
    "    ha='right',              # right-align so long names don't collide with the heatmap\n",
    "    va='center',\n",
    "    fontsize=9,              # shrink a bit if you have many rows\n",
    ")\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"./multiple_tasks/hidden_variance_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e470b72c-1ea5-4082-9ab9-bc952dcf2981",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram, optimal_leaf_ordering\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def _hierarchical_clustering(data, k_min=3, k_max=40, metric=\"euclidean\"):\n",
    "    \"\"\"\n",
    "    Hierarchical Ward clustering on `data` (observations  features).\n",
    "    \"\"\"\n",
    "    n_obs = data.shape[0]\n",
    "\n",
    "    pairwise_dists = pdist(data, metric=metric)\n",
    "    Z = linkage(pairwise_dists, method=\"ward\")\n",
    "    Z = optimal_leaf_ordering(Z, pairwise_dists)\n",
    "\n",
    "    best_k, best_score, best_labels = None, -np.inf, None\n",
    "    k_range = range(max(k_min, 2), min(k_max, n_obs - 1) + 1)\n",
    "    D_square = squareform(pairwise_dists)\n",
    "\n",
    "    for k in k_range:\n",
    "        labels = fcluster(Z, k, criterion=\"maxclust\")\n",
    "        score = silhouette_score(D_square, labels, metric=\"precomputed\")\n",
    "        if score > best_score:\n",
    "            best_k, best_score, best_labels = k, score, labels\n",
    "\n",
    "    leaf_order = dendrogram(Z, no_plot=True)[\"leaves\"]\n",
    "\n",
    "    return dict(\n",
    "        linkage=Z,\n",
    "        leaf_order=leaf_order,\n",
    "        labels=best_labels,\n",
    "        k=best_k,\n",
    "        silhouette=best_score,\n",
    "    )\n",
    "\n",
    "\n",
    "def cluster_variance_matrix(V, k_min=3, k_max=40):\n",
    "    \"\"\"\n",
    "    Cluster a variance matrix V (shape: N features  M neurons)\n",
    "    for both rows and columns.\n",
    "    \"\"\"\n",
    "    V = np.asarray(V)\n",
    "\n",
    "    row_res = _hierarchical_clustering(V,  k_min, k_max)\n",
    "    col_res = _hierarchical_clustering(V.T, k_min, k_max)\n",
    "\n",
    "    return dict(\n",
    "        row_order=row_res[\"leaf_order\"],\n",
    "        col_order=col_res[\"leaf_order\"],\n",
    "        row_labels=row_res[\"labels\"],\n",
    "        col_labels=col_res[\"labels\"],\n",
    "        row_k=row_res[\"k\"],\n",
    "        col_k=col_res[\"k\"],\n",
    "        row_linkage=row_res[\"linkage\"],   # full row hierarchy\n",
    "        col_linkage=col_res[\"linkage\"],   # full column hierarchy\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47aacc0-d4d3-4b17-aeec-1ea0ff31c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cluster_variance_matrix(cell_vars_rules_sorted_norm)\n",
    "cell_vars_rules_sorted_norm_ordered = cell_vars_rules_sorted_norm[np.ix_(result[\"row_order\"], result[\"col_order\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9771191-9cb1-4ad2-a670-a230e5614cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1,figsize=(24,8*2))\n",
    "sns.heatmap(cell_vars_rules_sorted_norm, ax=ax[0], cmap=\"coolwarm\", cbar=True, vmin=0, vmax=1)\n",
    "sns.heatmap(cell_vars_rules_sorted_norm_ordered, ax=ax[1], cmap=\"coolwarm\", cbar=True, vmin=0, vmax=1)\n",
    "ax[0].set_ylabel('Rule / Break-name', fontsize=12, labelpad=12)\n",
    "ax[0].set_yticks(np.arange(len(tb_break_name)))\n",
    "ax[0].set_yticklabels(\n",
    "    tb_break_name,\n",
    "    rotation=0,              \n",
    "    ha='right',              \n",
    "    va='center',\n",
    "    fontsize=9,              \n",
    ")\n",
    "ax[1].set_ylabel('Rule / Break-name', fontsize=12, labelpad=12)\n",
    "ax[1].set_yticks(np.arange(len(tb_break_name)))\n",
    "ax[1].set_yticklabels(\n",
    "    tb_break_name[result[\"row_order\"]],\n",
    "    rotation=0,              \n",
    "    ha='right',              \n",
    "    va='center',\n",
    "    fontsize=9,              \n",
    ")    \n",
    "fig.savefig(f\"./multiple_tasks/hidden_variance_cluster_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b189f0-c14e-4527-af3b-1a57534fdcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"row_linkage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b8843b-e5f8-4524-9735-e05092b9f0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_break_name\n",
    "print(len(result[\"row_linkage\"]))\n",
    "print(len(result[\"col_linkage\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fc1bb2-f9df-4e97-88c0-8214c5deba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(15*2,4))\n",
    "dendrogram(result[\"row_linkage\"], ax=axs[0], labels=tb_break_name, leaf_rotation=45)\n",
    "axs[0].set_title(f\"Row hierarchy (k = {result['row_k']})\")\n",
    "\n",
    "dendrogram(result[\"col_linkage\"], ax=axs[1], labels=np.array([i for i in range(cell_vars_rules_sorted_norm_ordered.shape[1])]), leaf_rotation=45)\n",
    "axs[1].set_title(f\"Col hierarchy (k = {result['col_k']})\")\n",
    "fig.savefig(f\"./multiple_tasks/hidden_variance_hierarchy_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad6b702-b4e0-4177-9233-de7d99462256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2192dc0e-0021-446d-8e39-8da925909b68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mpn)",
   "language": "python",
   "name": "mpn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
