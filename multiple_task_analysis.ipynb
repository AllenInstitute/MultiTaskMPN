{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff3dc007-c85f-4f89-bd95-e1a59ca7c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import sys \n",
    "import random \n",
    "from pathlib import Path\n",
    "import json\n",
    "import time \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.ticker as ticker\n",
    "ticker.Locator.MAXTICKS = 10000 \n",
    "import seaborn as sns \n",
    "from scipy.cluster.hierarchy import dendrogram, cophenet\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "from numpy.linalg import norm \n",
    "\n",
    "import copy\n",
    "\n",
    "import scienceplots\n",
    "plt.style.use('science')\n",
    "plt.style.use(['no-latex'])\n",
    "\n",
    "import helper           \n",
    "import clustering\n",
    "import clustering_metric\n",
    "import color_func\n",
    "\n",
    "c_vals = [\n",
    "    \"#e53e3e\",  # red\n",
    "    \"#3182ce\",  # blue\n",
    "    \"#38a169\",  # green\n",
    "    \"#d69e2e\",  # yellow-gold\n",
    "    \"#d53f8c\",  # pink-magenta\n",
    "    \"#4c51bf\",  # indigo\n",
    "    \"#dd6b20\",  # orange\n",
    "    \"#0ea5e9\",  # sky blue\n",
    "    \"#22c55e\",  # bright green\n",
    "    \"#a855f7\",  # purple\n",
    "    \"#f43f5e\",  # red-pink\n",
    "    \"#0f766e\",  # teal\n",
    "    \"#b83280\",  # magenta-violet\n",
    "    \"#ca8a04\",  # amber\n",
    "    \"#2b6cb0\",  # deep blue\n",
    "] * 10\n",
    "\n",
    "c_vals_l = [\n",
    "    \"#feb2b2\",  # light red\n",
    "    \"#90cdf4\",  # light blue\n",
    "    \"#9ae6b4\",  # light green\n",
    "    \"#faf089\",  # light yellow-gold\n",
    "    \"#fbb6ce\",  # light magenta\n",
    "    \"#c3dafe\",  # light indigo\n",
    "    \"#fed7aa\",  # light orange\n",
    "    \"#bae6fd\",  # light sky blue\n",
    "    \"#bbf7d0\",  # light bright green\n",
    "    \"#e9d5ff\",  # light purple\n",
    "    \"#fecdd3\",  # light red-pink\n",
    "    \"#a7f3d0\",  # light teal\n",
    "    \"#f9a8d4\",  # light violet-magenta\n",
    "    \"#fde68a\",  # light amber\n",
    "    \"#bfdbfe\",  # light deep blue\n",
    "] * 10\n",
    "\n",
    "c_vals_d = [\n",
    "    \"#9b2c2c\",  # dark red\n",
    "    \"#2c5282\",  # dark blue\n",
    "    \"#276749\",  # dark green\n",
    "    \"#975a16\",  # dark golden\n",
    "    \"#97266d\",  # dark magenta\n",
    "    \"#4338ca\",  # dark indigo\n",
    "    \"#7b341e\",  # dark orange\n",
    "    \"#0369a1\",  # dark sky blue\n",
    "    \"#15803d\",  # dark bright green\n",
    "    \"#6b21a8\",  # dark purple\n",
    "    \"#9f1239\",  # dark red-pink\n",
    "    \"#0f4c3a\",  # dark teal\n",
    "    \"#702459\",  # dark violet-magenta\n",
    "    \"#854d0e\",  # dark amber\n",
    "    \"#1e3a8a\",  # dark deep blue\n",
    "] * 10\n",
    "\n",
    "\n",
    "l_vals = ['solid', 'dashed', 'dotted', 'dashdot', '-', '--', '-.', ':', (0, (3, 1, 1, 1)), (0, (5, 10))]\n",
    "markers_vals = ['o', 'v', '*', '+', '>', '1', '2', '3', '4', 's', 'p', '*', 'h', 'H', '+', 'x', 'D', 'd', '|', '_']\n",
    "linestyles = [\"-\", \"--\", \"-.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fa232a6-7cf4-45d3-8c07-afef82ab35bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 540.10 GB\n",
      "Available: 382.94 GB\n",
      "Used: 152.98 GB\n",
      "Percentage: 29.1%\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "mem = psutil.virtual_memory()\n",
    "print(f\"Total: {mem.total / 1e9:.2f} GB\")\n",
    "print(f\"Available: {mem.available / 1e9:.2f} GB\")\n",
    "print(f\"Used: {mem.used / 1e9:.2f} GB\")\n",
    "print(f\"Percentage: {mem.percent}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1b40904-4bde-4690-8bcd-7f62de45fa4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'multiple_tasks/param_everything_seed154_noL2+hidden300+batch128+angle_result.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m out_path_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiple_tasks/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparam_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_seed\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m+hidden\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m+batch\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00maccfeature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_result.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m out_path \u001b[38;5;241m=\u001b[39m Path(out_path_name)\n\u001b[0;32m---> 13\u001b[0m size_bytes \u001b[38;5;241m=\u001b[39m \u001b[43mout_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mst_size  \n\u001b[1;32m     14\u001b[0m size_gb \u001b[38;5;241m=\u001b[39m size_bytes \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize_gb\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GiB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/mpn/lib/python3.9/pathlib.py:1160\u001b[0m, in \u001b[0;36mPath.stat\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstat\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;124;03m    Return the result of the stat() system call on this path, like\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;124;03m    os.stat() does.\u001b[39;00m\n\u001b[1;32m   1159\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'multiple_tasks/param_everything_seed154_noL2+hidden300+batch128+angle_result.npz'"
     ]
    }
   ],
   "source": [
    "# load and unpack parameters\n",
    "# make sure to change out_path and out_param_path simultaneously\n",
    "seed = \"154\" # 467, 870, 509\n",
    "task = \"everything\"\n",
    "hidden = \"300\"\n",
    "batch = \"128\"\n",
    "feature = \"noL2\" # noL2, L2\n",
    "accfeature = \"+angle\" # +angle, \"\"\n",
    "# this file might be large, for the sake of completeness\n",
    "out_path_name = \"multiple_tasks/\" + f\"param_{task}_seed{seed}_{feature}+hidden{hidden}+batch{batch}{accfeature}_result.npz\"\n",
    "out_path = Path(out_path_name)\n",
    "\n",
    "size_bytes = out_path.stat().st_size  \n",
    "size_gb = size_bytes / 1024**3 \n",
    "print(f\"{out_path} = {size_gb:.3f} GiB\")\n",
    "\n",
    "with np.load(out_path_name, allow_pickle=True) as data:\n",
    "    rules_epochs = data[\"rules_epochs\"].item()\n",
    "    hyp_dict = data[\"hyp_dict\"].item()\n",
    "    all_rules = data[\"all_rules\"]\n",
    "    test_task = data[\"test_task\"]\n",
    "    # Ms = data[\"Ms\"]\n",
    "    Ms_orig = data[\"Ms_orig\"]\n",
    "    hs = data[\"hs\"]\n",
    "    bs = data[\"bs\"]\n",
    "    xs = data[\"xs\"]\n",
    "\n",
    "print(f\"Ms_orig: {Ms_orig.shape}\")\n",
    "print(f\"hs: {hs.shape}\")\n",
    "print(f\"xs: {xs.shape}\")\n",
    "print(f\"bs: {bs.shape}\")\n",
    "\n",
    "out_param_path = \"multiple_tasks/\" + f\"param_{task}_seed{seed}_{feature}+hidden{hidden}+batch{batch}{accfeature}_param.json\"\n",
    "out_param_path = Path(out_param_path)\n",
    "\n",
    "with out_param_path.open() as f: \n",
    "    raw_cfg_param = json.load(f)\n",
    "\n",
    "task_params, train_params, net_params = raw_cfg_param[\"task_params\"], raw_cfg_param[\"train_params\"], raw_cfg_param[\"net_params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15de75c5-efa6-467c-b7ba-7e87d1011d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2025-11-19: make sure the bias is only cell-dependent but not time- or trail-dependent\n",
    "ref = bs[0, 0, :]              \n",
    "same_per_k = np.all(bs == ref, axis=(0, 1))  \n",
    "all_k_constant = np.all(same_per_k)\n",
    "print(all_k_constant)\n",
    "\n",
    "figbias, axbias = plt.subplots(1,1,figsize=(10,2))\n",
    "sns.heatmap(ref.reshape(1,-1), ax=axbias, cmap=\"coolwarm\", center=0)\n",
    "figbias.savefig(f\"./multiple_tasks/bias_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0997a7bd-97d4-415d-b4a2-5a1c1b58eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.serialization import add_safe_globals\n",
    "\n",
    "# add_safe_globals([np.core.multiarray._reconstruct])\n",
    "\n",
    "# netpathname = \"multiple_tasks/\" + f\"savednet_{task}_seed{seed}_{feature}+hidden{hidden}+batch{batch}{accfeature}.pt\"\n",
    "# checkpoint = torch.load(netpathname, map_location=\"cpu\")\n",
    "# net_params_loaded = checkpoint[\"state_dict\"]\n",
    "# print(net_params_loaded.keys())\n",
    "\n",
    "# output_W = net_params_loaded[\"W_output\"].cpu().numpy()\n",
    "# input_W = net_params_loaded[\"W_initial_linear.weight\"].cpu().numpy()\n",
    "# modulation_W = net_params_loaded[\"mp_layer1.W\"].cpu().numpy()\n",
    "\n",
    "# fig, ax = plt.subplots(1,1,figsize=(10,4))\n",
    "# vmax = np.max(np.abs(output_W))\n",
    "# sns.heatmap(\n",
    "#     output_W,\n",
    "#     ax=ax,\n",
    "#     cmap=\"coolwarm\",\n",
    "#     # square=True,\n",
    "#     vmin=-vmax,\n",
    "#     vmax=vmax,\n",
    "#     center=0\n",
    "# )\n",
    "# ax.set_xlabel(\"Hidden 2 Index\", fontsize=15)\n",
    "# ax.set_ylabel(\"Output Index\", fontsize=15)\n",
    "# fig.tight_layout()\n",
    "# fig.show()\n",
    "\n",
    "# fig, ax = plt.subplots(1,1,figsize=(10,4))\n",
    "# vmax = np.max(np.abs(input_W))\n",
    "# sns.heatmap(\n",
    "#     input_W.T,\n",
    "#     ax=ax,\n",
    "#     cmap=\"coolwarm\",\n",
    "#     # square=True,\n",
    "#     vmin=-vmax,\n",
    "#     vmax=vmax,\n",
    "#     center=0\n",
    "# )\n",
    "# ax.set_xlabel(\"Hidden 1 Index\", fontsize=15)\n",
    "# ax.set_ylabel(\"Input Index\", fontsize=15)\n",
    "# fig.tight_layout()\n",
    "# fig.show()\n",
    "\n",
    "# fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "# vmax = np.max(np.abs(modulation_W))\n",
    "# sns.heatmap(\n",
    "#     modulation_W.T,\n",
    "#     ax=ax,\n",
    "#     cmap=\"coolwarm\",\n",
    "#     square=True,\n",
    "#     vmin=-vmax,\n",
    "#     vmax=vmax,\n",
    "#     center=0\n",
    "# )\n",
    "# ax.set_xlabel(\"Hidden 1 Index\", fontsize=15)\n",
    "# ax.set_ylabel(\"Hidden 2 Index\", fontsize=15)\n",
    "# fig.tight_layout()\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b58a0c-0d4f-48d2-9f5b-7968fe60c122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upper_cluster_conn = 300\n",
    "\n",
    "# def analyze_connectivity(input_mat, xlabel=\"\", ylabel=\"\", square=False): \n",
    "#     \"\"\"\n",
    "#     \"\"\"\n",
    "#     result = clustering.cluster_variance_matrix_repeat(input_mat, k_min=1, k_max=upper_cluster_conn, \n",
    "#                                                        metric=\"euclidean\", method=\"ward\", n_repeats=100, silhouette_tol=0.10)\n",
    "#     input_mat_reordered = input_mat[np.ix_(result[\"row_order\"], result[\"col_order\"])]\n",
    "#     best_alt_k_row, best_alt_k_col = result[\"row_tol_k\"], result[\"col_tol_k\"]\n",
    "    \n",
    "#     rl = np.asarray(result[\"row_tol_labels\"])[result[\"row_order\"]]\n",
    "#     cl = np.asarray(result[\"col_tol_labels\"])[result[\"col_order\"]]\n",
    "#     rbreaks = clustering._breaks(rl)\n",
    "#     cbreaks = clustering._breaks(cl)\n",
    "\n",
    "#     fig, axs = plt.subplots(2,1,figsize=(10,4*2 if not square else 10*2))\n",
    "#     vmax = np.max(np.abs(input_mat))\n",
    "#     sns.heatmap(input_mat, ax=axs[0], cmap=\"coolwarm\", vmin=-vmax, vmax=vmax, center=0)\n",
    "#     sns.heatmap(input_mat_reordered, ax=axs[1], cmap=\"coolwarm\", vmin=-vmax, vmax=vmax, center=0)\n",
    "#     for ax in axs:\n",
    "#         ax.set_xlabel(xlabel, fontsize=15)\n",
    "#         ax.set_ylabel(ylabel, fontsize=15)\n",
    "\n",
    "#     for rb in rbreaks:\n",
    "#         axs[1].axhline(rb, color=\"k\", lw=0.6)\n",
    "#     for cb in cbreaks:\n",
    "#         axs[1].axvline(cb, color=\"k\", lw=0.6)\n",
    "        \n",
    "#     axs[1].set_title(f\"best_alt_k_row: {best_alt_k_row}; best_alt_k_col: {best_alt_k_col}\", fontsize=15)\n",
    "            \n",
    "#     fig.tight_layout()\n",
    "#     fig.show()\n",
    "\n",
    "# analyze_connectivity(output_W, xlabel=\"Hidden 2 Index\", ylabel=\"Output\")\n",
    "# analyze_connectivity(input_W.T, xlabel=\"Hidden 1 Index\", ylabel=\"Input\")\n",
    "# analyze_connectivity(modulation_W, xlabel=\"Hidden 1 Index\", ylabel=\"Hidden 2 Index\", square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcd3091-5cb7-4ef7-852f-14a2dc6e9fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "def find_pngs_with_components(root, components, recursive=True, case_sensitive=False):\n",
    "    \"\"\"\n",
    "    Return a list of Paths to .png files whose *filenames* contain all substrings in `components`.\n",
    "    \"\"\"\n",
    "    root = Path(root)\n",
    "    if not case_sensitive:\n",
    "        components = [c.lower() for c in components]\n",
    "\n",
    "    candidates = root.rglob(\"*.png\") if recursive else root.glob(\"*.png\")\n",
    "    matches = []\n",
    "    for p in candidates:\n",
    "        name = p.name if case_sensitive else p.name.lower()\n",
    "        if all(c in name for c in components):\n",
    "            matches.append(p)\n",
    "\n",
    "    matches.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return matches\n",
    "\n",
    "def show_image(path, size=(6,6)):\n",
    "    img = Image.open(path)\n",
    "    fig, ax = plt.subplots(1,1,figsize=size)\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    fig.show()\n",
    "\n",
    "search_root = \"./multiple_tasks/\"\n",
    "\n",
    "components_loss = [\"loss\", f\"seed{seed}\"]    \n",
    "matches_loss = find_pngs_with_components(search_root, components_loss,\n",
    "                                    recursive=True, case_sensitive=False)\n",
    "show_image(matches_loss[0])\n",
    "\n",
    "components_showcase = [\"lowD\", f\"seed{seed}\"]    \n",
    "matches_showcase = find_pngs_with_components(search_root, components_showcase,\n",
    "                                    recursive=True, case_sensitive=False)\n",
    "show_image(matches_showcase[0], size=(4*2,4*20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ce8f23-5541-4b82-bb67-ff782fb6e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_data_analysis = [xs, hs, Ms_orig]\n",
    "clustering_data_analysis_names = [\"input\", \"hidden\", \"modulation_all\"]\n",
    "\n",
    "clustering_data_hierarchy = {}\n",
    "clustering_corr_info = []\n",
    "col_clusters_all, row_clusters_all = [], []\n",
    "row_cluster_breaker_all = []\n",
    "input_hidden_comparison = []\n",
    "base_data = []\n",
    "metrics_all_all = []\n",
    "rbreaks_all, cbreaks_all = [], []\n",
    "\n",
    "selection_key = [\"CH_blocks\", \"DB_blocks\"]\n",
    "\n",
    "upper_cluster = 300\n",
    "lower_cluster = 5\n",
    "\n",
    "for clustering_index in range(len(clustering_data_analysis)): \n",
    "    print(\"======================================================\")\n",
    "    clustering_data = clustering_data_analysis[clustering_index]\n",
    "    clustering_name = clustering_data_analysis_names[clustering_index]\n",
    "    print(f\"clustering_name: {clustering_name}\")\n",
    "    \n",
    "    if hyp_dict['ruleset'] == \"everything\": \n",
    "        phase_to_indices = [\n",
    "            (\"stim1\",  [0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]),\n",
    "            (\"stim2\",  [6, 7, 8, 9, 10]),\n",
    "            (\"delay1\", [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]),\n",
    "            (\"delay2\", [6, 7, 8, 9, 10]),\n",
    "            (\"go1\",    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])\n",
    "        ]\n",
    "    elif hyp_dict['ruleset'] == \"contextdelaydm1\":\n",
    "        phase_to_indices = [\n",
    "            (\"stim1\", [0]), \n",
    "            (\"stim2\", [0]), \n",
    "            (\"delay1\", [0]), \n",
    "            (\"delay2\", [0]), \n",
    "            (\"go1\", [0]), \n",
    "        ]\n",
    "\n",
    "    elif hyp_dict['ruleset'] == \"delaydm1\":\n",
    "        phase_to_indices = [\n",
    "            (\"stim1\", [0]), \n",
    "            (\"stim2\", [0]), \n",
    "            (\"delay1\", [0]), \n",
    "            (\"delay2\", [0]), \n",
    "            (\"go1\", [0]), \n",
    "        ]\n",
    "\n",
    "    elif hyp_dict['ruleset'] == \"dmcgo\":\n",
    "        phase_to_indices = [\n",
    "            (\"stim1\", [0]), \n",
    "            (\"delay1\", [0]), \n",
    "            (\"go1\", [0]), \n",
    "        ]\n",
    "    \n",
    "    tb_break = [\n",
    "        [idx, rules_epochs[all_rules[idx]][phase]]\n",
    "        for phase, indices in phase_to_indices\n",
    "        for idx in indices\n",
    "    ]\n",
    "    \n",
    "    tb_break_name = [\n",
    "        f\"{all_rules[idx]}-{phase}\"\n",
    "        for phase, indices in phase_to_indices\n",
    "        for idx in indices\n",
    "    ]\n",
    "    \n",
    "    tb_break_name = np.array(tb_break_name)\n",
    "    \n",
    "    cell_vars_rules = [] \n",
    "    \n",
    "    for el in range(len(tb_break)):\n",
    "        n_rules = len(task_params['rules'])\n",
    "        n_cells = clustering_data.shape[-1]\n",
    "            \n",
    "        rule_idx, period_time = tb_break[el][0], tb_break[el][1]\n",
    "        \n",
    "        # print('Rule {} (idx {}), {}'.format(all_rules[rule_idx], rule_idx, period_time))\n",
    "        if len(clustering_data.shape) == 3:\n",
    "            # 2025-11-19: if period_time[1] is None, then go until the end along that axis\n",
    "            rule_cluster = clustering_data[test_task == rule_idx, period_time[0]:period_time[1], :]\n",
    "            cell_vars_rules.append(np.var(rule_cluster, axis=(0, 1))) \n",
    "        else:\n",
    "            clustering_data_old = clustering_data\n",
    "            if \"pre\" in clustering_name:\n",
    "                rule_cluster = clustering_data[test_task == rule_idx, period_time[0]:period_time[1]]\n",
    "                mean_var = np.var(rule_cluster, axis=(0, 1)).mean(axis=0)\n",
    "                cell_vars_rules.append(mean_var)\n",
    "                \n",
    "            elif \"post\" in clustering_name: \n",
    "                rule_cluster = clusterin2_data[test_task == rule_idx, period_time[0]:period_time[1]]\n",
    "                mean_var = np.var(rule_cluster, axis=(0, 1)).mean(axis=1)\n",
    "                cell_vars_rules.append(mean_var)\n",
    "                \n",
    "            elif \"all\" in clustering_name: \n",
    "                clustering_data = clustering_data.reshape(clustering_data.shape[0], clustering_data.shape[1], -1)\n",
    "                rule_cluster = clustering_data[test_task == rule_idx, period_time[0]:period_time[1], :]\n",
    "                cell_vars_rules.append(np.var(rule_cluster, axis=(0, 1))) \n",
    "                    \n",
    "    cell_vars_rules = np.array(cell_vars_rules)    \n",
    "    cell_vars_rules_norm = np.zeros_like(cell_vars_rules)\n",
    "\n",
    "    print(f\"cell_vars_rules.shape: {cell_vars_rules.shape}\")\n",
    "    \n",
    "    # normalize\n",
    "    cell_max_var = np.max(cell_vars_rules, axis=0) # Across rules\n",
    "    print(f\"cell_max_var.shape: {cell_max_var.shape}\")\n",
    "\n",
    "    for period_idx in range(len(tb_break)):\n",
    "        cell_vars_rules_norm[period_idx] = np.where(\n",
    "            cell_max_var > 0., cell_vars_rules[period_idx] / cell_max_var, 0.\n",
    "        )\n",
    "\n",
    "    # modulation only, reshape to (N, pre, post) shape after calculating the variance\n",
    "    # N here as the number of sessions after breakdown\n",
    "    if \"all\" in clustering_name: \n",
    "        N, MM = cell_vars_rules_norm.shape\n",
    "        M = int(np.sqrt(MM))\n",
    "        cell_vars_rules_norm_keepshape = cell_vars_rules_norm.reshape(N, M, M)\n",
    "    \n",
    "    # build rule-wise value lists and corresponding field names dynamically\n",
    "    rule_vals  = [cell_vars_rules_norm[i].tolist() for i in range(n_rules)]\n",
    "    # print(f\"rule_vals: {rule_vals}\")\n",
    "    rule_names = [f\"rule{i}\" for i in range(n_rules)]\n",
    "    \n",
    "    # structured array whose fields are rule0, rule1, …, rule{n_rules-1}\n",
    "    dtype = np.dtype([(name, float) for name in rule_names])\n",
    "    rules_struct = np.array(list(zip(*rule_vals)), dtype=dtype)\n",
    "    \n",
    "    # sort_idxs = np.argsort(rules_struct, order=rule_names)[::-1] # descending lexicographic sort across all rule columns \n",
    "    sort_idxs = np.arange(rules_struct.shape[0], dtype=np.intp) # identity map \n",
    "\n",
    "    # July 7th: first sorting based on the normalized magnitude\n",
    "    # all the following should be aligned with this change\n",
    "    # Aug 22th: sort based on the variance ordering OR using an identity map (i.e. do nothing)\n",
    "    cell_vars_rules_sorted_norm = cell_vars_rules_norm[:, sort_idxs]\n",
    "    base_data.append(cell_vars_rules_sorted_norm)\n",
    "    print(f\"cell_vars_rules_sorted_norm: {cell_vars_rules_sorted_norm.shape}\")\n",
    "\n",
    "    # not plotting the modulation result for its large dimensionality\n",
    "    if \"all\" not in clustering_name: \n",
    "        fig, ax = plt.subplots(2,1,figsize=(24,8*2))\n",
    "        for period_idx in range(cell_vars_rules_sorted_norm.shape[0]): \n",
    "            ax[0].plot(cell_vars_rules_sorted_norm[period_idx], color=c_vals[period_idx], label=tb_break_name[period_idx])\n",
    "        ax[0].set_xlabel('Cell_idx')\n",
    "        ax[0].set_ylabel('Normalized task variance')\n",
    "        ax[0].set_title(clustering_name, fontsize=15)\n",
    "        \n",
    "        sns.heatmap(cell_vars_rules_sorted_norm, ax=ax[1], cmap=\"coolwarm\", cbar=True, vmin=0, vmax=1)\n",
    "        ax[1].set_xlabel('Cell idx')\n",
    "        ax[1].set_ylabel('Rule / Break-name', fontsize=12, labelpad=12)\n",
    "        ax[1].set_yticks(np.arange(len(tb_break_name)))\n",
    "        ax[1].set_yticklabels(tb_break_name, rotation=0, ha='right', va='center', fontsize=9)\n",
    "        ax[1].set_title(clustering_name, fontsize=15)\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f\"./multiple_tasks/{clustering_name}_variance_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)   \n",
    "\n",
    "    # analyze input and hidden\n",
    "    if not (\"all\" in clustering_name): \n",
    "        # clustering & grouping & re-ordering\n",
    "        # first loop on input, second loop in hidden\n",
    "        result = clustering.cluster_variance_matrix_repeat(cell_vars_rules_sorted_norm, k_min=lower_cluster, k_max=upper_cluster, \n",
    "                                                           metric=\"euclidean\", method=\"ward\", n_repeats=100, silhouette_tol=0.05)\n",
    "        \n",
    "        eval_res = clustering_metric.evaluate_bicluster_clustering(\n",
    "            cell_vars_rules_sorted_norm, row_labels=result[\"row_tol_labels\"], col_labels=result[\"col_tol_labels\"]\n",
    "        )\n",
    "        \n",
    "        eval_metrics = eval_res[\"metrics\"]\n",
    "        eval_blocks = eval_res[\"blocks\"]\n",
    "        eval_stdmean = np.nanmedian(eval_blocks[\"std\"] / eval_blocks[\"means\"])\n",
    "\n",
    "        eval_random_metrics_all = []\n",
    "        eval_random_blocks_all = []\n",
    "        for repeat in range(1000):\n",
    "            rng = np.random.default_rng(seed=np.random.randint(0, 10000))\n",
    "            row_arr = rng.permutation(result[\"row_tol_labels\"])\n",
    "            col_arr = rng.permutation(result[\"col_tol_labels\"])\n",
    "            eval_res_random = clustering_metric.evaluate_bicluster_clustering(\n",
    "                cell_vars_rules_sorted_norm, row_labels=row_arr, col_labels=col_arr\n",
    "            )\n",
    "            eval_random_metrics_all.append(eval_res_random[\"metrics\"])\n",
    "            eval_random_blocks_all.append(eval_res_random[\"blocks\"])\n",
    "\n",
    "        eval_random_stdmean = [np.nanmedian(eval_random_blocks[\"std\"] / eval_random_blocks[\"means\"]) \\\n",
    "                                       for eval_random_blocks in eval_random_blocks_all]\n",
    "\n",
    "        metrics_all = {}\n",
    "        for metric_key in selection_key: \n",
    "            optimized_value = eval_metrics[metric_key]\n",
    "            random_values = [eval_random_metrics[metric_key] for eval_random_metrics in eval_random_metrics_all]\n",
    "            metrics_all[metric_key] = [optimized_value, np.mean(random_values), np.std(random_values, ddof=1)/np.sqrt(len(random_values))]\n",
    "\n",
    "        metrics_all[\"std/mean\"] = [eval_stdmean, np.mean(eval_random_stdmean), np.std(eval_random_stdmean, ddof=1)/np.sqrt(len(eval_random_stdmean))]\n",
    "        print(f\"metrics_all: {metrics_all}\")\n",
    "        \n",
    "        # registeration\n",
    "        metrics_all_all.append(metrics_all) \n",
    "\n",
    "        input_hidden_comparison.append([result, cell_vars_rules_sorted_norm])\n",
    "        \n",
    "        cell_vars_rules_sorted_norm_ordered = cell_vars_rules_sorted_norm[np.ix_(result[\"row_order\"], result[\"col_order\"])]\n",
    "        \n",
    "        rl = np.asarray(result[\"row_tol_labels\"])[result[\"row_order\"]]\n",
    "        cl = np.asarray(result[\"col_tol_labels\"])[result[\"col_order\"]]\n",
    "        rbreaks = clustering._breaks(rl)\n",
    "        cbreaks = clustering._breaks(cl)\n",
    "        rbreaks_all.append(rbreaks); cbreaks_all.append(cbreaks)\n",
    "\n",
    "        best_k_row, best_k_col = result[\"row_k\"], result[\"col_k\"]\n",
    "        best_alt_k_row, best_alt_k_col = result[\"row_tol_k\"], result[\"col_tol_k\"]\n",
    "        print(f\"best_k_row: {best_k_row}; best_k_col: {best_k_col}\")\n",
    "        print(f\"best_alt_k_row: {best_alt_k_row}; best_alt_k_col: {best_alt_k_col}\")\n",
    "        \n",
    "        row_t, col_t = result[\"row_cut_threshold\"], result[\"col_cut_threshold\"] \n",
    "\n",
    "        # extract the grouping information, i.e. which neuron belong to which cluster\n",
    "        # instead of the view of dendrogram\n",
    "        # 2025-10-20: we register the tolerant version of optimal cluster selection\n",
    "        col_labels, col_k = result[\"col_tol_labels\"], result[\"col_tol_k\"]\n",
    "        # group the neuron based on the labels\n",
    "        col_clusters = {int(lab): np.where(col_labels == lab)[0] for lab in np.unique(col_labels)}\n",
    "        # 2025-11-04: do similar things for row separation\n",
    "        # we checked so that the cluster label (name) is monotonically increasing \n",
    "        # i.e. near by cluster (e.g. cluster 1 and cluster 2) should be more similiar in population \n",
    "        # as well, since increasing the cutoff threshold in dendrogram will \"merge\" these clusters\n",
    "        row_labels, row_k = result[\"row_tol_labels\"], result[\"row_tol_k\"]\n",
    "        row_clusters = {int(lab): np.where(row_labels == lab)[0] for lab in np.unique(row_labels)}\n",
    "        # registeration\n",
    "        col_clusters_all.append(col_clusters)\n",
    "        row_clusters_all.append(row_clusters)\n",
    "        \n",
    "        # plot the optimization score as a function of number of clustering\n",
    "        # also plot the indicator for the optimal number of cluster (and with tolerance version)\n",
    "        figscore, axscore = plt.subplots(1,1,figsize=(10,3))\n",
    "        axscore.plot(result[\"row_score_recording_mean\"].keys(), result[\"row_score_recording_mean\"].values(), \n",
    "                     label=\"row\", color=c_vals[0])\n",
    "        axscore.axvline(best_k_row, color=c_vals[0], linestyle=\"-\")\n",
    "        axscore.axvline(best_alt_k_row, color=c_vals[0], linestyle=\"--\")\n",
    "        axscore.plot(result[\"col_score_recording_mean\"].keys(), result[\"col_score_recording_mean\"].values(), \n",
    "                     label=\"col\", color=c_vals[1])\n",
    "        axscore.axvline(best_k_col, color=c_vals[1], linestyle=\"-\")\n",
    "        axscore.axvline(best_alt_k_col, color=c_vals[1], linestyle=\"--\")\n",
    "        axscore.set_xlabel(\"Number of Cluster\")\n",
    "        axscore.set_ylabel(\"Silhouette Score\")\n",
    "        axscore.legend()\n",
    "        axscore.set_title(f\"{clustering_name}\", fontsize=15)\n",
    "        figscore.tight_layout()\n",
    "        figscore.savefig(f\"./multiple_tasks/{clustering_name}_variance_cluster_score_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "\n",
    "        # \n",
    "        ordered_row_name = tb_break_name[result[\"row_order\"]]\n",
    "        row_breakers = [0]\n",
    "        for row_group in row_clusters.values():\n",
    "            row_breakers.append(row_breakers[-1] + len(row_group))\n",
    "        row_breakers = row_breakers[1:]\n",
    "        print(f\"row_breakers: {row_breakers}\")\n",
    "        row_cluster_breaker_all.append(row_breakers)\n",
    "        \n",
    "        # pearson correlation matrix\n",
    "        figcorr, axcorrs = plt.subplots(1,3,figsize=(8*3,8))\n",
    "        metrics = [\"Correlation\", \"Cosine Similarity\", \"L2 Distance\"]\n",
    "        cell_vars_rules_sorted_norm_ordered_measure = np.corrcoef(cell_vars_rules_sorted_norm_ordered, rowvar=True)\n",
    "        cell_vars_rules_sorted_norm_ordered_measure_cos = cosine_similarity(cell_vars_rules_sorted_norm_ordered)\n",
    "        cell_vars_rules_sorted_norm_ordered_measure_L2  = squareform(pdist(cell_vars_rules_sorted_norm_ordered, metric='euclidean'))\n",
    "\n",
    "        # set uniform colorbar to cross-compare between analysis\n",
    "        sns.heatmap(cell_vars_rules_sorted_norm_ordered_measure, cmap=\"coolwarm\", square=True, vmin=-0.5, vmax=1.0, ax=axcorrs[0])\n",
    "        sns.heatmap(cell_vars_rules_sorted_norm_ordered_measure_cos, cmap=\"coolwarm\", square=True, vmin=-0.5, vmax=1.0, ax=axcorrs[1])\n",
    "        sns.heatmap(cell_vars_rules_sorted_norm_ordered_measure_L2, cmap=\"coolwarm\", square=True, ax=axcorrs[2])\n",
    "\n",
    "        for axcorr_index in range(len(axcorrs)): \n",
    "            axcorr = axcorrs[axcorr_index]\n",
    "            # plot the group information (delimiter between different cluster)\n",
    "            nn = cell_vars_rules_sorted_norm_ordered_measure.shape[0]\n",
    "            boundaries = row_breakers\n",
    "            for b in boundaries:\n",
    "                axcorr.axvline(b, 0, 1, color=\"k\", linewidth=1.2)\n",
    "                axcorr.axhline(b, 0, 1, color=\"k\", linewidth=1.2)\n",
    "            \n",
    "            axcorr.set_xticks(np.arange(len(tb_break_name)))\n",
    "            axcorr.set_xticklabels(tb_break_name[result[\"row_order\"]], rotation=90, ha='right', va='center', \\\n",
    "                                   rotation_mode='anchor', fontsize=9)    \n",
    "            axcorr.set_yticks(np.arange(len(tb_break_name)))\n",
    "            axcorr.set_yticklabels(tb_break_name[result[\"row_order\"]], rotation=0, ha='right', va='center', \\\n",
    "                                   rotation_mode='anchor', fontsize=9) \n",
    "            axcorr.tick_params(axis=\"both\", length=0)\n",
    "            axcorr.set_title(f\"{clustering_name}-{metrics[axcorr_index]}\", fontsize=15)\n",
    "        \n",
    "        figcorr.tight_layout()\n",
    "        figcorr.savefig(f\"./multiple_tasks/{clustering_name}_variance_cluster_corr_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "\n",
    "        # register correlation information\n",
    "        clustering_corr_info.append([\n",
    "            cell_vars_rules_sorted_norm_ordered_measure, ordered_row_name, result[\"col_order\"]\n",
    "        ])\n",
    "\n",
    "        # plot the effect of grouping & ordering through the feature axis\n",
    "        fig, ax = plt.subplots(2,1,figsize=(24,8*2))\n",
    "        sns.heatmap(cell_vars_rules_sorted_norm, ax=ax[0], cmap=\"coolwarm\", cbar=True, vmin=0, vmax=1)\n",
    "        sns.heatmap(cell_vars_rules_sorted_norm_ordered, ax=ax[1], cmap=\"coolwarm\", cbar=True, vmin=0, vmax=1)\n",
    "        for rb in rbreaks:\n",
    "            ax[1].axhline(rb, color=\"k\", lw=0.6)\n",
    "        for cb in cbreaks:\n",
    "            ax[1].axvline(cb, color=\"k\", lw=0.6)\n",
    "        ax[0].set_title(f\"Before Clustering; best k row: {best_alt_k_row}; best k col: {best_alt_k_col}\", fontsize=15)\n",
    "        ax[0].set_ylabel('Rule / Break-name', fontsize=12, labelpad=12)\n",
    "        ax[0].set_yticks(np.arange(len(tb_break_name)))\n",
    "        ax[0].set_yticklabels(tb_break_name, rotation=0, ha='right', va='center', fontsize=9)\n",
    "        ax[1].set_title(f\"After Clustering; best k row: {best_alt_k_row}; best k col: {best_alt_k_col}\", fontsize=15)\n",
    "        ax[1].set_ylabel('Rule / Break-name', fontsize=12, labelpad=12)\n",
    "        ax[1].set_yticks(np.arange(len(tb_break_name)))\n",
    "        ax[1].set_yticklabels(tb_break_name[result[\"row_order\"]], rotation=0, ha='right', va='center', fontsize=9)    \n",
    "        fig.savefig(f\"./multiple_tasks/{clustering_name}_variance_cluster_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "\n",
    "        # 2025-11-04: for plotting purpose\n",
    "        rbreaks_ = [0] + rbreaks + [cell_vars_rules_sorted_norm_ordered.shape[0]]\n",
    "        cbreaks_ = [0] + cbreaks + [cell_vars_rules_sorted_norm_ordered.shape[1]]\n",
    "        figvarmean, axsvarmean = plt.subplots(1,2,figsize=(4*2,4))\n",
    "        varmean = np.zeros((len(rbreaks_) - 1, len(cbreaks_) - 1))\n",
    "        for rr in range(len(rbreaks_) - 1):\n",
    "            for cc in range(len(cbreaks_) - 1):\n",
    "                # 2025-11-04: mean covariance in this bicluster\n",
    "                varmean_ = np.mean(cell_vars_rules_sorted_norm_ordered[rbreaks_[rr]:rbreaks_[rr+1], cbreaks_[cc]:cbreaks_[cc+1]])\n",
    "                varmean[rr,cc] = varmean_\n",
    "        sns.heatmap(varmean, ax=axsvarmean[0], cmap=\"coolwarm\", cbar=True, vmin=0, vmax=1)\n",
    "        axsvarmean[0].set_ylabel(\"Session Clusters\", fontsize=15)\n",
    "        axsvarmean[0].set_xlabel(\"Neuron Clusters\", fontsize=15)\n",
    "        # 2025-11-04: for sanity check; since the neuron clusters are ordered so that the adjacent ones \n",
    "        # are more similar to each other than the ones that are further, therefore the correlation matrix\n",
    "        # should have larger value near the diagonal \n",
    "        varmeanC = np.corrcoef(varmean, rowvar=False) \n",
    "        sns.heatmap(varmeanC, ax=axsvarmean[1], cmap=\"coolwarm\", cbar=True, vmin=0, vmax=1)\n",
    "        axsvarmean[1].set_xlabel(\"Neuron Clusters\", fontsize=15)\n",
    "        axsvarmean[1].set_ylabel(\"Neuron Clusters\", fontsize=15)\n",
    "        figvarmean.tight_layout()\n",
    "        figvarmean.savefig(f\"./multiple_tasks/{clustering_name}_variance_cluster_mean_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "\n",
    "        # plot the norm of normalized variance of each session (across the neuron dimension)\n",
    "        session_norm = cell_vars_rules_sorted_norm_ordered.sum(axis=1)\n",
    "        norm_order = np.argsort(-session_norm)\n",
    "        session_norm = session_norm[norm_order]\n",
    "        session_norm_name = (tb_break_name[result[\"row_order\"]])[norm_order]\n",
    "        fig, ax = plt.subplots(1,1,figsize=(15,4))\n",
    "        ax.plot([i for i in range(len(session_norm))], session_norm, \"-o\")\n",
    "        ax.set_xticks([i for i in range(len(session_norm))])\n",
    "        ax.set_xticklabels(session_norm_name, rotation=90, ha=\"right\")\n",
    "        ax.set_ylabel(\"Summation of Normalized Variance\", fontsize=15)\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f\"./multiple_tasks/{clustering_name}_variance_norm_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "        \n",
    "        # plot hierarchy of grouping \n",
    "        fig, axs = plt.subplots(2,1,figsize=(25,4*2))\n",
    "        dendrogram(result[\"row_linkage\"], ax=axs[0], labels=tb_break_name[result[\"row_order\"]], leaf_rotation=90)\n",
    "        axs[0].axhline(row_t, linestyle=\"--\", color=\"black\")\n",
    "        axs[0].set_title(f\"Row hierarchy (k = {result['row_k']})\", fontsize=15)\n",
    "        dendrogram(result[\"col_linkage\"], ax=axs[1], labels=np.array([i for i in range(cell_vars_rules_sorted_norm_ordered.shape[1])]), leaf_rotation=90)\n",
    "        axs[1].set_title(f\"Col hierarchy (k = {result['col_k']})\", fontsize=15)\n",
    "        axs[1].axhline(col_t, linestyle=\"--\", color=\"black\")\n",
    "        fig.suptitle(clustering_name, fontsize=15)\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f\"./multiple_tasks/{clustering_name}_variance_hierarchy_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "\n",
    "        # register hierarchy clustering\n",
    "        clustering_data_hierarchy[clustering_name] = result[\"col_linkage\"]\n",
    "\n",
    "    # align the correlation matrix for input and hidden based on an identical ordering \n",
    "    # this loop will be run during the hidden analysis iteration (not modulation iteration)\n",
    "    if (len(clustering_corr_info) == 2) and (\"all\" not in clustering_name):\n",
    "        input_order_row, hidden_order_row = clustering_corr_info[0][1], clustering_corr_info[1][1]\n",
    "        input_corr, hidden_corr = clustering_corr_info[0][0], clustering_corr_info[1][0]\n",
    "        shuffle_hidden_to_input = helper.permutation_indices_b_to_a(input_order_row, hidden_order_row)\n",
    "        # reordering\n",
    "        hidden_corr_input = hidden_corr[np.ix_(shuffle_hidden_to_input, shuffle_hidden_to_input)]\n",
    "\n",
    "        figinputhiddencorr, axinputhiddencorr = plt.subplots(1,2,figsize=(8*2,8))\n",
    "        sns.heatmap(input_corr, ax=axinputhiddencorr[0], cmap=\"coolwarm\", square=True, vmin=-0.5, vmax=1.0)\n",
    "        sns.heatmap(hidden_corr_input, ax=axinputhiddencorr[1], cmap=\"coolwarm\", square=True, vmin=-0.5, vmax=1.0)\n",
    "\n",
    "        for ax in axinputhiddencorr:\n",
    "            ax.set_xticks(np.arange(len(input_order_row)))\n",
    "            ax.set_xticklabels(input_order_row, rotation=90, ha='right', va='center', \\\n",
    "                                   rotation_mode='anchor', fontsize=9)    \n",
    "            ax.set_yticks(np.arange(len(input_order_row)))\n",
    "            ax.set_yticklabels(input_order_row, rotation=0, ha='right', va='center', \\\n",
    "                                   rotation_mode='anchor', fontsize=9) \n",
    "            ax.tick_params(axis=\"both\", length=0)\n",
    "\n",
    "        axinputhiddencorr[0].set_title(\"Input Correlation\", fontsize=15)\n",
    "        axinputhiddencorr[1].set_title(\"Hidden Correlation (Reordered By Input Correlation)\", fontsize=15)\n",
    "        for ax in axinputhiddencorr:\n",
    "            for b in row_cluster_breaker_all[0]:\n",
    "                ax.axvline(b, 0, 1, color=\"k\", linewidth=1.2)\n",
    "                ax.axhline(b, 0, 1, color=\"k\", linewidth=1.2)\n",
    "            \n",
    "        figinputhiddencorr.suptitle(\"Reorder Input & Hidden Correlation\")\n",
    "        figinputhiddencorr.savefig(f\"./multiple_tasks/input2hidden_variance_hierarchy_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "    \n",
    "    # 2025-11-04: check the consistency of row clusters between input & hidden\n",
    "    if (len(clustering_corr_info) == 2) and (\"all\" not in clustering_name):\n",
    "        belonging = np.zeros((2, len(tb_break_name)))\n",
    "        for ttind in range(len(tb_break_name)):\n",
    "            for rowind in range(2):\n",
    "                cluster_name = helper.find_key_by_membership(row_clusters_all[rowind], ttind)\n",
    "                belonging[rowind, ttind] = cluster_name\n",
    "\n",
    "        figbelonging, axbelonging = plt.subplots(1,1,figsize=(20,2))\n",
    "        sns.heatmap(belonging, ax=axbelonging, cmap=\"coolwarm\", cbar=True, square=True)\n",
    "        axbelonging.set_xticks([i for i in range(len(tb_break_name))])\n",
    "        axbelonging.set_xticklabels(tb_break_name, rotation=90, ha='right', va='center', \\\n",
    "                                   rotation_mode='anchor', fontsize=9)\n",
    "        figbelonging.tight_layout()\n",
    "        figbelonging.savefig(f\"./multiple_tasks/inputhidden_row_membership_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "\n",
    "        [result_input, cell_vars_rules_sorted_norm_input] = input_hidden_comparison[0]\n",
    "        [result_hidden, cell_vars_rules_sorted_norm_hidden] = input_hidden_comparison[1]\n",
    "        cell_vars_rules_sorted_norm_ordered_input = cell_vars_rules_sorted_norm_input[np.ix_(result_input[\"row_order\"], \n",
    "                                                                                             result_input[\"col_order\"])]\n",
    "        cell_vars_rules_sorted_norm_ordered_hidden = cell_vars_rules_sorted_norm_hidden[np.ix_(result_input[\"row_order\"], \n",
    "                                                                                               result_hidden[\"col_order\"])]\n",
    "        figsame, axssame = plt.subplots(2,1,figsize=(24,8*2))\n",
    "        sns.heatmap(cell_vars_rules_sorted_norm_ordered_input, ax=axssame[0], cmap=\"coolwarm\", cbar=True, vmin=0, vmax=1)\n",
    "        sns.heatmap(cell_vars_rules_sorted_norm_ordered_hidden, ax=axssame[1], cmap=\"coolwarm\", cbar=True, vmin=0, vmax=1)\n",
    "        for tem in range(2):\n",
    "            # 2025-11-05: input break\n",
    "            for rb in rbreaks_all[0]:\n",
    "                axssame[tem].axhline(rb, color=\"k\", lw=0.6)\n",
    "            for cb in cbreaks_all[tem]:\n",
    "                axssame[tem].axvline(cb, color=\"k\", lw=0.6)\n",
    "\n",
    "        axssame[0].set_title(\"Input Using Input Session Break\", fontsize=15)\n",
    "        axssame[1].set_title(\"Hidden Using Input Session Break\", fontsize=15)\n",
    "        for axsind in range(2):\n",
    "            axssame[axsind].set_xlabel(\"Hidden Neuron Index / Cluster\", fontsize=15)\n",
    "            axssame[axsind].set_ylabel(\"Input Neuron Index / Cluster\", fontsize=15)\n",
    "        figsame.tight_layout()\n",
    "        figsame.savefig(f\"./multiple_tasks/inputhidden_samerow_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "\n",
    "        # 2025-11-18: whether to use the common session grouping based on input or hidden\n",
    "        activecorr_lst = []\n",
    "        for ih_index in range(2): \n",
    "            common_rbreaks_ = [0] + rbreaks_all[ih_index] + [cell_vars_rules_sorted_norm_input.shape[0]]\n",
    "            cbreaks_input_ = [0] + cbreaks_all[0] + [cell_vars_rules_sorted_norm_input.shape[1]]\n",
    "            cbreaks_hidden_ = [0] + cbreaks_all[1] + [cell_vars_rules_sorted_norm_hidden.shape[1]]\n",
    "    \n",
    "            varmeaninput = np.zeros((len(common_rbreaks_) - 1, len(cbreaks_input_) - 1))\n",
    "            varmeanhidden = np.zeros((len(common_rbreaks_) - 1, len(cbreaks_hidden_) - 1))\n",
    "            for rr in range(len(common_rbreaks_) - 1):\n",
    "                for cc in range(len(cbreaks_input_) - 1):\n",
    "                    varmeaninput[rr,cc] = np.mean(cell_vars_rules_sorted_norm_ordered_input[common_rbreaks_[rr]:common_rbreaks_[rr+1], \n",
    "                                                  cbreaks_input_[cc]:cbreaks_input_[cc+1]])\n",
    "                for cc2 in range(len(cbreaks_hidden_) - 1):\n",
    "                    varmeanhidden[rr,cc2] = np.mean(cell_vars_rules_sorted_norm_ordered_hidden[common_rbreaks_[rr]:common_rbreaks_[rr+1], \n",
    "                                                  cbreaks_hidden_[cc2]:cbreaks_hidden_[cc2+1]])\n",
    "    \n",
    "            figmeanact, axsmeanact = plt.subplots(1,1,figsize=(10,4))\n",
    "            varmeanconcatenate = np.concatenate((varmeaninput, varmeanhidden), axis=1)\n",
    "            sns.heatmap(varmeanconcatenate, ax=axsmeanact, cmap=\"coolwarm\", cbar=True, vmin=0, vmax=1, square=True)\n",
    "            axsmeanact.axvline(len(cbreaks_input_), color=\"k\", lw=0.6)\n",
    "            axsmeanact.set_xlabel(\"Input / Hidden Cluster\", fontsize=15)\n",
    "            axsmeanact.set_ylabel(\"Common Session Cluster\", fontsize=15)\n",
    "            figmeanact.tight_layout()\n",
    "            figmeanact.savefig(f\"./multiple_tasks/inputhidden_aligned_activation_ih{ih_index}_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "\n",
    "            figcoactive, axscoactive = plt.subplots(1,3,figsize=(4*3,4))\n",
    "            s1, s2 = varmeaninput.shape[1], varmeanhidden.shape[1]\n",
    "            activecorr, activeL1, activecosine = np.zeros((s1,s2)), np.zeros((s1,s2)), np.zeros((s1,s2))\n",
    "            for inputind in range(varmeaninput.shape[1]):\n",
    "                for hiddenind in range(varmeanhidden.shape[1]):\n",
    "                    activecorr[inputind, hiddenind] = np.corrcoef(varmeaninput[:,inputind], varmeanhidden[:,hiddenind])[0,1]\n",
    "                    activeL1[inputind, hiddenind] = np.sum(np.abs(varmeaninput[:,inputind] - varmeanhidden[:,hiddenind]))\n",
    "                    activecosine[inputind, hiddenind] = np.dot(\n",
    "                        varmeaninput[:, inputind],\n",
    "                        varmeanhidden[:, hiddenind]\n",
    "                    ) / (\n",
    "                        norm(varmeaninput[:, inputind]) * norm(varmeanhidden[:, hiddenind])\n",
    "                    )\n",
    "                    \n",
    "            sns.heatmap(activecorr, ax=axscoactive[0], cmap=\"coolwarm\", cbar=True, square=True)\n",
    "            activecorr_lst.append(activecorr)\n",
    "            sns.heatmap(activeL1, ax=axscoactive[1], cmap=\"coolwarm\", cbar=True, square=True)\n",
    "            sns.heatmap(activecosine, ax=axscoactive[2], cmap=\"coolwarm\", cbar=True, square=True)\n",
    "            \n",
    "            for axsind in range(2):\n",
    "                axscoactive[axsind].set_xlabel(\"Hidden Cluster\", fontsize=15)\n",
    "                axscoactive[axsind].set_ylabel(\"Input Cluster\", fontsize=15)\n",
    "                \n",
    "            axscoactive[0].set_title(\"Pearson Correlation\", fontsize=15)\n",
    "            axscoactive[1].set_title(\"L1 Distance\", fontsize=15)\n",
    "            axscoactive[2].set_title(\"Cosine Similarity\", fontsize=15)\n",
    "            figcoactive.tight_layout()\n",
    "            figcoactive.savefig(f\"./multiple_tasks/inputhidden_coactivation_ih{ih_index}_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "\n",
    "        # 2025-11-19: this result should be aligned by using / comparing the session clusters of \n",
    "        figcoactivecompare, axcoactivecompare = plt.subplots(1,1,figsize=(5,5))\n",
    "        axcoactivecompare.scatter(activecorr_lst[0].flatten(), activecorr_lst[1].flatten(), c=c_vals[0], alpha=0.7)\n",
    "        x_fit, y_fit, r_value, slope, _, p_val = helper.linear_regression(activecorr_lst[0].flatten(), activecorr_lst[1].flatten(), \n",
    "                                                                          log=False, through_origin=True)\n",
    "        axcoactivecompare.plot(x_fit, y_fit, linestyle=\"--\", label=f\"R={r_value:.3f}; Slope: {slope:.3f}; p-value: {p_val:.3f}\")\n",
    "        axcoactivecompare.set_xlabel(\"Input Common Session Coactivation\", fontsize=15)\n",
    "        axcoactivecompare.set_ylabel(\"Hidden Common Session Coactivation\", fontsize=15)\n",
    "        axcoactivecompare.axhline(0, color=\"k\", lw=0.6)\n",
    "        axcoactivecompare.axvline(0, color=\"k\", lw=0.6)\n",
    "        axcoactivecompare.legend()\n",
    "        figcoactivecompare.tight_layout()\n",
    "        figcoactivecompare.savefig(f\"./multiple_tasks/inputhidden_coactivation_ihcompare_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "        \n",
    "    # use the clustering result for input and hidden to order the modulation information\n",
    "    # and/or cross-compare the clustering result from input & hidden \n",
    "    # trying to observe consistency in between\n",
    "    if (len(clustering_corr_info) == 2) and (\"all\" in clustering_name):\n",
    "        input_order_col_ind, hidden_order_col_ind = clustering_corr_info[0][2], clustering_corr_info[1][2]\n",
    "        # cell_vars_rules_norm_keepshape: 3D array\n",
    "        # sort the modulation matrix based on the pre (input) and post (hidden) neuron ordering\n",
    "        cell_vars_rules_norm_keepshape_ih = cell_vars_rules_norm_keepshape[:, input_order_col_ind, :]\n",
    "        cell_vars_rules_norm_keepshape_ih = cell_vars_rules_norm_keepshape_ih[:, :, hidden_order_col_ind]\n",
    "\n",
    "        flatten_by_pre = cell_vars_rules_norm_keepshape_ih.reshape(N, M*M)\n",
    "        flatten_by_post = cell_vars_rules_norm_keepshape_ih.transpose(0,2,1).reshape(N, M*M)\n",
    "\n",
    "        fig, axs = plt.subplots(2,1,figsize=(24,8*2))\n",
    "        sns.heatmap(flatten_by_pre, ax=axs[0], cmap=\"coolwarm\", cbar=True, vmin=0, vmax=1)\n",
    "        sns.heatmap(flatten_by_post, ax=axs[1], cmap=\"coolwarm\", cbar=True, vmin=0, vmax=1)\n",
    "        axs[0].set_title(\"Flatten By Pre (Input)\", fontsize=15)\n",
    "        axs[1].set_title(\"Flatten By Post (Hidden)\", fontsize=15)\n",
    "        for ax in axs: \n",
    "            ax.set_yticks(np.arange(len(tb_break_name)))\n",
    "            ax.set_yticklabels(tb_break_name, rotation=0, ha='right', va='center', fontsize=9)\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f\"./multiple_tasks/modulation_input2hidden_variance_hierarchy_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "\n",
    "        # \n",
    "        print(cell_vars_rules_sorted_norm.shape)\n",
    "        cluster_input, cluster_hidden = col_clusters_all[0], col_clusters_all[1]\n",
    "        cluster_combine = {}\n",
    "        newc = 0\n",
    "        for c1 in cluster_input.keys(): # input cluster name\n",
    "            for c2 in cluster_hidden.keys(): # hidden cluster name\n",
    "                newc += 1 # iterative counter of new cell type\n",
    "                # different newc stands for a unique cell type \n",
    "                # (defined as a multiplication cluster of pre [input] and post [hidden])\n",
    "                nc1_input, nc2_hidden = cluster_input[c1], cluster_hidden[c2]\n",
    "                nc_combine = [(int(i),int(j)) for i in nc1_input for j in nc2_hidden]\n",
    "                # transform to reshaped matrix index\n",
    "                nc_combine = [cc[0] * M + cc[1] for cc in nc_combine]\n",
    "                cluster_combine[newc] = nc_combine\n",
    "\n",
    "        # Oct 8th: qualitatively, training MPN without regularization will cause more hidden cluster\n",
    "        cluster_input_num, cluster_hidden_num = len(cluster_input), len(cluster_hidden)\n",
    "        print(f\"cluster_input_num: {cluster_input_num}; cluster_hidden_num: {cluster_hidden_num}\")\n",
    "        # by the setup, cluster_combine will be organized by shared pre\n",
    "        # same pre neuron will be placed adjacently\n",
    "        cluster_combine_pre = copy.deepcopy(cluster_combine)\n",
    "        # create order of key to put post neuron together \n",
    "        post_order = []\n",
    "        for c1 in range(cluster_input_num):\n",
    "            for c2 in range(cluster_hidden_num):\n",
    "                post_order.append(c1 + c2 * cluster_input_num + 1) \n",
    "\n",
    "        pre_order = list(cluster_combine.keys())\n",
    "        # print(f\"pre_order: {pre_order}\")\n",
    "        # print(f\"post_order: {post_order}\")\n",
    "\n",
    "        pre_order_group, post_order_group = [], [] \n",
    "        for ind in range(cluster_hidden_num):\n",
    "            pre_order_group.append([i+1 for i in range(ind * cluster_input_num, (ind+1) * cluster_input_num)])\n",
    "        for ind in range(cluster_input_num):\n",
    "            block = []\n",
    "            for ind2 in range(cluster_hidden_num):\n",
    "                block.append(ind + ind2 * cluster_input_num + 1)\n",
    "            post_order_group.append(block)\n",
    "        \n",
    "        group_neurons_comb_pre, group_neurons_comb_post = [], [] \n",
    "        for preo in pre_order:\n",
    "            group_neurons_comb_pre.extend(cluster_combine[preo])\n",
    "        for posto in post_order:\n",
    "            group_neurons_comb_post.extend(cluster_combine[posto])\n",
    "\n",
    "        # Sep 2nd: we dont want random order of cell type\n",
    "        # but based on certain pre or post neuron clustering \n",
    "        # random_celltype_orders = helper.concat_random_samples(cluster_combine, n_samples=1, seed=42)\n",
    "        # random_celltype_orders = random_celltype_orders[0] # take the first sample\n",
    "        # group_neurons_comb = random_celltype_orders[0] # concatenated list\n",
    "        # assert len(group_neurons_comb) == len(set(group_neurons_comb)) # every element is unique\n",
    "        # group_neurons = random_celltype_orders[1] # list of list, the order does not matter here\n",
    "\n",
    "        group_neurons = [group for group in cluster_combine.values()]\n",
    "\n",
    "        group_neurons_pre = []\n",
    "        for pre_order_ in pre_order_group:\n",
    "            block = []\n",
    "            for ind in pre_order_:\n",
    "                block.extend(group_neurons[ind-1])\n",
    "            group_neurons_pre.append(block)\n",
    "\n",
    "        group_neurons_post = []\n",
    "        for post_order_ in post_order_group:\n",
    "            block = []\n",
    "            for ind in post_order_:\n",
    "                block.extend(group_neurons[ind-1])\n",
    "            group_neurons_post.append(block)\n",
    "\n",
    "        # assert helper.all_leq(group_neurons_comb, cell_vars_rules_sorted_norm.shape[1])\n",
    "\n",
    "        # cell_vars_rules_sorted_norm_r1 = cell_vars_rules_sorted_norm[:,group_neurons_comb]\n",
    "        cell_vars_rules_sorted_norm_outer_bypre = cell_vars_rules_sorted_norm[:,group_neurons_comb_pre]\n",
    "        cell_vars_rules_sorted_norm_outer_bypost = cell_vars_rules_sorted_norm[:,group_neurons_comb_post]\n",
    "                \n",
    "        fig, axs = plt.subplots(6,1,figsize=(24,8*6))\n",
    "        sns.heatmap(cell_vars_rules_sorted_norm, ax=axs[0], cmap=\"coolwarm\", cbar=True, vmin=0, vmax=1)\n",
    "        sns.heatmap(cell_vars_rules_sorted_norm_outer_bypre, ax=axs[1], cmap=\"coolwarm\", cbar=True, vmin=0, vmax=1)\n",
    "        sns.heatmap(cell_vars_rules_sorted_norm_outer_bypost, ax=axs[2], cmap=\"coolwarm\", cbar=True, vmin=0, vmax=1)\n",
    "        axs[0].set_title(\"Original\", fontsize=15)\n",
    "        axs[1].set_title(f\"Ordering based on the Outer Product of Input & Hidden Clusters [Input Adjacent]\", fontsize=15)\n",
    "        axs[2].set_title(f\"Ordering based on the Outer Product of Input & Hidden Clusters [Hidden Adjacent]\", fontsize=15)\n",
    "        for iii in range(3):\n",
    "            axs[iii].set_yticks(np.arange(len(tb_break_name)))\n",
    "            axs[iii].set_yticklabels(tb_break_name, rotation=0, ha='right', va='center', fontsize=9)\n",
    "\n",
    "        group_all = [group_neurons, group_neurons_pre, group_neurons_post]\n",
    "        group_all_names = [\"Outer Product of Input & Hidden Clusters\", \"Shared Hidden\", \"Shared Input\"]\n",
    "\n",
    "        for group_neurons_index in range(len(group_all)):\n",
    "            group_neurons_ = group_all[group_neurons_index]\n",
    "            result_outer = clustering.cluster_variance_matrix_forgroup(cell_vars_rules_sorted_norm, row_groups=None, \n",
    "                                                                       col_groups=group_neurons_, k_min=int(lower_cluster-1), \n",
    "                                                                       k_max=upper_cluster, silhouette_tol=0.05)\n",
    "            prior_cluster_num = len(group_neurons_)\n",
    "            cell_vars_rules_sorted_norm_inputhidden = cell_vars_rules_sorted_norm[np.ix_(result_outer[\"row_order\"], \n",
    "                                                                                         result_outer[\"col_order\"])]\n",
    "    \n",
    "            sns.heatmap(cell_vars_rules_sorted_norm_inputhidden, ax=axs[3+group_neurons_index], \n",
    "                        cmap=\"coolwarm\", cbar=True, vmin=0, vmax=1)\n",
    "            axs[3+group_neurons_index].set_yticks(np.arange(len(tb_break_name)))\n",
    "            axs[3+group_neurons_index].set_yticklabels(tb_break_name[result_outer[\"row_order\"]], \n",
    "                                                       rotation=0, ha='right', va='center', fontsize=9)\n",
    "            axs[3+group_neurons_index].set_title(f\"Ordering based on the {group_all_names[group_neurons_index]}, #{prior_cluster_num} [Post-Clustering]\", fontsize=15)\n",
    "            \n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f\"./multiple_tasks/modulation_input2hiddentogether_variance_hierarchy_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "            \n",
    "    # plot the conditional grouping for modulation\n",
    "    # currently grouped based on modulation-pre and modulation-post separately\n",
    "    if (\"all\" in clustering_name): \n",
    "        assert len(clustering_data_old.shape) == 4\n",
    "        pre_num, post_num = clustering_data_old.shape[2], clustering_data_old.shape[3]\n",
    "        feature_group_post = [] \n",
    "        for i in range(post_num):\n",
    "            feature_group_post.append(helper.basic_sort([i + j * post_num for j in range(pre_num)], sort_idxs))\n",
    "        feature_group_pre = []\n",
    "        for i in range(pre_num):\n",
    "            feature_group_pre.append(helper.basic_sort([j for j in range(post_num * i, post_num * (i+1))], sort_idxs))\n",
    "\n",
    "        result_pre = clustering.cluster_variance_matrix_forgroup(cell_vars_rules_sorted_norm, row_groups=None, \n",
    "                                                                 col_groups=feature_group_pre, k_min=lower_cluster, \n",
    "                                                                 k_max=upper_cluster, silhouette_tol=0.05)\n",
    "        result_post = clustering.cluster_variance_matrix_forgroup(cell_vars_rules_sorted_norm, row_groups=None, \n",
    "                                                                  col_groups=feature_group_post, k_min=lower_cluster, \n",
    "                                                                  k_max=upper_cluster, silhouette_tol=0.05)\n",
    "\n",
    "        # a arbitrarily chosen large cluster (in size, small in number)\n",
    "        # to compare with the cutoff distance\n",
    "        # higher the cutoff distance, fewer clusters will be formed effectively\n",
    "        select_col_k = 10\n",
    "        print(f\"select_col_k: {select_col_k}\")\n",
    "        \n",
    "        # Oct 6th: do not give any prior grouping prior to the modulation information\n",
    "        # simply grouping and considering each individual column as separate\n",
    "        # having smaller G, e.g. 200, will make the following calculation in determining pre- and post-\n",
    "        # belonging identity more time costly\n",
    "        G_lst = [30, 100, 300, 1000]\n",
    "        figcol, axscol = plt.subplots(1,len(G_lst),figsize=(4*len(G_lst),4))\n",
    "        figppshare, axsppshare = plt.subplots(2,len(G_lst),figsize=(4*len(G_lst),4*2))\n",
    "\n",
    "        result_all_lst = []\n",
    "        result_all_name_lst = []\n",
    "\n",
    "        # 2025-11-04: input cluster & hidden cluster along the neuron dimension (N)\n",
    "        cluster_input, cluster_hidden = col_clusters_all[0], col_clusters_all[1]\n",
    "        # sanity check: order it based on the key\n",
    "        cluster_input = dict(sorted(cluster_input.items()))\n",
    "        cluster_hidden = dict(sorted(cluster_hidden.items()))\n",
    "        \n",
    "        for G_idx in range(len(G_lst)): \n",
    "            G = G_lst[G_idx]\n",
    "            col_groups_all, col_labels_all, centroids_all = clustering.make_col_groups_with_kmeans(cell_vars_rules_sorted_norm, \n",
    "                                                                                                   n_groups=G)\n",
    "    \n",
    "            # plot the statistics of grouping on all modulations without considering their pre/post identity \n",
    "            group_lengths = [len(sublist) for sublist in col_groups_all]\n",
    "            \n",
    "            axscol[G_idx].hist(group_lengths, bins=20, edgecolor='black', alpha=0.7)\n",
    "            # if uniform grouping, what is the expected length for each group \n",
    "            axscol[G_idx].axvline(MM / G, linestyle=\"--\", color=\"black\")\n",
    "            axscol[G_idx].set_xlabel('Length of Formed Group', fontsize=15)\n",
    "            axscol[G_idx].set_ylabel('Frequency', fontsize=15)\n",
    "\n",
    "            # modulation analysis: using different G will generate different dendogram effectively, since we use the mean/median \n",
    "            # of each group to calculate the distance\n",
    "            result_all = clustering.cluster_variance_matrix_forgroup(cell_vars_rules_sorted_norm, row_groups=None, \n",
    "                                                                     col_groups=col_groups_all, k_min=lower_cluster, \n",
    "                                                                     k_max=G, silhouette_tol=0.05)\n",
    "    \n",
    "            print(f\"G = {G}; result_all['row_k']: {result_all['row_k']}; result_all['col_k']: {result_all['col_k']}\")\n",
    "            assert result_all[\"col_k\"] < G\n",
    "            axscol[G_idx].set_title(f\"G={G}, Neuron Cluster={result_all['col_k']}\", fontsize=15)\n",
    "\n",
    "            result_all_lst.append(result_all)\n",
    "            result_all_name_lst.append(f\"G={G}\")\n",
    "    \n",
    "            # Oct 21th: after grouping and clustering the modulation, we ask for two modulation within the same cluster, \n",
    "            # how likely they share the same presynaptic neuron (or neuron cluster), \n",
    "            # postsynaptic neuron (or neuron cluster), or neither, or both for neuron cluster \n",
    "            # it is not possible for different modulation sharing the same pre and post neuron,\n",
    "            # but they may still share the same neuron cluster\n",
    "            # here we are curious about their collective behavior and calculate the total summation of count\n",
    "            row_all, col_all = result_all[\"row_labels\"], result_all[\"col_labels\"]\n",
    "            assert np.max(col_all) == result_all[\"col_k\"]\n",
    "\n",
    "            # some helper function\n",
    "            def find_key_by_element(d, element):\n",
    "                for key, values in d.items():\n",
    "                    if element in values:\n",
    "                        return key\n",
    "                return None\n",
    "                \n",
    "            def same_pre(i, j, M):\n",
    "                return (i // M) == (j // M)\n",
    "    \n",
    "            def same_post(i, j, M):\n",
    "                return (i % M) == (j % M)\n",
    "\n",
    "            def same_pre_cluster(i, j, M, pre_cluster):\n",
    "                pre_i = i // M\n",
    "                pre_j = j // M\n",
    "                pre_i_belong = find_key_by_element(pre_cluster, pre_i)\n",
    "                pre_j_belong = find_key_by_element(pre_cluster, pre_j)\n",
    "                assert pre_i_belong is not None\n",
    "                assert pre_j_belong is not None\n",
    "                return pre_i_belong, pre_j_belong, pre_i_belong == pre_j_belong \n",
    "\n",
    "            def same_post_cluster(i, j, M, post_cluster):\n",
    "                post_i = i % M\n",
    "                post_j = j % M\n",
    "                post_i_belong = find_key_by_element(post_cluster, post_i)\n",
    "                post_j_belong = find_key_by_element(post_cluster, post_j)\n",
    "                assert post_i_belong is not None\n",
    "                assert post_j_belong is not None\n",
    "                return post_i_belong, post_j_belong, post_i_belong == post_j_belong \n",
    "                \n",
    "            # Oct 21th: \"both\" for debugging purpose, effectively with \"True\" or \"False\" should obtain identical result\n",
    "            # Use \"True\" for the actual implementation; \n",
    "            parallel_cal = \"True\" \n",
    "            if parallel_cal in (\"False\", \"both\", ): \n",
    "                # put here for future sanity check of consistency\n",
    "                same_pre_all, same_post_all, no_same_pre_post_all = 0, 0, 0\n",
    "                same_pre_cluster_all, same_post_cluster_all, same_pre_post_cluster_all, no_same_pre_post_cluster_all = 0, 0, 0, 0\n",
    "                # looping through different identified cluster number\n",
    "                for cluster_num in np.unique(col_all):\n",
    "                    # find indices that match to the desired cluster\n",
    "                    idx = np.where(col_all == cluster_num)[0]\n",
    "                    # combinations of all indices pair by removing the permutation\n",
    "                    idx_comb = [[idx[i], idx[j]] for i in range(len(idx)) for j in range(i + 1, len(idx))]\n",
    "                    for idx_ in idx_comb: \n",
    "                        same_pre_check = same_pre(idx_[0], idx_[1], M)\n",
    "                        same_post_check = same_post(idx_[0], idx_[1], M)\n",
    "                        # at most one is true, since we exclude i == j case in idx_comb\n",
    "                        # impossible to having two different modulation entry sharing the same\n",
    "                        # presynaptic and postsynaptic neuron \n",
    "                        assert same_pre_check + same_post_check <= 1\n",
    "                        non_check = 0 if (same_pre_check + same_post_check == 1) else 1\n",
    "                        \n",
    "                        same_pre_all += same_pre_check; same_post_all += same_post_check; no_same_pre_post_all += non_check \n",
    "                        \n",
    "                        # Oct 20th: check with pre/post [cluster] belonging\n",
    "                        _, _, same_pre_cluster_check = same_pre_cluster(idx_[0], idx_[1], M, cluster_input)\n",
    "                        _, _, same_post_cluster_check = same_post_cluster(idx_[0], idx_[1], M, cluster_hidden)\n",
    "                        assert same_pre_cluster_check + same_post_cluster_check <= 2\n",
    "                        non_cluster_check = 1 if (same_pre_cluster_check + same_post_cluster_check == 0) else 0\n",
    "                        both_cluster_check = 1 if (same_pre_cluster_check + same_post_cluster_check == 2) else 0\n",
    "                        same_pre_cluster_all += same_pre_cluster_check; same_post_cluster_all += same_post_cluster_check\n",
    "                        same_pre_post_cluster_all += both_cluster_check; no_same_pre_post_cluster_all += non_cluster_check\n",
    "\n",
    "                print(f\"same_pre_all: {same_pre_all}; same_post_all: {same_post_all}; no_same_pre_post_all: {no_same_pre_post_all}\")\n",
    "                print(f\"same_pre_cluster_all: {same_pre_cluster_all}; same_post_cluster_all: {same_post_cluster_all}\")\n",
    "                print(f\"same_pre_post_cluster_all: {same_pre_post_cluster_all}; no_same_pre_post_cluster_all: {no_same_pre_post_cluster_all}\")\n",
    "            \n",
    "            if parallel_cal in (\"True\", \"both\", ):\n",
    "                # membership using the actual clustering information\n",
    "                same_pre_all, same_post_all, no_same_pre_post_all, same_pre_cluster_all, same_post_cluster_all, \\\n",
    "                    same_pre_post_cluster_all, no_same_pre_post_cluster_all = clustering_metric.count_pairs_with_clusters(col_all, M, \n",
    "                                                                                                                         cluster_input, \n",
    "                                                                                                                         cluster_hidden)\n",
    "                # (control) membership using the random clustering information \n",
    "                same_pre_all_c, same_post_all_c, no_same_pre_post_all_c, same_pre_cluster_all_c, same_post_cluster_all_c, \\\n",
    "                    same_pre_post_cluster_all_c, no_same_pre_post_cluster_all_c = clustering_metric.count_pairs_with_clusters_control(col_all, M, \n",
    "                                                                                                                                      cluster_input, \n",
    "                                                                                                                                      cluster_hidden)\n",
    "                \n",
    "                print(f\"same_pre_all: {same_pre_all}; same_post_all: {same_post_all}; no_same_pre_post_all: {no_same_pre_post_all}\")\n",
    "                print(f\"same_pre_cluster_all: {same_pre_cluster_all}; same_post_cluster_all: {same_post_cluster_all}\")\n",
    "                print(f\"same_pre_post_cluster_all: {same_pre_post_cluster_all}; no_same_pre_post_cluster_all: {no_same_pre_post_cluster_all}\")\n",
    "\n",
    "                print(f\"same_pre_all_c: {same_pre_all_c}; same_post_all_c: {same_post_all_c}; no_same_pre_post_all_c: {no_same_pre_post_all_c}\")\n",
    "                print(f\"same_pre_cluster_all_c: {same_pre_cluster_all_c}; same_post_cluster_all_c: {same_post_cluster_all_c}\")\n",
    "                print(f\"same_pre_post_cluster_all_c: {same_pre_post_cluster_all_c}; no_same_pre_post_cluster_all_c: {no_same_pre_post_cluster_all_c}\")\n",
    "    \n",
    "            bar_all_lst = [[same_pre_all, same_post_all, no_same_pre_post_all], \n",
    "                           [same_pre_cluster_all, same_post_cluster_all, same_pre_post_cluster_all, no_same_pre_post_cluster_all]]\n",
    "            bar_all_c_lst = [[same_pre_all_c, same_post_all_c, no_same_pre_post_all_c], \n",
    "                           [same_pre_cluster_all_c, same_post_cluster_all_c, same_pre_post_cluster_all_c, no_same_pre_post_cluster_all_c]]\n",
    "            bar_name_lst = [[\"Share-Pre\", \"Share-Post\", \"Neither\"], \n",
    "                           [\"Share-Pre-Cluster\", \"Share-Post-Cluster\", \"Share-Both-Cluster\", \"Neither\"]]\n",
    "\n",
    "            N_cluster = np.max(col_all)\n",
    "            \n",
    "            for idx in range(len(bar_all_lst)):\n",
    "                bar_all = np.array(bar_all_lst[idx])\n",
    "                bar_all_c = np.array(bar_all_c_lst[idx])\n",
    "\n",
    "                over_membership = (bar_all - bar_all_c) / bar_all_c\n",
    "                \n",
    "                axsppshare[idx,G_idx].bar([i for i in range(len(over_membership))], over_membership)\n",
    "                axsppshare[idx,G_idx].set_xticks([i for i in range(len(over_membership))])\n",
    "                axsppshare[idx,G_idx].set_xticklabels(bar_name_lst[idx], rotation=90, ha=\"right\")\n",
    "                axsppshare[idx,G_idx].set_ylabel(\"Over-membership\", fontsize=15)\n",
    "                axsppshare[idx,G_idx].set_title(f\"G = {G}; # Cluster = {N_cluster}\")\n",
    "\n",
    "            axsppshare[0,G_idx].set_title(\"Same Neuron Check\")\n",
    "            axsppshare[1,G_idx].set_title(\"Same Neuron Cluster Check\")\n",
    "\n",
    "            # Oct 21th: next analyze for each individual modulation cluster, the belonging to different individual \n",
    "            # pre and post cluster; only test in the minimal modulation cluster selection case\n",
    "            def value_counts_desc(arr):\n",
    "                \"\"\"\n",
    "                Return dict: {value: count}, sorted by count descending.\n",
    "                \"\"\"\n",
    "                unique, counts = np.unique(arr, return_counts=True)\n",
    "                sorted_idx = np.argsort(-counts)  # sort by count descending\n",
    "                return {unique[i]: counts[i] for i in sorted_idx}\n",
    "\n",
    "            # 2025-11-17: revise to plot for G=300 case\n",
    "            if G_idx == 2: \n",
    "                print(f\"Plot for G={G_lst[G_idx]} Case\")\n",
    "                # for plotting purpose\n",
    "                mp = 5; cnt = 0\n",
    "                # order the modulation cluster size from largest to smallest\n",
    "                # select the ones with large cluster size for the following analysis\n",
    "                all_choice_order_dict = value_counts_desc(col_all)\n",
    "                # 2025-11-04: print the keys of modulation cluster name/labels after \n",
    "                # ordering based on the size from the largest to the smallest\n",
    "                all_choice_order = list(all_choice_order_dict.keys())\n",
    "                print(f\"all_choice_order: {all_choice_order}\")\n",
    "\n",
    "                # 2025-11-04: calculate each cluster size and normalize it\n",
    "                cluster_size_all = []\n",
    "                for cluster_num in all_choice_order:\n",
    "                    idx = np.where(col_all == cluster_num)[0]\n",
    "                    cluster_size_all.append(len(idx))\n",
    "                # 2025-11-04: normalize to percentage\n",
    "                cluster_size_percent = [i / np.sum(cluster_size_all) for i in cluster_size_all]\n",
    "                # 2025-11-17: the summation of percentage is trivially 1\n",
    "                assert np.isclose(np.sum(cluster_size_percent), 1.0)\n",
    "                \n",
    "                for cluster_num in all_choice_order:\n",
    "                    idx = np.where(col_all == cluster_num)[0]\n",
    "\n",
    "                # size of cluster product by input and hidden \n",
    "                in_num = [len(cluster_input[key]) for key in cluster_input.keys()]\n",
    "                hid_num = [len(cluster_hidden[key]) for key in cluster_hidden.keys()]\n",
    "                all_num = np.zeros((len(cluster_input), len(cluster_hidden)))\n",
    "                for ci in range(len(in_num)):\n",
    "                    for ch in range(len(hid_num)):\n",
    "                        all_num[ci,ch] = in_num[ci] * hid_num[ch]\n",
    "                        \n",
    "                figac, axac = plt.subplots(1,1,figsize=(4,4))\n",
    "                sns.heatmap(all_num, ax=axac, cmap=\"coolwarm\")\n",
    "                axac.set_xlabel(\"Hidden Cluster Index\", fontsize=15)\n",
    "                axac.set_ylabel(\"Input Cluster Index\", fontsize=15)\n",
    "                axac.set_title(\"Number of Total Modulation\", fontsize=15)\n",
    "                figac.tight_layout()\n",
    "                figac.savefig(f\"./multiple_tasks/{clustering_name}_inhidpair_num_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)  \n",
    "\n",
    "                corr_lst, om_lst, num_lst = [], [], []\n",
    "                over_membership_lst = []\n",
    "                \n",
    "                figsm, axsm = plt.subplots(2,mp,figsize=(4*mp,4*2))\n",
    "                # 2025-10-27: loop through different modulation cluster\n",
    "                for cidx, cluster_num in enumerate(all_choice_order):\n",
    "                    Z_count = np.zeros((len(cluster_input), len(cluster_hidden)))\n",
    "                    idx = np.where(col_all == cluster_num)[0]\n",
    "                    # for modulation in this cluster, loop through index to check its belonging\n",
    "                    for idx_ in idx: \n",
    "                        # 2025-10-27: slightly abuse the usage of same_pre_cluster; here only \n",
    "                        # extract the pre-cluster and post-cluster identity for idx_ \n",
    "                        pre_i, _, _ = same_pre_cluster(idx_, idx_, M, cluster_input)\n",
    "                        post_i, _, _ = same_post_cluster(idx_, idx_, M, cluster_hidden)\n",
    "                        # register of pre & post cluster belonging \n",
    "                        # -1 is for syntax consistency\n",
    "                        Z_count[pre_i-1, post_i-1] += 1\n",
    "\n",
    "                    # calculate Pearson correlation to the total membership count\n",
    "                    # normalization issue is handled internally within the calculation\n",
    "                    corr = np.corrcoef(all_num.flatten(), Z_count.flatten())[0,1]\n",
    "                    corr_lst.append(corr)\n",
    "                    # calculate over/under-membership, defined as the relative \"exceeding\" \n",
    "                    # compared to the membership by assuming uniform spliting on all_num\n",
    "                    # to different modulation cluster\n",
    "                    # this should captures the modulation-cluster-wise fluctuation \n",
    "                    # in input-hidden-bicluster attendence\n",
    "                    # 2025-11-04: we should revise so that the average division is based on the \n",
    "                    # modulation cluster size (i.e. weighted), not purely uniform devision (1/N_cluster)\n",
    "                    # all_num_avg = all_num / N_cluster\n",
    "                    all_num_avg = all_num * cluster_size_percent[cidx]\n",
    "                    # 2025-11-04: we dont take the absolute value so the over- and under-membership\n",
    "                    # are separately treated\n",
    "                    # 2025-11-17: revise the calculation (center at 1 instead)\n",
    "                    over_membership = Z_count / all_num_avg\n",
    "                    # 2025-11-04: the mean score is calculated through absolute value to prevent\n",
    "                    # mutual cancellation\n",
    "                    om = np.mean(np.abs(over_membership))\n",
    "                    om_lst.append(om); num_lst.append(len(idx))\n",
    "                    over_membership_lst.append(over_membership)\n",
    "                    \n",
    "                    if cnt < mp: \n",
    "                        sns.heatmap(Z_count, ax=axsm[0,cnt], cmap=\"coolwarm\")\n",
    "                        sns.heatmap(over_membership, ax=axsm[1,cnt], cmap=\"coolwarm\", vmin=0, vmax=2)\n",
    "                        for axindex in range(2): \n",
    "                            axsm[axindex,cnt].set_xlabel(\"Hidden Cluster Index\", fontsize=15)\n",
    "                            axsm[axindex,cnt].set_ylabel(\"Input Cluster Index\", fontsize=15)\n",
    "                        axsm[0,cnt].set_title(f\"Modulation Cluster {cluster_num}; corr: {corr:.3f}\", fontsize=12)\n",
    "                        axsm[1,cnt].set_title(f\"Modulation Cluster {cluster_num}; om: {om:.3f}\", fontsize=12)\n",
    "                        cnt += 1\n",
    "                        \n",
    "                figsm.tight_layout()\n",
    "                figsm.savefig(f\"./multiple_tasks/{clustering_name}_specific_case_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)  \n",
    "\n",
    "                figomcluster, axsomcluster = plt.subplots(4,1,figsize=(10,4*4))\n",
    "                \n",
    "                over_membership_array_all = np.array([arr.flatten() for arr in over_membership_lst])\n",
    "                # 2025-11-04: calculate the average over/under-membership for input & hidden cluster\n",
    "                over_membership_array_input = np.array([np.mean(np.abs(arr), axis=1) for arr in over_membership_lst])\n",
    "                over_membership_array_hidden = np.array([np.mean(np.abs(arr), axis=0) for arr in over_membership_lst])\n",
    "\n",
    "                over_membership_array = [over_membership_array_all, over_membership_array_input, over_membership_array_hidden]\n",
    "\n",
    "                for omindex, over_membership_array_ in enumerate(over_membership_array): \n",
    "                    cluster_colors = color_func.rainbow_generate(over_membership_array_.shape[1])\n",
    "                    for cindex in range(over_membership_array_.shape[1]):\n",
    "                        axsomcluster[omindex].plot(over_membership_array_[:,cindex], color=cluster_colors[cindex], alpha=0.5)\n",
    "                    \n",
    "                axsomcluster[0].set_ylabel(\"Overmembership at Bi-Cluster\", fontsize=15)\n",
    "                axsomcluster[1].set_ylabel(\"Mean Overmembership at Input-Cluster\", fontsize=15)\n",
    "                axsomcluster[2].set_ylabel(\"Mean Overmembership at Hidden-Cluster\", fontsize=15)\n",
    "                \n",
    "                axsomcluster[3].plot(cluster_size_percent, \"-o\", linewidth=2, color=cluster_colors[0])\n",
    "                axsomcluster[3].axhline(1 / len(cluster_size_percent), linestyle=\"--\")\n",
    "                axsomcluster[3].set_ylabel(\"Cluster Size (Normalized)\", fontsize=15)\n",
    "                for axomcluster in axsomcluster: \n",
    "                    axomcluster.set_xlabel(\"Modulation Cluster Order (Largest to Smallest)\", fontsize=15)\n",
    "                for indx in range(3):\n",
    "                    axsomcluster[indx].set_xlim([0, 5])\n",
    "                    axsomcluster[indx].set_ylim([0, 3])\n",
    "                figomcluster.tight_layout()\n",
    "                figomcluster.savefig(f\"./multiple_tasks/{clustering_name}_om_across_cluster_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)  \n",
    "                \n",
    "                figgroupcorr, axsgroupcorr = plt.subplots(1,5,figsize=(4*5,4))\n",
    "                axsgroupcorr[0].hist(corr_lst, bins=\"auto\")\n",
    "                axsgroupcorr[0].set_xlabel(\"Correlation per Group\", fontsize=15)\n",
    "                axsgroupcorr[1].hist(om_lst, bins=\"auto\")\n",
    "                axsgroupcorr[1].set_xlabel(\"Mean Absolute Average OM\", fontsize=15)\n",
    "                for index in range(len(corr_lst)):\n",
    "                    axsgroupcorr[2].scatter(corr_lst[index], om_lst[index], c=c_vals[0], alpha=0.7)\n",
    "                    axsgroupcorr[3].scatter(num_lst[index], corr_lst[index], c=c_vals[0], alpha=0.7)\n",
    "                    axsgroupcorr[4].scatter(num_lst[index], om_lst[index], c=c_vals[0], alpha=0.7)\n",
    "\n",
    "                # 2025-11-04: do a linear regression\n",
    "                x_fit, y_fit, _, _, _, _ = helper.linear_regression(num_lst, corr_lst, log=False, through_origin=False)\n",
    "                axsgroupcorr[3].plot(x_fit, y_fit)\n",
    "                x_fit2, y_fit2, _, _, _, _ = helper.linear_regression(num_lst, om_lst, log=False, through_origin=False)\n",
    "                axsgroupcorr[4].plot(x_fit2, y_fit2)\n",
    "                    \n",
    "                axsgroupcorr[2].set_xlabel(\"Corr\", fontsize=15)\n",
    "                axsgroupcorr[2].set_ylabel(\"OM\", fontsize=15)\n",
    "                axsgroupcorr[3].set_xlabel(\"Modulation Cluster Size\", fontsize=15)\n",
    "                axsgroupcorr[3].set_ylabel(\"Corr\", fontsize=15)\n",
    "                axsgroupcorr[4].set_xlabel(\"Modulation Cluster Size\", fontsize=15)\n",
    "                axsgroupcorr[4].set_ylabel(\"OM\", fontsize=15)\n",
    "                \n",
    "                figgroupcorr.tight_layout()\n",
    "                figgroupcorr.savefig(f\"./multiple_tasks/{clustering_name}_corr_allgroup_case_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)  \n",
    "\n",
    "        figcol.tight_layout()\n",
    "        figcol.savefig(f\"./multiple_tasks/{clustering_name}_allneuron_grouplength_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)  \n",
    "        figppshare.tight_layout()\n",
    "        figppshare.savefig(f\"./multiple_tasks/{clustering_name}_prepost_belonging_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)  \n",
    "\n",
    "        # all following analysis based on the maximal G (G_lst[-1])\n",
    "        cell_vars_rules_sorted_norm_pre = cell_vars_rules_sorted_norm[np.ix_(result_pre[\"row_order\"], result_pre[\"col_order\"])]\n",
    "        cell_vars_rules_sorted_norm_post = cell_vars_rules_sorted_norm[np.ix_(result_post[\"row_order\"], result_post[\"col_order\"])]\n",
    "\n",
    "        # 2025-11-12: for all G results (after reordering)\n",
    "        # up to here, the covariance matrix is determined. \n",
    "        cell_vars_rules_sorted_norm_all_lst = []\n",
    "        for result_all_ in result_all_lst:\n",
    "            cell_vars_rules_sorted_norm_all = cell_vars_rules_sorted_norm[np.ix_(result_all_[\"row_order\"], result_all_[\"col_order\"])]\n",
    "            cell_vars_rules_sorted_norm_all_lst.append(cell_vars_rules_sorted_norm_all)\n",
    "\n",
    "        clustering_data_hierarchy[\"modulation_all_pre\"] = result_pre[\"col_linkage\"]\n",
    "        clustering_data_hierarchy[\"modulation_all_post\"] = result_post[\"col_linkage\"]\n",
    "\n",
    "        # check if length is consistent\n",
    "        assert len(G_lst) == len(result_all_lst)\n",
    "\n",
    "        # plot original, pre, post, plus all results under different G\n",
    "        tf = 3 + len(result_all_lst)\n",
    "        figprepost, axprepost = plt.subplots(tf,1,figsize=(24,8*tf))\n",
    "        sns.heatmap(cell_vars_rules_sorted_norm, ax=axprepost[0], cmap=\"coolwarm\", cbar=True, vmin=0, vmax=1)\n",
    "        sns.heatmap(cell_vars_rules_sorted_norm_pre, ax=axprepost[1], cmap=\"coolwarm\", cbar=True, vmin=0, vmax=1)\n",
    "        sns.heatmap(cell_vars_rules_sorted_norm_post, ax=axprepost[2], cmap=\"coolwarm\", cbar=True, vmin=0, vmax=1)\n",
    "\n",
    "        # plot the headmap for all G result\n",
    "        for k in range(len(cell_vars_rules_sorted_norm_all_lst)):\n",
    "            sns.heatmap(cell_vars_rules_sorted_norm_all_lst[k], ax=axprepost[3+k], cmap=\"coolwarm\", cbar=True, vmin=0, vmax=1)\n",
    "\n",
    "        for ax in axprepost:\n",
    "            ax.set_yticks(np.arange(len(tb_break_name)))\n",
    "            \n",
    "        axprepost[0].set_yticklabels(tb_break_name, rotation=0, ha='right', va='center', fontsize=9)\n",
    "        axprepost[1].set_yticklabels(tb_break_name[result_pre[\"row_order\"]], rotation=0, ha='right', va='center', fontsize=9)\n",
    "        axprepost[2].set_yticklabels(tb_break_name[result_post[\"row_order\"]], rotation=0, ha='right', va='center', fontsize=9)\n",
    "\n",
    "        # plot the row\n",
    "        # 2025-10-30: modulation, clustering by G groups from MinibatchKmeans, \"border\" between clusters\n",
    "        modulation_cluster_norm = []\n",
    "        for k in range(len(cell_vars_rules_sorted_norm_all_lst)):\n",
    "            axprepost[3+k].set_yticklabels(tb_break_name[result_all_lst[k][\"row_order\"]], rotation=0, ha='right', va='center', fontsize=9)\n",
    "            result_all = result_all_lst[k]\n",
    "            rl_all = np.asarray(result_all[\"row_labels\"])[result_all[\"row_order\"]]\n",
    "            cl_all = np.asarray(result_all[\"col_labels\"])[result_all[\"col_order\"]]\n",
    "            rbreaks_all_ = clustering._breaks(rl_all)\n",
    "            cbreaks_all_ = clustering._breaks(cl_all)\n",
    "            for rb in rbreaks_all_:\n",
    "                axprepost[3+k].axhline(rb, color=\"k\", lw=0.6)\n",
    "            for cb in cbreaks_all_:\n",
    "                axprepost[3+k].axvline(cb, color=\"k\", lw=0.6)\n",
    "\n",
    "            # 2025-11-12: calculate the mean of the reordered covaraince matrix under different G value\n",
    "            # notice G indicates the group size, not the actual \n",
    "            rbreaks_all_end = [0] + rbreaks_all_ + [cell_vars_rules_sorted_norm_all_lst[0].shape[0]]\n",
    "            cbreaks_all_end = [0] + cbreaks_all_ + [cell_vars_rules_sorted_norm_all_lst[0].shape[1]]\n",
    "\n",
    "            varmeanmod = np.zeros((len(rbreaks_all_end) - 1, len(cbreaks_all_end) - 1))\n",
    "            for rr in range(len(rbreaks_all_end) - 1):\n",
    "                for cc in range(len(cbreaks_all_end) - 1):\n",
    "                    varmeanmod[rr,cc] = np.mean(cell_vars_rules_sorted_norm_all_lst[k][rbreaks_all_end[rr]:rbreaks_all_end[rr+1], \n",
    "                                                  cbreaks_all_end[cc]:cbreaks_all_end[cc+1]])\n",
    "\n",
    "            modulation_cluster_norm.append(varmeanmod)            \n",
    "            \n",
    "        axprepost[0].set_title(\"Original\", fontsize=15)\n",
    "        axprepost[1].set_title(f\"Group by Pre\", fontsize=15)\n",
    "        axprepost[2].set_title(f\"Group by Post\", fontsize=15)\n",
    "\n",
    "        # 2025-10-30: modulation, clustering by pre/post, \"border\" between clusters\n",
    "        rl_pre = np.asarray(result_pre[\"row_labels\"])[result_pre[\"row_order\"]]\n",
    "        cl_pre = np.asarray(result_pre[\"col_labels\"])[result_pre[\"col_order\"]]\n",
    "        rbreaks_pre = clustering._breaks(rl_pre)\n",
    "        cbreaks_pre = clustering._breaks(cl_pre)\n",
    "        for rb in rbreaks_pre:\n",
    "            axprepost[1].axhline(rb, color=\"k\", lw=0.6)\n",
    "        for cb in cbreaks_pre:\n",
    "            axprepost[1].axvline(cb, color=\"k\", lw=0.6)\n",
    "\n",
    "        rl_post = np.asarray(result_post[\"row_labels\"])[result_post[\"row_order\"]]\n",
    "        cl_post = np.asarray(result_post[\"col_labels\"])[result_post[\"col_order\"]]\n",
    "        rbreaks_post = clustering._breaks(rl_post)\n",
    "        cbreaks_post = clustering._breaks(cl_post)\n",
    "        for rb in rbreaks_post:\n",
    "            axprepost[2].axhline(rb, color=\"k\", lw=0.6)\n",
    "        for cb in cbreaks_post:\n",
    "            axprepost[2].axvline(cb, color=\"k\", lw=0.6)\n",
    "        \n",
    "        for k in range(len(cell_vars_rules_sorted_norm_all_lst)):\n",
    "            axprepost[3+k].set_title(f\"Group by All, G={G_lst[k]}\", fontsize=15)\n",
    "            \n",
    "        figprepost.tight_layout()\n",
    "        figprepost.savefig(f\"./multiple_tasks/{clustering_name}_bygroup_variance_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "\n",
    "        figmodnorm, axsmodnorm = plt.subplots(len(modulation_cluster_norm),1,figsize=(10,4*len(modulation_cluster_norm)))\n",
    "        for idx in range(len(modulation_cluster_norm)):\n",
    "            sns.heatmap(modulation_cluster_norm[idx], ax=axsmodnorm[idx], cmap=\"coolwarm\")\n",
    "            axsmodnorm[idx].set_xlabel(\"Modulation Index\", fontsize=15)\n",
    "            axsmodnorm[idx].set_ylabel(\"Session Index\", fontsize=15)\n",
    "\n",
    "        figmodnorm.tight_layout()\n",
    "        figmodnorm.savefig(f\"./multiple_tasks/{clustering_name}_modulation_clusternorm_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "\n",
    "        fig, axs = plt.subplots(tf,1,figsize=(25*1,10*tf))\n",
    "        # pre-neuron modulation clustering across session\n",
    "        dendrogram(result_pre[\"row_linkage\"], ax=axs[0], labels=tb_break_name[result_pre[\"row_order\"]], leaf_rotation=90)\n",
    "        axs[0].axhline(result_pre[\"row_best_cut_distance\"], linestyle=\"--\", color=\"black\", linewidth=3)\n",
    "        axs[0].set_title(f\"Modulation Pre Row hierarchy (k = {result_pre['row_k']})\", fontsize=15)\n",
    "        # pre-neuron modulation clustering across neuron\n",
    "        dendrogram(result_pre[\"col_linkage\"], ax=axs[1], no_labels=True, leaf_rotation=90)\n",
    "        axs[1].axhline(result_pre[\"col_best_cut_distance\"], linestyle=\"--\", color=\"black\", linewidth=3)\n",
    "        axs[1].axhline(result_pre[\"col_cut_distance_by_k\"][select_col_k], linestyle=\"--\", color=c_vals[0], linewidth=3)\n",
    "        axs[1].set_title(f\"Modulation Pre Col hierarchy (k = {result_pre['col_k']})\", fontsize=15)\n",
    "        # post-neuron modulation clustering across neuron \n",
    "        dendrogram(result_post[\"col_linkage\"], ax=axs[2], no_labels=True, leaf_rotation=90)\n",
    "        axs[2].axhline(result_post[\"col_best_cut_distance\"], linestyle=\"--\", color=\"black\", linewidth=3)\n",
    "        axs[2].axhline(result_post[\"col_cut_distance_by_k\"][select_col_k], linestyle=\"--\", color=c_vals[0], linewidth=3)\n",
    "        axs[2].set_title(f\"Modulation Post Col hierarchy (k = {result_post['col_k']})\", fontsize=15)\n",
    "        # all-neuron modulation clustering across neuron\n",
    "        for k in range(len(result_all_lst)):\n",
    "            dendrogram(result_all_lst[k][\"col_linkage\"], ax=axs[3+k], no_labels=True, leaf_rotation=90)\n",
    "            axs[3+k].axhline(result_all_lst[k][\"col_best_cut_distance\"], linestyle=\"--\", color=\"black\", linewidth=3)\n",
    "            axs[3+k].axhline(result_all_lst[k][\"col_cut_distance_by_k\"][select_col_k], linestyle=\"--\", color=c_vals[0], linewidth=3)\n",
    "            axs[3+k].set_title(f\"Modulation All Col hierarchy (k = {result_all_lst[k]['col_k']}); G={G_lst[k]}\", fontsize=15)\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f\"./multiple_tasks/{clustering_name}_bygroup_variance_hierarchy_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300) \n",
    "\n",
    "        figscore, axscore = plt.subplots(1,1,figsize=(10,3))\n",
    "        # axscore.plot(result_pre[\"row_score_recording\"].keys(), result_pre[\"row_score_recording\"].values(), \\\n",
    "                     # label=\"row pre\", color=c_vals[0])\n",
    "        axscore.plot(result_pre[\"col_score_recording\"].keys(), result_pre[\"col_score_recording\"].values(), \\\n",
    "                     label=\"col pre\", color=c_vals[1])\n",
    "        # axscore.plot(result_post[\"row_score_recording\"].keys(), result_post[\"row_score_recording\"].values(), \\\n",
    "                     # label=\"row post\", color=c_vals[2])\n",
    "        axscore.plot(result_post[\"col_score_recording\"].keys(), result_post[\"col_score_recording\"].values(), \\\n",
    "                     label=\"col post\", color=c_vals[3])\n",
    "        for k in range(len(result_all_lst)): \n",
    "            # axscore.plot(result_all_lst[k][\"row_score_recording\"].keys(), result_all_lst[k][\"row_score_recording\"].values(), \n",
    "                         # label=\"row all\", color=c_vals[4+k], linestyle=\"--\")\n",
    "            axscore.plot(result_all_lst[k][\"col_score_recording\"].keys(), result_all_lst[k][\"col_score_recording\"].values(), \\\n",
    "                         label=f\"col all, G={G_lst[k]}\", color=c_vals[5+k], linestyle=\"--\")\n",
    "        axscore.set_xlabel(\"Number of Cluster\", fontsize=15)\n",
    "        axscore.set_ylabel(\"Silhouette Score\", fontsize=15)\n",
    "        axscore.legend()\n",
    "        axscore.set_xscale(\"log\")\n",
    "        figscore.tight_layout()\n",
    "        figscore.savefig(f\"./multiple_tasks/{clustering_name}_variance_cluster_score_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "\n",
    "        input_info, hidden_info = base_data[0], base_data[1]\n",
    "\n",
    "        # compare modulation grouping result\n",
    "        # either using pre, post, or different level of K-means pre-clusters\n",
    "        all_mod = [result_pre, result_post] + result_all_lst\n",
    "        all_mod_name = [\"Pre\", \"Post\"] + result_all_name_lst\n",
    "\n",
    "        all_mod_metric_allk = []\n",
    "        # loop through downsampled clusters along the neuron dimension\n",
    "        select_col_allk = np.arange(lower_cluster, 15)\n",
    "        for select_col_k_ in select_col_allk:\n",
    "            all_mod_metric = []\n",
    "            for mod_ in all_mod: \n",
    "                eval_res_modulation = clustering_metric.evaluate_bicluster_clustering(\n",
    "                    cell_vars_rules_sorted_norm, row_labels=mod_[\"row_labels_by_k\"][select_col_k_], \n",
    "                    col_labels=mod_[\"col_labels_by_k\"][select_col_k_]\n",
    "                )\n",
    "                \n",
    "                all_mod_metric.append([\n",
    "                    eval_res_modulation[\"metrics\"][\"CH_blocks\"],\n",
    "                    eval_res_modulation[\"metrics\"][\"DB_blocks\"],\n",
    "                    eval_res_modulation[\"metrics\"][\"XB_blocks\"], \n",
    "                ])\n",
    "\n",
    "            all_mod_metric_allk.append(all_mod_metric)\n",
    "\n",
    "        all_mod_metric_allk = np.array(all_mod_metric_allk)\n",
    "        print(all_mod_metric_allk.shape)\n",
    "        \n",
    "        # compare the effect by clustering based on presynaptic neuron, postsynaptic neuron, or all\n",
    "        # hypothesis: all >≈ postsynaptic > presynaptic\n",
    "        fig, ax = plt.subplots(1,all_mod_metric_allk.shape[-1], figsize=(4*all_mod_metric_allk.shape[-1], 4))\n",
    "        for metric_index in range(all_mod_metric_allk.shape[2]):\n",
    "            for model_index in range(all_mod_metric_allk.shape[1]):\n",
    "                ax[metric_index].plot([j for j in range(len(select_col_allk))], all_mod_metric_allk[:,model_index,metric_index], \\\n",
    "                           \"-o\", label=all_mod_name[model_index], color=c_vals[model_index])\n",
    "                ax[metric_index].set_xticks([j for j in range(len(select_col_allk))])\n",
    "                ax[metric_index].set_xticklabels(select_col_allk)\n",
    "                \n",
    "        ax[0].set_ylabel(\"Calinski Harabasz\", fontsize=15)\n",
    "        ax[1].set_ylabel(\"Davies Bouldin\", fontsize=15)\n",
    "        ax[2].set_ylabel(\"Xie-Beni\", fontsize=15)\n",
    "        \n",
    "        for ax_ in ax:\n",
    "            ax_.legend()\n",
    "            ax_.set_xlabel(\"# Cluster\", fontsize=15)\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f\"./multiple_tasks/between_modulation_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8d8bc2-acb2-4686-9f12-6f4afe225348",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d82038-9aa4-4a2f-b3b7-a7f9a52c9b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_num = 5\n",
    "modulation_norm = modulation_cluster_norm[2]\n",
    "print(f\"modulation_norm: {modulation_norm.shape}\")\n",
    "assert modulation_norm.shape[1] == max(all_choice_order)\n",
    "print(np.min(all_choice_order))\n",
    "\n",
    "fig, axs = plt.subplots(3,plot_num,figsize=(4*plot_num,4*3))\n",
    "for i in range(plot_num):\n",
    "    cluster_name = all_choice_order[i] - 1\n",
    "    modulation_norm_cl = modulation_norm[:,cluster_name].reshape(-1,1)\n",
    "    data = over_membership_lst[i]\n",
    "    hidden_mean = np.mean(data, axis=0)\n",
    "    input_mean = np.mean(data, axis=1)\n",
    "    mean, std = np.mean(hidden_mean), np.std(hidden_mean) / np.sqrt(len(hidden_mean))\n",
    "    idx = np.where(hidden_mean > 2)[0]\n",
    "    axs[0,i].plot(hidden_mean, \"-o\", color=c_vals[0])\n",
    "    # axs[0,i].axhline(mean, linestyle=\"--\", linewidth=2)\n",
    "    # axs[0,i].axhline(2 * std, linestyle=\"--\", linewidth=2)\n",
    "    axs[0,i].set_title(f\"{idx + 1}\")\n",
    "    # axs[0,i].set_title(f\"Cluster Name: {cluster_name}\")\n",
    "    idx = np.where(input_mean > 2)[0]\n",
    "    axs[1,i].plot(input_mean, \"-o\", color=c_vals[0])\n",
    "    axs[1,i].set_title(f\"{idx + 1}\")\n",
    "    sns.heatmap(modulation_norm_cl, ax=axs[2,i], cmap=\"coolwarm\", vmin=np.min(modulation_norm), vmax=np.max(modulation_norm))\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c487802-47e8-445e-86cc-2e609bd2fae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2025-11-20: filter out some small modulation cluster\n",
    "valid_mod_cluster = np.array(cluster_size_percent) > (1 / len(cluster_size_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4c19df-939b-4a68-90d4-f951b5043a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2025-11-20: filter out some small input * hidden bi-cluster\n",
    "inp_hidden_mask = all_num > (np.sum(all_num) / (all_num.shape[0] * all_num.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c1bd18-77d4-4095-832e-ea288d7fb164",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lst = []\n",
    "data_all = [[], []]\n",
    "fig, axs = plt.subplots(2,plot_num,figsize=(4*plot_num,4*2))\n",
    "for i in range(len(over_membership_lst)):\n",
    "    data = over_membership_lst[i]\n",
    "    xx, yy = activecorr.flatten(), data.flatten()\n",
    "    if valid_mod_cluster[i]: \n",
    "        data_all[0].extend(list(activecorr[inp_hidden_mask].flatten())); \n",
    "        data_all[1].extend(list(data[inp_hidden_mask].flatten()))\n",
    "    x_fit, y_fit, r_value, slope, _, p_val = helper.linear_regression(xx, yy, log=False, through_origin=False)\n",
    "    data_lst.append([slope, p_val])\n",
    "    if i < plot_num: \n",
    "        sns.kdeplot(x=xx, y=yy, fill=True, cmap='coolwarm', ax=axs[0,i], levels=200, thresh=0, bw_adjust=0.5, gridsize=300)\n",
    "        axs[1,i].scatter(xx, yy, alpha=0.5, color=c_vals[0])\n",
    "        axs[1,i].plot(x_fit, y_fit, label=f\"R={r_value:.3f}; Slope: {slope:.3f}; p-value: {p_val:.3f}\")\n",
    "        axs[1,i].legend()\n",
    "        for j in range(2):\n",
    "            axs[j,i].set_xlabel(\"Co-activation Level\", fontsize=15)\n",
    "            axs[j,i].set_ylabel(\"Overmembership Level\", fontsize=15)\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"./multiple_tasks/inputhiddencoactive_om_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6b6302-5b55-483c-817b-930057b80b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(6,6))\n",
    "ax.scatter(data_all[0], data_all[1], alpha=0.5, s=10, color=c_vals[0])\n",
    "# ax.set_yscale(\"log\")\n",
    "ax.set_ylim([0, 5])\n",
    "ax.set_xlabel(\"Coactivation Level\", fontsize=15)\n",
    "ax.set_ylabel(\"Modulation-Cluster-Specific Overmembership\", fontsize=15)\n",
    "ax.axhline(1, color=c_vals[1])\n",
    "\n",
    "unique_x = np.unique(data_all[0])\n",
    "median_y = [np.median(np.array(data_all[1])[data_all[0] == ux]) for ux in unique_x]\n",
    "ax.plot(unique_x, median_y, color=c_vals[2], linewidth=2, label=\"Median Y given X\")\n",
    "ax.legend(loc=\"best\", frameon=True, fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767cadd3-8fe7-4e11-99d5-a822b5bfbe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_lst = np.linspace(1,3,100)\n",
    "fig, axs = plt.subplots(1,4,figsize=(4*4,4))\n",
    "\n",
    "fits = []\n",
    "for cindex, cutoff in enumerate(cutoff_lst): \n",
    "    prop_y_gt2 = []\n",
    "    \n",
    "    for ux in unique_x:\n",
    "        y_vals = np.array(data_all[1])[np.array(data_all[0]) == ux]\n",
    "        prop_y_gt2.append(np.mean(y_vals > cutoff))  \n",
    "\n",
    "    x_fit, y_fit, r_value, slope, _, p_val = helper.linear_regression(unique_x, prop_y_gt2, \n",
    "                                                                      log=False, through_origin=False)\n",
    "    fits.append([r_value, slope])\n",
    "\n",
    "    if cindex % 25 == 0: \n",
    "        cindex = int(cindex/25)\n",
    "        axs[cindex].scatter(unique_x, prop_y_gt2, c=c_vals[0])\n",
    "        axs[cindex].set_xlabel(\"Coactivation Level\", fontsize=15)\n",
    "        axs[cindex].set_ylabel(\"Percentage of Overmembership\", fontsize=15)\n",
    "        axs[cindex].plot(x_fit, y_fit, linestyle=\"--\", label=f\"r_value: {r_value:.3f}; slope: {slope:.3f}; \")\n",
    "        axs[cindex].legend()\n",
    "        axs[cindex].set_title(f\"cutoff: {cutoff}\", fontsize=15)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"./multiple_tasks/coact_om_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "\n",
    "fits = np.array(fits)\n",
    "figfit, axsfit = plt.subplots(1,2,figsize=(4*2,4))\n",
    "axsfit[0].plot(cutoff_lst, fits[:,0])\n",
    "axsfit[1].plot(cutoff_lst, fits[:,1])\n",
    "axsfit[0].set_xlabel(\"Threshold\", fontsize=15)\n",
    "axsfit[1].set_xlabel(\"Threshold\", fontsize=15)\n",
    "axsfit[0].set_ylabel(\"R-Value\", fontsize=15)\n",
    "axsfit[1].set_ylabel(\"Slope\", fontsize=15)\n",
    "figfit.tight_layout()\n",
    "figfit.savefig(f\"./multiple_tasks/coact_om_threshold_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12075952-5af8-4389-9ad4-b4ed8d5f8b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_to_indices_fix = [\n",
    "    (\"fix1\",    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])\n",
    "]\n",
    "\n",
    "tb_break_fix = [\n",
    "    [idx, rules_epochs[all_rules[idx]][phase]]\n",
    "    for phase, indices in phase_to_indices_fix\n",
    "    for idx in indices\n",
    "]\n",
    "\n",
    "tb_break_fix_name = [\n",
    "    f\"{all_rules[idx]}\"\n",
    "    for phase, indices in phase_to_indices_fix\n",
    "    for idx in indices\n",
    "]\n",
    "\n",
    "clustering_datas = [xs, hs, Ms_orig]\n",
    "clustering_data_names = [\"input\", \"hidden\", \"modulation\"]\n",
    "sindex = 1\n",
    "\n",
    "states = []\n",
    "for el in range(len(tb_break_fix)):        \n",
    "    rule_idx, period_time = tb_break[el][0], tb_break[el][1]\n",
    "    if sindex in (0, 1,):\n",
    "        rule_context_final_state = clustering_datas[sindex][test_task == rule_idx, period_time[1], :]\n",
    "    else:\n",
    "        rule_context_final_state = clustering_datas[sindex][test_task == rule_idx, period_time[1], :, :]\n",
    "        rule_context_final_state = rule_context_final_state.reshape(rule_context_final_state.shape[0], -1)\n",
    "    states.append(rule_context_final_state)\n",
    "\n",
    "print(tb_break_fix_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595b0913-6874-4587-ab6c-2844a5640cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = np.vstack(states)              \n",
    "N_per_task = [s.shape[0] for s in states]\n",
    "\n",
    "mean_vec = X_all.mean(axis=0, keepdims=True)\n",
    "X_centered = X_all - mean_vec      \n",
    "\n",
    "U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)\n",
    "W = Vt[:2].T                         \n",
    "\n",
    "Z_all = X_centered @ W                 \n",
    "\n",
    "Z_tasks = []\n",
    "start = 0\n",
    "for n in N_per_task:\n",
    "    Z_tasks.append(Z_all[start:start+n])   \n",
    "    start += n\n",
    "\n",
    "print(len(Z_tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644226ce-c77a-414a-96e1-73e640eb2392",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Circle\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "\n",
    "for k, Zk in enumerate(Z_tasks):\n",
    "    color = c_vals[k % len(c_vals)]\n",
    "    ax.scatter(Zk[:, 0], Zk[:, 1], s=20, alpha=0.5, color=color)\n",
    "\n",
    "    med = np.mean(Zk, axis=0)   \n",
    "    med_x, med_y = med[0], med[1]\n",
    "\n",
    "    ax.scatter(med_x, med_y, s=150, color=color, edgecolor=\"black\",\n",
    "               marker=\"X\", linewidth=1.5, zorder=5)\n",
    "\n",
    "    dists = np.linalg.norm(Zk - med, axis=1)\n",
    "    radius = dists.max()\n",
    "\n",
    "    circle = Circle(\n",
    "        (med_x, med_y),\n",
    "        radius,\n",
    "        edgecolor=color,\n",
    "        facecolor=\"none\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=2,\n",
    "        alpha=0.8\n",
    "    )\n",
    "    ax.add_patch(circle)\n",
    "\n",
    "    label = f\"{tb_break_fix_name[k]} (med=({med_x:.2f}, {med_y:.2f}))\"\n",
    "\n",
    "    ax.scatter([], [], color=color, marker=\"X\", s=150, edgecolor=\"black\",\n",
    "               label=label)\n",
    "\n",
    "ax.set_xlabel(\"Context endpt. state PC1\", fontsize=15)\n",
    "ax.set_ylabel(\"Context endpt. state PC2\", fontsize=15)\n",
    "ax.set_title(f\"{clustering_data_names[sindex]}\", fontsize=15)\n",
    "ax.legend(frameon=True, fontsize=12)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c3c2ef-bdcd-4d32-bd30-99634d9a2a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33a9769-7d93-46f0-b6c9-222fae6c296c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mpn)",
   "language": "python",
   "name": "mpn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
