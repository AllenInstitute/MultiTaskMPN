{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "374fb8f2-9b83-44ce-821b-8917a114c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# PyTorch Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Data Handling and Image Processing\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Style for Matplotlib\n",
    "import scienceplots\n",
    "plt.style.use('science')\n",
    "plt.style.use(['no-latex'])\n",
    "\n",
    "# Scientific Computing and Machine Learning\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.linalg import subspace_angles\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Custom Modules and Extensions\n",
    "sys.path.append(\"../netrep/\")\n",
    "sys.path.append(\"../svcca/\")\n",
    "import cca_core\n",
    "from netrep.metrics import LinearMetric\n",
    "import networks as nets  # Contains RNNs\n",
    "import net_helpers\n",
    "import tasks\n",
    "import helper\n",
    "import mpn\n",
    "\n",
    "import scienceplots\n",
    "plt.style.use('science')\n",
    "plt.style.use(['no-latex'])\n",
    "\n",
    "# Memory Optimization\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd1556c8-a4b6-434b-a60f-37035980bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 Red, 1 blue, 2 green, 3 purple, 4 orange, 5 teal, 6 gray, 7 pink, 8 yellow\n",
    "c_vals = ['#e53e3e', '#3182ce', '#38a169', '#805ad5','#dd6b20', '#319795', '#718096', '#d53f8c', '#d69e2e',]\n",
    "c_vals_l = ['#feb2b2', '#90cdf4', '#9ae6b4', '#d6bcfa', '#fbd38d', '#81e6d9', '#e2e8f0', '#fbb6ce', '#faf089',]\n",
    "c_vals_d = ['#9b2c2c', '#2c5282', '#276749', '#553c9a', '#9c4221', '#285e61', '#2d3748', '#97266d', '#975a16',]\n",
    "l_vals = ['solid', 'dashed', 'dotted', 'dashdot', '-', '--', '-.', ':', (0, (3, 1, 1, 1)), (0, (5, 10))]\n",
    "markers_vals = ['o', 'v', '^', '<', '>', '1', '2', '3', '4', 's', 'p', '*', 'h', 'H', '+', 'x', 'D', 'd', '|', '_']\n",
    "hyp_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34978bf6-67b8-41bd-a022-a7b46a320686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set seed 65\n",
      "Fixation_off: True; Task_info: True\n",
      "Rules: ['delaydm1']\n",
      "  Input size 7, Output size 3\n",
      "Using CUDA...\n"
     ]
    }
   ],
   "source": [
    "# Reload modules if changes have been made to them\n",
    "from importlib import reload\n",
    "\n",
    "reload(nets)\n",
    "reload(net_helpers)\n",
    "\n",
    "fixseed = False # randomize setting the seed may lead to not perfectly solved results\n",
    "seed = random.randint(1,1000) if not fixseed else 8 # random set the seed to test robustness by default\n",
    "print(f\"Set seed {seed}\")\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "hyp_dict['task_type'] = 'multitask' # int, NeuroGym, multitask\n",
    "hyp_dict['mode_for_all'] = \"random_batch\"\n",
    "hyp_dict['ruleset'] = 'delaydm1' # low_dim, all, test\n",
    "\n",
    "accept_rules = ('fdgo', 'fdanti', 'delaygo', 'delayanti', 'reactgo', 'reactanti', \n",
    "                'delaydm1', 'delaydm2', 'dmsgo', 'dmcgo', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm', 'dm1')\n",
    "\n",
    "\n",
    "rules_dict = \\\n",
    "    {'all' : ['fdgo', 'reactgo', 'delaygo', 'fdanti', 'reactanti', 'delayanti',\n",
    "              'dm1', 'dm2', 'contextdm1', 'contextdm2', 'multidm',\n",
    "              'delaydm1', 'delaydm2', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm',\n",
    "              'dmsgo', 'dmsnogo', 'dmcgo', 'dmcnogo'],\n",
    "     'low_dim' : ['fdgo', 'reactgo', 'delaygo', 'fdanti', 'reactanti', 'delayanti',\n",
    "                 'delaydm1', 'delaydm2', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm',\n",
    "                 'dmsgo', 'dmsnogo', 'dmcgo', 'dmcnogo'],\n",
    "\n",
    "     'gofamily': ['fdgo', 'fdanti', 'reactgo', 'reactanti', 'delaygo', 'delayanti'],\n",
    "\n",
    "     'delaygo': ['delaygo'],\n",
    "     'delaygofamily': ['delaygo', 'delayanti'],\n",
    "     'fdgo': ['fdgo'],\n",
    "     'fdfamily': ['fdgo', 'fdanti'],\n",
    "     'reactgo': ['reactgo'],\n",
    "     'reactfamily': ['reactgo', 'reactanti'],\n",
    "     \n",
    "     'delaydm1': ['delaydm1'],\n",
    "     'delaydmfamily': ['delaydm1', 'delaydm2'],\n",
    "     \n",
    "     'dmsgofamily': ['dmsgo', 'dmsnogo'],\n",
    "     'dmsgo': ['dmsgo'],\n",
    "     'dmcgo': ['dmcgo'],\n",
    "     'contextdelaydm1': ['contextdelaydm1'], \n",
    "     'contextdelayfamily': ['contextdelaydm1', 'contextdelaydm2'],\n",
    "     'dm1': ['dm1']\n",
    "    }\n",
    "    \n",
    "\n",
    "# This can either be used to set parameters OR set parameters and train\n",
    "train = True # whether or not to train the network\n",
    "verbose = True\n",
    "hyp_dict['run_mode'] = 'minimal' # minimal, debug\n",
    "hyp_dict['chosen_network'] = \"dmpn\"\n",
    "\n",
    "# suffix for saving images\n",
    "hyp_dict['addon_name'] = \"\"\n",
    "\n",
    "mpn_depth = 1\n",
    "\n",
    "# for coding \n",
    "if hyp_dict['chosen_network'] in (\"gru\", \"vanilla\"):\n",
    "    mpn_depth = 1\n",
    "\n",
    "def current_basic_params():\n",
    "    task_params = {\n",
    "        'task_type': hyp_dict['task_type'],\n",
    "        'rules': rules_dict[hyp_dict['ruleset']],\n",
    "        'dt': 40, # ms, directly influence sequence lengths,\n",
    "        'ruleset': hyp_dict['ruleset'],\n",
    "        'n_eachring': 8, # Number of distinct possible inputs on each ring\n",
    "        'in_out_mode': 'low_dim',  # high_dim or low_dim or low_dim_pos (Robert vs. Laura's paper, resp)\n",
    "        'sigma_x': 0.00, # Laura raised to 0.1 to prevent overfitting (Robert uses 0.01)\n",
    "        'mask_type': 'cost', # 'cost', None\n",
    "        'fixate_off': True, # Second fixation signal goes on when first is off\n",
    "        'task_info': True, \n",
    "        'randomize_inputs': False,\n",
    "        'n_input': 20, # Only used if inputs are randomized,\n",
    "        'modality_diff': True # if two stimulus are included in the task, put them into different modality \n",
    "    }\n",
    "\n",
    "    print(f\"Fixation_off: {task_params['fixate_off']}; Task_info: {task_params['task_info']}\")\n",
    "\n",
    "    train_params = {\n",
    "        'lr': 1e-4,\n",
    "        'n_batches': 640,\n",
    "        'batch_size': 640,\n",
    "        'gradient_clip': 10,\n",
    "        'valid_n_batch': 200,\n",
    "        'n_datasets': 100, # Number of distinct batches\n",
    "        'n_epochs_per_set': 100, # longer/shorter training\n",
    "        # 'task_mask': None, # None, task\n",
    "        # 'weight_reg': 'L2',\n",
    "        # 'reg_lambda': 1e-4,\n",
    "    }\n",
    "\n",
    "    if not train: # some \n",
    "        assert train_params['n_epochs_per_set'] == 0\n",
    "\n",
    "    n_hidden = 100\n",
    "\n",
    "    net_params = {\n",
    "        'net_type': hyp_dict['chosen_network'], # mpn1, dmpn, vanilla\n",
    "        'n_neurons': [1] + [n_hidden] * mpn_depth + [1],\n",
    "        'output_bias': False, # Turn off biases for easier interpretation\n",
    "        'loss_type': 'MSE', # XE, MSE\n",
    "        'activation': 'tanh', # linear, ReLU, sigmoid, tanh, tanh_re, tukey, heaviside\n",
    "        'cuda': True,\n",
    "        'monitor_freq': 100,\n",
    "        'monitor_valid_out': True, # Whether or not to save validation output throughout training\n",
    "        'output_matrix': '',# \"\" (default); \"untrained\", or \"orthogonal\"\n",
    "        'input_layer_add': True, \n",
    "        'input_layer_bias': True,\n",
    "        \n",
    "        # for one-layer MPN, GRU or Vanilla\n",
    "        'ml_params': {\n",
    "            'bias': True, # Bias of layer\n",
    "            'mp_type': 'mult',\n",
    "            'm_update_type': 'hebb_assoc', # hebb_assoc, hebb_pre\n",
    "            'eta_type': 'scalar', # scalar, pre_vector, post_vector, matrix\n",
    "            'eta_train': True,\n",
    "            # 'eta_init': 'gaussian', \n",
    "            'lam_type': 'scalar', # scalar, pre_vector, post_vector, matrix\n",
    "            'm_time_scale': 400, # ms, sets lambda\n",
    "            'lam_train': True,\n",
    "            'W_freeze': False  \n",
    "        },\n",
    "\n",
    "        # Vanilla RNN params\n",
    "        'leaky': True,\n",
    "        'alpha': 0.2,\n",
    "    }\n",
    "\n",
    "    # for multiple MPN layers, assert \n",
    "    if mpn_depth > 1:\n",
    "        for mpl_idx in range(mpn_depth - 1):\n",
    "            assert f'ml_params{mpl_idx}' in net_params.keys()\n",
    "\n",
    "    # actually I don't think it is needed\n",
    "    # putting here to warn the parameter checking every time \n",
    "    # when switching network\n",
    "    if hyp_dict['chosen_network'] in (\"gru\", \"vanilla\"):\n",
    "        assert f'ml_params' in net_params.keys()\n",
    "\n",
    "    return task_params, train_params, net_params\n",
    "\n",
    "task_params, train_params, net_params = current_basic_params()\n",
    "\n",
    "shift_index = 1 if not task_params['fixate_off'] else 0\n",
    "\n",
    "if hyp_dict['task_type'] in ('multitask',):\n",
    "    task_params, train_params, net_params = tasks.convert_and_init_multitask_params(\n",
    "        (task_params, train_params, net_params)\n",
    "    )\n",
    "\n",
    "    net_params['prefs'] = tasks.get_prefs(task_params['hp'])\n",
    "\n",
    "    print('Rules: {}'.format(task_params['rules']))\n",
    "    print('  Input size {}, Output size {}'.format(\n",
    "        task_params['n_input'], task_params['n_output'],\n",
    "    ))\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "if net_params['cuda']:\n",
    "    print('Using CUDA...')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('Using CPU...')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a341b36-dc7a-42b0-bffb-cb17d380041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_dict[\"mess_with_training\"] = False\n",
    "\n",
    "if hyp_dict['mess_with_training']:\n",
    "    hyp_dict['addon_name'] += \"messwithtraining\"\n",
    "\n",
    "params = task_params, train_params, net_params\n",
    "\n",
    "# random matrix\n",
    "random_matrix = None\n",
    "if task_params[\"randomize_inputs\"]:\n",
    "    random_matrix = task_params[\"randomize_matrix\"]\n",
    "\n",
    "if net_params['net_type'] == 'mpn1':\n",
    "    netFunction = mpn.MultiPlasticNet\n",
    "elif net_params['net_type'] == 'dmpn':\n",
    "    netFunction = mpn.DeepMultiPlasticNet\n",
    "elif net_params['net_type'] == 'vanilla':\n",
    "    netFunction = nets.VanillaRNN\n",
    "elif net_params['net_type'] == 'gru':\n",
    "    netFunction = nets.GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07e4fe48-2af6-4741-8d42-01803ba0abf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Align ['delaydm1'] With Same Time\n",
      "delaydm1\n"
     ]
    }
   ],
   "source": [
    "test_n_batch = train_params[\"valid_n_batch\"]\n",
    "color_by = \"stim\"\n",
    "\n",
    "task_random_fix = True\n",
    "if task_random_fix:\n",
    "    print(f\"Align {task_params['rules']} With Same Time\")\n",
    "\n",
    "if task_params['task_type'] in ('multitask',): # Test batch consists of all the rules\n",
    "    task_params['hp']['batch_size_train'] = test_n_batch\n",
    "    # using homogeneous cutting off\n",
    "    test_mode_for_all = \"random\"\n",
    "    # ZIHAN\n",
    "    # generate test data using \"random\"\n",
    "    test_data, test_trials_extra = tasks.generate_trials_wrap(task_params, test_n_batch, \\\n",
    "                rules=task_params['rules'], mode_input=test_mode_for_all, fix=task_random_fix)\n",
    "    _, test_trials, test_rule_idxs = test_trials_extra\n",
    "\n",
    "    task_params['dataset_name'] = 'multitask'\n",
    "\n",
    "    if task_params['in_out_mode'] in ('low_dim_pos',):\n",
    "        output_dim_labels = ('Fixate', 'Cos', '-Cos', 'Sin', '-Sin')\n",
    "    elif task_params['in_out_mode'] in ('low_dim',):\n",
    "        output_dim_labels = ('Fixate', 'Cos', 'Sin')\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    labels = []\n",
    "    for rule_idx, rule in enumerate(task_params['rules']):\n",
    "        print(rule)\n",
    "        if rule in accept_rules:\n",
    "            if hyp_dict['ruleset'] in ('dmsgo', 'dmcgo'):\n",
    "                labels.append(test_trials[rule_idx].meta['matches'])\n",
    "            else:\n",
    "                labels.append(test_trials[rule_idx].meta['resp1' if color_by == \"resp\" else 'stim1'])\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "    labels = np.concatenate(labels, axis=0).reshape(-1,1)\n",
    "\n",
    "test_input, test_output, test_mask = test_data\n",
    "\n",
    "permutation = np.random.permutation(test_input.shape[0])\n",
    "test_input = test_input[permutation]\n",
    "test_output = test_output[permutation]\n",
    "test_mask = test_mask[permutation]\n",
    "labels = labels[permutation]\n",
    "\n",
    "test_input_np = test_input.detach().cpu().numpy()\n",
    "test_output_np = test_output.detach().cpu().numpy()\n",
    "\n",
    "n_batch_all = test_input_np.shape[0] # Total number of batches, might be different than test_n_batch\n",
    "max_seq_len = test_input_np.shape[1]\n",
    "\n",
    "if task_params[\"randomize_inputs\"]: \n",
    "    rmat = task_params[\"randomize_matrix\"]\n",
    "    test_input_np = np.matmul(test_input_np, np.linalg.pinv(rmat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e78b44e9-b6e3-4c1c-a5c7-5a608a6d090b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 100, 100, 3]\n",
      "MultiPlastic Net:\n",
      "  output neurons: 3\n",
      "  Act: tanh\n",
      "\n",
      "=== Layer Universal Setup ===\n",
      "  MP Layer1 parameters:\n",
      "    n_neurons - input: 100, output: 100\n",
      "    M matrix parameters:    update bounds - Max mult: 1.0, Min mult: -1.0\n",
      "      type: mult // Update - type: hebb_assoc // Act fn: linear\n",
      "      Eta: matrix (train) // Lambda: matrix (train) // Lambda_max: 0.90 (tau: 4.0e+02)\n",
      "How about Test Data at dataset 0\n",
      "Train parameters:\n",
      "  Loss: MSE // LR: 1.00e-04 // Optim: adam\n",
      "  Grad type: backprop // Gradient clip: 1.0e+01\n",
      "Weight reg: None\n",
      "Iter: 0, LR: 1.000e-04 - train_loss:3.850e-01, rounded train_acc:0.618, valid_loss:3.853e-01, rounded valid_acc:0.638\n",
      "Iter: 100, LR: 1.000e-04 - train_loss:1.805e-01, rounded train_acc:0.277, valid_loss:1.759e-01, rounded valid_acc:0.246\n",
      "Iter: 200, LR: 1.000e-04 - train_loss:1.755e-01, rounded train_acc:0.275, valid_loss:1.710e-01, rounded valid_acc:0.246\n",
      "Iter: 300, LR: 1.000e-04 - train_loss:1.747e-01, rounded train_acc:0.275, valid_loss:1.705e-01, rounded valid_acc:0.246\n",
      "Iter: 400, LR: 1.000e-04 - train_loss:1.738e-01, rounded train_acc:0.275, valid_loss:1.701e-01, rounded valid_acc:0.246\n",
      "Iter: 500, LR: 1.000e-04 - train_loss:1.726e-01, rounded train_acc:0.275, valid_loss:1.696e-01, rounded valid_acc:0.246\n",
      "How about Test Data at dataset 1\n",
      "Iter: 600, LR: 1.000e-04 - train_loss:1.740e-01, rounded train_acc:0.252, valid_loss:1.685e-01, rounded valid_acc:0.246\n",
      "Iter: 700, LR: 1.000e-04 - train_loss:1.726e-01, rounded train_acc:0.254, valid_loss:1.677e-01, rounded valid_acc:0.246\n",
      "Iter: 800, LR: 1.000e-04 - train_loss:1.677e-01, rounded train_acc:0.254, valid_loss:1.635e-01, rounded valid_acc:0.246\n",
      "Iter: 900, LR: 1.000e-04 - train_loss:1.523e-01, rounded train_acc:0.252, valid_loss:1.485e-01, rounded valid_acc:0.246\n",
      "Iter: 1000, LR: 1.000e-04 - train_loss:1.480e-01, rounded train_acc:0.254, valid_loss:1.455e-01, rounded valid_acc:0.246\n",
      "How about Test Data at dataset 2\n",
      "Iter: 1100, LR: 1.000e-04 - train_loss:1.431e-01, rounded train_acc:0.271, valid_loss:1.466e-01, rounded valid_acc:0.246\n",
      "Iter: 1200, LR: 1.000e-04 - train_loss:1.377e-01, rounded train_acc:0.271, valid_loss:1.450e-01, rounded valid_acc:0.246\n",
      "Iter: 1300, LR: 1.000e-04 - train_loss:1.298e-01, rounded train_acc:0.289, valid_loss:1.400e-01, rounded valid_acc:0.255\n",
      "Iter: 1400, LR: 1.000e-04 - train_loss:1.243e-01, rounded train_acc:0.372, valid_loss:1.360e-01, rounded valid_acc:0.300\n",
      "Iter: 1500, LR: 1.000e-04 - train_loss:1.186e-01, rounded train_acc:0.385, valid_loss:1.297e-01, rounded valid_acc:0.305\n",
      "Iter: 1600, LR: 1.000e-04 - train_loss:1.108e-01, rounded train_acc:0.425, valid_loss:1.188e-01, rounded valid_acc:0.356\n",
      "Iter: 1700, LR: 1.000e-04 - train_loss:1.017e-01, rounded train_acc:0.458, valid_loss:1.117e-01, rounded valid_acc:0.381\n",
      "Iter: 1800, LR: 1.000e-04 - train_loss:9.440e-02, rounded train_acc:0.476, valid_loss:1.083e-01, rounded valid_acc:0.406\n",
      "Iter: 1900, LR: 1.000e-04 - train_loss:9.001e-02, rounded train_acc:0.489, valid_loss:1.052e-01, rounded valid_acc:0.412\n",
      "Iter: 2000, LR: 1.000e-04 - train_loss:8.939e-02, rounded train_acc:0.485, valid_loss:1.037e-01, rounded valid_acc:0.425\n",
      "How about Test Data at dataset 4\n",
      "Iter: 2100, LR: 1.000e-04 - train_loss:8.704e-02, rounded train_acc:0.490, valid_loss:9.498e-02, rounded valid_acc:0.454\n",
      "Iter: 2200, LR: 1.000e-04 - train_loss:8.370e-02, rounded train_acc:0.482, valid_loss:9.314e-02, rounded valid_acc:0.448\n",
      "Iter: 2300, LR: 1.000e-04 - train_loss:8.784e-02, rounded train_acc:0.499, valid_loss:9.952e-02, rounded valid_acc:0.425\n",
      "Iter: 2400, LR: 1.000e-04 - train_loss:8.271e-02, rounded train_acc:0.487, valid_loss:9.427e-02, rounded valid_acc:0.408\n",
      "Iter: 2500, LR: 1.000e-04 - train_loss:7.767e-02, rounded train_acc:0.493, valid_loss:9.322e-02, rounded valid_acc:0.413\n",
      "Iter: 2600, LR: 1.000e-04 - train_loss:8.171e-02, rounded train_acc:0.490, valid_loss:9.147e-02, rounded valid_acc:0.446\n",
      "Iter: 2700, LR: 1.000e-04 - train_loss:7.944e-02, rounded train_acc:0.490, valid_loss:9.068e-02, rounded valid_acc:0.431\n",
      "Iter: 2800, LR: 1.000e-04 - train_loss:7.597e-02, rounded train_acc:0.486, valid_loss:8.702e-02, rounded valid_acc:0.444\n",
      "Iter: 2900, LR: 1.000e-04 - train_loss:7.665e-02, rounded train_acc:0.503, valid_loss:8.499e-02, rounded valid_acc:0.437\n",
      "Iter: 3000, LR: 1.000e-04 - train_loss:7.406e-02, rounded train_acc:0.520, valid_loss:9.021e-02, rounded valid_acc:0.481\n",
      "Iter: 3100, LR: 1.000e-04 - train_loss:8.406e-02, rounded train_acc:0.468, valid_loss:8.611e-02, rounded valid_acc:0.404\n",
      "Iter: 3200, LR: 1.000e-04 - train_loss:7.596e-02, rounded train_acc:0.479, valid_loss:8.045e-02, rounded valid_acc:0.424\n",
      "Iter: 3300, LR: 1.000e-04 - train_loss:7.455e-02, rounded train_acc:0.493, valid_loss:7.921e-02, rounded valid_acc:0.423\n",
      "Iter: 3400, LR: 1.000e-04 - train_loss:7.258e-02, rounded train_acc:0.496, valid_loss:7.811e-02, rounded valid_acc:0.424\n",
      "Iter: 3500, LR: 1.000e-04 - train_loss:7.162e-02, rounded train_acc:0.493, valid_loss:7.682e-02, rounded valid_acc:0.427\n",
      "Iter: 3600, LR: 1.000e-04 - train_loss:7.186e-02, rounded train_acc:0.527, valid_loss:7.813e-02, rounded valid_acc:0.416\n",
      "Iter: 3700, LR: 1.000e-04 - train_loss:6.617e-02, rounded train_acc:0.515, valid_loss:7.692e-02, rounded valid_acc:0.436\n",
      "Iter: 3800, LR: 1.000e-04 - train_loss:6.540e-02, rounded train_acc:0.556, valid_loss:7.623e-02, rounded valid_acc:0.477\n",
      "Iter: 3900, LR: 1.000e-04 - train_loss:6.789e-02, rounded train_acc:0.533, valid_loss:7.817e-02, rounded valid_acc:0.453\n",
      "Iter: 4000, LR: 1.000e-04 - train_loss:6.281e-02, rounded train_acc:0.556, valid_loss:7.528e-02, rounded valid_acc:0.468\n",
      "How about Test Data at dataset 8\n",
      "Iter: 4100, LR: 1.000e-04 - train_loss:6.526e-02, rounded train_acc:0.534, valid_loss:7.142e-02, rounded valid_acc:0.503\n",
      "Iter: 4200, LR: 1.000e-04 - train_loss:6.371e-02, rounded train_acc:0.538, valid_loss:7.056e-02, rounded valid_acc:0.510\n",
      "Iter: 4300, LR: 1.000e-04 - train_loss:6.237e-02, rounded train_acc:0.547, valid_loss:7.026e-02, rounded valid_acc:0.515\n",
      "Iter: 4400, LR: 1.000e-04 - train_loss:6.119e-02, rounded train_acc:0.553, valid_loss:6.952e-02, rounded valid_acc:0.523\n",
      "Iter: 4500, LR: 1.000e-04 - train_loss:6.047e-02, rounded train_acc:0.553, valid_loss:6.900e-02, rounded valid_acc:0.521\n",
      "Iter: 4600, LR: 1.000e-04 - train_loss:7.489e-02, rounded train_acc:0.512, valid_loss:7.787e-02, rounded valid_acc:0.539\n",
      "Iter: 4700, LR: 1.000e-04 - train_loss:6.516e-02, rounded train_acc:0.544, valid_loss:7.254e-02, rounded valid_acc:0.525\n",
      "Iter: 4800, LR: 1.000e-04 - train_loss:6.410e-02, rounded train_acc:0.551, valid_loss:7.197e-02, rounded valid_acc:0.527\n",
      "Iter: 4900, LR: 1.000e-04 - train_loss:6.362e-02, rounded train_acc:0.546, valid_loss:7.167e-02, rounded valid_acc:0.529\n",
      "Iter: 5000, LR: 1.000e-04 - train_loss:6.210e-02, rounded train_acc:0.558, valid_loss:7.240e-02, rounded valid_acc:0.535\n",
      "Iter: 5100, LR: 1.000e-04 - train_loss:6.996e-02, rounded train_acc:0.560, valid_loss:6.961e-02, rounded valid_acc:0.533\n",
      "Iter: 5200, LR: 1.000e-04 - train_loss:6.531e-02, rounded train_acc:0.579, valid_loss:7.171e-02, rounded valid_acc:0.525\n",
      "Iter: 5300, LR: 1.000e-04 - train_loss:6.291e-02, rounded train_acc:0.583, valid_loss:7.125e-02, rounded valid_acc:0.520\n",
      "Iter: 5400, LR: 1.000e-04 - train_loss:6.476e-02, rounded train_acc:0.567, valid_loss:7.545e-02, rounded valid_acc:0.482\n",
      "Iter: 5500, LR: 1.000e-04 - train_loss:8.071e-02, rounded train_acc:0.532, valid_loss:7.992e-02, rounded valid_acc:0.515\n",
      "Iter: 5600, LR: 1.000e-04 - train_loss:6.633e-02, rounded train_acc:0.586, valid_loss:6.815e-02, rounded valid_acc:0.563\n",
      "Iter: 5700, LR: 1.000e-04 - train_loss:6.650e-02, rounded train_acc:0.589, valid_loss:6.900e-02, rounded valid_acc:0.527\n",
      "Iter: 5800, LR: 1.000e-04 - train_loss:6.868e-02, rounded train_acc:0.573, valid_loss:6.812e-02, rounded valid_acc:0.549\n",
      "Iter: 5900, LR: 1.000e-04 - train_loss:6.381e-02, rounded train_acc:0.606, valid_loss:6.994e-02, rounded valid_acc:0.551\n",
      "Iter: 6000, LR: 1.000e-04 - train_loss:6.410e-02, rounded train_acc:0.602, valid_loss:7.251e-02, rounded valid_acc:0.543\n",
      "Iter: 6100, LR: 1.000e-04 - train_loss:7.623e-02, rounded train_acc:0.546, valid_loss:6.867e-02, rounded valid_acc:0.531\n",
      "Iter: 6200, LR: 1.000e-04 - train_loss:6.699e-02, rounded train_acc:0.559, valid_loss:6.875e-02, rounded valid_acc:0.521\n",
      "Iter: 6300, LR: 1.000e-04 - train_loss:6.679e-02, rounded train_acc:0.564, valid_loss:7.001e-02, rounded valid_acc:0.518\n",
      "Iter: 6400, LR: 1.000e-04 - train_loss:7.058e-02, rounded train_acc:0.558, valid_loss:6.537e-02, rounded valid_acc:0.566\n",
      "Iter: 6500, LR: 1.000e-04 - train_loss:6.420e-02, rounded train_acc:0.580, valid_loss:6.596e-02, rounded valid_acc:0.566\n",
      "Iter: 6600, LR: 1.000e-04 - train_loss:6.953e-02, rounded train_acc:0.584, valid_loss:6.543e-02, rounded valid_acc:0.564\n",
      "Iter: 6700, LR: 1.000e-04 - train_loss:7.151e-02, rounded train_acc:0.566, valid_loss:7.633e-02, rounded valid_acc:0.524\n",
      "Iter: 6800, LR: 1.000e-04 - train_loss:5.988e-02, rounded train_acc:0.600, valid_loss:6.559e-02, rounded valid_acc:0.564\n",
      "Iter: 6900, LR: 1.000e-04 - train_loss:7.556e-02, rounded train_acc:0.568, valid_loss:8.116e-02, rounded valid_acc:0.518\n",
      "Iter: 7000, LR: 1.000e-04 - train_loss:6.044e-02, rounded train_acc:0.597, valid_loss:6.626e-02, rounded valid_acc:0.553\n",
      "Iter: 7100, LR: 1.000e-04 - train_loss:5.758e-02, rounded train_acc:0.620, valid_loss:6.469e-02, rounded valid_acc:0.554\n",
      "Iter: 7200, LR: 1.000e-04 - train_loss:5.546e-02, rounded train_acc:0.627, valid_loss:6.435e-02, rounded valid_acc:0.553\n",
      "Iter: 7300, LR: 1.000e-04 - train_loss:5.509e-02, rounded train_acc:0.630, valid_loss:6.468e-02, rounded valid_acc:0.550\n",
      "Iter: 7400, LR: 1.000e-04 - train_loss:5.346e-02, rounded train_acc:0.638, valid_loss:6.410e-02, rounded valid_acc:0.551\n",
      "Iter: 7500, LR: 1.000e-04 - train_loss:5.302e-02, rounded train_acc:0.638, valid_loss:6.509e-02, rounded valid_acc:0.552\n",
      "Iter: 7600, LR: 1.000e-04 - train_loss:5.827e-02, rounded train_acc:0.558, valid_loss:6.441e-02, rounded valid_acc:0.536\n",
      "Iter: 7700, LR: 1.000e-04 - train_loss:5.660e-02, rounded train_acc:0.562, valid_loss:6.345e-02, rounded valid_acc:0.542\n",
      "Iter: 7800, LR: 1.000e-04 - train_loss:5.660e-02, rounded train_acc:0.570, valid_loss:6.191e-02, rounded valid_acc:0.555\n",
      "Iter: 7900, LR: 1.000e-04 - train_loss:5.500e-02, rounded train_acc:0.579, valid_loss:6.350e-02, rounded valid_acc:0.548\n",
      "Iter: 8000, LR: 1.000e-04 - train_loss:5.390e-02, rounded train_acc:0.583, valid_loss:6.448e-02, rounded valid_acc:0.551\n",
      "How about Test Data at dataset 16\n",
      "Iter: 8100, LR: 1.000e-04 - train_loss:5.759e-02, rounded train_acc:0.598, valid_loss:6.277e-02, rounded valid_acc:0.593\n",
      "Iter: 8200, LR: 1.000e-04 - train_loss:5.598e-02, rounded train_acc:0.605, valid_loss:6.165e-02, rounded valid_acc:0.567\n",
      "Iter: 8300, LR: 1.000e-04 - train_loss:5.943e-02, rounded train_acc:0.605, valid_loss:7.083e-02, rounded valid_acc:0.554\n",
      "Iter: 8400, LR: 1.000e-04 - train_loss:5.494e-02, rounded train_acc:0.611, valid_loss:6.191e-02, rounded valid_acc:0.586\n",
      "Iter: 8500, LR: 1.000e-04 - train_loss:5.376e-02, rounded train_acc:0.609, valid_loss:6.027e-02, rounded valid_acc:0.596\n",
      "Iter: 8600, LR: 1.000e-04 - train_loss:5.873e-02, rounded train_acc:0.590, valid_loss:6.647e-02, rounded valid_acc:0.577\n",
      "Iter: 8700, LR: 1.000e-04 - train_loss:6.165e-02, rounded train_acc:0.591, valid_loss:5.897e-02, rounded valid_acc:0.597\n",
      "Iter: 8800, LR: 1.000e-04 - train_loss:5.480e-02, rounded train_acc:0.597, valid_loss:5.842e-02, rounded valid_acc:0.588\n",
      "Iter: 8900, LR: 1.000e-04 - train_loss:5.505e-02, rounded train_acc:0.592, valid_loss:6.448e-02, rounded valid_acc:0.577\n",
      "Iter: 9000, LR: 1.000e-04 - train_loss:5.574e-02, rounded train_acc:0.622, valid_loss:5.719e-02, rounded valid_acc:0.594\n",
      "Iter: 9100, LR: 1.000e-04 - train_loss:5.971e-02, rounded train_acc:0.583, valid_loss:6.130e-02, rounded valid_acc:0.563\n",
      "Iter: 9200, LR: 1.000e-04 - train_loss:6.745e-02, rounded train_acc:0.553, valid_loss:6.625e-02, rounded valid_acc:0.544\n",
      "Iter: 9300, LR: 1.000e-04 - train_loss:6.278e-02, rounded train_acc:0.568, valid_loss:6.993e-02, rounded valid_acc:0.555\n",
      "Iter: 9400, LR: 1.000e-04 - train_loss:5.395e-02, rounded train_acc:0.601, valid_loss:6.299e-02, rounded valid_acc:0.569\n",
      "Iter: 9500, LR: 1.000e-04 - train_loss:6.221e-02, rounded train_acc:0.590, valid_loss:7.600e-02, rounded valid_acc:0.545\n",
      "Iter: 9600, LR: 1.000e-04 - train_loss:5.552e-02, rounded train_acc:0.601, valid_loss:6.023e-02, rounded valid_acc:0.554\n",
      "Iter: 9700, LR: 1.000e-04 - train_loss:5.187e-02, rounded train_acc:0.605, valid_loss:5.830e-02, rounded valid_acc:0.560\n",
      "Iter: 9800, LR: 1.000e-04 - train_loss:5.034e-02, rounded train_acc:0.615, valid_loss:5.684e-02, rounded valid_acc:0.570\n",
      "Iter: 9900, LR: 1.000e-04 - train_loss:4.930e-02, rounded train_acc:0.611, valid_loss:5.664e-02, rounded valid_acc:0.585\n",
      "Iter: 10000, LR: 1.000e-04 - train_loss:4.864e-02, rounded train_acc:0.605, valid_loss:5.567e-02, rounded valid_acc:0.595\n",
      "Iter: 10100, LR: 1.000e-04 - train_loss:5.211e-02, rounded train_acc:0.611, valid_loss:5.621e-02, rounded valid_acc:0.597\n",
      "Iter: 10200, LR: 1.000e-04 - train_loss:5.016e-02, rounded train_acc:0.622, valid_loss:5.602e-02, rounded valid_acc:0.591\n",
      "Iter: 10300, LR: 1.000e-04 - train_loss:4.948e-02, rounded train_acc:0.615, valid_loss:5.566e-02, rounded valid_acc:0.603\n",
      "Iter: 10400, LR: 1.000e-04 - train_loss:4.855e-02, rounded train_acc:0.621, valid_loss:5.542e-02, rounded valid_acc:0.615\n",
      "Iter: 10500, LR: 1.000e-04 - train_loss:4.856e-02, rounded train_acc:0.625, valid_loss:5.389e-02, rounded valid_acc:0.619\n",
      "Iter: 10600, LR: 1.000e-04 - train_loss:4.932e-02, rounded train_acc:0.632, valid_loss:5.397e-02, rounded valid_acc:0.581\n",
      "Iter: 10700, LR: 1.000e-04 - train_loss:4.913e-02, rounded train_acc:0.633, valid_loss:7.874e-02, rounded valid_acc:0.544\n",
      "Iter: 10800, LR: 1.000e-04 - train_loss:5.503e-02, rounded train_acc:0.624, valid_loss:6.013e-02, rounded valid_acc:0.579\n",
      "Iter: 10900, LR: 1.000e-04 - train_loss:4.777e-02, rounded train_acc:0.638, valid_loss:5.678e-02, rounded valid_acc:0.587\n",
      "Iter: 11000, LR: 1.000e-04 - train_loss:4.770e-02, rounded train_acc:0.636, valid_loss:7.383e-02, rounded valid_acc:0.548\n",
      "Iter: 11100, LR: 1.000e-04 - train_loss:4.608e-02, rounded train_acc:0.629, valid_loss:5.889e-02, rounded valid_acc:0.586\n",
      "Iter: 11200, LR: 1.000e-04 - train_loss:4.285e-02, rounded train_acc:0.644, valid_loss:5.254e-02, rounded valid_acc:0.619\n",
      "Iter: 11300, LR: 1.000e-04 - train_loss:4.346e-02, rounded train_acc:0.636, valid_loss:7.169e-02, rounded valid_acc:0.545\n",
      "Iter: 11400, LR: 1.000e-04 - train_loss:4.371e-02, rounded train_acc:0.644, valid_loss:5.852e-02, rounded valid_acc:0.586\n",
      "Iter: 11500, LR: 1.000e-04 - train_loss:5.451e-02, rounded train_acc:0.608, valid_loss:6.676e-02, rounded valid_acc:0.570\n",
      "Iter: 11600, LR: 1.000e-04 - train_loss:5.541e-02, rounded train_acc:0.594, valid_loss:5.693e-02, rounded valid_acc:0.582\n",
      "Iter: 11700, LR: 1.000e-04 - train_loss:5.679e-02, rounded train_acc:0.600, valid_loss:5.622e-02, rounded valid_acc:0.608\n",
      "Iter: 11800, LR: 1.000e-04 - train_loss:4.887e-02, rounded train_acc:0.633, valid_loss:5.221e-02, rounded valid_acc:0.594\n",
      "Iter: 11900, LR: 1.000e-04 - train_loss:4.820e-02, rounded train_acc:0.646, valid_loss:5.750e-02, rounded valid_acc:0.611\n",
      "Iter: 12000, LR: 1.000e-04 - train_loss:5.218e-02, rounded train_acc:0.617, valid_loss:5.368e-02, rounded valid_acc:0.599\n",
      "Iter: 12100, LR: 1.000e-04 - train_loss:6.086e-02, rounded train_acc:0.556, valid_loss:5.651e-02, rounded valid_acc:0.640\n",
      "Iter: 12200, LR: 1.000e-04 - train_loss:5.385e-02, rounded train_acc:0.584, valid_loss:6.160e-02, rounded valid_acc:0.588\n",
      "Iter: 12300, LR: 1.000e-04 - train_loss:6.006e-02, rounded train_acc:0.589, valid_loss:5.425e-02, rounded valid_acc:0.622\n",
      "Iter: 12400, LR: 1.000e-04 - train_loss:5.130e-02, rounded train_acc:0.590, valid_loss:6.218e-02, rounded valid_acc:0.560\n",
      "Iter: 12500, LR: 1.000e-04 - train_loss:6.291e-02, rounded train_acc:0.572, valid_loss:7.579e-02, rounded valid_acc:0.512\n",
      "Iter: 12600, LR: 1.000e-04 - train_loss:5.118e-02, rounded train_acc:0.628, valid_loss:6.168e-02, rounded valid_acc:0.610\n",
      "Iter: 12700, LR: 1.000e-04 - train_loss:5.263e-02, rounded train_acc:0.636, valid_loss:5.207e-02, rounded valid_acc:0.640\n",
      "Iter: 12800, LR: 1.000e-04 - train_loss:4.617e-02, rounded train_acc:0.652, valid_loss:5.310e-02, rounded valid_acc:0.637\n",
      "Iter: 12900, LR: 1.000e-04 - train_loss:4.658e-02, rounded train_acc:0.650, valid_loss:5.864e-02, rounded valid_acc:0.612\n",
      "Iter: 13000, LR: 1.000e-04 - train_loss:4.990e-02, rounded train_acc:0.643, valid_loss:5.240e-02, rounded valid_acc:0.634\n",
      "Iter: 13100, LR: 1.000e-04 - train_loss:4.596e-02, rounded train_acc:0.658, valid_loss:5.098e-02, rounded valid_acc:0.626\n",
      "Iter: 13200, LR: 1.000e-04 - train_loss:5.281e-02, rounded train_acc:0.642, valid_loss:5.061e-02, rounded valid_acc:0.668\n",
      "Iter: 13300, LR: 1.000e-04 - train_loss:4.414e-02, rounded train_acc:0.662, valid_loss:5.792e-02, rounded valid_acc:0.629\n",
      "Iter: 13400, LR: 1.000e-04 - train_loss:4.553e-02, rounded train_acc:0.663, valid_loss:5.079e-02, rounded valid_acc:0.643\n",
      "Iter: 13500, LR: 1.000e-04 - train_loss:4.162e-02, rounded train_acc:0.677, valid_loss:5.152e-02, rounded valid_acc:0.645\n",
      "Iter: 13600, LR: 1.000e-04 - train_loss:4.417e-02, rounded train_acc:0.627, valid_loss:5.925e-02, rounded valid_acc:0.586\n",
      "Iter: 13700, LR: 1.000e-04 - train_loss:4.253e-02, rounded train_acc:0.623, valid_loss:5.864e-02, rounded valid_acc:0.620\n",
      "Iter: 13800, LR: 1.000e-04 - train_loss:4.383e-02, rounded train_acc:0.622, valid_loss:5.798e-02, rounded valid_acc:0.633\n",
      "Iter: 13900, LR: 1.000e-04 - train_loss:4.430e-02, rounded train_acc:0.624, valid_loss:5.434e-02, rounded valid_acc:0.648\n",
      "Iter: 14000, LR: 1.000e-04 - train_loss:5.281e-02, rounded train_acc:0.558, valid_loss:6.674e-02, rounded valid_acc:0.603\n",
      "Iter: 14100, LR: 1.000e-04 - train_loss:4.909e-02, rounded train_acc:0.636, valid_loss:5.565e-02, rounded valid_acc:0.629\n",
      "Iter: 14200, LR: 1.000e-04 - train_loss:4.376e-02, rounded train_acc:0.662, valid_loss:5.570e-02, rounded valid_acc:0.654\n",
      "Iter: 14300, LR: 1.000e-04 - train_loss:5.004e-02, rounded train_acc:0.636, valid_loss:5.328e-02, rounded valid_acc:0.642\n",
      "Iter: 14400, LR: 1.000e-04 - train_loss:4.174e-02, rounded train_acc:0.670, valid_loss:5.401e-02, rounded valid_acc:0.625\n",
      "Iter: 14500, LR: 1.000e-04 - train_loss:4.058e-02, rounded train_acc:0.663, valid_loss:4.718e-02, rounded valid_acc:0.664\n",
      "Iter: 14600, LR: 1.000e-04 - train_loss:7.454e-02, rounded train_acc:0.563, valid_loss:8.318e-02, rounded valid_acc:0.570\n",
      "Iter: 14700, LR: 1.000e-04 - train_loss:4.281e-02, rounded train_acc:0.658, valid_loss:4.874e-02, rounded valid_acc:0.662\n",
      "Iter: 14800, LR: 1.000e-04 - train_loss:3.918e-02, rounded train_acc:0.665, valid_loss:5.248e-02, rounded valid_acc:0.647\n",
      "Iter: 14900, LR: 1.000e-04 - train_loss:4.022e-02, rounded train_acc:0.675, valid_loss:5.777e-02, rounded valid_acc:0.605\n",
      "Iter: 15000, LR: 1.000e-04 - train_loss:4.171e-02, rounded train_acc:0.668, valid_loss:5.196e-02, rounded valid_acc:0.651\n",
      "Iter: 15100, LR: 1.000e-04 - train_loss:5.057e-02, rounded train_acc:0.617, valid_loss:5.995e-02, rounded valid_acc:0.605\n",
      "Iter: 15200, LR: 1.000e-04 - train_loss:5.860e-02, rounded train_acc:0.619, valid_loss:6.220e-02, rounded valid_acc:0.568\n",
      "Iter: 15300, LR: 1.000e-04 - train_loss:4.031e-02, rounded train_acc:0.673, valid_loss:4.867e-02, rounded valid_acc:0.615\n",
      "Iter: 15400, LR: 1.000e-04 - train_loss:5.510e-02, rounded train_acc:0.636, valid_loss:5.593e-02, rounded valid_acc:0.591\n",
      "Iter: 15500, LR: 1.000e-04 - train_loss:4.050e-02, rounded train_acc:0.669, valid_loss:4.781e-02, rounded valid_acc:0.634\n",
      "Iter: 15600, LR: 1.000e-04 - train_loss:4.373e-02, rounded train_acc:0.634, valid_loss:5.172e-02, rounded valid_acc:0.626\n",
      "Iter: 15700, LR: 1.000e-04 - train_loss:4.400e-02, rounded train_acc:0.642, valid_loss:5.054e-02, rounded valid_acc:0.624\n",
      "Iter: 15800, LR: 1.000e-04 - train_loss:4.512e-02, rounded train_acc:0.631, valid_loss:4.650e-02, rounded valid_acc:0.648\n",
      "Iter: 15900, LR: 1.000e-04 - train_loss:4.150e-02, rounded train_acc:0.645, valid_loss:4.753e-02, rounded valid_acc:0.650\n",
      "Iter: 16000, LR: 1.000e-04 - train_loss:5.774e-02, rounded train_acc:0.627, valid_loss:5.054e-02, rounded valid_acc:0.611\n",
      "How about Test Data at dataset 32\n",
      "Iter: 16100, LR: 1.000e-04 - train_loss:4.914e-02, rounded train_acc:0.623, valid_loss:4.661e-02, rounded valid_acc:0.622\n",
      "Iter: 16200, LR: 1.000e-04 - train_loss:4.470e-02, rounded train_acc:0.653, valid_loss:5.034e-02, rounded valid_acc:0.624\n",
      "Iter: 16300, LR: 1.000e-04 - train_loss:4.298e-02, rounded train_acc:0.648, valid_loss:5.189e-02, rounded valid_acc:0.622\n",
      "Iter: 16400, LR: 1.000e-04 - train_loss:4.678e-02, rounded train_acc:0.634, valid_loss:4.737e-02, rounded valid_acc:0.651\n",
      "Iter: 16500, LR: 1.000e-04 - train_loss:4.170e-02, rounded train_acc:0.669, valid_loss:4.907e-02, rounded valid_acc:0.643\n",
      "Iter: 16600, LR: 1.000e-04 - train_loss:4.780e-02, rounded train_acc:0.630, valid_loss:5.369e-02, rounded valid_acc:0.604\n",
      "Iter: 16700, LR: 1.000e-04 - train_loss:4.388e-02, rounded train_acc:0.643, valid_loss:4.604e-02, rounded valid_acc:0.616\n",
      "Iter: 16800, LR: 1.000e-04 - train_loss:4.342e-02, rounded train_acc:0.648, valid_loss:4.275e-02, rounded valid_acc:0.629\n",
      "Iter: 16900, LR: 1.000e-04 - train_loss:6.522e-02, rounded train_acc:0.598, valid_loss:5.616e-02, rounded valid_acc:0.600\n",
      "Iter: 17000, LR: 1.000e-04 - train_loss:4.311e-02, rounded train_acc:0.649, valid_loss:4.803e-02, rounded valid_acc:0.603\n",
      "Iter: 17100, LR: 1.000e-04 - train_loss:5.092e-02, rounded train_acc:0.613, valid_loss:4.744e-02, rounded valid_acc:0.630\n",
      "Iter: 17200, LR: 1.000e-04 - train_loss:4.306e-02, rounded train_acc:0.667, valid_loss:4.787e-02, rounded valid_acc:0.620\n",
      "Iter: 17300, LR: 1.000e-04 - train_loss:4.236e-02, rounded train_acc:0.669, valid_loss:4.457e-02, rounded valid_acc:0.620\n",
      "Iter: 17400, LR: 1.000e-04 - train_loss:4.383e-02, rounded train_acc:0.666, valid_loss:5.246e-02, rounded valid_acc:0.608\n",
      "Iter: 17500, LR: 1.000e-04 - train_loss:4.835e-02, rounded train_acc:0.654, valid_loss:4.759e-02, rounded valid_acc:0.603\n",
      "Iter: 17600, LR: 1.000e-04 - train_loss:4.167e-02, rounded train_acc:0.652, valid_loss:4.391e-02, rounded valid_acc:0.661\n",
      "Iter: 17700, LR: 1.000e-04 - train_loss:4.177e-02, rounded train_acc:0.656, valid_loss:5.072e-02, rounded valid_acc:0.595\n",
      "Iter: 17800, LR: 1.000e-04 - train_loss:4.459e-02, rounded train_acc:0.673, valid_loss:4.559e-02, rounded valid_acc:0.656\n",
      "Iter: 17900, LR: 1.000e-04 - train_loss:3.875e-02, rounded train_acc:0.670, valid_loss:4.795e-02, rounded valid_acc:0.608\n",
      "Iter: 18000, LR: 1.000e-04 - train_loss:3.713e-02, rounded train_acc:0.650, valid_loss:4.776e-02, rounded valid_acc:0.598\n",
      "Iter: 18100, LR: 1.000e-04 - train_loss:4.466e-02, rounded train_acc:0.646, valid_loss:5.173e-02, rounded valid_acc:0.616\n",
      "Iter: 18200, LR: 1.000e-04 - train_loss:4.409e-02, rounded train_acc:0.631, valid_loss:4.400e-02, rounded valid_acc:0.637\n",
      "Iter: 18300, LR: 1.000e-04 - train_loss:3.827e-02, rounded train_acc:0.644, valid_loss:4.714e-02, rounded valid_acc:0.629\n",
      "Iter: 18400, LR: 1.000e-04 - train_loss:4.176e-02, rounded train_acc:0.629, valid_loss:5.032e-02, rounded valid_acc:0.603\n",
      "Iter: 18500, LR: 1.000e-04 - train_loss:4.779e-02, rounded train_acc:0.619, valid_loss:5.216e-02, rounded valid_acc:0.604\n",
      "Iter: 18600, LR: 1.000e-04 - train_loss:5.597e-02, rounded train_acc:0.583, valid_loss:5.050e-02, rounded valid_acc:0.580\n",
      "Iter: 18700, LR: 1.000e-04 - train_loss:4.535e-02, rounded train_acc:0.610, valid_loss:4.686e-02, rounded valid_acc:0.624\n",
      "Iter: 18800, LR: 1.000e-04 - train_loss:7.140e-02, rounded train_acc:0.537, valid_loss:7.172e-02, rounded valid_acc:0.537\n",
      "Iter: 18900, LR: 1.000e-04 - train_loss:5.007e-02, rounded train_acc:0.618, valid_loss:5.284e-02, rounded valid_acc:0.610\n",
      "Iter: 19000, LR: 1.000e-04 - train_loss:5.359e-02, rounded train_acc:0.610, valid_loss:5.392e-02, rounded valid_acc:0.599\n",
      "Iter: 19100, LR: 1.000e-04 - train_loss:3.926e-02, rounded train_acc:0.661, valid_loss:4.319e-02, rounded valid_acc:0.631\n",
      "Iter: 19200, LR: 1.000e-04 - train_loss:4.038e-02, rounded train_acc:0.666, valid_loss:4.891e-02, rounded valid_acc:0.567\n",
      "Iter: 19300, LR: 1.000e-04 - train_loss:4.630e-02, rounded train_acc:0.643, valid_loss:4.668e-02, rounded valid_acc:0.598\n",
      "Iter: 19400, LR: 1.000e-04 - train_loss:3.587e-02, rounded train_acc:0.682, valid_loss:4.165e-02, rounded valid_acc:0.629\n",
      "Iter: 19500, LR: 1.000e-04 - train_loss:3.487e-02, rounded train_acc:0.668, valid_loss:4.050e-02, rounded valid_acc:0.646\n",
      "Iter: 19600, LR: 1.000e-04 - train_loss:4.329e-02, rounded train_acc:0.631, valid_loss:4.950e-02, rounded valid_acc:0.609\n",
      "Iter: 19700, LR: 1.000e-04 - train_loss:4.673e-02, rounded train_acc:0.620, valid_loss:5.441e-02, rounded valid_acc:0.615\n",
      "Iter: 19800, LR: 1.000e-04 - train_loss:3.830e-02, rounded train_acc:0.657, valid_loss:5.387e-02, rounded valid_acc:0.620\n",
      "Iter: 19900, LR: 1.000e-04 - train_loss:3.938e-02, rounded train_acc:0.664, valid_loss:5.053e-02, rounded valid_acc:0.609\n",
      "Iter: 20000, LR: 1.000e-04 - train_loss:5.201e-02, rounded train_acc:0.610, valid_loss:5.068e-02, rounded valid_acc:0.603\n",
      "Iter: 20100, LR: 1.000e-04 - train_loss:4.592e-02, rounded train_acc:0.647, valid_loss:5.825e-02, rounded valid_acc:0.618\n",
      "Iter: 20200, LR: 1.000e-04 - train_loss:4.811e-02, rounded train_acc:0.627, valid_loss:4.850e-02, rounded valid_acc:0.625\n",
      "Iter: 20300, LR: 1.000e-04 - train_loss:5.091e-02, rounded train_acc:0.619, valid_loss:6.306e-02, rounded valid_acc:0.574\n",
      "Iter: 20400, LR: 1.000e-04 - train_loss:4.228e-02, rounded train_acc:0.663, valid_loss:5.387e-02, rounded valid_acc:0.593\n",
      "Iter: 20500, LR: 1.000e-04 - train_loss:4.420e-02, rounded train_acc:0.659, valid_loss:5.579e-02, rounded valid_acc:0.587\n",
      "Iter: 20600, LR: 1.000e-04 - train_loss:4.411e-02, rounded train_acc:0.635, valid_loss:5.106e-02, rounded valid_acc:0.611\n",
      "Iter: 20700, LR: 1.000e-04 - train_loss:4.147e-02, rounded train_acc:0.626, valid_loss:4.921e-02, rounded valid_acc:0.592\n",
      "Iter: 20800, LR: 1.000e-04 - train_loss:3.953e-02, rounded train_acc:0.655, valid_loss:4.823e-02, rounded valid_acc:0.627\n",
      "Iter: 20900, LR: 1.000e-04 - train_loss:3.601e-02, rounded train_acc:0.637, valid_loss:4.498e-02, rounded valid_acc:0.621\n",
      "Iter: 21000, LR: 1.000e-04 - train_loss:4.521e-02, rounded train_acc:0.626, valid_loss:6.233e-02, rounded valid_acc:0.566\n",
      "Iter: 21100, LR: 1.000e-04 - train_loss:4.786e-02, rounded train_acc:0.633, valid_loss:4.966e-02, rounded valid_acc:0.615\n",
      "Iter: 21200, LR: 1.000e-04 - train_loss:4.823e-02, rounded train_acc:0.616, valid_loss:5.120e-02, rounded valid_acc:0.614\n",
      "Iter: 21300, LR: 1.000e-04 - train_loss:3.875e-02, rounded train_acc:0.647, valid_loss:4.862e-02, rounded valid_acc:0.620\n",
      "Iter: 21400, LR: 1.000e-04 - train_loss:4.770e-02, rounded train_acc:0.616, valid_loss:4.684e-02, rounded valid_acc:0.641\n",
      "Iter: 21500, LR: 1.000e-04 - train_loss:5.117e-02, rounded train_acc:0.615, valid_loss:5.263e-02, rounded valid_acc:0.640\n",
      "Iter: 21600, LR: 1.000e-04 - train_loss:5.373e-02, rounded train_acc:0.617, valid_loss:5.506e-02, rounded valid_acc:0.613\n",
      "Iter: 21700, LR: 1.000e-04 - train_loss:3.906e-02, rounded train_acc:0.672, valid_loss:4.426e-02, rounded valid_acc:0.636\n",
      "Iter: 21800, LR: 1.000e-04 - train_loss:3.850e-02, rounded train_acc:0.670, valid_loss:4.139e-02, rounded valid_acc:0.638\n",
      "Iter: 21900, LR: 1.000e-04 - train_loss:3.807e-02, rounded train_acc:0.671, valid_loss:5.372e-02, rounded valid_acc:0.611\n",
      "Iter: 22000, LR: 1.000e-04 - train_loss:3.735e-02, rounded train_acc:0.672, valid_loss:5.056e-02, rounded valid_acc:0.639\n",
      "Iter: 22100, LR: 1.000e-04 - train_loss:4.122e-02, rounded train_acc:0.623, valid_loss:4.447e-02, rounded valid_acc:0.615\n",
      "Iter: 22200, LR: 1.000e-04 - train_loss:4.813e-02, rounded train_acc:0.627, valid_loss:4.627e-02, rounded valid_acc:0.650\n",
      "Iter: 22300, LR: 1.000e-04 - train_loss:4.740e-02, rounded train_acc:0.606, valid_loss:5.888e-02, rounded valid_acc:0.594\n",
      "Iter: 22400, LR: 1.000e-04 - train_loss:3.534e-02, rounded train_acc:0.646, valid_loss:4.751e-02, rounded valid_acc:0.588\n",
      "Iter: 22500, LR: 1.000e-04 - train_loss:4.247e-02, rounded train_acc:0.609, valid_loss:4.927e-02, rounded valid_acc:0.578\n",
      "Iter: 22600, LR: 1.000e-04 - train_loss:3.433e-02, rounded train_acc:0.695, valid_loss:4.181e-02, rounded valid_acc:0.671\n",
      "Iter: 22700, LR: 1.000e-04 - train_loss:3.754e-02, rounded train_acc:0.695, valid_loss:4.755e-02, rounded valid_acc:0.660\n",
      "Iter: 22800, LR: 1.000e-04 - train_loss:3.944e-02, rounded train_acc:0.685, valid_loss:4.262e-02, rounded valid_acc:0.673\n",
      "Iter: 22900, LR: 1.000e-04 - train_loss:6.143e-02, rounded train_acc:0.627, valid_loss:6.521e-02, rounded valid_acc:0.634\n",
      "Iter: 23000, LR: 1.000e-04 - train_loss:4.473e-02, rounded train_acc:0.664, valid_loss:4.351e-02, rounded valid_acc:0.663\n",
      "Iter: 23100, LR: 1.000e-04 - train_loss:3.836e-02, rounded train_acc:0.652, valid_loss:5.283e-02, rounded valid_acc:0.654\n",
      "Iter: 23200, LR: 1.000e-04 - train_loss:3.951e-02, rounded train_acc:0.664, valid_loss:3.905e-02, rounded valid_acc:0.685\n",
      "Iter: 23300, LR: 1.000e-04 - train_loss:3.536e-02, rounded train_acc:0.665, valid_loss:4.763e-02, rounded valid_acc:0.669\n",
      "Iter: 23400, LR: 1.000e-04 - train_loss:4.295e-02, rounded train_acc:0.667, valid_loss:5.011e-02, rounded valid_acc:0.676\n",
      "Iter: 23500, LR: 1.000e-04 - train_loss:4.121e-02, rounded train_acc:0.654, valid_loss:4.742e-02, rounded valid_acc:0.676\n",
      "Iter: 23600, LR: 1.000e-04 - train_loss:4.264e-02, rounded train_acc:0.665, valid_loss:3.771e-02, rounded valid_acc:0.702\n",
      "Iter: 23700, LR: 1.000e-04 - train_loss:4.108e-02, rounded train_acc:0.662, valid_loss:4.693e-02, rounded valid_acc:0.674\n",
      "Iter: 23800, LR: 1.000e-04 - train_loss:5.234e-02, rounded train_acc:0.605, valid_loss:5.014e-02, rounded valid_acc:0.626\n",
      "Iter: 23900, LR: 1.000e-04 - train_loss:3.618e-02, rounded train_acc:0.667, valid_loss:4.132e-02, rounded valid_acc:0.693\n",
      "Iter: 24000, LR: 1.000e-04 - train_loss:5.413e-02, rounded train_acc:0.618, valid_loss:5.323e-02, rounded valid_acc:0.628\n",
      "Iter: 24100, LR: 1.000e-04 - train_loss:3.582e-02, rounded train_acc:0.681, valid_loss:4.643e-02, rounded valid_acc:0.641\n",
      "Iter: 24200, LR: 1.000e-04 - train_loss:3.804e-02, rounded train_acc:0.679, valid_loss:4.395e-02, rounded valid_acc:0.674\n",
      "Iter: 24300, LR: 1.000e-04 - train_loss:3.331e-02, rounded train_acc:0.691, valid_loss:4.495e-02, rounded valid_acc:0.669\n",
      "Iter: 24400, LR: 1.000e-04 - train_loss:3.319e-02, rounded train_acc:0.697, valid_loss:4.674e-02, rounded valid_acc:0.631\n",
      "Iter: 24500, LR: 1.000e-04 - train_loss:2.869e-02, rounded train_acc:0.710, valid_loss:3.976e-02, rounded valid_acc:0.656\n",
      "Iter: 24600, LR: 1.000e-04 - train_loss:4.019e-02, rounded train_acc:0.689, valid_loss:4.636e-02, rounded valid_acc:0.639\n",
      "Iter: 24700, LR: 1.000e-04 - train_loss:4.131e-02, rounded train_acc:0.689, valid_loss:4.333e-02, rounded valid_acc:0.660\n",
      "Iter: 24800, LR: 1.000e-04 - train_loss:3.647e-02, rounded train_acc:0.713, valid_loss:4.534e-02, rounded valid_acc:0.665\n",
      "Iter: 24900, LR: 1.000e-04 - train_loss:3.364e-02, rounded train_acc:0.729, valid_loss:3.922e-02, rounded valid_acc:0.668\n",
      "Iter: 25000, LR: 1.000e-04 - train_loss:4.415e-02, rounded train_acc:0.683, valid_loss:4.495e-02, rounded valid_acc:0.629\n",
      "Iter: 25100, LR: 1.000e-04 - train_loss:3.644e-02, rounded train_acc:0.645, valid_loss:4.975e-02, rounded valid_acc:0.639\n",
      "Iter: 25200, LR: 1.000e-04 - train_loss:3.936e-02, rounded train_acc:0.652, valid_loss:5.256e-02, rounded valid_acc:0.660\n",
      "Iter: 25300, LR: 1.000e-04 - train_loss:3.864e-02, rounded train_acc:0.634, valid_loss:4.370e-02, rounded valid_acc:0.650\n",
      "Iter: 25400, LR: 1.000e-04 - train_loss:3.034e-02, rounded train_acc:0.700, valid_loss:3.928e-02, rounded valid_acc:0.684\n",
      "Iter: 25500, LR: 1.000e-04 - train_loss:3.405e-02, rounded train_acc:0.667, valid_loss:4.311e-02, rounded valid_acc:0.667\n",
      "Iter: 25600, LR: 1.000e-04 - train_loss:3.554e-02, rounded train_acc:0.688, valid_loss:3.862e-02, rounded valid_acc:0.709\n",
      "Iter: 25700, LR: 1.000e-04 - train_loss:3.471e-02, rounded train_acc:0.679, valid_loss:3.925e-02, rounded valid_acc:0.683\n",
      "Iter: 25800, LR: 1.000e-04 - train_loss:3.706e-02, rounded train_acc:0.677, valid_loss:4.093e-02, rounded valid_acc:0.682\n",
      "Iter: 25900, LR: 1.000e-04 - train_loss:4.046e-02, rounded train_acc:0.667, valid_loss:3.957e-02, rounded valid_acc:0.704\n",
      "Iter: 26000, LR: 1.000e-04 - train_loss:3.088e-02, rounded train_acc:0.725, valid_loss:3.827e-02, rounded valid_acc:0.719\n",
      "Iter: 26100, LR: 1.000e-04 - train_loss:3.556e-02, rounded train_acc:0.684, valid_loss:4.062e-02, rounded valid_acc:0.711\n",
      "Iter: 26200, LR: 1.000e-04 - train_loss:3.118e-02, rounded train_acc:0.711, valid_loss:4.239e-02, rounded valid_acc:0.681\n",
      "Iter: 26300, LR: 1.000e-04 - train_loss:3.137e-02, rounded train_acc:0.684, valid_loss:4.063e-02, rounded valid_acc:0.705\n",
      "Iter: 26400, LR: 1.000e-04 - train_loss:3.277e-02, rounded train_acc:0.694, valid_loss:4.055e-02, rounded valid_acc:0.711\n",
      "Iter: 26500, LR: 1.000e-04 - train_loss:2.900e-02, rounded train_acc:0.711, valid_loss:3.847e-02, rounded valid_acc:0.719\n",
      "Iter: 26600, LR: 1.000e-04 - train_loss:3.780e-02, rounded train_acc:0.693, valid_loss:3.740e-02, rounded valid_acc:0.685\n",
      "Iter: 26700, LR: 1.000e-04 - train_loss:3.812e-02, rounded train_acc:0.688, valid_loss:4.847e-02, rounded valid_acc:0.656\n",
      "Iter: 26800, LR: 1.000e-04 - train_loss:3.778e-02, rounded train_acc:0.695, valid_loss:4.158e-02, rounded valid_acc:0.695\n",
      "Iter: 26900, LR: 1.000e-04 - train_loss:3.207e-02, rounded train_acc:0.720, valid_loss:3.937e-02, rounded valid_acc:0.699\n",
      "Iter: 27000, LR: 1.000e-04 - train_loss:4.284e-02, rounded train_acc:0.655, valid_loss:5.336e-02, rounded valid_acc:0.631\n",
      "Iter: 27100, LR: 1.000e-04 - train_loss:3.696e-02, rounded train_acc:0.700, valid_loss:4.284e-02, rounded valid_acc:0.686\n",
      "Iter: 27200, LR: 1.000e-04 - train_loss:3.017e-02, rounded train_acc:0.717, valid_loss:4.435e-02, rounded valid_acc:0.677\n",
      "Iter: 27300, LR: 1.000e-04 - train_loss:3.135e-02, rounded train_acc:0.711, valid_loss:4.358e-02, rounded valid_acc:0.679\n",
      "Iter: 27400, LR: 1.000e-04 - train_loss:3.494e-02, rounded train_acc:0.699, valid_loss:4.153e-02, rounded valid_acc:0.680\n",
      "Iter: 27500, LR: 1.000e-04 - train_loss:2.987e-02, rounded train_acc:0.720, valid_loss:4.276e-02, rounded valid_acc:0.671\n",
      "Iter: 27600, LR: 1.000e-04 - train_loss:3.688e-02, rounded train_acc:0.666, valid_loss:4.006e-02, rounded valid_acc:0.692\n",
      "Iter: 27700, LR: 1.000e-04 - train_loss:3.540e-02, rounded train_acc:0.668, valid_loss:4.076e-02, rounded valid_acc:0.669\n",
      "Iter: 27800, LR: 1.000e-04 - train_loss:3.324e-02, rounded train_acc:0.703, valid_loss:4.097e-02, rounded valid_acc:0.691\n",
      "Iter: 27900, LR: 1.000e-04 - train_loss:3.353e-02, rounded train_acc:0.683, valid_loss:4.902e-02, rounded valid_acc:0.651\n",
      "Iter: 28000, LR: 1.000e-04 - train_loss:3.498e-02, rounded train_acc:0.702, valid_loss:4.068e-02, rounded valid_acc:0.696\n",
      "Iter: 28100, LR: 1.000e-04 - train_loss:3.378e-02, rounded train_acc:0.693, valid_loss:4.129e-02, rounded valid_acc:0.690\n",
      "Iter: 28200, LR: 1.000e-04 - train_loss:3.238e-02, rounded train_acc:0.688, valid_loss:4.629e-02, rounded valid_acc:0.718\n",
      "Iter: 28300, LR: 1.000e-04 - train_loss:3.412e-02, rounded train_acc:0.710, valid_loss:4.016e-02, rounded valid_acc:0.732\n",
      "Iter: 28400, LR: 1.000e-04 - train_loss:3.815e-02, rounded train_acc:0.688, valid_loss:5.437e-02, rounded valid_acc:0.674\n",
      "Iter: 28500, LR: 1.000e-04 - train_loss:3.761e-02, rounded train_acc:0.717, valid_loss:3.982e-02, rounded valid_acc:0.742\n",
      "Iter: 28600, LR: 1.000e-04 - train_loss:4.217e-02, rounded train_acc:0.668, valid_loss:4.159e-02, rounded valid_acc:0.663\n",
      "Iter: 28700, LR: 1.000e-04 - train_loss:3.083e-02, rounded train_acc:0.703, valid_loss:3.481e-02, rounded valid_acc:0.728\n",
      "Iter: 28800, LR: 1.000e-04 - train_loss:3.133e-02, rounded train_acc:0.697, valid_loss:4.059e-02, rounded valid_acc:0.702\n",
      "Iter: 28900, LR: 1.000e-04 - train_loss:2.954e-02, rounded train_acc:0.703, valid_loss:3.745e-02, rounded valid_acc:0.712\n",
      "Iter: 29000, LR: 1.000e-04 - train_loss:5.810e-02, rounded train_acc:0.623, valid_loss:6.067e-02, rounded valid_acc:0.634\n",
      "Iter: 29100, LR: 1.000e-04 - train_loss:3.331e-02, rounded train_acc:0.700, valid_loss:4.413e-02, rounded valid_acc:0.691\n",
      "Iter: 29200, LR: 1.000e-04 - train_loss:3.084e-02, rounded train_acc:0.710, valid_loss:4.303e-02, rounded valid_acc:0.708\n",
      "Iter: 29300, LR: 1.000e-04 - train_loss:3.597e-02, rounded train_acc:0.690, valid_loss:4.565e-02, rounded valid_acc:0.680\n",
      "Iter: 29400, LR: 1.000e-04 - train_loss:2.877e-02, rounded train_acc:0.713, valid_loss:4.313e-02, rounded valid_acc:0.703\n",
      "Iter: 29500, LR: 1.000e-04 - train_loss:3.349e-02, rounded train_acc:0.704, valid_loss:4.393e-02, rounded valid_acc:0.706\n",
      "Iter: 29600, LR: 1.000e-04 - train_loss:2.951e-02, rounded train_acc:0.717, valid_loss:3.983e-02, rounded valid_acc:0.731\n",
      "Iter: 29700, LR: 1.000e-04 - train_loss:3.501e-02, rounded train_acc:0.706, valid_loss:4.151e-02, rounded valid_acc:0.719\n",
      "Iter: 29800, LR: 1.000e-04 - train_loss:2.918e-02, rounded train_acc:0.725, valid_loss:4.553e-02, rounded valid_acc:0.696\n",
      "Iter: 29900, LR: 1.000e-04 - train_loss:4.309e-02, rounded train_acc:0.689, valid_loss:5.362e-02, rounded valid_acc:0.669\n",
      "Iter: 30000, LR: 1.000e-04 - train_loss:4.200e-02, rounded train_acc:0.695, valid_loss:5.155e-02, rounded valid_acc:0.663\n",
      "Iter: 30100, LR: 1.000e-04 - train_loss:3.117e-02, rounded train_acc:0.696, valid_loss:4.140e-02, rounded valid_acc:0.678\n",
      "Iter: 30200, LR: 1.000e-04 - train_loss:2.905e-02, rounded train_acc:0.706, valid_loss:3.708e-02, rounded valid_acc:0.688\n",
      "Iter: 30300, LR: 1.000e-04 - train_loss:3.270e-02, rounded train_acc:0.707, valid_loss:4.797e-02, rounded valid_acc:0.655\n",
      "Iter: 30400, LR: 1.000e-04 - train_loss:2.994e-02, rounded train_acc:0.710, valid_loss:4.440e-02, rounded valid_acc:0.671\n",
      "Iter: 30500, LR: 1.000e-04 - train_loss:2.788e-02, rounded train_acc:0.720, valid_loss:5.038e-02, rounded valid_acc:0.661\n",
      "Iter: 30600, LR: 1.000e-04 - train_loss:3.515e-02, rounded train_acc:0.720, valid_loss:4.049e-02, rounded valid_acc:0.699\n",
      "Iter: 30700, LR: 1.000e-04 - train_loss:3.121e-02, rounded train_acc:0.741, valid_loss:4.092e-02, rounded valid_acc:0.696\n",
      "Iter: 30800, LR: 1.000e-04 - train_loss:2.900e-02, rounded train_acc:0.739, valid_loss:4.184e-02, rounded valid_acc:0.695\n",
      "Iter: 30900, LR: 1.000e-04 - train_loss:2.964e-02, rounded train_acc:0.739, valid_loss:3.770e-02, rounded valid_acc:0.702\n",
      "Iter: 31000, LR: 1.000e-04 - train_loss:3.183e-02, rounded train_acc:0.729, valid_loss:3.692e-02, rounded valid_acc:0.702\n",
      "Iter: 31100, LR: 1.000e-04 - train_loss:3.023e-02, rounded train_acc:0.702, valid_loss:4.570e-02, rounded valid_acc:0.675\n",
      "Iter: 31200, LR: 1.000e-04 - train_loss:3.203e-02, rounded train_acc:0.720, valid_loss:3.797e-02, rounded valid_acc:0.700\n",
      "Iter: 31300, LR: 1.000e-04 - train_loss:3.779e-02, rounded train_acc:0.723, valid_loss:4.808e-02, rounded valid_acc:0.696\n",
      "Iter: 31400, LR: 1.000e-04 - train_loss:3.802e-02, rounded train_acc:0.702, valid_loss:4.899e-02, rounded valid_acc:0.676\n",
      "Iter: 31500, LR: 1.000e-04 - train_loss:3.065e-02, rounded train_acc:0.731, valid_loss:5.312e-02, rounded valid_acc:0.657\n",
      "Iter: 31600, LR: 1.000e-04 - train_loss:3.274e-02, rounded train_acc:0.690, valid_loss:3.781e-02, rounded valid_acc:0.684\n",
      "Iter: 31700, LR: 1.000e-04 - train_loss:5.762e-02, rounded train_acc:0.633, valid_loss:6.663e-02, rounded valid_acc:0.642\n",
      "Iter: 31800, LR: 1.000e-04 - train_loss:2.734e-02, rounded train_acc:0.716, valid_loss:3.742e-02, rounded valid_acc:0.686\n",
      "Iter: 31900, LR: 1.000e-04 - train_loss:2.639e-02, rounded train_acc:0.728, valid_loss:4.033e-02, rounded valid_acc:0.686\n",
      "Iter: 32000, LR: 1.000e-04 - train_loss:3.141e-02, rounded train_acc:0.720, valid_loss:4.074e-02, rounded valid_acc:0.679\n",
      "How about Test Data at dataset 64\n",
      "Iter: 32100, LR: 1.000e-04 - train_loss:4.776e-02, rounded train_acc:0.685, valid_loss:5.075e-02, rounded valid_acc:0.658\n",
      "Iter: 32200, LR: 1.000e-04 - train_loss:3.409e-02, rounded train_acc:0.711, valid_loss:3.878e-02, rounded valid_acc:0.694\n",
      "Iter: 32300, LR: 1.000e-04 - train_loss:3.253e-02, rounded train_acc:0.733, valid_loss:3.537e-02, rounded valid_acc:0.685\n",
      "Iter: 32400, LR: 1.000e-04 - train_loss:4.187e-02, rounded train_acc:0.688, valid_loss:3.923e-02, rounded valid_acc:0.684\n",
      "Iter: 32500, LR: 1.000e-04 - train_loss:3.442e-02, rounded train_acc:0.698, valid_loss:4.274e-02, rounded valid_acc:0.671\n",
      "Iter: 32600, LR: 1.000e-04 - train_loss:3.208e-02, rounded train_acc:0.702, valid_loss:4.268e-02, rounded valid_acc:0.667\n",
      "Iter: 32700, LR: 1.000e-04 - train_loss:2.757e-02, rounded train_acc:0.717, valid_loss:3.699e-02, rounded valid_acc:0.669\n",
      "Iter: 32800, LR: 1.000e-04 - train_loss:2.691e-02, rounded train_acc:0.719, valid_loss:4.757e-02, rounded valid_acc:0.658\n",
      "Iter: 32900, LR: 1.000e-04 - train_loss:5.834e-02, rounded train_acc:0.598, valid_loss:5.677e-02, rounded valid_acc:0.605\n",
      "Iter: 33000, LR: 1.000e-04 - train_loss:2.867e-02, rounded train_acc:0.701, valid_loss:4.572e-02, rounded valid_acc:0.607\n",
      "Iter: 33100, LR: 1.000e-04 - train_loss:3.976e-02, rounded train_acc:0.604, valid_loss:4.655e-02, rounded valid_acc:0.614\n",
      "Iter: 33200, LR: 1.000e-04 - train_loss:3.163e-02, rounded train_acc:0.685, valid_loss:3.816e-02, rounded valid_acc:0.658\n",
      "Iter: 33300, LR: 1.000e-04 - train_loss:3.996e-02, rounded train_acc:0.626, valid_loss:5.293e-02, rounded valid_acc:0.613\n",
      "Iter: 33400, LR: 1.000e-04 - train_loss:3.156e-02, rounded train_acc:0.691, valid_loss:3.721e-02, rounded valid_acc:0.665\n",
      "Iter: 33500, LR: 1.000e-04 - train_loss:3.273e-02, rounded train_acc:0.704, valid_loss:5.392e-02, rounded valid_acc:0.658\n",
      "Iter: 33600, LR: 1.000e-04 - train_loss:3.173e-02, rounded train_acc:0.738, valid_loss:3.856e-02, rounded valid_acc:0.678\n",
      "Iter: 33700, LR: 1.000e-04 - train_loss:3.193e-02, rounded train_acc:0.725, valid_loss:3.983e-02, rounded valid_acc:0.684\n",
      "Iter: 33800, LR: 1.000e-04 - train_loss:3.082e-02, rounded train_acc:0.718, valid_loss:4.036e-02, rounded valid_acc:0.654\n",
      "Iter: 33900, LR: 1.000e-04 - train_loss:3.112e-02, rounded train_acc:0.705, valid_loss:4.002e-02, rounded valid_acc:0.701\n",
      "Iter: 34000, LR: 1.000e-04 - train_loss:3.928e-02, rounded train_acc:0.697, valid_loss:3.988e-02, rounded valid_acc:0.658\n",
      "Iter: 34100, LR: 1.000e-04 - train_loss:3.172e-02, rounded train_acc:0.711, valid_loss:4.252e-02, rounded valid_acc:0.668\n",
      "Iter: 34200, LR: 1.000e-04 - train_loss:2.920e-02, rounded train_acc:0.707, valid_loss:3.788e-02, rounded valid_acc:0.682\n",
      "Iter: 34300, LR: 1.000e-04 - train_loss:2.972e-02, rounded train_acc:0.716, valid_loss:4.151e-02, rounded valid_acc:0.661\n",
      "Iter: 34400, LR: 1.000e-04 - train_loss:4.848e-02, rounded train_acc:0.646, valid_loss:6.549e-02, rounded valid_acc:0.614\n",
      "Iter: 34500, LR: 1.000e-04 - train_loss:3.436e-02, rounded train_acc:0.706, valid_loss:6.103e-02, rounded valid_acc:0.641\n",
      "Iter: 34600, LR: 1.000e-04 - train_loss:2.622e-02, rounded train_acc:0.756, valid_loss:3.717e-02, rounded valid_acc:0.693\n",
      "Iter: 34700, LR: 1.000e-04 - train_loss:2.512e-02, rounded train_acc:0.761, valid_loss:3.687e-02, rounded valid_acc:0.711\n",
      "Iter: 34800, LR: 1.000e-04 - train_loss:2.550e-02, rounded train_acc:0.749, valid_loss:3.882e-02, rounded valid_acc:0.718\n",
      "Iter: 34900, LR: 1.000e-04 - train_loss:3.112e-02, rounded train_acc:0.733, valid_loss:4.745e-02, rounded valid_acc:0.649\n",
      "Iter: 35000, LR: 1.000e-04 - train_loss:2.570e-02, rounded train_acc:0.760, valid_loss:3.822e-02, rounded valid_acc:0.717\n",
      "Iter: 35100, LR: 1.000e-04 - train_loss:3.138e-02, rounded train_acc:0.695, valid_loss:3.951e-02, rounded valid_acc:0.670\n",
      "Iter: 35200, LR: 1.000e-04 - train_loss:2.966e-02, rounded train_acc:0.722, valid_loss:3.826e-02, rounded valid_acc:0.689\n",
      "Iter: 35300, LR: 1.000e-04 - train_loss:5.343e-02, rounded train_acc:0.644, valid_loss:6.069e-02, rounded valid_acc:0.626\n",
      "Iter: 35400, LR: 1.000e-04 - train_loss:4.080e-02, rounded train_acc:0.657, valid_loss:4.672e-02, rounded valid_acc:0.636\n",
      "Iter: 35500, LR: 1.000e-04 - train_loss:4.182e-02, rounded train_acc:0.684, valid_loss:5.126e-02, rounded valid_acc:0.661\n",
      "Iter: 35600, LR: 1.000e-04 - train_loss:2.629e-02, rounded train_acc:0.721, valid_loss:3.671e-02, rounded valid_acc:0.722\n",
      "Iter: 35700, LR: 1.000e-04 - train_loss:2.510e-02, rounded train_acc:0.719, valid_loss:3.430e-02, rounded valid_acc:0.713\n",
      "Iter: 35800, LR: 1.000e-04 - train_loss:2.361e-02, rounded train_acc:0.730, valid_loss:3.397e-02, rounded valid_acc:0.716\n",
      "Iter: 35900, LR: 1.000e-04 - train_loss:2.337e-02, rounded train_acc:0.741, valid_loss:3.694e-02, rounded valid_acc:0.713\n",
      "Iter: 36000, LR: 1.000e-04 - train_loss:2.398e-02, rounded train_acc:0.738, valid_loss:4.039e-02, rounded valid_acc:0.693\n",
      "Iter: 36100, LR: 1.000e-04 - train_loss:2.678e-02, rounded train_acc:0.731, valid_loss:3.402e-02, rounded valid_acc:0.711\n",
      "Iter: 36200, LR: 1.000e-04 - train_loss:2.884e-02, rounded train_acc:0.720, valid_loss:3.780e-02, rounded valid_acc:0.688\n",
      "Iter: 36300, LR: 1.000e-04 - train_loss:2.730e-02, rounded train_acc:0.723, valid_loss:3.763e-02, rounded valid_acc:0.682\n",
      "Iter: 36400, LR: 1.000e-04 - train_loss:2.346e-02, rounded train_acc:0.747, valid_loss:3.630e-02, rounded valid_acc:0.691\n",
      "Iter: 36500, LR: 1.000e-04 - train_loss:2.330e-02, rounded train_acc:0.719, valid_loss:3.636e-02, rounded valid_acc:0.684\n",
      "Iter: 36600, LR: 1.000e-04 - train_loss:3.398e-02, rounded train_acc:0.713, valid_loss:4.255e-02, rounded valid_acc:0.696\n",
      "Iter: 36700, LR: 1.000e-04 - train_loss:3.155e-02, rounded train_acc:0.721, valid_loss:4.622e-02, rounded valid_acc:0.696\n",
      "Iter: 36800, LR: 1.000e-04 - train_loss:3.403e-02, rounded train_acc:0.728, valid_loss:3.641e-02, rounded valid_acc:0.718\n",
      "Iter: 36900, LR: 1.000e-04 - train_loss:3.167e-02, rounded train_acc:0.710, valid_loss:3.909e-02, rounded valid_acc:0.676\n",
      "Iter: 37000, LR: 1.000e-04 - train_loss:2.981e-02, rounded train_acc:0.719, valid_loss:3.949e-02, rounded valid_acc:0.708\n",
      "Iter: 37100, LR: 1.000e-04 - train_loss:3.417e-02, rounded train_acc:0.659, valid_loss:3.773e-02, rounded valid_acc:0.693\n",
      "Iter: 37200, LR: 1.000e-04 - train_loss:3.346e-02, rounded train_acc:0.702, valid_loss:3.391e-02, rounded valid_acc:0.718\n",
      "Iter: 37300, LR: 1.000e-04 - train_loss:2.993e-02, rounded train_acc:0.725, valid_loss:3.718e-02, rounded valid_acc:0.716\n",
      "Iter: 37400, LR: 1.000e-04 - train_loss:2.879e-02, rounded train_acc:0.721, valid_loss:3.372e-02, rounded valid_acc:0.709\n",
      "Iter: 37500, LR: 1.000e-04 - train_loss:4.014e-02, rounded train_acc:0.679, valid_loss:5.079e-02, rounded valid_acc:0.650\n",
      "Iter: 37600, LR: 1.000e-04 - train_loss:2.569e-02, rounded train_acc:0.736, valid_loss:4.445e-02, rounded valid_acc:0.710\n",
      "Iter: 37700, LR: 1.000e-04 - train_loss:2.514e-02, rounded train_acc:0.743, valid_loss:3.506e-02, rounded valid_acc:0.731\n",
      "Iter: 37800, LR: 1.000e-04 - train_loss:2.771e-02, rounded train_acc:0.734, valid_loss:4.032e-02, rounded valid_acc:0.691\n",
      "Iter: 37900, LR: 1.000e-04 - train_loss:2.385e-02, rounded train_acc:0.747, valid_loss:3.682e-02, rounded valid_acc:0.711\n",
      "Iter: 38000, LR: 1.000e-04 - train_loss:2.143e-02, rounded train_acc:0.770, valid_loss:4.246e-02, rounded valid_acc:0.712\n",
      "Iter: 38100, LR: 1.000e-04 - train_loss:2.844e-02, rounded train_acc:0.721, valid_loss:4.382e-02, rounded valid_acc:0.685\n",
      "Iter: 38200, LR: 1.000e-04 - train_loss:2.477e-02, rounded train_acc:0.744, valid_loss:3.598e-02, rounded valid_acc:0.709\n",
      "Iter: 38300, LR: 1.000e-04 - train_loss:2.386e-02, rounded train_acc:0.739, valid_loss:3.219e-02, rounded valid_acc:0.722\n",
      "Iter: 38400, LR: 1.000e-04 - train_loss:2.367e-02, rounded train_acc:0.747, valid_loss:3.428e-02, rounded valid_acc:0.702\n",
      "Iter: 38500, LR: 1.000e-04 - train_loss:2.292e-02, rounded train_acc:0.744, valid_loss:3.219e-02, rounded valid_acc:0.696\n",
      "Iter: 38600, LR: 1.000e-04 - train_loss:2.338e-02, rounded train_acc:0.755, valid_loss:3.262e-02, rounded valid_acc:0.716\n",
      "Iter: 38700, LR: 1.000e-04 - train_loss:2.111e-02, rounded train_acc:0.770, valid_loss:3.171e-02, rounded valid_acc:0.722\n",
      "Iter: 38800, LR: 1.000e-04 - train_loss:2.114e-02, rounded train_acc:0.777, valid_loss:3.375e-02, rounded valid_acc:0.718\n",
      "Iter: 38900, LR: 1.000e-04 - train_loss:2.758e-02, rounded train_acc:0.771, valid_loss:3.468e-02, rounded valid_acc:0.736\n",
      "Iter: 39000, LR: 1.000e-04 - train_loss:2.114e-02, rounded train_acc:0.784, valid_loss:3.351e-02, rounded valid_acc:0.714\n",
      "Iter: 39100, LR: 1.000e-04 - train_loss:3.074e-02, rounded train_acc:0.706, valid_loss:4.912e-02, rounded valid_acc:0.692\n",
      "Iter: 39200, LR: 1.000e-04 - train_loss:3.957e-02, rounded train_acc:0.678, valid_loss:4.748e-02, rounded valid_acc:0.685\n",
      "Iter: 39300, LR: 1.000e-04 - train_loss:3.102e-02, rounded train_acc:0.714, valid_loss:4.630e-02, rounded valid_acc:0.695\n",
      "Iter: 39400, LR: 1.000e-04 - train_loss:2.550e-02, rounded train_acc:0.726, valid_loss:3.998e-02, rounded valid_acc:0.693\n",
      "Iter: 39500, LR: 1.000e-04 - train_loss:2.614e-02, rounded train_acc:0.738, valid_loss:3.598e-02, rounded valid_acc:0.693\n",
      "Iter: 39600, LR: 1.000e-04 - train_loss:3.760e-02, rounded train_acc:0.703, valid_loss:5.405e-02, rounded valid_acc:0.660\n",
      "Iter: 39700, LR: 1.000e-04 - train_loss:3.880e-02, rounded train_acc:0.690, valid_loss:4.024e-02, rounded valid_acc:0.671\n",
      "Iter: 39800, LR: 1.000e-04 - train_loss:2.581e-02, rounded train_acc:0.743, valid_loss:3.250e-02, rounded valid_acc:0.690\n",
      "Iter: 39900, LR: 1.000e-04 - train_loss:3.029e-02, rounded train_acc:0.732, valid_loss:3.818e-02, rounded valid_acc:0.690\n",
      "Iter: 40000, LR: 1.000e-04 - train_loss:2.404e-02, rounded train_acc:0.741, valid_loss:3.417e-02, rounded valid_acc:0.680\n",
      "Iter: 40100, LR: 1.000e-04 - train_loss:2.514e-02, rounded train_acc:0.722, valid_loss:4.210e-02, rounded valid_acc:0.688\n",
      "Iter: 40200, LR: 1.000e-04 - train_loss:2.769e-02, rounded train_acc:0.719, valid_loss:3.640e-02, rounded valid_acc:0.700\n",
      "Iter: 40300, LR: 1.000e-04 - train_loss:3.713e-02, rounded train_acc:0.697, valid_loss:3.914e-02, rounded valid_acc:0.686\n",
      "Iter: 40400, LR: 1.000e-04 - train_loss:2.202e-02, rounded train_acc:0.742, valid_loss:3.342e-02, rounded valid_acc:0.695\n",
      "Iter: 40500, LR: 1.000e-04 - train_loss:2.238e-02, rounded train_acc:0.747, valid_loss:3.275e-02, rounded valid_acc:0.703\n",
      "Iter: 40600, LR: 1.000e-04 - train_loss:4.735e-02, rounded train_acc:0.691, valid_loss:5.670e-02, rounded valid_acc:0.636\n",
      "Iter: 40700, LR: 1.000e-04 - train_loss:3.156e-02, rounded train_acc:0.743, valid_loss:3.096e-02, rounded valid_acc:0.737\n",
      "Iter: 40800, LR: 1.000e-04 - train_loss:2.555e-02, rounded train_acc:0.751, valid_loss:3.457e-02, rounded valid_acc:0.726\n",
      "Iter: 40900, LR: 1.000e-04 - train_loss:2.999e-02, rounded train_acc:0.717, valid_loss:3.884e-02, rounded valid_acc:0.729\n",
      "Iter: 41000, LR: 1.000e-04 - train_loss:2.565e-02, rounded train_acc:0.743, valid_loss:3.451e-02, rounded valid_acc:0.712\n",
      "Iter: 41100, LR: 1.000e-04 - train_loss:2.775e-02, rounded train_acc:0.720, valid_loss:3.221e-02, rounded valid_acc:0.688\n",
      "Iter: 41200, LR: 1.000e-04 - train_loss:3.305e-02, rounded train_acc:0.730, valid_loss:3.745e-02, rounded valid_acc:0.710\n",
      "Iter: 41300, LR: 1.000e-04 - train_loss:3.104e-02, rounded train_acc:0.709, valid_loss:4.435e-02, rounded valid_acc:0.649\n",
      "Iter: 41400, LR: 1.000e-04 - train_loss:2.523e-02, rounded train_acc:0.743, valid_loss:5.019e-02, rounded valid_acc:0.657\n",
      "Iter: 41500, LR: 1.000e-04 - train_loss:3.606e-02, rounded train_acc:0.681, valid_loss:5.937e-02, rounded valid_acc:0.595\n",
      "Iter: 41600, LR: 1.000e-04 - train_loss:2.594e-02, rounded train_acc:0.702, valid_loss:3.780e-02, rounded valid_acc:0.705\n",
      "Iter: 41700, LR: 1.000e-04 - train_loss:3.868e-02, rounded train_acc:0.652, valid_loss:5.017e-02, rounded valid_acc:0.652\n",
      "Iter: 41800, LR: 1.000e-04 - train_loss:2.610e-02, rounded train_acc:0.710, valid_loss:3.365e-02, rounded valid_acc:0.715\n",
      "Iter: 41900, LR: 1.000e-04 - train_loss:3.530e-02, rounded train_acc:0.668, valid_loss:5.788e-02, rounded valid_acc:0.638\n",
      "Iter: 42000, LR: 1.000e-04 - train_loss:3.022e-02, rounded train_acc:0.704, valid_loss:3.430e-02, rounded valid_acc:0.706\n",
      "Iter: 42100, LR: 1.000e-04 - train_loss:3.252e-02, rounded train_acc:0.720, valid_loss:3.136e-02, rounded valid_acc:0.735\n",
      "Iter: 42200, LR: 1.000e-04 - train_loss:2.790e-02, rounded train_acc:0.731, valid_loss:4.153e-02, rounded valid_acc:0.706\n",
      "Iter: 42300, LR: 1.000e-04 - train_loss:2.439e-02, rounded train_acc:0.737, valid_loss:3.338e-02, rounded valid_acc:0.729\n",
      "Iter: 42400, LR: 1.000e-04 - train_loss:2.801e-02, rounded train_acc:0.724, valid_loss:3.394e-02, rounded valid_acc:0.705\n",
      "Iter: 42500, LR: 1.000e-04 - train_loss:2.493e-02, rounded train_acc:0.727, valid_loss:3.918e-02, rounded valid_acc:0.686\n",
      "Iter: 42600, LR: 1.000e-04 - train_loss:2.104e-02, rounded train_acc:0.773, valid_loss:2.975e-02, rounded valid_acc:0.717\n",
      "Iter: 42700, LR: 1.000e-04 - train_loss:2.953e-02, rounded train_acc:0.739, valid_loss:3.206e-02, rounded valid_acc:0.731\n",
      "Iter: 42800, LR: 1.000e-04 - train_loss:1.924e-02, rounded train_acc:0.789, valid_loss:3.216e-02, rounded valid_acc:0.716\n",
      "Iter: 42900, LR: 1.000e-04 - train_loss:1.936e-02, rounded train_acc:0.797, valid_loss:2.941e-02, rounded valid_acc:0.732\n",
      "Iter: 43000, LR: 1.000e-04 - train_loss:1.881e-02, rounded train_acc:0.795, valid_loss:3.263e-02, rounded valid_acc:0.710\n",
      "Iter: 43100, LR: 1.000e-04 - train_loss:2.259e-02, rounded train_acc:0.758, valid_loss:3.308e-02, rounded valid_acc:0.734\n",
      "Iter: 43200, LR: 1.000e-04 - train_loss:2.171e-02, rounded train_acc:0.772, valid_loss:3.191e-02, rounded valid_acc:0.743\n",
      "Iter: 43300, LR: 1.000e-04 - train_loss:2.027e-02, rounded train_acc:0.780, valid_loss:3.225e-02, rounded valid_acc:0.758\n",
      "Iter: 43400, LR: 1.000e-04 - train_loss:1.912e-02, rounded train_acc:0.787, valid_loss:3.213e-02, rounded valid_acc:0.739\n",
      "Iter: 43500, LR: 1.000e-04 - train_loss:2.047e-02, rounded train_acc:0.788, valid_loss:3.219e-02, rounded valid_acc:0.751\n",
      "Iter: 43600, LR: 1.000e-04 - train_loss:2.987e-02, rounded train_acc:0.712, valid_loss:3.031e-02, rounded valid_acc:0.703\n",
      "Iter: 43700, LR: 1.000e-04 - train_loss:2.307e-02, rounded train_acc:0.754, valid_loss:3.335e-02, rounded valid_acc:0.711\n",
      "Iter: 43800, LR: 1.000e-04 - train_loss:2.338e-02, rounded train_acc:0.757, valid_loss:3.185e-02, rounded valid_acc:0.722\n",
      "Iter: 43900, LR: 1.000e-04 - train_loss:2.112e-02, rounded train_acc:0.769, valid_loss:3.263e-02, rounded valid_acc:0.716\n",
      "Iter: 44000, LR: 1.000e-04 - train_loss:2.046e-02, rounded train_acc:0.771, valid_loss:3.858e-02, rounded valid_acc:0.698\n",
      "Iter: 44100, LR: 1.000e-04 - train_loss:2.290e-02, rounded train_acc:0.754, valid_loss:3.140e-02, rounded valid_acc:0.726\n",
      "Iter: 44200, LR: 1.000e-04 - train_loss:3.829e-02, rounded train_acc:0.699, valid_loss:5.085e-02, rounded valid_acc:0.670\n",
      "Iter: 44300, LR: 1.000e-04 - train_loss:2.217e-02, rounded train_acc:0.772, valid_loss:3.379e-02, rounded valid_acc:0.713\n",
      "Iter: 44400, LR: 1.000e-04 - train_loss:2.064e-02, rounded train_acc:0.788, valid_loss:3.267e-02, rounded valid_acc:0.740\n",
      "Iter: 44500, LR: 1.000e-04 - train_loss:2.628e-02, rounded train_acc:0.769, valid_loss:3.849e-02, rounded valid_acc:0.685\n",
      "Iter: 44600, LR: 1.000e-04 - train_loss:2.794e-02, rounded train_acc:0.744, valid_loss:3.604e-02, rounded valid_acc:0.705\n",
      "Iter: 44700, LR: 1.000e-04 - train_loss:2.549e-02, rounded train_acc:0.744, valid_loss:3.743e-02, rounded valid_acc:0.701\n",
      "Iter: 44800, LR: 1.000e-04 - train_loss:2.755e-02, rounded train_acc:0.759, valid_loss:3.230e-02, rounded valid_acc:0.721\n",
      "Iter: 44900, LR: 1.000e-04 - train_loss:2.477e-02, rounded train_acc:0.771, valid_loss:4.195e-02, rounded valid_acc:0.707\n",
      "Iter: 45000, LR: 1.000e-04 - train_loss:2.266e-02, rounded train_acc:0.769, valid_loss:3.339e-02, rounded valid_acc:0.737\n",
      "Iter: 45100, LR: 1.000e-04 - train_loss:2.858e-02, rounded train_acc:0.765, valid_loss:4.161e-02, rounded valid_acc:0.693\n",
      "Iter: 45200, LR: 1.000e-04 - train_loss:2.590e-02, rounded train_acc:0.777, valid_loss:3.490e-02, rounded valid_acc:0.704\n",
      "Iter: 45300, LR: 1.000e-04 - train_loss:2.520e-02, rounded train_acc:0.782, valid_loss:3.790e-02, rounded valid_acc:0.704\n",
      "Iter: 45400, LR: 1.000e-04 - train_loss:2.551e-02, rounded train_acc:0.777, valid_loss:3.280e-02, rounded valid_acc:0.717\n",
      "Iter: 45500, LR: 1.000e-04 - train_loss:2.276e-02, rounded train_acc:0.787, valid_loss:3.038e-02, rounded valid_acc:0.713\n",
      "Iter: 45600, LR: 1.000e-04 - train_loss:2.864e-02, rounded train_acc:0.734, valid_loss:3.596e-02, rounded valid_acc:0.696\n",
      "Iter: 45700, LR: 1.000e-04 - train_loss:2.203e-02, rounded train_acc:0.759, valid_loss:2.777e-02, rounded valid_acc:0.700\n",
      "Iter: 45800, LR: 1.000e-04 - train_loss:2.166e-02, rounded train_acc:0.765, valid_loss:3.336e-02, rounded valid_acc:0.715\n",
      "Iter: 45900, LR: 1.000e-04 - train_loss:2.125e-02, rounded train_acc:0.773, valid_loss:3.035e-02, rounded valid_acc:0.723\n",
      "Iter: 46000, LR: 1.000e-04 - train_loss:1.951e-02, rounded train_acc:0.776, valid_loss:2.787e-02, rounded valid_acc:0.721\n",
      "Iter: 46100, LR: 1.000e-04 - train_loss:2.463e-02, rounded train_acc:0.755, valid_loss:3.270e-02, rounded valid_acc:0.694\n",
      "Iter: 46200, LR: 1.000e-04 - train_loss:2.150e-02, rounded train_acc:0.773, valid_loss:3.808e-02, rounded valid_acc:0.725\n",
      "Iter: 46300, LR: 1.000e-04 - train_loss:2.805e-02, rounded train_acc:0.760, valid_loss:2.902e-02, rounded valid_acc:0.705\n",
      "Iter: 46400, LR: 1.000e-04 - train_loss:2.896e-02, rounded train_acc:0.766, valid_loss:3.662e-02, rounded valid_acc:0.707\n",
      "Iter: 46500, LR: 1.000e-04 - train_loss:2.260e-02, rounded train_acc:0.789, valid_loss:3.058e-02, rounded valid_acc:0.730\n",
      "Iter: 46600, LR: 1.000e-04 - train_loss:3.311e-02, rounded train_acc:0.728, valid_loss:3.235e-02, rounded valid_acc:0.680\n",
      "Iter: 46700, LR: 1.000e-04 - train_loss:2.712e-02, rounded train_acc:0.749, valid_loss:2.789e-02, rounded valid_acc:0.709\n",
      "Iter: 46800, LR: 1.000e-04 - train_loss:2.418e-02, rounded train_acc:0.756, valid_loss:2.870e-02, rounded valid_acc:0.708\n",
      "Iter: 46900, LR: 1.000e-04 - train_loss:4.135e-02, rounded train_acc:0.711, valid_loss:4.762e-02, rounded valid_acc:0.655\n",
      "Iter: 47000, LR: 1.000e-04 - train_loss:2.538e-02, rounded train_acc:0.737, valid_loss:5.738e-02, rounded valid_acc:0.675\n",
      "Iter: 47100, LR: 1.000e-04 - train_loss:2.493e-02, rounded train_acc:0.737, valid_loss:2.963e-02, rounded valid_acc:0.719\n",
      "Iter: 47200, LR: 1.000e-04 - train_loss:2.341e-02, rounded train_acc:0.728, valid_loss:3.148e-02, rounded valid_acc:0.734\n",
      "Iter: 47300, LR: 1.000e-04 - train_loss:2.188e-02, rounded train_acc:0.744, valid_loss:2.986e-02, rounded valid_acc:0.721\n",
      "Iter: 47400, LR: 1.000e-04 - train_loss:2.317e-02, rounded train_acc:0.742, valid_loss:2.887e-02, rounded valid_acc:0.728\n",
      "Iter: 47500, LR: 1.000e-04 - train_loss:2.205e-02, rounded train_acc:0.750, valid_loss:3.287e-02, rounded valid_acc:0.723\n",
      "Iter: 47600, LR: 1.000e-04 - train_loss:2.483e-02, rounded train_acc:0.727, valid_loss:3.057e-02, rounded valid_acc:0.681\n",
      "Iter: 47700, LR: 1.000e-04 - train_loss:2.270e-02, rounded train_acc:0.742, valid_loss:3.682e-02, rounded valid_acc:0.691\n",
      "Iter: 47800, LR: 1.000e-04 - train_loss:2.293e-02, rounded train_acc:0.747, valid_loss:3.080e-02, rounded valid_acc:0.695\n",
      "Iter: 47900, LR: 1.000e-04 - train_loss:3.179e-02, rounded train_acc:0.720, valid_loss:3.779e-02, rounded valid_acc:0.684\n",
      "Iter: 48000, LR: 1.000e-04 - train_loss:2.311e-02, rounded train_acc:0.740, valid_loss:3.441e-02, rounded valid_acc:0.701\n",
      "Iter: 48100, LR: 1.000e-04 - train_loss:2.242e-02, rounded train_acc:0.770, valid_loss:3.036e-02, rounded valid_acc:0.728\n",
      "Iter: 48200, LR: 1.000e-04 - train_loss:2.382e-02, rounded train_acc:0.745, valid_loss:3.167e-02, rounded valid_acc:0.729\n",
      "Iter: 48300, LR: 1.000e-04 - train_loss:2.687e-02, rounded train_acc:0.755, valid_loss:3.342e-02, rounded valid_acc:0.713\n",
      "Iter: 48400, LR: 1.000e-04 - train_loss:2.803e-02, rounded train_acc:0.756, valid_loss:2.901e-02, rounded valid_acc:0.727\n",
      "Iter: 48500, LR: 1.000e-04 - train_loss:1.949e-02, rounded train_acc:0.775, valid_loss:3.162e-02, rounded valid_acc:0.719\n",
      "Iter: 48600, LR: 1.000e-04 - train_loss:2.653e-02, rounded train_acc:0.761, valid_loss:2.657e-02, rounded valid_acc:0.747\n",
      "Iter: 48700, LR: 1.000e-04 - train_loss:3.083e-02, rounded train_acc:0.715, valid_loss:3.543e-02, rounded valid_acc:0.717\n",
      "Iter: 48800, LR: 1.000e-04 - train_loss:2.496e-02, rounded train_acc:0.779, valid_loss:2.700e-02, rounded valid_acc:0.759\n",
      "Iter: 48900, LR: 1.000e-04 - train_loss:2.758e-02, rounded train_acc:0.781, valid_loss:3.821e-02, rounded valid_acc:0.733\n",
      "Iter: 49000, LR: 1.000e-04 - train_loss:3.655e-02, rounded train_acc:0.733, valid_loss:4.237e-02, rounded valid_acc:0.712\n",
      "Iter: 49100, LR: 1.000e-04 - train_loss:2.595e-02, rounded train_acc:0.781, valid_loss:2.579e-02, rounded valid_acc:0.749\n",
      "Iter: 49200, LR: 1.000e-04 - train_loss:2.124e-02, rounded train_acc:0.790, valid_loss:2.870e-02, rounded valid_acc:0.737\n",
      "Iter: 49300, LR: 1.000e-04 - train_loss:2.356e-02, rounded train_acc:0.787, valid_loss:2.658e-02, rounded valid_acc:0.748\n",
      "Iter: 49400, LR: 1.000e-04 - train_loss:2.213e-02, rounded train_acc:0.781, valid_loss:3.852e-02, rounded valid_acc:0.735\n",
      "Iter: 49500, LR: 1.000e-04 - train_loss:2.063e-02, rounded train_acc:0.795, valid_loss:3.194e-02, rounded valid_acc:0.755\n",
      "How about Test Data at dataset 99\n",
      "Iter: 49600, LR: 1.000e-04 - train_loss:2.790e-02, rounded train_acc:0.740, valid_loss:2.754e-02, rounded valid_acc:0.735\n",
      "Iter: 49700, LR: 1.000e-04 - train_loss:2.368e-02, rounded train_acc:0.756, valid_loss:3.272e-02, rounded valid_acc:0.697\n",
      "Iter: 49800, LR: 1.000e-04 - train_loss:2.228e-02, rounded train_acc:0.747, valid_loss:2.686e-02, rounded valid_acc:0.722\n",
      "Iter: 49900, LR: 1.000e-04 - train_loss:2.563e-02, rounded train_acc:0.764, valid_loss:3.629e-02, rounded valid_acc:0.706\n",
      "Iter: 50000, LR: 1.000e-04 - train_loss:2.175e-02, rounded train_acc:0.779, valid_loss:2.741e-02, rounded valid_acc:0.720\n"
     ]
    }
   ],
   "source": [
    "# we use net at different training stage on the same test_input\n",
    "net, _, (counter_lst, netout_lst, db_lst, Winput_lst, Winputbias_lst, \\\n",
    "         Woutput_lst, Wall_lst, marker_lst, loss_lst, acc_lst) = net_helpers.train_network(params, device=device, verbose=verbose, train=train, hyp_dict=hyp_dict, netFunction=netFunction, test_input=test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47256480-5b47-496e-887e-b8de35dcc8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATsAAAEUCAYAAABUJpEkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8WgzjOAAAACXBIWXMAAA9hAAAPYQGoP6dpAABWxklEQVR4nO2deZxN9f/Hn+eusw9jmWEwIktkrUFlaaGQpKKM0PKVtIg2SxGF0CKiKJUtDUmKFJVsoZLk10a2CWMWw8y4s931/P44955779w7q5k7mM/z8fCYez7ncz7nc485r3l/Pp/35/2WZFmWEQgEgsscTVV3QCAQCAKBEDuBQFAtEGInEAiqBbqq7sClSuvWrWnatGmJ9ZKTk4mNja3Qe1dGm5XVrmhT/D9VVZtHjx7lr7/+chfIgnJxxx13VGi9yrj3xdCuaLPiuVT6WtVtFq4rhrGXIAkJCZdMu5XV14rmUvru4pmWDyF2lyCX0kskXsyKRzzT8iHErpK5VH4xLyXEM614qsMzFWJXyVSHX6JAI55pxVMdnqkQO4FAUC0QYicQCKoFQuwEAkG1QIidQCCoFgixKyfJycn079+fxMTEqu6KQBBQ8rdtI2ft2oDdT7bZkO32UtdPTEykf//+JCcne5VLsixCPJWH/v37s379+qruhkAQcE736AFA3dWr0cXEVOq9ZIuFtIED0dSqRd0lS8p0beF3VFh2AoGg1Mhms/rZduIE2fPmYc/KAsCelYVp6VJkh0OtYz9zBltKinpsPXyY7HffLd29bDZSevXCkZ2N7dixC+67EDuBoBohWyyYli9HtlrLdb1L2AAyX36Z3M8/x/rvvwCY3nsP05Il2E+fBsCRk0PawIGkDx6sXnNu0iRyV68u1b2sR4+Wq49FIcROIKhG5G3ciOnDDynYs6d8DXhYdnJOjvLBOZ/myM1VfmZnA3DuhRd8Lpfz85WfNluxt8ldt46MkSO9ygp++YW8TZvK129EiCeBoNogm81kz50LgCY8vFxtOFwC59muy0p0ip797FlkWcZy4IBaJ3/nTix//KEKoWy1Iun8y49p6VJMHvNz+quuQrZayf/uO+wpKYT07l2uvguxEwguY+yZmVj++IPg7t0x/9//uU+Uc10y47HHfMrk3Fyy3ngD6+HDgDKXd27iRPW8NiaGzEmTvC+yWCA42Luv6enIZrOX0AEYrr4a8y+/YE9PR1unTrn6DULsBILLmnOTJmH980+Ct2/H6hHIsqRhZFnIXb8e699/q8cFO3diPXgQAON112H2M2T2N2eYNmiQ3/YloxHZbMaeno6+Zcty91PM2QkElwCyxYL1yBGv1dDS4FoskG02bKmpaF2uIuUQu6IE0rVA4e9Y36SJ/7YsllLfVzIYFLE7cwZddHSpryuMEDuB4BIgc/p0zvzvf6TceisFv/xS6uvkggLlp8WCPTUVXcOGynF5xK6Q0H4Tez1nQmr5CqfDgTY2lpiNG9FERgIgFZojLEnsPou7hZVNlLk5yWjEkZkJViuaunXL3G8XQuwEgkuAgu3b1c/WQ4dKfZ1LoGSzGXtKCtoGDZyNlN31xCWcAIaHH+Gdq+7lzav8h4bS1a+PJiwMyWAAQN+sGQBSWBgAZx54QJ3j88fSZneQ6BS7+WdjWNq0HwBaIXYCweWNFBrq/hwUVPoLXW4h589jT0tDf8UVgP85MwDTihWcmzzZ7zlPyy6330AA/qx5Jf9ENiaoa1evutp69ZS+OsXO2LEjkU8/Ta033lDr2FJScOTlkT13Lg6TyX0fj3a+jr2BNeci+eyKnkq7FyB2YoFCILgEkEJCkJ1+bIXFzpacTPqQIdRdtQqdU2TAe6h6ZvhwAPTOjHiuc7LNBpIEGg2pt9+u3sMfnmKXmu0ehv4b0Yhromp41dW5MoDp9Uqfg4MJHTAA26lT7koaDZlTpmD+5Rdy161Ti+fHj1A/v3uV96KFa1hcHoRlJxBcAF/+fpZVe88UW8d+5gznP/zQy3opMx5zXJJTQOwZGWQvXEjB7t1KFQ+/NoAzDz3k04yuSRPQ6dRhbMqtt5Jy881Y9u3zErpzL73E6R49yNu8WS3zHMae9hC7GhYTmvBwbJKGP+I6AmBo1065xnkfl4Xn+gngOHcOs5/5x28jr/b7CCKeeAJJkvyeKw1C7ASCC+DlDSd4bdOpYusU7NpFzvLlnHn00VK1KZvNnO7Rg4KffnKXeYidKwJI/nffkbtqFXmuze4a9+ss22zYTpzwarfm9OmkWrQkRTRANpvJnDVLHeaeHTcOgB9iriXTEKbOEeZv2QJAZkY2mR5D0IOp+epnu6RFCg5mZZM+TGw2HO2n6/k9qAGHUvNUgZSMRqWyh9hlv/mmz3evOW1akc9F361HkedKgxA7gaCc5Jj9hx3KzLXi8HDadW2jsicn48jJwZaSooqX5eBB8n/4wet6x/nzAOR+9pla5jXH5rzWfvYsgFvUPO5pP+NrbQZ17Uq/+X/zZMexWP7+m/xvvnGftNuRgTlXD+WVdu5tWpJej6nAxi0Lj7EhX3H70Ly3lB3/ZnNLyxpKd7R6JIOB5FBlPs0RFMyoj4+QsPiQasmpq7JOqxTg/2peydxW3gscmpo1ffrt4nxYVJHnSoMQO0G1J3fDBqz//Vfm60Ysc68mZuUpc2A2h8wtc/5k4TZ3pA/XFikA6/HjpA8eTNbrr1OwaxcZjz5K5ssve7Xr2j+qbq2y28FmI+Suu5Rjp/AVttxsSUmYf/tNqWM2M+q6iYzuoSw2aOvV8xoC2lNTfb6PA+X84chG5GudFphOh6lAEfV3rxrE3FYJ9F2TRdp5K41rGzHazZg1ithZNMoSgNXuFt2QPn2oMWECxs6dAdh72sKU9oqFO7vNA3xfvzP/hSq+f+lBNVl+wm35AWiRmXlXYwDyLaWPaecPIXaCao0sy2S/8QbnJkxwl5nN5Kxe7RWqyB//prmHcoMW/QOgCsPmn0+q1psjK0uZKwMszi1b+d9+63ejPIAjL0/56bTwXOJmaN0aTWSkW+xOnnQ7CQM5n3zC2aefxnbyJNlvv82p0GiO62sBHsNIJzY/4u6Q3HJwIKq5cp1eT67Z/Ry+r99Z/RwZrMNot2LR6JmXVptfa7dWvqPNXV/S6Qjp0we7DJv/yuSxVUnsq30VMqB1KM/qiesm4EDizdZDWfhbnlef7Eg0iFIEMN9a/P9HSQixE1RrZNeigYfVk/PZZ5xbuAjL77+Tmm3Bfu4cBbt2AcqGds/4bC7O5iqWnSlf+ZmdayXv66+R7XbyN29WV0lN77/vtx+O/HysSUlKn5yWnT01FVmW1cUEyWjEkZ2N6f33mbP0Jx5reD9BN9zg05bpo4+w7NunHp/Xh/B44+F8/luGWmYxewyLnd/dLmnVoiy94g+nWHb+HZAjg7UYHFaWNbuDz86EqeXf/Z3lVc/ukLn1rT+Z+HmSWmaTtGhlt3glhdXHotHjj2C90q+ExYf48vezfuuUBiF2gmqNa27LMwrIAZOBAbfMYfN/Nvq+/Rebpr7LuRde4Pz775M5aRJnn322yPaynGKXqw8GjUa15LT16xfbj/T77uPMAw8AIOe5rRv7qVPkrluHHYkbtodzX4+ZZOtD+fikkaTwWE6176Y66m6NuYYZbR/ipTONvObCDg5/lmO6KKZvPKmWnde7/faCe/UCwKZxi905o9PFQ6fn6IlMv32OCNKhc/gOLRfvdA+RP/wxlV//y1GH+S4KC5t25ONow8PwR7DBLVNrfs3wW6c0CLETVGv8id0hWwgAS48oFk9mliI+OStXAkqUD3/ZDLLybKSt+QJQhoSS0Yjt+HHQ64kYNarYfnjO6zk8xC5n1SpMH37ISee8Vq4+mC8b9SDSrtQZth1qr/8K4/XX8+bVw9hTtx27ott7DTd3BvnuTzV5iJ2hbVvlWXgMY88FKWK31hHHzG3+rSm9TiLTWHyoqHe2pvDYx0d8yi1aHWatgXuvra1859ZtKajj9hEcdPw79XOIh9jptML1RCAoM2fHj+fc+PGA4rQL8P6OFP6xKEKQVKBMuEvmAq/rpJAQCmy+Yjdm3i4O/eZ+sWWrFfOBA+ji4oqM3eYP1zAWjYa8r74C4N/IRup5h6ShRd5p9bjzq79Tw+k64o+th9xCOvw6ZcX0vMEtdhrn7gzPObtNsdezI7oDf2T7SkT7hkr9UKMWs9boc740rGzShxx9CDVClOdSYHVwvsBOXJSR94ddyXMv3cs1cWE8dUt9gvXuPmjKr3ViB4Wg+mL28GNz+Zst2p4KeHvp5+m8dyzIVivZp31dO/6wRfBH8zvV43Nr16E9doSIJ54oU7/k/HxFfCVJdfQ9a6yhnnegAZuNWhRwFqVvmho1/LTkyxW1lfqew1jX4oWrrGaQhswCB6+1ecBvG/d3rstDN0i0axDq93xp2NRAmWt0iV2+xY6pwM4j3WK4tnE4EM7ixkpdTys6KrT8kiUsO0G1YfNfmfSc84ffc7LZTL5HTDZP8rRB5Grdguc4c4b/Ro0u8j46hzI/lZl8Bm3duuTcdieb/jzHwze8xOrGvVQXD3+Yli7Fcf48mrAwfq/VguNhytAu0+AeLtolDVYH1JHc27eK8vkrTMMoIxrZ4SV2GAz8WusqnrhOWZGOCNYWcbVCeJCWbs2UPwi1Sik+YUb/UhNu1KKRILvAjtUuEx7ke29Pl5kX+zYs1f38IcSunIi8sRc/st3utXK6cFsK53JtWO2yz5ybXFDA6SfH+G1nWbM7uO+mWV5lf9e4osj7XhetvJzn9aEY2rXj7oUHeWHdf6QHR7Hiytv5qsnNRV5rWrJEiU4SE8OLzYczuosyzPacG5MlCatGRx2de9I/Jbt0UUyiQnREOMwcjmiEVassEhzN1zO1g3t3h07rXxaa1VUEP9RDuL58shVbnm3jU/eGKyMAuKOt4ghcO0y5V7dmEV71gvQagvQajmcoUwUuS68oaob6X7H1pKi8sULsyklsbCzr168nIcF/iBtB1ZM1ezbpgwerm89d8jb/h2Ryjhz3qiubzZg1BopDW78+5iuacTAijvmtBhdZr1Gk8kLm6EMwduxIQSH/sD+bdVH706/nXL71WEwAJRpI4dXbcwb30NohSVglHWE6t2Av2Hqa0lA3Qk+EbOa72C7cfeNrZOnDSLd4y4CmiIkxg06pF2pwW18hBi01Q3QM6ewdLr1uuPIMbm2t7IgY1UOxUBtGec/xBeklNJLE578piyAuUSzM6Jvr8f6wK0v1HRMSEli/fj2xrmAETsScneCyxZ6WBii5SnUNGqjW3Mc/nSHsSAb9POrKBQWYtb5iF2LLJ0+n5EqQtVpeuWIQB5oWH2aofoQWcJAaXIu/W3WjwbETdGsWSeIvyjyf3bXTwPlzedPbuTVjv3sbWHo6xvh48DBMcvXufA1fNexOY1Myer2WPk1r8s2fmfx4+HyxfWpVL4SPR7QA3C6FsiQxrPsryHu8h8DaIsRO5ywPNfoONds3COWTn93zmKN61KNepIHrm4bz2+QOgDLf1ijK6FUvSK/xGoLXCfcvdg/dcOHJuIVlJ7hs0Tj9zwqOJzH/h9Ocz3e/VJbs82ijozG0b4+mRg1ks5m0YN+9l3Xz3T5mS+t042+plnosyf49+hs0V+aV3m6VwMgVR8i3OIj0mAc75QhmbqsERtygbOXKMkYQ0t+9sOHIyCDX4D357xJGF0nhsRj1Wqbc0YjC9Eh1OxQvf1jZCeERI4Ao2b3zQ5Z8JWBAh1o+ZeAeYnq6grjQ67zL6oTrGdEtxmu+7drG4V4+cwDGQtfVKGG+8EIQYie46LGnp5N6111+N7cXhytm2+GzVpbsSuN8gYfYHfwH3RVXUHvePEIHDUIuKOCFa570aaNePffw8dOoTlg9dhl0j/FvAYUYtBg8/MHyrQ4v94lkexDf1+/sdtwF/m3oPe915/Fm6ufpbR9W9516otdr1aElQIxz+KzxEOF6kQaGXVeXaXfGqWVPm3/l9pM7MRShK+0bhNKtWQRGu3fo9Mn9GjLutgZ+LTvPJzF9QJzPeReFxS3I47nUDtNdUAinkhBiJ7ggZIeD/O3b/TrZVhTmfftwnDtHwY8/lu1CV4DKIqLyuhx5paAgr8CUA/7byoRjnxEdriMyrkGRzYfU8R+FI66W0WtVMc/iILgoZXFywuwevtmRsHnIx0912/rdSmUwKmUL72/KK3fG8fZgJTCn5zYsg07i6Z6xxNVyrybHhml47NBadVgKMLRLHdVy02okDFqNuoDhIipUz+BO/lMZuv77n7ypHn3bFB2dRF/IKdio03BH2yiGX1eXb5/2XeioSITYCS6I/O++I/Oll3yCMNqSkzkzciS2tLRSW2TmffvUcEieaCKUFTzXxviikC0WJfeocwO/bLeTHFKHVJvvXFziFb35935l9VUyGr0CUw44sY2ux37km7FtKM7Q8Dec+/SumtQO0xNWyPrxtGD8kWJx1880Rvic9yd2RqfYdW4SQb+2UUQGK2LVPNu9yV/vZ2U1cswYIp95BoszOsnsexoz5pZY1WFXq5Ew6CQvF5mihuwuXCGtNCVYZpIkqXN4oDyXl++MY2zP2GKuqhiE2AkuCDXMtvOX3JaSgmnZMvI2bsR66BDp995L2sCBauw1UOK7ucITyQ4HBbt24cjL4+wzz5Dx1FM+95A98igUh2nFCtIGDSLlppuwp6fzdnAnHr3+RV6WrvWpa9doGb1NEbh8fZDXi220uy1Bz8n6phZv0W5VP4QgyVsE9Aad5+NQCdZr+GZMa8bd5msp1gjR8VumlgKnoHn5wDkpbGUB6IO9RbxOuJ41TY7TJ3m3WmbQ+YqPJjyc0DvvxBWcpFGUEa1G8hK7whZYSTicll1ZR6H+/OoqCyF2gnLz0Y+p3HhaCcPt2g5lev99TB995BMZJO3uu9XP58aPJ33YMECxDM+98ALn33sPANsR732UssOB6cMPAXDk5BTbH3WbFcqe169CrvI6X9PmazWaCuzcurc2axr3VMuMDvdclefCpAaYpf/NXU+nYXg97z4ZgxRRsjm8h/XBBg3REQZax4b49OHG5pEcyNbySZM+gJLEpjQYg3y3akXrrF7zZyVZWqDM64HbeVen9RW7klrp1iyCwfF1GHhN7RLv50lJFm9FIsROUG6W7k5TP8sWC7IsY0tLo1/PuTyf2Rx7Ea+I5Q/3Lgbr0aMAnN62mwI/fm7Wv//G5gx95MjO5syIEdhOnSJrzhwsHhnuwTu/gb+8pEu2+2bN+vqPcwD859ypcJ39NPXefJ0oZwhyT8tOkqDHrR14zPGncgzomjX3ak+vV0S/sD+Zwyl+NYJ9Fxoeu1G594kwJRLw+y3u9qnjD0OIb5YxlxVcFiKcfXJ9VZ1G8lr4AJA0xUuFQadhXG//ixcXC0LsBOXH7pwbQxGX/O+/x/yXsuVqf62WfB5X9E4BFacoDb32BcZfq2zB8syK5Slo5j17sB4+TP62beR9+SUZjz/u1ZRssSAFB6ufC6PzM++0zblJfkeMYqH2bxWGsWNHguLjAe9hmYSSEjC8j5LP1CHDfZ3qcJeHq4ZrHm3y7Y3oeqV77s3lZFvTz/aqqFAdA+Nk0oOiOGvwna8rCqMfseMCFookdRjru5BQiYukAUOInaDcuIaNNkmLbDZjS0nl/zyGYMfCfeenPKP/yjablygdjWjIL7Vbec3N+Ru6amu5xeX8Bx+o1oxstaqx3WSLhfZ5J3yuLczPx70zfoUUWjWt5bE9SSr0QQbCg3RM7uf2dXOtkMZEGnjZ6e7RvmEozWOU4Wuon0UNrUYiJlTiTFBNHuj+Sol9dhHhZ+tUaL9+BN10U6mu/3hECxYPd7u4aJ2KptVIPsPwywEhdoIy48jL49ykSbg2YBVojWCxsDUvgknXuCN87IzpwOlg9xxOcqaZlCH3czi8ITm6YOSCAsUtxGP4ubJJH6/Ybv4sNE8BzFmxAotrA7/ViiY0FBmwmS0+K4i6uKL9v1xodd5i97+u0fzvBmXHhK6u4nahip4fK0pncFtuYUYtdcL1PHGTO06bJEk8e5UizgND09SVSaNW8nIcvrux8nPVyJZF9rXwii8oiW2ipk4t8hpPWtUL4Zo4d8BMyWOB4sQ5c6HaFWvaTezTwK9DdGUixE5QZmzHjlGwc6d6bNbqOWly8I3JNzPU485IGn/UaModC/5muyOGpzs/ywvXPMGZkSPJ//ZbDK1bq/Udkga75+KGU+ysklYNLunjb2c2qxm7pNBQNtfvQo89UeRLbstnunU3dZcvL/G7RYV4C4hBp+GO9opga51x3+IbKwLR1k+II8+4dXqtxOaxV3NNnHeAy+Y1PExDj/vYPMSuTW0tv03uwJV1/QxVnRS3ktm9WemHwy4kD8vOM4+Ecq7MzRXLoGvrcGd7/zs1KgshdoIiydu8may5c3Hk5HglTLZnKluoJOfb+lTn50k43IBfCnyj1rpe4D+cw9tcZ2y4Y+ENsLuiUnhYSLLBwLmJE7H8+69y7HQIvuuWN3mxozJHZ/n9d697mPfvJ33wYAp27kQKCmJPtJKgOc/h/vVudGv3Er/voOPfcUUNPzsVnPNXrhe+SZ1gfpvcgUa1fIWodDsAXHXc37vwgoDB4Fo0cLc3Y0Acc+9zRx0uTuzevLcJP01sV4q+uHH1QKeBKXeUbAVfaohAAAK/yA4HWa++CigrcTKwafP/0b/XrZxIOc/JSHeIo/MG/7kDPMl1bqZ3/fTCcwXRqAhIxmOPUX/LFq9hbFEuGdaDB9kX1ZJ8nZFbDAayZMWqMXs44oaGFB/RBKBOQRbo/fizFRK7CyXCqDQULbkdmQvvLTXofYWsZb0QNfgm+B/GutBqpCI39BeF5zDW5Y6initTSxcnQuwuU2S7nbPPP4++cWMi/TjqFoctJYX0we4QRo7cXA5FxDG9Zk/y955h7dEokuLHKHsrS+np4Ir2m+UnZ4HDQ+xMkoHVjXsxMOl75XtYLBiuuabYtu1ZWcxoNxKL1oBn1DnPkOFhEX5E1olRJ2G2yYRbc9VMXp6oYldBr3zjCA2v7X2La27rpJYVdv41GnxfzcL7SivazcNlRfrzzYuJLPmPxcWOGMZepthOnsSybx+5a9eW6TrZ4fC5xnH2rCpW5/PtyHbFNaS0OYt/qd2KH6PbA5Bl8CN2Hit/Z2UjK668nYORjZUCq5UCQ9FCBYojssVPeKYCj7LQUP9zXwatRPNopf16+Rle28ZcuMTuQvIfeKHR0Cr7uNeG/cKWnMGP2IUHeb+uZbXcSuKujv7n0N69v6nXqu2lirDsLlOszjkvbb16JdT0JnftWnLXrPEqs6WkIEvKUFWrAUNBLhhLHrq6eKX9SPWzZ3jxvyKv4GhEQ9prfFdc04KVF2+fvSafB7f2OV8aCnRGbkj7ndZZxwgK85+Q5vqmEfydomTqaj6gF8ZOnXzquDbMFxVrDeDtn17nZGhdoEORdVzomzXD0KYNoffc4y4rZLUZ/QxjC7vFVDT3d67L/Z19Y/V1aVL2xY6LESF2lyl5GzYAqE62pcWyf7/XsRQaiiMrC4dr+CnLaAoKoHxJpbwsu/Hxykb8EIev2CWHxyA7HLwQcpO6Cqtx5ijN0QVjcFgx6UL4oPkAxvy9qsj71S04R/+TO5CCXvI590yvWPq2qcnnv53l3W0pxIx40G8bBp2Gufc18XLTKEyTnGSa5CQXed4TTVAQtRcs8CozFnJ58aN1FW7JVTfEMPYyxJGXh+XPPxUHW5vN57wsy1gOHfIpy9++HbPHVq6Y9evRhIcj5+Soafaycy3YPAI+TuxTdAgkf5wI87U08/xsE/svuC7XzjjglcvUFbpo8I0zmdZuBDta92JnTEe21St6Ts/YuDERTzyB5Fx4ePDwBpqYTrFix2SGdqlLVKgSZNIzEoc/ujePrNStUPpCVpsxsuh8rMF6DYPKuAdVIMTussR69Cg4HBhatfK7VzJv40YyRo5U96UCWPbtI/OllzhhC8b4yqvU+egjkixGzMGKNePKFp+4L5OTodHqdf3aec/ztJLP+dyvgcFKkJ/oG8XxU922FHbZtWr1bGjQFYD/q9mMKx4aAig+fEWhCzIQdu+96vHA/7bw9s9vUNNiKvKa8lB70SJqTJhQ7usLu54E+dsK5mTXhHZMvIAsW9UVIXaXIY70dAB0DRt6u3U4sTvDMrm2ZTny8rCfO0eOLpjHrp/Iu6aG6Js2ZeCif3i9Xl/AO56a52KAodAeyhDJ934mWcew69wC+ZaHr1hZea/lQAAakYPFodx7ez3fEE4utEXEYSvr8L4kDFddRUifPuW+vvAcna6MIZYEJSPErhC5ubls3LiRXr16VXVXyo0jLw8kCSkszGtTvWy1Itvt7qjCzkgWqX36kDVjBvtqKVuTbA53qsFjRmW4ZPITYw1855F0ku8WqmyrhNHDsouJKDkdXklEX9mArDzfIXphXPN8PteXcZW6sim8QFGJgZ+rLULsChEaGsrtt9+OtYhQ3pcCcl4eUnCwMk/lYdllPP44GU8+6X6T7HYcHq4WGQ2UcEXhQVqszii2OkkJE+4v7NAAP9t99EUYJAadhsa1lFWNiohhlplr4+0f3OkDH+kWwwQ/84dah39B1ISWP5t9ZeDpajIwKptojz8IG59qzYYnW1VFty4rArYa+8EHH5CamkrDhg2x2+08/PDDFdJuTk4ODz74IHPmzKFRI/fG4r1797Jy5Uo6dOjArl27mDhxIldcUXRi40sV+7lzmH/6iZC+fdUyR24uUmgoaLWqZWc/c0Z1R9E3Vea45IIC7CdPqtelG2sAUGB1qLlOtRLc2fMtn/u+dI2WAX19N3Lr/Vh2oAjciv+1wGJzYLZ519FKYC+jJfNfoY3qzaODueWqGnRvFklmno37P1AWYIJbtihbw1WE0WOBYky9c17bzgrvZhCUj4CI3XvvvcexY8eYPXs2hw8f5tprr+Whhx664ExCH330ESdOnGDt2rW84Qy2CGA2mxk0aBA///wz0dHRNG/enIceeoht27YBYDKZWL16tU97cXFxl9zwNeeTT8hdswZ9y5bomyhzYXJeHpqQECStVrXssufOVa9xRRWxnz2L+cABtTyjTiPIBbOH2Om0GvAz7VU4OoiLEI3D764KvVYi1Kgl1Kj1GX52ahLOnqPuBYM64XrOmLwt6zCjlh3j2tJxmuIaYy20Ud01TI6JNFDXwyoy1Pde/Q295x4KCuXLuBgIcs7Z9Tm1C6nLpSHQlxqVLnZms5mJEyey3+m/deWVV3LgwAG/Qrdp0yZ69+7tVeZwONiyZYtfEXJZhy+//LJX+Y4dO4iIiCA6WpkU79SpEz/99BNpaWlER0cTHh7OiBEjKuT7VTWaSCUdn+X339E3aULWa6+Rt3Ej+latQKdTLTvrv/+ib9YM8+EjmLOUhYls5x8IY5cuhL/wIqeWnwQsFFhlt9jpNODrBudX7IYf+YpBHWrylVURmPG96jH7OyWCid1jl0SNEB2Jj7QgYbFifb3UrxF95rmDdN7cMpLVezO82nZtp3q5fyOmrD/hYwl6znlpJAmdBmwO3znFyKeeIpKLD51WInHbC4TZ8gi+ZWtVd+eypNLn7Hbv3k1mZiaHDx9m1apVvPjii6SmpvqtazKZmDNnjnpss9kYO3Ysder4T99WFElJSURFudO5abVawsPD+atQGG9/mM1mPvnkE5KSkkhMTMRsLhzXSyE5OZn+/fuTmJhYpr5VOB6BK0FxKwFltdFl2Tmys7Gnp3P8uj707/kWt9d9kAyPnKW6xo0Z+XkqyVmKqpltHpZdERacTu/7d/LepO8Jw2211Y5wD78shdSpRUwI0397l4XpnxEdYeDKOm5Xi55X1fBp27Vl6452tbjbz7amwqvCOmdWLd0l4ogrBQURbssjatq0EkOgC4onMTGR/v37k5zs7eRd6U81xRmbzGAwMHjwYF544QXuuecezvvJFDVo0CCaNGnCzJkzsVqtjBkzhkcffZT27duX6Z4ZGRkEBXn7KQUFBZGVlVXitUajkSFDhpCUlERCQgJGo/+tArGxsaxfv56EhIQy9a2icTijBRfOjaoJDQWdDux2zOlnmH31Azyb7p6zfLDbyyQ7A2vmGkL4v1PuZDQFVgf5znk1WxG/IpoiRNBT0oI85qEKDzsBeifOp9OcqQDcG6/8Qfv5hXY+8d/AO0y4wU96wMIb6V1pDv1UvSiRdDrqb99OcPeSQ1EFiqduqc/tbXxjFF7sJCQksH79emJjvdMzVvqvQqRzmBXvjOkfFhaGw+HgxyISHg8YMIAWLVoQHx/P6NGjad267PsiIyMjfZI25+TkULv25eV1bs/KIneVc6tUoZ0SkmvODkg+dZadMR3ItXkLgis0U4rB/Qvdun4IBR6WnTVEqfN4bLbXtTpDEe4jHs/dM0pH24a+W600QUHqzoaB19Tmt8kd/OY5BW8LTfZxN/YVQNfmfrHFqvw8eH000wY0rupuVBiVLnYdOnRAkiTsHi4QhY89ycvLY+PGjYwdO5YVK1aU654tW7Yk3elYC8rQ1GQyEVeKsNyXEtlvvql+dmX3cqFadoB8zndXA4DkrJ/R9joAtj7XhjaxoRxMyeexj5WUhkczlGF860gZo909eVfUAoUnRqeLScdGYbTzE9W3LHhadvkWt5UY58ziVdiyu6qekvNBezlkihFUCJUudvXr16dHjx7s2rULgDNnziBJEjff7Jt5ymQy8eSTT/LSSy/x4IMP0qdPH8aNG+djpZVE9+7dSU9P55Rzp8D27duJj4+/rMTOtHIlBT/9pB7LNhumxYvVYykoSLXsCjL8i12B1kBI376k5DgIM2qJDNZh1PlPthJm1PDJ9hfVY63HnN3c+5rwwfVOAfK07CpwF4CnxecSu2d6xfo9D1C/hjJfaPYzfBZUTwLierJs2TKmTJnC33//zbFjx9iwYQOhfpw6J06cyPTp06lfvz4AXbt2Ra/X89prrzF+/Hif+omJiezYsQOACRMm0L17dx5//HF0Oh3Lli1jxowZdOnShe3bt7Ny5crK/ZIBxJaSgun9970LrVZyPN1ptFrVsjPtPwCxvk6pZq0BKSiIjFwrtcOUukU5/EYGadE63POCnmLXvXkk5nwdZ8Fb7Jxt+Rt2lhUvy84pYN2aRfDZPmXVtrBlF+NcHEk7f+k6hwsqloCIXaNGjViyZEmJ9RYUCnsD0LlzZzp37uy3fkJCAgkJCSxcuNDnXM+ePenZU8ny/sADD5Sxxxc3Fg/fOBdy4egmkqRadvmn0yDW5xLytUakoCDO5dioFabMnRmLELtaoTo8Z+20hVZjPRPNuHDN2ZVn69Oyh5rxwJLD6rGn2BU4LbvIYJ0qo4VXY5s6V3ddFp5AcImsVQk88Uw16KLwaqyk0SjWHXil6GvXIJQ3BimrsgVOy+5srpVaocVbdoVdTXw2qrvEzp9lVw6xa9PAe0HD835XOIUsPEirTnEUjhoSE2lg05jW9G8XhUAAInjnJYnDj9uOTzhxrVYVO8/EM/3aRXFzyxoEyTbOBNXklCacc7k2rqyrrF4G6Yr4+1coEU3hPAUuK9JT2VxhnSpiT7un5fZsr1juu7aO10pr4WEsQN0IYdUJ3AixuwTxFLuQ/v2xp6RgzjrP1phraH/uEDUtOWiiotShpdVD7IKd1laQZGd1k9tYnQShBgt1nSHHjX5E48mb6iHpznqV+TjrusRPlnl3z0xOh9TBoFOmJa6oXc6wxh54urEYdBqa1PH2o/SXJEYg8ESI3SWIaxibHFIHvSaShroz7D96ljedeVU7ksHiPrdg+fNPACxBIeq1rmFqMHaynGW5FgcNo4qOSHJXx9pw0nvoXJz7WqPcNBrlpqHVSHw8ogVNahcdiLI0/K9rNPfF+99FM/y6aF79+qTfcwKBJ0LsLkFclt2j178IOfC9/j+vTFq/URtJo1EtO1uU25naZdmFOCygcYtgw5qK2PlboDDqJJ8goD7Ouh6WVY1Jk9RcFq3qhXChPHFT/SLPDbymNgNFiHJBKRBidxEjyzL2kydx5OdjaKFEwrClpWH5v//zyhrmMJmwSn7+K13zaK3bgtPH2jW3FWbPB00NQMkAX5xlZ9BpsBdaACm80UEyKGIrhYUR0qsXIZdY9BjB5Y8Qu4uY/M2byZo5E4C6q1cj5+djT00Fu10JAX5Cqec4exarxjsF3kc/pjKgfRMyXp7L3J0ebTrdNsKs+eCcymtQ06iKnL8FCp1GQmrgHRjTJ0JxgwbUeOEFgnr0KPf3FQgqEyF2FzGeCXEynngCR4Y77JG2fn1V7NYbmqO15Xhdu2BrCgu2pniVtYgOpq1z25YmNhYylfLYmu4FhKAiQg1ra9Wi/vbt4Iwn529BIOS220r5zQSCwCP87C5iJA93D0+hA9CEuOfC3m3czysfKyhD08IkjmxJRLDy9y0/0h0mqWaI+29eYX+1ohAb7AWXGkLsLmb0xSSmMXj7kB0P957Ev7tjbR6/0T2vt3DolV7nc83uBQdPsSuthl0qoZMEAhdiGHsR428LFkD0F19gO3kSz/jnSU06gDskHUa9hhpOEbu5ZSSdr/C2/B68PppJX/xHt2YRDO3inu+LClUEdtHQK6kZouPftHyv6zQSOGTh1ya49BBidxFTWOy+r9eJFtn/Ub9mTewpKXiKnUfsTQB0kqRabJ6Wm4terWrSq5VvYMYgvYbfJndQj5tFe+dXdYldKUe7AsFFgxC7i5jCm/vnth5CmDWXJen5NCjBsvI8HVOB2akUi05GE6A5uyCb/7D4AkFZEWJ3EeO5uV/TSonYnKMPZdB7Bxl/ja+AGXWSmqZQI0GnK8Lp1apGhTrdajRK3sNALFBsfqA+OoP4FRVUDOI36WLGokQGrjltGrkNm8LH7ujLu8+6IwXXr2HgdJaFMKMWs9MalCSJ8CAts++p2Fy5rv34FRiXs0jqNIqu/JsIqg1i5uUixZGXR05iIlJwMMHdu2OSvC25UznuWCINairnQoxuAawsw8s1fBWuJ4JLDSF2FynWI0oOCNmZPeyPM+75u3s61uLkOfdcVr+2is+cxSMEeWWtlro0LlBzdgJBRSHE7iLDYTLhyM5GKpTCceomJdfuowc/4872tdQ8EVPuaESzukpUkfP5nkmNKqd/LhENxDBWIKhIhNhdZKT260dq//7IFovf831P7aKlRyQRg1aiUS1F7PKtDt50RiGuLKdfV7uS8LMTXGKIBYqLFbN7mJqdrwxhx/2xDC0yOo2EXithtcsYdBqC9RruaBdFTISB65pG0LdNTe7uWDlhj4QzseBSRYjdRYrsFLugm2/mUKoyb9coJwU0imnlkhxXIpqX+7vTRE6vxMTGGjEWEFyiiF/dixSX2NV47jl+OW4iKlRHk3CIeOIJ5byzXlHZwCqLSbc3okuT8JIrCgQXGSW+Kc8991wg+iEAr2Tgrjk7yWjk37R8WtcPIWb1asIGDvSqW1Q2sMri+qYRvHv/lSVXFAguMkocxi5dupS6devSt29frr766kD0qdoie8zTyWYzaLVIOh3HMwq45aoa3nWduhjkJ0GOQCDwpUSx+/bbb+nYsSNbt25l/vz51K1blzvvvJOgoAtLoiLwRfbIGiabzUgGAzaHzOksC42ivF1R1GGs2JEvEJSKEt+Udu3aAdC9e3fq1avHW2+9RYcOHXj33Xc5fPhwCVdfviQnJ9O/f38SExMvuK2MZ54hdeBAHCaTWiabzUhGI2arAxkI9dgdAR6WXYCHsQLBxU5iYiL9+/cnOTnZq7xEy27QoEG0aNGCpUuX0qZNG5577jkGDBiATqfjhx9+4ODBg9xxxx2V1vGLldjYWNavX18hbVn27QPcKRIBTB98AIDZuSuicG6IqlqgEAgudhISEkhISKB///5e5SWK3ffff0/z5s358ccfadq0qde5OnXq8Pfff1dsT6sxeZs3+5S5opj4y3gPYs5OICgtJYrd5MmTef755/2e++CDDwgPF24I5UV2OMj78kv1OH/TJq/zUa+/TqpVseyKsuCEZScQlI4S35QHHniAn3/+WT3++uuvsTnDCM2bN4/p06dXXu8uc/I3bSJ77ly/57QNGxLUqZO6ud9YhAUndjQIBKWjRLGbNm0a06ZNU4979OjBkiVLKrVT1QV7oYxhQTfcoH7WREYCUOAcxopVV4HgwijxDerQoQNfffWVehwaGoq+uKxXglLj6VcXMmAAUa++SuqTL3IoIg6Nc3rAZdmVNsWhQCDwT4lv0H///ed17HA42L9/f6V1qDrhyMxE36IF4Q89RPiDDwIw4mAdnu30NJpQJZm1uYhh7McjWjC+d4OA9lcguJQpcYGiUaNGxMfH07NnT2RZZvPmzYwaNSoQfbvssWdmoomKUoXOEyk0lP/OFjA68RjgO4xtVS+EVh6hngQCQfGUKHb/+9//iImJYenSpTgcDqZOncqdd94ZiL5d9jiystA1auT3nDYqik9/dc/pFbVAIRAISkepJoJuv/121qxZw6effkp2djZbt26t7H5VC+ScHHVurjCaGjXQeYQDFnN2AsGFUap4dt9++y2pqUpYcFmWmTdvHjfddFOldqw64MjJQRMW5vecpmZNdHa32IkENwLBhVGi2L355pt8//33FBQU0LhxY06ePMlAZ5ghwYUh5+YiFSF2xk6d0P2cFdgOCQSXMSWOjfR6Pd988w0PPPAAS5Ys4fvvvyfS6QMmKD+yxYJsNhdt2QUHoxPWnEBQYZQodjVr1sRqtWKz2fj1118BxddOcGE4cnMBipyzA9BpizwlEAjKSInD2FOnTlGrVi1++OEHHnzwQfr27cvp06d9IgoIyoackwNQ5DBWIBBULCVaduPHj2f//v1ce+21LFmyBLPZzIQJEwLRt8sahzP5tRQcXGQdq10u8pxAICgbJYrdV199pW4Pi4+PZ968eSI8e0XgzDGx7riDPvP+9F/FJsROIKgoShS7BQsW+ET8dLmhCMqPbLUCsPQvM2nnreSY7T51XJbdqkdaBLRvAsHlSIlid++991KnTh2vsrfffrvSOlRdcIldw0hl2vRoer5PHYvdwZV1g2geI7aFCQQXSolit3jxYtq0aUN0dDRNmjThiiuuYG4RMdgEZcApdrXDlCmC1Xsz6DjNO8CCxSZj0IqdEwJBRVDiamzjxo1JTExEq1X8IGRZZsaMGZXescsdV15YjVPMNv2V6VPHapeLDMcuEAjKRoliN2/ePGJiYrzKRHTiC8c1jLU4ihYzq93htT9WIBCUnxLHSIWFTpZlpkyZUmkdqi64xK6gGPcSZRgrxE4gqAhKFDuNRoNWq1X/6fV61q1bF4i+Xd5YLKDTqdnD/GG1izk7gaCiKPFNGjJkCDabDbvdjs1mY+/evSIHRQUgW61Ier1fsXOlR8y3Ogg2CLETCCqCEt+k9957D8mZwUqSJDp06EBmpu9kuqBsyFYrksGA2Zkq0RPXyDbPYhdiJxBUECUuUJw9e5azZ8+qxxaLhX/++adSO1UtsFpBr6egkNg9fEM0S3enAZBndhBqENEABIKKoESxa9KkCQ0aNFCtu4iICF566aVK79jljmyxOIexbrG7OjaE2JoGHDIs2p7Cf+fMwrITCCqIEsVu9OjRvPXWW4HoS7XCNWeX72HZGXUaNSLx+zuULXmhQuwEggqhxDfprbfeoqCgQD0+ceJEpXaouiBbrdgNRs7n24kIUoaqBp2ERvJ2NQkWw1iBoEIoUeyefvppunXrph6npKTw2WefVWqnqgUWC7nGMGSgfg0DAEatBldenZgIZRuZ1e67gCEQCMpOiWLXunVrNm7cqB537tyZvLy8Su1UdUC2WjEZlIjP9SIVsdN7WHY1QpQZhnyLEDuBoCIoUexycnKoW7euV5kYyl44stXKeb0SpTi2piJ2Oo2EaxTbur4S6eQqkQhbIKgQSlygSEtL49lnn+XWW29FlmWvYJ6CC8Bq5bwhCoD6NYxqscvJ+Mq6wewa31bM2QkEFUSJlt0rr7yCRqNh6NCh3H///ej1embPnh2IvlUJubm5bNy4kV69elXqfWSLhXRdOHqtRHS48scjJsKguqIY9RohdAJBBVKqVIqvv/46Z86c4ezZs0yfPh2DwRCIvlUJoaGh3H777VidG/UrC9lqJVkbScOaRjROd5NW9UPUHRXBOuFyIhBUJCW+UdOmTePmm29Wj7dt28bmzZvLfcPx48ezc+fOcl9fmJycHAYOHOgzj7h3717Gjh3LsmXLGDlyJMePH6+we1YEstVKsiaMRlFGujWL4O3BTbi5ZaS6o8KoF9FOBIKKpMQ5u1q1anlFJr799ttZvnx5uW62c+dOli5dSp8+fcp1fWE++ugjTpw4wdq1a3njjTfUcrPZzKBBg/j555+Jjo6mefPmPPTQQ2zbtg0Ak8nE6tWrfdqLi4ur9OGritWKSTLQIFSHRpLo2kxJPN4sWsk2FlcrKDD9EAiqCSWKnSzLtG3b1qssLS2tzDc6f/48Bw4c4KqrriqyzqZNm+jdu7dXmcPhYMuWLX5F6OGHHwbg5Zdf9irfsWMHERERREdHA9CpUyd++ukn0tLSiI6OJjw8nBEjRpT5O1QkssWCGR3GQsPVbs0i2fpcGyKDS/yvEQgEZaDEYezhw4eZP38+hw4d4uDBg7zxxhscPny4zDdavHgxjzzySLF1TCYTc+bMUY9tNhtjx471SfhTEklJSURFRanHWq2W8PBw/vrrrxKvNZvNfPLJJyQlJZGYmIjZbPZbLzk5mf79+5OYmFimvrmQrVbMaAnyM1wVQicQlJ/ExET69+/vkxWxVHN2O3fupHXr1lx99dXs2rXLa8hYGjZs2MBtt92G0Wgstt6gQYNo0qQJM2fOxGq1MmbMGB599FHat29fpvtlZGQQFOQ9DAwKCiIrK6vEa41GI0OGDCEpKYmEhIQi+xwbG8v69etJSEgoU99UrFbMaHwsO4FAcGEkJCSwfv16YmNjvcpLfNPCw8P59NNPyczM5OzZs6xbt45333231Dc+ffo0mZmZpU6sPWDAAFq0aEF8fDyjR4+mdevWpb6Xi8jISGTZOyhmTk4OtWvXLnNblYXLsjPqhdgJBIGg1G9aeHg4YWFhfPnll3zwwQelvsHmzZtJTk5m1qxZzJo1i3///ZdPPvmEb775xm/9vLw8Nm7cyNixY1mxYkWp7+NJy5YtSU9PV4/NZjMmk4m4uLhytVcZyBYLFlkiSIidQBAQSjU59Ndff7FkyRI+/vhjrFYrGk3pX9CHHnrI63jRokUMGTKEG2+80aeuyWRizJgxTJkyhbi4OH788UfGjRvH7Nmz1Xh6paF79+6kp6dz6tQpGjRowPbt24mPj7+oxM5utWGRNQSJYaxAEBCKfNOys7NZtGgRnTp14oYbbmDXrl18/PHHpKen8/zzz5f5RqdOnWLcuHGkpqby5ptv8vXXX/vUmThxItOnT1dFqWvXrtxzzz289tprfttMTEzkscceA2DChAnq8Fqn07Fs2TJmzJjBsmXLWLVqFStXrixznysTi135aRR5YQWCwCD7YejQoXJwcLDctm1beeHChXJOTo48a9Ysf1WrLXfccUe5r3VYrfLft/SRO7zym/zDP5kV1ymBQKBS+B31O4x95JFHqFWrFu3atWPQoEGEhoaWaRgpKB65oACzRtkPKxYoBILA4FfsunfvTvfu3TGZTHz11VeYzWZOnz6tnt+yZQu33HJLwDp5ueEwmbBonQE7xZydQBAQil2gCA8PZ/DgwYAStHPx4sVkZWWxePFi/v3334B08HJENpkwaxXLLlhYdgJBQCi1q37Lli1p2bIlFouFL7/8sjL7dNnjyMnB4hrGigUKgSAglNmsMBgMTJ48uTL6Um1wnD+vztkJPzuBIDCU60277bbbKrof1QqHyYTZNWcnxE4gCAjiTasC5Lw8rMFK/gmxQCEQBAbxplUBst2OWa8EGBDDWIEgMIg3rSqw27FoDGgl0GvFAoVAEAiE2FUFsoxZZxDzdQJBABFvW1XgcGDW6MUQViAIIOJtqwJkhwOL1iAWJwSCACLetqpAlrFo9cKhWCAIIELsqgK7XQxjBYIAI962qkCWsWp0GMQwViAIGOJtqwJkhwOHpEEjRrECQcAQYlcVOBzYJS06oXYCQcAQYlcVyDIOSYNWiJ1AEDCE2FUFdjt2SYNWPH2BIGCI160KkGUZu0ZYdgJBIBFiVxU4HDjQiDk7gSCACLGrChwO5zBWiJ1AECiE2FUFztVYMWcnEAQO8bpVBbKMXZKEZScQBBAhdlWArFp2QuwEgkAhxK4qcM7ZiQUKgSBwCLGrChwOHIhhrEAQSITYVQXqamxVd0QgqD6I160KkGVZuJ4IBAFGiF1V4BrGSkLsBIJAIcSuKhDDWIEg4IjXrSpwOLAjhrECQSARYlcFKH52YjVWIAgkQuyqAlnGLgIBCAQBRYhdVeBwYEcSc3YCQQARr1tVIObsBIKAI8SuCpDFDgqBIOAIsasKxAKFQBBwhNhVBbKMHUksUAgEAUSIXRUgq3N2Vd0TgaD6IF63QuTm5rJx40Z69epVafewO2QAMYwVCAKIELtChIaGcvvtt2O1WivtHjaH8lPsjRUIAkdAxG7btm2sXr2ajz76iHvvvZd9+/ZVWNs5OTkMHDiQEydOeJXv3buXsWPHsmzZMkaOHMnx48cr7J4XSromFIC6Efoq7olAUH3QBeIm99xzD3PmzOHhhx8mMzOTO++8k1OnTl1wux999BEnTpxg7dq1vPHGG2q52Wxm0KBB/Pzzz0RHR9O8eXMeeughtm3bBoDJZGL16tU+7cXFxVXq8NXFaW0EAI2ijJV+L4FAoBAQsdu2bRtNmjRRj202m996mzZtonfv3l5lDoeDLVu2+BWhhx9+GICXX37Zq3zHjh1EREQQHR0NQKdOnfjpp59IS0sjOjqa8PBwRowYcUHf6UJI0Yajx0GdcGHZCQSBIiDD2DZt2hAaqgzdNmzYwOzZs/3WM5lMzJkzRz222WyMHTuWOnXqlOl+SUlJREVFqcdarZbw8HD++uuvEq81m8188sknJCUlkZiYiNls9lsvOTmZ/v37k5iYWKa+ARSgJViyoxFzdgJBhZOYmEj//v1JTk72Kg/YAsUvv/zCSy+9RHx8PPfdd5/fOoMGDaJJkybMnDkTq9XKmDFjePTRR2nfvn2Z7pWRkUFQUJBXWVBQEFlZWSVeazQaGTJkCElJSSQkJGA0+h9qxsbGsn79ehISEsrUN0DZPYFc5usEgUWWZc6fP4/FYlGPs7KysNvtVdwzQXEkJCSwfv16YmNjvcoDJnadOnXilVdeoWnTpnTr1o2cnBy/9QYMGECLFi2Ij49n9OjRtG7dusz3ioyMRJa9xSQnJ4fatWuXq+8VjV1GiF0ls2XLFoYNG4YkSTzxxBPMmjWLGTNmMHLkSCIiInwWtPwxb948YmNj2b17NwAvvvgitWrV4uTJk8Ve9+uvv15Q30+ePEm9evU4dOjQBbUj8KbSxe6nn34iOjpaXQ298cYb+fXXX9m0aZPf+nl5eWzcuJGxY8eyYsWKct2zZcuWpKenq8dmsxmTyURcXFy52qto7EgIF7vK5ZZbbmHatGkAPP/880yYMIEXX3yR999/n+XLl3PkyJES2xg7diwdO3ZUj1999VUaNGhQ7DVZWVlF/m6Xlnr16vHaa69xxRVXXFA7Am8qfYFCp9PRunVr6tevD8CxY8fQ6/W0a9fOp67JZGLMmDFMmTKFuLg4fvzxR8aNG8fs2bORyjC/1b17d9LT0zl16hQNGjRg+/btxMfHX1Rid6lbdo6CAmylsI4qGl2jRmgKTVGUloMHD1KnTh369OnDli1bSnVN4d+74n4PTSYTEyZMICYmplz9c6HT6Rg2bNgFtSHwpdLF7tprr+Xhhx9mwYIFaDQafvzxR7766iuaNWvmU3fixIlMnz5dFcauXbui1+t57bXXGD9+vE/9xMREduzYAcCECRPo3r07jz/+ODqdjmXLljFjxgy6dOnC9u3bWblyZeV+0TKgJNup6l5cGLYTJ8h45JGA37f24sUYmjcv17Vbt26lT58+1KpVi759+7Ju3Tqef/55hg4dytSpU3nvvfeYPHkyr732Gg8++GC52j9y5AhHjhxh1qxZ3HrrrdSrV48xY8aQlJTEI488wqeffsoTTzzBHXfcwezZs4mNjSUrKwur1cpzzz0HwLvvvsvrr7/OihUr6Nq1K+PHj+edd95h9erVnD9/nj///JOoqCieffbZIvsyb948wsPDsdvt/Pfff0yZMgW9Xln9X79+PTt37qRFixb8+eefjBo1ipYtW7J7925WrVpFmzZtOHr0KP369cNut/P000/Ttm1bli5dymeffcb48eMZNmwYU6dOZcWKFUyePJnhw4djNBrZvHkzO3bsYNeuXezZs4eYmBh+/fVXnnjiCfWdP336NNOmTaNdu3bk5OQQGhrK9ddfz3PPPce+fftITEzktttuY9iwYezfv58PPviALl26lP0/vDCyoFzccccd5b72lacWyQNe3VWBvQk89vx82XzoUMD/2fPzS93H48ePy4A8btw4+cUXX5Rr1aolHz9+3KvO5MmT5SlTpqjHw4YNk5csWaIe9+jRQ966dat6HBcX59OGJ1OmTPFqT5Zl+ciRI7LBYJAPHTok//jjj/Lx48flw4cPy6GhoXJBQYEsy7J82223yd988416TdeuXb3u26BBA/nTTz+VZVmWz549K4eHhxfZh4KCAtlgMKj9fPTRR+WFCxfKsizLv//+u9yoUSM5Ly9PlmVZHjFihPzZZ5/Jp0+flmNiYuSUlBT1e8ydO1eWZVn+8MMP5QceeKDIZzZ06FD5gQcekPPz8+W1a9fKsizL999/vzxt2jRZlmV5z549cufOndX68fHx8hdffCHLsix/88038uDBg9XvVaNGDfncuXOyLMvyihUr5BMnThT5PUui8DsaED87gTd2h3zJu51ogoLKbWEFmscee4zGjRv7XdXXaDTFHlcEWq2WiIgImjdvTnOPZ7Z161aWL1+OTqcjPz+f1NRUr2sKt+GaP4yKisJkMhV5P6PRyK+//sr27dv5/vvvOXPmjNr2ihUr6NKlC8HBwQAsXrwYgAULFtCoUSN1CD516lS1vZKekVarJT4+nqCgIO6++25AsSz37NnD/Pnzsdls6v3//PNP9u7dq/rN9u7dW/WtjYqKol+/fixfvpwxY8Zw5swZGjZsWOT3LCtib2wVYLc70IknH3AGDhxI48aNi63jcDgq7H6uVVxA9TN1sXfvXh588EHatm3LQw89pC5GFOVwD74CWBTHjx9n0KBBREdHM2LECNq0aaO27XA4fDwVgCLL/eHvGRX+fqNHj+a7775jxIgR3HXXXQDY7XbsdjuSJBV5r0cffZTFixdz8ODBcnliFId45QKMLMvYHSLiycVCeHi46kcH8Mcff1yQH114eDjZ2dkA/PPPP0XWW7FiBX369KFz584AqjtLRcwtr1u3jquvvlq1mDzbHjJkCHv27OH8+fOAIkBr1qzhrrvu4vjx414uOatWrVK/k+cz+r//+79in9G5c+dITExkxowZBAcHq/ffsWMHNWrUoF27dmzcuFGt77l1s2vXrsiyzMyZMyt866YYxgYam82ZbEeIXWXyww8/sHTpUkBZ+OrcuTNjx471qXf//fczceJEPv74YywWCx07dmTJkiW0atWK/fv38/fff/POO+8QExPDmjVrSEtLY8aMGUybNs3vquuQIUN4/PHHWbhwIV27diU1NZXp06eTmprKhAkTeOqpp6hfvz6PPPIIM2fO5IsvviAvL4977rmHzz//nDFjxrBgwQL+/vtv3n33XZo3b86HH35IWloa06dPZ/z48bz//vuA4lLzyiuvqENSF4MGDWLv3r2sWbMGgG7durF8+XLat2/Ptddey9y5c3n22Wfp3LkzFouFIUOGUKNGDdauXcukSZPo2rUrDoeDvn37AspQc8OGDaxYsQKbzUbz5s354osvuO6668jOzmbXrl0cO3YMo9FIQkICUVFRTJ48mblz59K2bVvMZjPNmjVj586d9OjRgw0bNjBx4kROnz5NZGSkz2LlyJEjyc3NLZMHRmmQ5NLargIv+vfvz/r168t8nSMnh/HPr+RMy44sHRNfCT0TCC5t1qxZQ7du3S7YhafwOyqGsQFGNpuxSxq0lTARLhBcqvz22288/fTTAKSnp1+w0PlDvHGViCM/n7PPPUfBrl1qmSp2l7qjnUBQgURGRpKRkcH777/PwIEDK+UeYs6uEtEEB+PIySHrrbcwbNpEzcmTkQsKsEsadCIBhUCg0rRp03JvDy0t4o2rZCIefxx9XBwFO3ZgT09HNptxSBo0wvdEIAgo4o2rZIxt2xL+0EMAyFarOozVldJnSiAQVAxC7AKBc08iViv5P/yAQ9KiFZadQBBQxBsXACSn2MlWK7b//sMhSWiLCAoqEAgqByF2AcBT7GSLBbl2XbFAIRAEGPHGBQJPsTObcWg0CK27+JEDGJbdZrORmZlZqrpff/0148aN89pyJSgZ8coFAMljzk5ZoNCK7WKVTFWGZS8rKSkpDBkyhA4dOqhlzzzzDP/73//81u/Tpw+ZmZns3bu3QvtxuSP87AKAZDAAHpadpBFiV8nccsstNG3alI8//pjnn3/eK9pJ3759OXLkCI0aNSq2jbFjx7Ju3Tr1+NVXX62UILD16tVj9uzZ3HTTTWrZ0KFDi4yAIkmSTzIZQckIyy4QuIaxFovHdjEhdoHm4MGDnD17lj59+lBQUFCqa8oSlv1CKNxux44d6dSpU6Xcq7oiLLsA4DmMRRW7qu3ThZJvdZCUUTrBqEga1w4iWF++h1fZYdnXr1/PhAkTMBgMrF69mpiYGLp37069evVYsWIF586dY82aNTRu3Jjff/+d++67j/h432AQR48e5ZlnngHgyy+/BODw4cO88847XHvttWi1Wg4cOOA3j4uLtLQ0Fi1aRJMmTTh48CDdunVTQz5ZLBZeeukl6tatq+ZTnjt3rhpaSZIk6taty2+//cbrr7/OvHnzmDFjBl999RU33ngjo0eP5p133uHYsWNER0czevRovvrqK+bMmcPnn3/OzTffzOOPP17m0PC7d+/mhRdeoGXLlqxZswaTycRNN93E9ddfz/Lly9Vry4sQuwAgabWg0aiW3eUwjE3KKOD+DwKf6m/liBZcVS+kTNcsXLgQvV7PokWL6NOnj1p+1113sX//fvX40UcfZZfHPuay0r9/f8LCwhg3bhwtWrQA4MEHH+Spp55Cq9Uyb948UlNTmTRpEj179qR9+/Ze0YldNG3alKeeekrNjpaXl0fPnj354YcfaNq0KQDbt28vti8bNmxg586dTJkyBavVSv369Tly5AiRkZG8+OKL2Gw2nnnmGVUU8/LyWLJkCXv37mXdunXY7Xbee+890tPTmThxIt98843a9vz581URDg4O5r333sNoNBIdHc2UKVMAJaPfuHHjOHToEI0bN2bUqFF8+OGHjBo1igMHDjB69GgOHjxIcHAwjzzyCH/99RcPP/wwBQUFbN++nTp16lCnTh1GjRrFxIkTy/1/4okQuwAh6fU48vIAZ3axSzwse+PaQawc0aJK7ltWAhmW/eabbyYnJ4d9+/bRrFkzatSooUYYnjBhArt27WL+/PnUqFGDtLS0ItvxjEq8ZcsWCgoKVKEDCAsLK7Yfw4cPp2nTpixYsIDw8HAAMjMziYyMZMmSJeo+1OjoaPbt2wfAkiVL1EURrVbLL7/8orZX3HPSarXY7Xa6dOniFbG4rKHhQZmrnDRpEmfOnCE7O5urr7662O9ZFoTYBQq9HtmZGNzOpT+MDdZrymxhVTWliaZREWHZH330URYtWsQNN9yg5mQAZYHj6NGjLFq0iJo1azJ8+HBkWS7VPQuHPS+JDz/8kLVr17J06VIaNGigWlyBCs3uCg0/d+5cevfuzalTp0q8P0BERAR33303S5cuJSQkhFGjRpWqP6XhEn/lLh0kgwGHU+wcSJd8wp3LhYoOyw7K0PWLL74gIyODyMhItXzBggVMnjyZmjVrqu4rSUlJJQ6db775Zmw2G0lJSWpZRkZGsf1cuHAhY8eOpUGDBhQUFHDmzBlkWeaTTz5h2LBhfPXVV2rdn376iaSkJIYNG+blu3fixAn27NkDeD+n9PR00tLSir1/eULDu3DlodBqtaXOu1EahGUXIBznzpHnnOfIs0uEGsXfmcqkqsKyA9SsWZO77rqLm2++2at89uzZLFu2jG7dupGVlcXw4cN55513eO6555g0aRLp6elMmzaNe+65hwULFvDPP//w8ccfM3ToUL755htmzpxJt27d0Ol0mEwm1q9fz3XXXaeGT/dkypQpbNy4EVmWycjI4Mknn2TWrFmMHz+e++67j3HjxvHKK6/QqFEjQkJC6NKlC2PHjuX8+fOMHz+eli1bAqgLNc899xyff/456enpGAwGrr76al588UXeeust5syZA8CYMWMYNWoUV111VblCw7uIj48nIiKC22+/vUz/5yUhwrKXk7KGZT/dowegWHUDer3FhD4NGXhN7crqnkBwSTN//nxGjx59QW0UfkeFZRcgak6fjpybS44+GMduiAwWIZ4EAk9mzJhBfHw8DRs2rPA0iiDELmAEd+sGwLlMM+z+m4hg8egFAk8iIiL45ZdfOHPmDPfff3+Fty/euABzPl+Z1I0IEpadQODJhQ5bS0KIXSXz9OpjWOzuZXqX2IlhrEAQWITYVTIhBg16u9vNJNSgpVX9EKIjDFXYK4Gg+iHErpKZcVfjqu6CQCBAOBULBIJqghA7gUBQLRBiJxAIqgVC7AQCQbVAiF0lk5iYWNVduOwQz7TiqQ7PVIhdJVMdfokCjXimFU91eKZC7C5BKusXszLavVReokvpu4tnWj6E2F2CXEovkXgxKx7xTMuHCPFUTlq3bu0VJrsokpOTKzztXWW0WVntijbF/1NVtXn06FH++usv9ViInUAgqBaIYaxAIKgWCLETCATVAiF2AoGgWiDETiAQVAuE2AkEgmqBELtKYu/evYwdO5Zly5YxcuRIjh8/XtVdumj4+eefefPNN5k6dSq33XYb27dvB4p/ZpVx7nJl/Pjx7Ny5ExDP1AtZUOEUFBTIcXFxcmpqqizLsrx79265R48eVdupi4Tc3Fx5woQJ6vGaNWvk4OBgOSkpqchnVtzzLO+5y5UdO3bIdevWlbdu3Vopz+1SfqZC7CqBb7/9Vm7Tpo16bLPZZKPRqP6CVGcOHDggA/Lhw4dlWZbl8+fPy4C8atWqIp9Zcc+zvOcuR7Kzs+X58+fLPXr0kLdu3Vopz+1SfqZiGFsJJCUlERUVpR5rtVrCw8O9vLmrK23atGHXrl3q7pMTJ06oP4t6ZsU9z/KeuxxZvHgxjzzyiHpcGc/tUn6mIgdFJZCRkUFQUJBXWVBQEFlZWVXToYsISZK4/vrr1eNZs2YxduxYbDZbkc+suOdZ3nOXGxs2bOC2227DaDSqZZXx3C7lZyosu0ogMjISudAuvJycHGrXrl1FPbo4+eijj4iJiWHOnDnFPrPKOHc5cfr0aTIzM7n66qu9ysUz9UaIXSXQsmVL0tPT1WOz2YzJZCIuLq4Ke3Vx8fXXX2O323n99dcxm80EBQUV+cyKe57lPXc5sXnzZpKTk5k1axazZs3i33//5ZNPPiEuLk48U0+qbLbwMsZqtcr169eXT548KcuyLG/evFnu0qVLFffq4mH79u3y4sWL5ZSUFDklJUVet26dvGPHjiKfWXHPs7znLmfi4uLkrVu3Vspzu5SfqZizqwR0Oh3Lli1jxowZdOnShe3bt7Ny5cqq7tZFwbFjx+jXrx8mk8mrPDs7u8hnVtzzLO+5y5FTp07x9ttvk5qayptvvkleXp54ph6IEE8CgaBaIObsBAJBtUCInUAgqBYIsRMIBNUCIXYCgaBaIMROIBBUC4TYCQSCaoEQO4FAUC0QYicQXAAFBQWcP3++qrshKAVC7AQXNRMmTFA/z5s3j5SUlCLr7t27l9GjRyNJEsOHD2fWrFnMnDmToUOH8vLLL2O320t1zyNHjpQqisfu3bvp0aMHTz31VKnaFVQtYruY4KJFlmXy8/PV43///Zd69eoVWT8+Pp74+HgWLFjA448/TpcuXQCw2Wy0bt0aWZaZOnVqiffdtGkT/fr1o0aNGsXWu/766xk1apQaVl5wcSMsO8FFy6+//kp8fDwAJpOJyMjIcrWj0+lo3bo1v//+e4l1//33X+bMmVPqtiVJKlefBIFHWHaCi5bNmzczatQoAL777jt69uxZrnYOHTrE7t27WbFiBQB2u53Zs2cTGxtLVlYWVquV5557Drvdzueff05mZiYLFy6kZs2aPPvss+j1enbv3s2qVato06YNR48epV+/fnTt2lVtb+XKlciyzOeff86CBQuoX78+APPnz8dut1OjRg327t3LW2+9RWZmJitXrqRhw4bY7XY2bdrE0qVLL/yBCYpFiJ3goiE5OZn58+erx1u2bCEnJweA7du3c8MNN/Dtt9/Srl07EhISim3r+++/JykpiZycHBITE5k5cya9evUC4Pjx47z66qucPXsWo9FI7969ufrqq+nduzcTJkxg0aJFPPbYYzRu3BiAlJQU7rnnHvbv309MTAxTp05l3759qtjt2bOHDz/8EIPBwC+//MKqVat45plnWLNmDT/++COrV68G4MCBA3z44Yfk5uZSs2ZNBg0aBCAWOAKEEDvBRUNsbCyzZs0ClOi3er2eadOmATB27FjeeOONUrfVs2dPdc5u+PDh9OjRg/379/P2229z5ZVXsnXrVpYvX45OpyM/P5/U1NQi21q7di2NGjUiJiYGwGfer02bNhgMBgCioqJU8frkk08ICQnhiy++AKBBgwYEBQVx0003ceutt/Lee+9xww038PTTT5f6ewnKjxA7wUXJL7/8QqdOnQBIT08nOjq63G0ZDAYGDx7M2LFjmTp1KkePHuXBBx/ko48+onPnzuoCg81mQ6dzvxJnz54lIyMDh8PhE4rcE61WW2R5/fr1GTBggFf5kSNHOHr0KHv27GHjxo3ceOON/PPPP175IwQVjxA7wUWD5zB2z549tG/fnl27dnHw4EHCw8NVN5TSDGML8/PPP3PFFVcQGRnJihUr6NOnD507dwbg5MmTAKxcuZIHHniA8PBwsrOzyczMBOCuu+5i2rRpnDhxgkaNGgGwatUqBg8eXOw9R4wYwfjx45k9ezYajYa8vDzWrFlDUlISAwYMoHv37nTv3p0jR45gNpuF2FUyInin4KJk7NixzJ07V/385ptvFmlBudi7dy8rVqxg/vz5DB06VHU3+f333zGbzcyZM4cmTZrwxx9/MHPmTO69917y8vLIyspi06ZNjBkzhltuuYVVq1axbds22rRpw6hRo9BqtezYsYMPPviArl274nA46Nu3L6dPn2bSpEkcOXKEadOmUadOHSZNmoQkSUybNo3evXvz8ccf8+uvv3LVVVdhsVh45JFHePvttykoKKBJkybk5+cjSRIjRowIwFOt3gixE1x0pKamsmzZMsaPH48sy4wZM4a33367qrsluMQRfnaCi45NmzbRu3dvAPbt28c111xTxT0SXA4IsRNcdBw7dox27doBsGvXLlX4BIILQQxjBQJBtUBYdgKBoFogxE4gEFQLhNgJBIJqwf8DxVxf15HT+ZgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if train:\n",
    "    fig, ax = plt.subplots(1,1,figsize=(3,3))\n",
    "    ax.plot(net.hist['iters_monitor'][1:], net.hist['train_acc'][1:], color=c_vals[0], label='Full train accuracy')\n",
    "    ax.plot(net.hist['iters_monitor'][1:], net.hist['valid_acc'][1:], color=c_vals[1], label='Full valid accuracy')\n",
    "    if net.weight_reg is not None:\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['train_loss_output_label'], color=c_vals_l[0], zorder=-1, label='Output label')\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['train_loss_reg_term'], color=c_vals_l[0], zorder=-1, label='Reg term', linestyle='dashed')\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['valid_loss_output_label'], color=c_vals_l[1], zorder=-1, label='Output valid label')\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['valid_loss_reg_term'], color=c_vals_l[1], zorder=-1, label='Reg valid term', linestyle='dashed')\n",
    "    \n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    # ax.set_ylabel('Loss ({})'.format(net.loss_type))\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_xlabel('# Batches')\n",
    "    plt.savefig(f\"./paper/loss_{hyp_dict['ruleset']}_{task_params['fixate_off']}.png\")\n",
    "    \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e544aca8-271f-49d3-9ed3-ab7023f23600",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'lam0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mnet_helpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet_eta_lambda_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyp_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mmfs1/gscratch/amath/zihan-zhang/mpn/net_helpers.py:136\u001b[0m, in \u001b[0;36mnet_eta_lambda_analysis\u001b[0;34m(net, net_params, hyp_dict, verbose)\u001b[0m\n\u001b[1;32m    132\u001b[0m     full_eta \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mhist[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meta\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(mpl_idx\u001b[38;5;241m+\u001b[39mlayer_index)]\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m net\u001b[38;5;241m.\u001b[39mmp_layers[mpl_idx]\u001b[38;5;241m.\u001b[39mlam_type \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpre_vector\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost_vector\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatrix\u001b[39m\u001b[38;5;124m'\u001b[39m,):\n\u001b[1;32m    135\u001b[0m     full_lam \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([\n\u001b[0;32m--> 136\u001b[0m         lam\u001b[38;5;241m.\u001b[39mflatten()[np\u001b[38;5;241m.\u001b[39mnewaxis, :] \u001b[38;5;28;01mfor\u001b[39;00m lam \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhist\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlam\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmpl_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    137\u001b[0m     ], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     full_lam \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mhist[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlam\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(mpl_idx\u001b[38;5;241m+\u001b[39mlayer_index)]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'lam0'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if train:\n",
    "    net_helpers.net_eta_lambda_analysis(net, net_params, hyp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b385cd9-3f03-4c33-a943-ef80ca4df69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyp_dict['chosen_network'] == \"dmpn\":\n",
    "    if net_params[\"input_layer_add\"]:\n",
    "        input_matrix = net.W_initial_linear.weight.data.detach().cpu().numpy()\n",
    "        figinp, axsinp = plt.subplots(1,1,figsize=(4,4))\n",
    "        sns.heatmap(input_matrix, ax=axsinp, square=True, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c10ab17-bab6-4228-9096-af042e9ac385",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "use_finalstage = False\n",
    "if use_finalstage:\n",
    "    # plotting output in the validation set\n",
    "    net_out, db = net.iterate_sequence_batch(test_input, run_mode='track_states')\n",
    "    W_output = net.W_output.detach().cpu().numpy()\n",
    "\n",
    "    W_all_ = []\n",
    "    for i in range(len(net.mp_layers)):\n",
    "        W_all_.append(net.mp_layers[i].W.detach().cpu().numpy())\n",
    "    W_ = W_all_[0]\n",
    "    \n",
    "else:\n",
    "    ind = len(marker_lst)-1 \n",
    "    # ind = 0\n",
    "    network_at_percent = (marker_lst[ind]+1)/train_params['n_datasets']*100\n",
    "    print(f\"Using network at {network_at_percent}%\")\n",
    "    net_out = netout_lst[ind]\n",
    "    db = db_lst[ind]\n",
    "    W_output = Woutput_lst[ind]\n",
    "    if net_params[\"net_type\"] == \"dmpn\":\n",
    "        W_ = Wall_lst[ind][0]\n",
    "\n",
    "if net_params['loss_type'] in ('MSE',):\n",
    "    fig, axs = plt.subplots(5, 1, figsize=(4, 5*2))\n",
    "    figin, axsin = plt.subplots(5, 1, figsize=(4, 5*2))\n",
    "\n",
    "    if test_output_np.shape[-1] == 1:\n",
    "        for batch_idx, ax in enumerate(axs):\n",
    "            ax.plot(net_out[batch_idx, :, 0], color=c_vals[batch_idx])\n",
    "            ax.plot(test_output_np[batch_idx, :, 0], color=c_vals_l[batch_idx])\n",
    "\n",
    "    else:\n",
    "        for batch_idx, ax in enumerate(axs):\n",
    "            task_label = test_input_np[batch_idx, 0, 6-shift_index:]\n",
    "            # task_label_index = np.where(task_label == 1)[0][0] \n",
    "            task_label_index = np.where(np.isclose(task_label, 1, atol=0.1))[0][0]\n",
    "            for out_idx in range(test_output_np.shape[-1]):\n",
    "                axs[batch_idx].plot(net_out[batch_idx, :, out_idx], color=c_vals[out_idx])\n",
    "                axs[batch_idx].plot(test_output_np[batch_idx, :, out_idx], color=c_vals_l[out_idx], linewidth=5, alpha=0.5)\n",
    "\n",
    "            input_batch = test_input[batch_idx,:,:].cpu().numpy()\n",
    "            for inp_idx in range(input_batch.shape[-1]):\n",
    "                axsin[batch_idx].plot(input_batch[:,inp_idx], color=c_vals[inp_idx], label=inp_idx)\n",
    "            axsin[batch_idx].legend()\n",
    "\n",
    "    fig.suptitle(f\"Validation Set Output Comparison using Network at Percentage {network_at_percent}%\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"./paper/lowD_{hyp_dict['ruleset']}_{hyp_dict['chosen_network']}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "\n",
    "\n",
    "    figin.suptitle(f\"Validation Set Output Comparison using Network at Percentage {network_at_percent}%\")\n",
    "    figin.tight_layout()\n",
    "    figin.savefig(f\"./paper/lowD_{hyp_dict['ruleset']}_{hyp_dict['chosen_network']}_{hyp_dict['addon_name']}_input.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9efbd2f-a48d-4f8e-ba55-b0b102463546",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_marker = 1 if net_params[\"input_layer_add\"] else 0\n",
    "\n",
    "# here db is selected based on learning stage selection \n",
    "def modulation_extraction(db):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print(db.keys())\n",
    "    Ms = np.concatenate((\n",
    "        db[f'M{ll_marker}'].detach().cpu().numpy().reshape(n_batch_all, max_seq_len, -1),\n",
    "    ), axis=-1)\n",
    "\n",
    "    Ms_orig = np.concatenate((\n",
    "        db[f'M{ll_marker}'].detach().cpu().numpy(),\n",
    "    ), axis=-1)\n",
    "\n",
    "    bs = np.concatenate((\n",
    "        db[f'b{ll_marker}'].detach().cpu().numpy(),\n",
    "    ), axis=-1) \n",
    "\n",
    "    hs = np.concatenate((\n",
    "        db[f'hidden{ll_marker}'].detach().cpu().numpy().reshape(n_batch_all, max_seq_len, -1),\n",
    "    ), axis=-1)\n",
    "\n",
    "    return Ms, Ms_orig, hs, bs\n",
    "    \n",
    "if net_params[\"net_type\"] in (\"dmpn\", ):\n",
    "    if mpn_depth == 1:\n",
    "        Ms, Ms_orig, hs, bs = modulation_extraction(db)\n",
    "    else:\n",
    "        modulations, hiddens = [], []\n",
    "        for i in range(mpn_depth):\n",
    "            modulations.append(db[f'M{i}'].detach().cpu().numpy().reshape(n_batch_all, max_seq_len, -1))\n",
    "            hiddens.append(db[f'hidden{i}'].detach().cpu().numpy().reshape(n_batch_all, max_seq_len, -1),)\n",
    "\n",
    "        Ms = modulations[0]\n",
    "        hs = hiddens[0]\n",
    "        \n",
    "elif net_params[\"net_type\"] in (\"vanilla\", \"gru\"):\n",
    "    hs = db['hidden'].detach().cpu().numpy()\n",
    "\n",
    "pca_type = 'full' # full, cell_types\n",
    "pca_target_lst = ['hs', 'Ms'] # hs, 'Ms' \n",
    "if net_params[\"net_type\"] in (\"vanilla\", \"gru\"):\n",
    "    pca_target_lst = ['hs'] # if not dmpn, no M information effectively\n",
    "\n",
    "# using recorded information\n",
    "recordkyle_all, recordkyle_nameall = [], []\n",
    "for test_subtrial in test_trials:\n",
    "    metaepoch = test_subtrial.epochs\n",
    "    periodname = list(metaepoch.keys())\n",
    "    recordkyle, recordkyle_name = [], []\n",
    "    for keyiter in range(len(periodname)):\n",
    "        try:\n",
    "            recordkyle_name.append(periodname[keyiter])\n",
    "            if test_mode_for_all == \"random\":\n",
    "                recordkyle.append(metaepoch[periodname[keyiter]][1])\n",
    "            elif test_mode_for_all == \"random_batch\":\n",
    "                recordkyle.append(list(metaepoch[periodname[keyiter]][1]))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    if test_mode_for_all in (\"random\",):\n",
    "        fillrecordkyle = []\n",
    "        for timestamp in recordkyle:\n",
    "            fillrecordkyle.append([timestamp for _ in range(hs.shape[0])])\n",
    "        recordkyle = fillrecordkyle\n",
    "\n",
    "    recordkyle.insert(0, [0 for _ in range(len(recordkyle[1]))])\n",
    "    recordkyle = np.array(recordkyle).T.tolist()\n",
    "    recordkyle_all.extend(recordkyle)\n",
    "    recordkyle_nameall.append(recordkyle_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f41fdc5-88a2-459a-8ea5-8177aa1a9393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sep 30th\n",
    "# This part of code should be adaptive for multitask, which may have different breaks and periods for each task\n",
    "unique_lists = set(tuple(lst) for lst in recordkyle_all)\n",
    "# here select task specific information\n",
    "# which maximally should have length of number of tasks\n",
    "unique_recordkyle_all = [list(lst) for lst in unique_lists]\n",
    "if not task_random_fix:\n",
    "    assert len(unique_recordkyle_all) >= len(rules_dict[hyp_dict['ruleset']])\n",
    "else:\n",
    "    print(\"Test DataSet Random Seed Is Fixed\")\n",
    "\n",
    "all_session_breakdown = []\n",
    "for task_specific_time in unique_recordkyle_all:\n",
    "    session_breakdown = []\n",
    "    for sindex in range(0,len(task_specific_time)-1):\n",
    "        # all sessions should be the same for each task\n",
    "        # but different across tasks\n",
    "        # though the time of when response period starts might be similar across\n",
    "        session_breakdown.append([task_specific_time[sindex], task_specific_time[sindex+1]]) \n",
    "    session_breakdown.append([task_specific_time[0], task_specific_time[-1]])\n",
    "    all_session_breakdown.append(session_breakdown)\n",
    "\n",
    "# break down time\n",
    "all_breaks = []\n",
    "for session_breakdown in all_session_breakdown:\n",
    "    breaks = [cut[1] for cut in session_breakdown[:-1]]\n",
    "    print(f\"Task {all_session_breakdown.index(session_breakdown)}; breaks: {breaks}\")\n",
    "    all_breaks.append(breaks)\n",
    "\n",
    "# for delay-task\n",
    "assert len(all_breaks)\n",
    "response_start = all_breaks[0][-2]\n",
    "stimulus_start = all_breaks[0][0]\n",
    "stimulus_end = all_breaks[0][1]\n",
    "print(f\"response_start: {response_start}\")\n",
    "print(f\"stimulus_start: {stimulus_start}\")\n",
    "print(f\"stimulus_end: {stimulus_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5adc633-8a73-4e8b-8733-b4ce98482ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages_num = len(Wall_lst) # how many recorded neurons in total\n",
    "break_info = all_breaks[0]\n",
    "\n",
    "input_nums = Ms_orig.shape[-1]\n",
    "batch_nums = Ms_orig.shape[0]\n",
    "neuron_nums = Ms_orig.shape[2]\n",
    "colors = helper.generate_rainbow_colors(Ms_orig.shape[2])\n",
    "\n",
    "def generate_random_orthonormal_matrix(N, num_columns=3):\n",
    "    \"\"\"\n",
    "    generates an N x num_columns random matrix with orthonormal columns.\n",
    "    \"\"\"\n",
    "    random_matrix = np.random.randn(N, num_columns)    \n",
    "    Q, R = np.linalg.qr(random_matrix)    \n",
    "    return Q[:, :num_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e212e03-6fb4-439b-9fd6-5aefd8a09a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check from equation 2-7\n",
    "def plot_trajectory_by_index(label_index, stage_iter, verbose=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    W_ = Wall_lst[stage_iter][0]\n",
    "    W_output = Woutput_lst[stage_iter]\n",
    "    _, Ms_orig, _, bs = modulation_extraction(db_lst[stage_iter]) # batch * seq_len * hidden_neuron * input_neuron\n",
    "\n",
    "    if verbose:\n",
    "        figsize1, figsize2 = 2, 6\n",
    "        figexh1, axsexh1 = plt.subplots(3,3,figsize=(figsize2*3,figsize1*3))  \n",
    "        figexh2, axsexh2 = plt.subplots(4,3,figsize=(figsize2*3,figsize1*4))  \n",
    "        figdiff, axsdiff = plt.subplots(1,2,figsize=(4*2,2))\n",
    "\n",
    "    task_middle_dict = {}\n",
    "    task_labels_across_batch = []\n",
    "\n",
    "    saver_shape1 = (3,3)\n",
    "    saver1 = np.empty((test_input.shape[0], saver_shape1[0], saver_shape1[1]), dtype=object)\n",
    "    saver_shape2 = (4,3)\n",
    "    saver2 = np.empty((test_input.shape[0], saver_shape2[0]+1, saver_shape2[1]), dtype=object)\n",
    "    saver2_random = np.empty((test_input.shape[0], saver_shape2[0]+1, saver_shape2[1]), dtype=object) # projection to random space\n",
    "\n",
    "    random_output_Y_lst = [generate_random_orthonormal_matrix(W_output.shape[1]) for _ in range(10)]\n",
    "\n",
    "    for batch_iter in range(test_input.shape[0]):\n",
    "        writeon = 0\n",
    "        labels_for_batch = labels[batch_iter,0]\n",
    "        \n",
    "        if labels_for_batch in label_index: # >=0: for all label; ==0, say, for specific label on the ring (regardless on which task is using)\n",
    "            xx = test_input[batch_iter, :, :].cpu().numpy()[0,6-shift_index:]\n",
    "            which_task = np.where(xx)[0][0] # extract here, will repeat later at different time slices\n",
    "            \n",
    "            if labels_for_batch not in task_middle_dict.keys():\n",
    "                task_middle_dict[which_task] = []\n",
    "                writeon = 1\n",
    "                \n",
    "            x_batch_taskinfo = test_input[batch_iter, :, :][:,6-shift_index:].cpu().numpy()[0,:]\n",
    "            task_specific = np.where(x_batch_taskinfo == 1)[0]\n",
    "            assert len(task_specific) == 1\n",
    "            task_specific = task_specific[0]\n",
    "            \n",
    "            task_labels_across_batch.append(task_specific) # load task information (which task) across batches\n",
    "        \n",
    "            res_eq26, res_eq8, res_eq11 = [], [], []\n",
    "            res_meta = []\n",
    "\n",
    "            for i in range(saver_shape1[0]):\n",
    "                for j in range(saver_shape1[1]):\n",
    "                    saver1[batch_iter, i, j] = np.array([])\n",
    "        \n",
    "            for i in range(saver_shape2[0]+1):\n",
    "                for j in range(saver_shape2[1]):\n",
    "                    saver2[batch_iter, i, j] = np.array([])\n",
    "                    saver2_random[batch_iter, i, j] = np.array([])\n",
    "        \n",
    "            for time_iter in range(test_input.shape[1]):\n",
    "                x = test_input[batch_iter, time_iter, :].cpu().numpy().reshape(-1,1)\n",
    "                \n",
    "                input_length = len(x)\n",
    "        \n",
    "                x_fixon, x_fixoff, x_stimulus, x_task = [np.zeros((input_length, 1)) for _ in range(4)]\n",
    "                # one-hot encoded vector for fixation\n",
    "                x_fixon[0,0] = x[0,0] \n",
    "                # one-hot encoded vector for fixation off (set to dummy if not presented)\n",
    "                x_fixoff[1,0] = x[1,0] if task_params['fixate_off'] else 0\n",
    "                # one-hot encoded vector for stimulus\n",
    "                x_stimulus[2-shift_index:6-shift_index,0] = x[2-shift_index:6-shift_index,0]\n",
    "                # one-hot encoded vector for task\n",
    "                # task (dynamically setting for all element after the 6th elements)\n",
    "                tasks_info = x[6-shift_index:,0]\n",
    "                x_task[6-shift_index:,0] = tasks_info\n",
    "\n",
    "                which_task = np.where(tasks_info)[0][0]\n",
    "                \n",
    "                Mt = Ms_orig[batch_iter, time_iter, :, :] \n",
    "                bt = bs[batch_iter, time_iter, :].reshape(-1,1) # hidden_neuron * 1\n",
    "                \n",
    "                middle =  W_ + W_ * Mt\n",
    "\n",
    "                if time_iter >= response_start + 1 and len(label_index) == 1:\n",
    "                    if writeon:\n",
    "                        task_middle_dict[which_task].append(middle)\n",
    "                \n",
    "                y_fix = W_output[0,:].reshape(1,-1)\n",
    "                Y_resp1 = W_output[1,:].reshape(1,-1)\n",
    "                Y_resp2 = W_output[2,:].reshape(1,-1)\n",
    "\n",
    "                if task_params['fixate_off']:\n",
    "                    allX1 = [x_fixon+x_task, x_fixoff+x_task, x_stimulus+x_fixon+x_task]\n",
    "                else:\n",
    "                    allX1 = [x_fixon+x_task, x_task, x_stimulus+x_fixon+x_task]\n",
    "                allX1name = [\"x_fixon+x_task\", \"x_fixoff+x_task\", \"x_stimulus+x_fixon+x_task\"]\n",
    "                allX2 = [x_fixon, x_fixoff, x_stimulus, x_task]\n",
    "                allX2name = [\"x_fixon\", \"x_fixoff\", \"x_stimulus\", \"x_task\"]\n",
    "                allY = [y_fix, Y_resp1, Y_resp2]\n",
    "                allYname = [\"y_fix\", \"Y_resp1\", \"Y_resp2\"]\n",
    "        \n",
    "                for yiter in range(len(allY)):\n",
    "                    for xiter in range(len(allX1)):\n",
    "                        # res1 = helper.to_unit_vector(allY[yiter]) @ helper.to_unit_vector(middle @ allX1[xiter])\n",
    "                        step1 = middle @ allX1[xiter] + bt # adjust according to specific bias \n",
    "                        res1 = allY[yiter] @ step1 \n",
    "                        saver1[batch_iter, xiter, yiter] = np.append(saver1[batch_iter, xiter, yiter], res1[0,0])\n",
    "        \n",
    "                for y1 in range(len(allY)):\n",
    "                    for x1 in range(len(allX2)):\n",
    "                        # res2 = helper.to_unit_vector(allY[yiter]) @ helper.to_unit_vector(middle @ allX2[xiter])\n",
    "                        step1 = middle @ allX2[x1]\n",
    "                        res2 = allY[y1] @ step1\n",
    "                        res2_random = [((random_output_Y[:,y1].reshape(1,-1)) @ middle @ allX2[x1])[0,0] for random_output_Y in random_output_Y_lst]\n",
    "                        \n",
    "                        saver2[batch_iter, x1, y1] = np.append(saver2[batch_iter, x1, y1], res2[0,0])\n",
    "                        saver2_random[batch_iter, x1, y1] = np.append(saver2_random[batch_iter, x1, y1], np.mean(res2_random))\n",
    "\n",
    "                # how about bias projection to output\n",
    "                for y_iter2 in range(len(allY)):\n",
    "                    step1 = bt \n",
    "                    res2 = allY[y_iter2] @ step1\n",
    "                    saver2[batch_iter, len(allX2), y_iter2] = np.append(saver2[batch_iter, len(allX2), y_iter2], res2[0,0])\n",
    "\n",
    "            if verbose:\n",
    "                for i in range(saver_shape1[0]):\n",
    "                    for j in range(saver_shape1[1]):\n",
    "                        axsexh1[i,j].plot(saver1[batch_iter,i,j], color=c_vals[labels_for_batch], linestyle=l_vals[task_specific])\n",
    "            \n",
    "                for i in range(saver_shape2[0]):\n",
    "                    for j in range(saver_shape2[1]):            \n",
    "                        axsexh2[i,j].plot(saver2[batch_iter,i,j], color=c_vals[labels_for_batch], linestyle=l_vals[task_specific])\n",
    "        \n",
    "                # # extract fixon-task information explicitly\n",
    "                axsdiff[0].plot(saver2[batch_iter,0,1] + saver2[batch_iter,3,1], color=c_vals[labels_for_batch], linestyle=l_vals[task_specific])\n",
    "                axsdiff[0].plot(saver2_random[batch_iter,0,1] + saver2_random[batch_iter,3,1], color=c_vals_l[labels_for_batch], linestyle=l_vals[task_specific])\n",
    "                \n",
    "                axsdiff[1].plot(saver2[batch_iter,0,2] + saver2[batch_iter,3,2], color=c_vals[labels_for_batch], linestyle=l_vals[task_specific])\n",
    "                axsdiff[1].plot(saver2_random[batch_iter,0,2] + saver2_random[batch_iter,3,2], color=c_vals_l[labels_for_batch], linestyle=l_vals[task_specific])\n",
    "\n",
    "    if verbose:\n",
    "        # plot fixon/task information for one specific stimulus on one figure\n",
    "        # show perfect cancellation until fixon info goes away (during response period)\n",
    "        figpaper, axspaper = plt.subplots(8,1,figsize=(4, figsize1*8))\n",
    "\n",
    "        temp_saver = []\n",
    "\n",
    "        for batch_iter in range(test_input.shape[0]):\n",
    "            labels_for_batch = labels[batch_iter,0]\n",
    "            if labels_for_batch in label_index and labels_for_batch not in temp_saver:\n",
    "                f_fixon, f_task, f_bias = saver2[batch_iter, 0, 1], saver2[batch_iter, 3, 1], saver2[batch_iter, -1, 1]\n",
    "                axspaper[len(temp_saver)].plot(f_fixon, color=c_vals[0], linestyle=l_vals[0], label=\"Fixon\")\n",
    "                axspaper[len(temp_saver)].plot(f_task+f_bias, color=c_vals[1], linestyle=l_vals[1], label=\"Task+Bias\")\n",
    "                axspaper[len(temp_saver)].plot(f_fixon+f_task+f_bias, color=c_vals[2], linestyle=l_vals[3], linewidth=3, \\\n",
    "                                                       label=\"Combine\")\n",
    "                axspaper[len(temp_saver)].axhline(0, color=c_vals[3])\n",
    "\n",
    "                temp_saver.append(labels_for_batch)\n",
    "\n",
    "        for axsp in axspaper:\n",
    "            axsp.legend()\n",
    "            axsp.set_ylim([-2.0, 2.0])\n",
    "        figpaper.tight_layout()   \n",
    "        figpaper.savefig(\"./paper/show.png\")\n",
    "        \n",
    "        for i in range(saver_shape1[0]):\n",
    "            for j in range(saver_shape1[1]):\n",
    "                axsexh1[i,j].set_ylim([-2.0, 2.0])\n",
    "                axsexh1[i,j].set_title(f\"{allX1name[i]} & {allYname[j]}\")\n",
    "        \n",
    "        for i in range(saver_shape2[0]):\n",
    "            for j in range(saver_shape2[1]):\n",
    "                axsexh2[i,j].set_ylim([-2.0, 2.0])\n",
    "                axsexh2[i,j].set_title(f\"{allX2name[i]} & {allYname[j]}\")\n",
    "        \n",
    "        for ax in np.concatenate((axsexh1.flatten(), axsexh2.flatten())):\n",
    "            for breaks in all_breaks:\n",
    "                for bb in breaks:\n",
    "                    ax.axvline(bb, linestyle=\"--\", c=c_vals[all_breaks.index(breaks)])\n",
    "    \n",
    "        label_index_name = \"all\" if len(label_index) == 8 else label_index\n",
    "        \n",
    "        figexh1.suptitle(f\"Exhaustive Search 1 {color_by} at Stage {stage_iter}\")\n",
    "        figexh1.tight_layout()\n",
    "        figexh1.savefig(f\"./paper/es1_{task_params['fixate_off']}_{network_at_percent}_{label_index_name}.png\")\n",
    "        \n",
    "        figexh2.suptitle(f\"Exhaustive Search 2 {color_by} Stage {stage_iter}\")\n",
    "        figexh2.tight_layout()\n",
    "        figexh2.savefig(f\"./results/es2_{task_params['fixate_off']}_{network_at_percent}_{label_index_name}.png\")\n",
    "\n",
    "        axsdiff[0].set_title(\"Stimulus 1\")\n",
    "        axsdiff[1].set_title(\"Stimulus 2\")\n",
    "        figdiff.suptitle(f\"Fixon-Task at Stage {stage_iter}\")\n",
    "        figdiff.tight_layout()\n",
    "        figdiff.savefig(f\"./paper/diff_{task_params['fixate_off']}_{network_at_percent}_{label_index_name}.png\")\n",
    "\n",
    "    return task_middle_dict if len(label_index) == 1 else {}, task_labels_across_batch, saver2, saver2_random # only do it for single task learnig for clarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954cbe71-7953-48eb-9f2f-af7d5b3be088",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trajectory, all_trajectory_random = [], []\n",
    "for stage_iter in range(stages_num):\n",
    "    task_middle_dict, task_labels_across_batch, save_trajectory, save_trajectory_random = plot_trajectory_by_index(np.unique(labels), \\\n",
    "                                                                                                                    stage_iter, \\\n",
    "                                                                                                                    verbose=(stage_iter==stages_num-1)\n",
    "                                                                                          )\n",
    "    all_trajectory.append(save_trajectory)\n",
    "    all_trajectory_random.append(save_trajectory_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d7d96a-195a-4b05-adda-ef5af48cc49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_trajectory(save_trajectory, save_trajectory_random):\n",
    "    \"\"\"\n",
    "    Analyze trajectories by calculating mean absolute values for fixations and tasks.\n",
    "    \"\"\"\n",
    "    def process_trajectory(trajectory, ind=False):\n",
    "        results = []\n",
    "        for batch in trajectory:\n",
    "            stim1_fixon = batch[0, 1][stimulus_start:response_start]\n",
    "            stim1_task = batch[3, 1][stimulus_start:response_start]\n",
    "            if ind: \n",
    "                bias = batch[4, 1][stimulus_start:response_start]\n",
    "            else:\n",
    "                bias = np.zeros_like(stim1_fixon.shape)\n",
    "                \n",
    "            results.append([np.mean(np.abs(stim1_fixon + stim1_task + bias)), np.mean(np.abs(stim1_fixon)), np.mean(np.abs(stim1_task))])\n",
    "        return np.array(results)\n",
    "    \n",
    "    # Process both trajectories\n",
    "    result = process_trajectory(save_trajectory, True)\n",
    "    result_random = process_trajectory(save_trajectory_random)\n",
    "    \n",
    "    # Return the mean of the computed values\n",
    "    return np.mean(result[:, 0]), np.mean(result[:, 1]), np.mean(result[:,2]), np.mean(result_random[:, 0]), np.mean(result_random[:, 1]), np.mean(result_random[:, 2])\n",
    "\n",
    "fixon_task_diff = np.array([analyze_trajectory(all_trajectory[i], all_trajectory_random[i]) for i in range(len(all_trajectory_random))])\n",
    "\n",
    "figfixontaskcancel, axsfixontaskcancel = plt.subplots(figsize=(4,2))\n",
    "axsfixontaskcancel.plot(counter_lst, fixon_task_diff[:,0], \"-o\", c=c_vals[0], label=\"abs(fixon-task)\")\n",
    "axsfixontaskcancel.plot(counter_lst, fixon_task_diff[:,1], \"-o\", c=c_vals[1], label=\"abs(fixon)\")\n",
    "axsfixontaskcancel.plot(counter_lst, fixon_task_diff[:,2], \"-o\", c=c_vals[2], label=\"abs(task)\")\n",
    "# axsfixontaskcancel[0].plot(fixon_task_diff[:,3], \"-o\", c=c_vals_l[0], label=\"abs(fixon-task) random\")\n",
    "# axsfixontaskcancel[0].plot(fixon_task_diff[:,4], \"-o\", c=c_vals_l[1], label=\"abs(fixon) random\")\n",
    "# axsfixontaskcancel[0].plot(fixon_task_diff[:,5], \"-o\", c=c_vals_l[2], label=\"abs(task) random\")\n",
    "axsfixontaskcancel.legend()\n",
    "axsfixontaskcancel.set_ylabel(\"Projection Magnitude\")\n",
    "# axsfixontaskcancel.set_title(\"Average Cancellation Effect Before Response Period\")\n",
    "\n",
    "axsfixontaskcancel.set_xlabel(\"# Dataset\")\n",
    "axsfixontaskcancel.set_xscale(\"log\")\n",
    "figfixontaskcancel.tight_layout()\n",
    "figfixontaskcancel.savefig(\"./paper/cancel.png\")\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(4,2))  # Create a new figure\n",
    "\n",
    "ax1.plot(counter_lst, loss_lst, \"-o\", c=c_vals[0], label=\"Loss\")\n",
    "ax1.set_ylabel(\"Loss\", color=c_vals[0])\n",
    "ax1.tick_params(axis='y', colors=c_vals[0])\n",
    "ax1.set_yscale(\"log\")  # Keep log scale for loss\n",
    "ax1.set_xlabel(\"Counter\")\n",
    "\n",
    "# Create a second y-axis for accuracy (right)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(counter_lst, acc_lst, \"-o\", c=c_vals[1], label=\"Accuracy\")\n",
    "ax2.set_ylabel(\"Accuracy\", color=c_vals[1])\n",
    "ax2.tick_params(axis='y', colors=c_vals[1])\n",
    "\n",
    "ax1.set_xlabel(\"# Dataset\")\n",
    "ax1.set_xscale(\"log\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"./paper/loss_acc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80c1aa8-307f-4c84-9255-4e9d7cdc75db",
   "metadata": {},
   "outputs": [],
   "source": [
    "modulation_dict_diff_lst = []\n",
    "modulation_dict_lst = []\n",
    "hidden_output_dict_lst = []\n",
    "hidden_dict_lst = []\n",
    "hidden_all_dict_lst = []\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=stages_num,\n",
    "    specs=[[{'type': 'scatter3d'}] * stages_num],\n",
    "    subplot_titles=[f\"Stage {i+1}\" for i in range(stages_num)]  # Add titles for each subplot\n",
    ")\n",
    "\n",
    "camera = dict(\n",
    "    eye=dict(x=1.25, y=1.25, z=1.25),  # Position of the camera\n",
    "    up=dict(x=0, y=0, z=1),            # Orientation of the camera\n",
    "    center=dict(x=0, y=0, z=0),        # Focal point of the camera\n",
    ")\n",
    "\n",
    "for stage_iter in range(stages_num):\n",
    "    Woutput = Woutput_lst[stage_iter]\n",
    "    _, Ms_orig, hs, bs = modulation_extraction(db_lst[stage_iter])\n",
    "    \n",
    "    hs_stimulus = hs[:,stimulus_start-1:stimulus_end,:]\n",
    "    Ms_stimulus = Ms_orig[:,stimulus_start-1:stimulus_end,:,:]\n",
    "\n",
    "    modulation_diff_dict, modulation_dict, hidden_output_dict, hidden_dict, hidden_all_dict = {}, {}, {}, {}, {}\n",
    "    \n",
    "    for batch_iter in range(batch_nums):\n",
    "        hs_stimulus_batch = hs_stimulus[batch_iter,:,:]\n",
    "        hs_stimulus_batch_output = hs_stimulus_batch @ Woutput.T\n",
    "\n",
    "        trace = go.Scatter3d(\n",
    "            x=hs_stimulus_batch_output[:, 0],\n",
    "            y=hs_stimulus_batch_output[:, 1],\n",
    "            z=hs_stimulus_batch_output[:, 2],\n",
    "            mode='lines+markers', \n",
    "            line=dict(color=c_vals[labels[batch_iter][0]], width=2),  \n",
    "            marker=dict(size=5, symbol='circle'), \n",
    "            name=f\"Batch {batch_iter} - Stage {stage_iter}\",\n",
    "            showlegend=False \n",
    "        )\n",
    "        fig.add_trace(trace, row=1, col=stage_iter + 1)\n",
    "        \n",
    "        # modulation \n",
    "        Ms_stimulus_fixon = Ms_stimulus[batch_iter,-1,:,0] - Ms_stimulus[batch_iter,0,:,0]\n",
    "        Ms_stimulus_task = Ms_stimulus[batch_iter,-1,:,-1] - Ms_stimulus[batch_iter,0,:,-1]\n",
    "        \n",
    "        modulation_diff_dict[labels[batch_iter,0]] = Ms_stimulus_fixon # change of modulation on fixon during stimulus period\n",
    "        modulation_dict[labels[batch_iter,0]] =  Ms_stimulus[batch_iter,-1,:,0]\n",
    "        hidden_output_dict[labels[batch_iter,0]] = hs_stimulus_batch_output\n",
    "        hidden_dict[labels[batch_iter,0]] = hs_stimulus_batch[-1,:]\n",
    "        hidden_all_dict[labels[batch_iter,0]] = hs_stimulus_batch\n",
    "\n",
    "    modulation_dict_diff_lst.append(modulation_diff_dict)\n",
    "    modulation_dict_lst.append(modulation_dict)\n",
    "    hidden_output_dict_lst.append(hidden_output_dict)\n",
    "    hidden_dict_lst.append(hidden_dict)\n",
    "    hidden_all_dict_lst.append(hidden_all_dict)\n",
    "\n",
    "for stage_iter in range(stages_num):\n",
    "    fig.update_layout(\n",
    "        **{\n",
    "            f\"scene{stage_iter + 1}\": dict(\n",
    "                xaxis=dict(range=[-1.3, 1.3], title=\"X\"),\n",
    "                yaxis=dict(range=[-1.3, 1.3], title=\"Y\"),\n",
    "                zaxis=dict(range=[-1.3, 1.3], title=\"Z\"),\n",
    "                aspectmode='cube',  \n",
    "                camera=camera,       \n",
    "                domain=dict(\n",
    "                    x=[stage_iter / stages_num, (stage_iter + 1) / stages_num - 0.02],  \n",
    "                    y=[0, 1]  \n",
    "                )\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"3D Interactive Plot for Different Stages\",\n",
    "    height=600, \n",
    "    width=600 * stages_num, \n",
    "    margin=dict(l=10, r=10, t=50, b=10),  \n",
    ")\n",
    "\n",
    "output_file = \"./save/3d_interactive_plot_compact.html\"\n",
    "fig.write_html(output_file)\n",
    "\n",
    "print(f\"Plot saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb427a04-1279-4f63-867b-87dee94a7745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "modulation_change_stage, m_corr_stage, h_corr_stage = [], [], []\n",
    "fig_hc, axs_hc = plt.subplots(figsize=(6,2))\n",
    "\n",
    "def binarize(arr, threshold):\n",
    "    \"\"\"\"\"\"\n",
    "    return (np.abs(arr) > threshold).astype(int)\n",
    "\n",
    "def normalized_participation_ratio(cov_matrix):\n",
    "    \"\"\"\"\"\"\n",
    "    eigenvalues = np.linalg.eigvalsh(cov_matrix)  \n",
    "    sum_eigen = np.sum(eigenvalues)\n",
    "    sum_eigen_sq = np.sum(eigenvalues ** 2)    \n",
    "    N = len(eigenvalues)\n",
    "    npr = (sum_eigen ** 2) / (N * sum_eigen_sq)\n",
    "    return npr\n",
    "    \n",
    "for i in range(stages_num):\n",
    "    def analyze_hm_change(lst):\n",
    "        md = lst[i]\n",
    "        md_m = [np.array(value) for value in md.values()]\n",
    "        md_m = np.column_stack(md_m)\n",
    "        md_m = md_m.T # num_stimulus * hidden_size\n",
    "        mc_stage = list(np.sum(np.abs(md_m), axis=1))\n",
    "        # synptic_corr = np.corrcoef(md_m, rowvar=True)\n",
    "        synptic_corr = cosine_similarity(md_m)\n",
    "                \n",
    "        mean_corr = np.nanmean(np.triu(synptic_corr))\n",
    "        return mean_corr, mc_stage, md_m\n",
    "        \n",
    "    m_mean_corr, _, md_m = analyze_hm_change(modulation_dict_lst)\n",
    "    m_diff_mean_corr, mc_stage, md_m_diff = analyze_hm_change(modulation_dict_diff_lst)\n",
    "    h_mean_corr, _, md_h = analyze_hm_change(hidden_dict_lst)\n",
    "        \n",
    "    m_corr_stage.append(m_mean_corr)\n",
    "    h_corr_stage.append(h_mean_corr)\n",
    "    modulation_change_stage.append(mc_stage)\n",
    "\n",
    "    if i == stages_num - 1:\n",
    "        sns.heatmap(md_m_diff, ax=axs_hc, cmap=\"coolwarm\")\n",
    "        fig_hc.savefig(\"./paper/modulation_heatmap.png\")\n",
    "\n",
    "modulation_change_stage = np.array(modulation_change_stage)\n",
    "\n",
    "figmc,axsmc = plt.subplots(1, 3, figsize=(4*3,2))\n",
    "# for i in range(modulation_change_stage.shape[1]):\n",
    "#     axsmc[0].plot(counter_lst, modulation_change_stage[:,i], \"-o\", c=c_vals[i])\n",
    "axsmc[0].plot(counter_lst, np.mean(modulation_change_stage, axis=1), \"-o\", c=c_vals[0])\n",
    "axsmc[0].fill_between(counter_lst, np.mean(modulation_change_stage, axis=1) - np.std(modulation_change_stage, axis=1), \\\n",
    "                                   np.mean(modulation_change_stage, axis=1) + np.std(modulation_change_stage, axis=1), color=c_vals_l[0])\n",
    "axsmc[0].set_ylabel(\"Change of Modulation\")\n",
    "# normalized? \n",
    "axsmc[1].plot(counter_lst, m_corr_stage/m_corr_stage[0], \"-o\")\n",
    "axsmc[1].set_ylabel(\"Average Synaptic Correlation between Stimulus\", fontsize=10)\n",
    "axsmc[2].plot(counter_lst, h_corr_stage/h_corr_stage[0], \"-o\")\n",
    "axsmc[2].set_ylabel(\"Postsynaptic Activity Correlation between Stimulus\", fontsize=10)\n",
    "\n",
    "import os \n",
    "\n",
    "def save_dict_with_count_npz(directory, data_dict, it=\"\"):\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    # Count existing files in the directory\n",
    "    file_count = len([f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))])\n",
    "\n",
    "    # Generate filename based on file count\n",
    "    filename = os.path.join(directory, f\"{it}{file_count}.npz\")\n",
    "\n",
    "    # Save dictionary as an NPZ file\n",
    "    np.savez(filename, **data_dict)\n",
    "\n",
    "    print(f\"Dictionary saved as: {filename}\")\n",
    "\n",
    "\n",
    "data_json = {\"counter_lst\": counter_lst, \"m_corr_stage\": m_corr_stage/m_corr_stage[0], \"h_corr_stage\": h_corr_stage/h_corr_stage[0]}\n",
    "save_dict_with_count_npz(\"./paper_data\", data_json, it=\"corr\")\n",
    "\n",
    "for ax in axsmc:\n",
    "    ax.set_xlabel(\"# Dataset\")\n",
    "    ax.set_xscale(\"log\")\n",
    "    \n",
    "figmc.tight_layout()\n",
    "figmc.savefig(\"./paper/modulation_change.png\")\n",
    "\n",
    "def traj_length(array):\n",
    "    \"\"\"\"\"\"\n",
    "    diffs = np.diff(array, axis=0)  \n",
    "    return np.sum(np.linalg.norm(diffs, axis=1))\n",
    "\n",
    "hidden_length_all = []\n",
    "for stage_iter in range(stages_num):\n",
    "    hidden_stage = hidden_output_dict_lst[stage_iter] \n",
    "    hidden_stage = {k: hidden_stage[k] for k in sorted(hidden_stage.keys())}\n",
    "    hidden_length = [traj_length(arr) for arr in hidden_stage.values()]    \n",
    "    hidden_length_all.append(hidden_length)\n",
    "    \n",
    "hidden_length_all = np.array(hidden_length_all)\n",
    "figt, axst = plt.subplots(figsize=(3,3))\n",
    "for i in range(hidden_length_all.shape[1]):\n",
    "    axst.plot(counter_lst, hidden_length_all[:,i], \"-o\", c=c_vals[i])\n",
    "axst.set_xlabel(\"# Dataset\")\n",
    "axst.set_xscale(\"log\")\n",
    "axst.set_ylabel(\"Length of Hidden State Trajectory\")\n",
    "figt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904b7833-7825-487f-9a02-f5938cbecbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def load_all_npz_files(directory):\n",
    "    npz_files = sorted(glob.glob(os.path.join(directory, \"*.npz\")))\n",
    "\n",
    "    data_list = []\n",
    "    for file in npz_files:\n",
    "        data = np.load(file)  \n",
    "        data_list.append({key: data[key] for key in data.files})  \n",
    "\n",
    "        print(f\"Loaded: {file}\") \n",
    "\n",
    "    return data_list\n",
    "\n",
    "plotall = True \n",
    "if plotall:\n",
    "    directory = \"./paper_data\"\n",
    "    all_data = load_all_npz_files(directory)\n",
    "    counter_lst_all, m_corr_all, h_corr_all = [], [], []\n",
    "    for i, data in enumerate(all_data):\n",
    "        counter_lst_all.append(data[\"counter_lst\"])\n",
    "        m_corr_all.append(data[\"m_corr_stage\"])\n",
    "        h_corr_all.append(data[\"h_corr_stage\"])\n",
    "\n",
    "    counter_lst_all = np.array(counter_lst_all)\n",
    "    m_corr_all = np.array(m_corr_all)\n",
    "    h_corr_all = np.array(h_corr_all)\n",
    "\n",
    "    mean_counter = np.mean(counter_lst_all, axis=0)\n",
    "    mean_m_corr = np.mean(m_corr_all, axis=0)\n",
    "    std_m_corr = np.std(m_corr_all, axis=0)\n",
    "    \n",
    "    mean_h_corr = np.mean(h_corr_all, axis=0)\n",
    "    std_h_corr = np.std(h_corr_all, axis=0)\n",
    "    \n",
    "    figmcall, axsmcall = plt.subplots(1,2,figsize=(4*2,2))\n",
    "    axsmcall[0].plot(mean_counter, mean_m_corr, \"-o\", label=\"Mean m_corr\", color=c_vals[0])\n",
    "    axsmcall[0].fill_between(mean_counter, mean_m_corr - std_m_corr, mean_m_corr + std_m_corr, color=c_vals_l[0], alpha=0.2)\n",
    "    axsmcall[0].set_ylabel(\"Cos of Modulation\", fontsize=10)\n",
    "    axsmcall[1].plot(mean_counter, mean_h_corr, \"-o\", label=\"Mean h_corr\", color=c_vals[0])\n",
    "    axsmcall[1].fill_between(mean_counter, mean_h_corr - std_h_corr, mean_h_corr + std_h_corr, color=c_vals_l[0], alpha=0.2)\n",
    "    axsmcall[1].set_ylabel(\"Cos of Hidden Activity\", fontsize=10)\n",
    "    for ax in axsmcall:\n",
    "        ax.set_xlabel(\"# Dataset\")\n",
    "        ax.set_xscale(\"log\")\n",
    "    figmcall.tight_layout()\n",
    "    figmcall.savefig(\"./paper/modulation_analysis_during_learning.png\")\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ffac89-2d1b-460f-a42d-a782c3940f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixon_task_projoutput = []\n",
    "for stage_iter in range(stages_num):\n",
    "    W = Wall_lst[stage_iter][0]\n",
    "    W_output = Woutput_lst[stage_iter]\n",
    "\n",
    "    _, Ms_orig, _, bs = modulation_extraction(db_lst[stage_iter]) # batch * seq_len * hidden_neuron * input_neuron\n",
    "    bias = np.mean(bs, axis=0)\n",
    "\n",
    "    W_output_random_lst = [generate_random_orthonormal_matrix(W_output.shape[1]) for _ in range(10)]\n",
    "    W_fixon = W[:,0].reshape(-1,1)\n",
    "    W_task = W[:,5].reshape(-1,1)\n",
    "    \n",
    "    fixon_output, task_output = W_output[1:,:] @ W_fixon, W_output[1:,:] @ W_task\n",
    "    bias_output = np.mean(bias @ (W_output[1:,:].T), axis=0)\n",
    "    fixon_proj_output_norm1 = fixon_output[0] + bias_output[0]\n",
    "    task_proj_output_norm1 = task_output[0] \n",
    "    fixon_proj_output_norm2 = fixon_output[1] + bias_output[1]\n",
    "    task_proj_output_norm2 = task_output[1] \n",
    "\n",
    "    fixon_output_random, task_output_random = [(W_output_random.T @ W_fixon) for W_output_random in W_output_random_lst], [(W_output_random.T @ W_task) for W_output_random in W_output_random_lst]\n",
    "    fixon_proj_output_norm_random = np.mean([np.sum(fixon_output_r) for fixon_output_r in fixon_output_random])\n",
    "    task_proj_output_norm_random = np.mean([np.sum(task_output_r) for task_output_r in task_output_random])\n",
    "    \n",
    "    fixon_task_projoutput.append([fixon_proj_output_norm1, task_proj_output_norm1, fixon_proj_output_norm2, task_proj_output_norm2, fixon_proj_output_norm_random, task_proj_output_norm_random])\n",
    "\n",
    "fixon_task_projoutput = np.array(fixon_task_projoutput)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,2))\n",
    "ax.plot(counter_lst, fixon_task_projoutput[:, 0], \"-o\", color=c_vals[0], linestyle=l_vals[0], label=\"fixon proj output1\")\n",
    "ax.plot(counter_lst, fixon_task_projoutput[:, 1], \"-o\", color=c_vals[0], linestyle=l_vals[1], label=\"task proj output1\")\n",
    "ax.plot(counter_lst, fixon_task_projoutput[:, 0]+fixon_task_projoutput[:, 1], \"-o\", color=c_vals[1], linestyle=l_vals[2], linewidth=1, label=\"fixon/task sum output1\")\n",
    "ax.axhline(0, color=c_vals[1], linestyle=l_vals[2])\n",
    "\n",
    "# ax.plot(counter_lst, fixon_task_projoutput[:, 2], \"-o\", color=c_vals[1], linestyle=l_vals[0], label=\"fixon proj output2\")\n",
    "# ax.plot(counter_lst, fixon_task_projoutput[:, 3], \"-o\", color=c_vals[1], linestyle=l_vals[1], label=\"task proj output2\")\n",
    "# ax.plot(counter_lst, fixon_task_projoutput[:, 2]+fixon_task_projoutput[:, 3], \"-o\", color=c_vals[1], linestyle=l_vals[2], linewidth=3, label=\"fixon/task sum output2\")\n",
    "# ax.plot(counter_lst, fixon_task_projoutput[:,2], \"-o\", c=c_vals_l[0], label=\"fixon proj output random\")\n",
    "# ax.plot(counter_lst, fixon_task_projoutput[:,3], \"-o\",c=c_vals_l[1], label=\"task proj output random\")\n",
    "ax.legend()    \n",
    "ax.set_xlabel(\"# Dataset\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_ylabel(\"Projection\")\n",
    "fig.savefig(\"./paper/w_to_output.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb3d781-4921-407a-a0e6-570f200cd040",
   "metadata": {},
   "outputs": [],
   "source": [
    "fighs, axshs = plt.subplots(3,2,figsize=(3*2,3*3))\n",
    "pca_er_stage = []\n",
    "\n",
    "start_stage = stages_num-2\n",
    "for stage_iter in range(start_stage, stages_num):\n",
    "    PCA_downsample = 3\n",
    "    _, Ms_orig, hs, bs = modulation_extraction(db_lst[stage_iter])\n",
    "\n",
    "    prlst = [normalized_participation_ratio(np.cov(hs[i,:,:], rowvar=False)) for i in range(hs.shape[0])]\n",
    "\n",
    "    pca = PCA(n_components = PCA_downsample)\n",
    "    # # use hs \n",
    "    # hs_flattened = hs.reshape(hs.shape[0]*hs.shape[1], hs.shape[2])\n",
    "    # hs_pca = pca.fit_transform(hs_flattened)\n",
    "    # hs_reconstructed = hs_pca.reshape(hs.shape[0], hs.shape[1], PCA_downsample)\n",
    "    # lowd_data = hs_reconstructed\n",
    "    # # use Ms\n",
    "    # Ms_flattened = Ms_orig.reshape(Ms_orig.shape[0] * Ms_orig.shape[1] * Ms_orig.shape[3], Ms_orig.shape[2])\n",
    "    # ms_pca = pca.fit_transform(Ms_flattened)\n",
    "    # Ms_reconstructed = ms_pca.reshape(Ms_orig.shape[0], Ms_orig.shape[1], Ms_orig.shape[3], PCA_downsample)\n",
    "    # lowd_data_lst = [Ms_reconstructed[:,:,0,:]] # for instance, modulation of fixon\n",
    "    Ms_end_of_stimulus = Ms_orig[:,stimulus_end:stimulus_end+1,:,:]\n",
    "    Ms_end_of_stimulus_flattened = Ms_end_of_stimulus.reshape(Ms_end_of_stimulus.shape[0] * Ms_end_of_stimulus.shape[1] * Ms_end_of_stimulus.shape[3], Ms_end_of_stimulus.shape[2])\n",
    "    pca.fit(Ms_end_of_stimulus_flattened)\n",
    "    Ms_flattened = Ms_orig.reshape(Ms_orig.shape[0] * Ms_orig.shape[1] * Ms_orig.shape[3], Ms_orig.shape[2])\n",
    "    projected_data = pca.transform(Ms_flattened)\n",
    "    Ms_reconstructed = projected_data.reshape(Ms_orig.shape[0], Ms_orig.shape[1], Ms_orig.shape[3], PCA_downsample)\n",
    "\n",
    "    lowd_data_lst = [Ms_reconstructed[:,:,0,:]] # for instance, modulation of fixon\n",
    "\n",
    "    # pca_er_stage.append(pca.explained_variance_ratio_)\n",
    "    \n",
    "    for i in range(hs.shape[0]):\n",
    "        for dd in range(len(lowd_data_lst)):\n",
    "            lowd_data = lowd_data_lst[dd]\n",
    "            data_batch = lowd_data[i,:,:]\n",
    "\n",
    "            axshs[0,stage_iter-start_stage].scatter(data_batch[stimulus_start:stimulus_end,0], data_batch[stimulus_start:stimulus_end,1], marker=markers_vals[dd], c=c_vals[labels[i,0]], alpha=alphas)\n",
    "            axshs[0,stage_iter-start_stage].set_xlabel(\"PC 1\")\n",
    "            axshs[0,stage_iter-start_stage].set_ylabel(\"PC 2\")\n",
    "            axshs[1,stage_iter-start_stage].scatter(data_batch[stimulus_start:stimulus_end,0], data_batch[stimulus_start:stimulus_end,2], marker=markers_vals[dd], c=c_vals[labels[i,0]], alpha=alphas)\n",
    "            axshs[2,stage_iter-start_stage].scatter(data_batch[stimulus_start:stimulus_end,1], data_batch[stimulus_start:stimulus_end,2], marker=markers_vals[dd], c=c_vals[labels[i,0]], alpha=alphas)\n",
    "\n",
    "fighs.tight_layout()\n",
    "fighs.savefig(\"./paper/m_pca.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b13ffb-b00b-40c0-87a3-8c7e19d096c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_select = 10\n",
    "\n",
    "# overall_corr_change, over_stimulus_info_reserved = [], []\n",
    "\n",
    "# useseed = None\n",
    "\n",
    "# for batch_iter in range(batch_nums):\n",
    "#     if batch_iter % 20 == 0:\n",
    "#         print(f\"batch_iter: {batch_iter}\")\n",
    "        \n",
    "#     task_selection = task_labels_across_batch[batch_iter]\n",
    "    \n",
    "#     stimulus_change_neuron_overtime = []\n",
    "#     stimulus_change_cts_neuron_overtime = []\n",
    "#     delay_change_neuron_overtime = []\n",
    "#     stimulus_information_reserved_overtime = []\n",
    "\n",
    "#     if batch_iter == batch_select:\n",
    "#         fig, axs = plt.subplots(stages_num,6,figsize=(8*2,2*stages_num))\n",
    "#         for ax in axs.flatten():\n",
    "#             for breaktime in break_info:\n",
    "#                 ax.axvline(breaktime, linestyle=\"--\")\n",
    "\n",
    "#     neuron_respond_stimulus_change, neuron_respond_delay_change = [], []\n",
    "#     for stage_iter in range(stages_num):\n",
    "#         W_ = Wall_lst[stage_iter][0]\n",
    "#         _, Ms_orig, _, bs = modulation_extraction(db_lst[stage_iter]) # batch * seq_len * hidden_neuron * input_neuron\n",
    "#         middle = W_[np.newaxis, np.newaxis, :, :] * (Ms_orig + 1)\n",
    "\n",
    "#         useful_info = Ms_orig\n",
    "    \n",
    "#         # how modulation signal is changed during the stimulus period (last timestep minus first timestep)\n",
    "#         stimulus_change_neuron = np.abs(useful_info[batch_iter, stimulus_end-1, :, :] - useful_info[batch_iter, stimulus_start, :, :]) # discrete case (end-before)\n",
    "#         stimulus_change_neuron_cts = np.abs(useful_info[batch_iter, stimulus_start:stimulus_end, :, :] - useful_info[batch_iter, stimulus_start, :, :]) # continuous case (whole trace)\n",
    "#         delay_change_neuron = np.abs(useful_info[batch_iter, response_start-1, :, :] - useful_info[batch_iter, stimulus_end, :, :])\n",
    "\n",
    "#         # which neuron has significant change of modulation signal\n",
    "#         neuron_to_stimulus = (np.abs(stimulus_change_neuron[:, 0]) > 1e-2).astype(int) # N*1 array\n",
    "#         neuron_respond_stimulus_change.append(neuron_to_stimulus)\n",
    "#         neuron_to_delay = (np.abs(delay_change_neuron[:, 0]) > 1e-2).astype(int) # N*1 array\n",
    "#         neuron_respond_delay_change.append(neuron_to_delay)\n",
    "\n",
    "#         unmatched_neuron = sum(1 for l1, l2 in zip(neuron_to_stimulus, neuron_to_delay) if l2 == 1 and l1 != 1)\n",
    "        \n",
    "#         # to match up dimension only\n",
    "#         assert task_params['fixate_off'] is False\n",
    "#         # fixon and task (even in multiple, but matched one) will have modulation change oppositely\n",
    "#         # assert np.sum(np.abs(stimulus_change_neuron[:,0] - stimulus_change_neuron[:,5+task_selection])) < 1e-5 \n",
    "        \n",
    "#         stimulus_change_neuron_overtime.append(stimulus_change_neuron)\n",
    "#         stimulus_change_cts_neuron_overtime.append(stimulus_change_neuron_cts)\n",
    "#         delay_change_neuron_overtime.append(delay_change_neuron)\n",
    "        \n",
    "#         info_reserved = np.abs(useful_info[batch_iter,break_info[2]-1,:,5+task_selection] - useful_info[batch_iter,break_info[0],:,5+task_selection])\n",
    "#         stimulus_information_reserved_overtime.append(np.mean(info_reserved))\n",
    "\n",
    "#         # plot_neuron = [1]\n",
    "#         plot_neuron = [i for i in range(neuron_nums)]\n",
    "        \n",
    "#         if batch_iter == batch_select:\n",
    "#             for neuron, color in enumerate(colors[:useful_info.shape[2]]):\n",
    "#                 if neuron_to_stimulus[neuron] and neuron in plot_neuron:\n",
    "#                     for j in range(6):\n",
    "#                         axs[stage_iter,j].plot(useful_info[batch_iter, :, neuron, j], alpha=0.2, c=color)\n",
    "#                         axs[stage_iter,j].set_title(f\"Input {j}\")\n",
    "\n",
    "#     neuron_respond_stimulus_change = np.array(neuron_respond_stimulus_change)\n",
    "#     not_all_zero_neuron_stimulus = np.where(np.any(neuron_respond_stimulus_change != 0, axis=0))[0]\n",
    "#     neuron_respond_delay_change = np.array(neuron_respond_delay_change)\n",
    "#     not_all_zero_neuron_delay = np.where(np.any(neuron_respond_delay_change != 0, axis=0))[0]\n",
    "\n",
    "#     if batch_iter == batch_select:\n",
    "#         ffs, ggs = plt.subplots(1,2,figsize=(10*2,4))\n",
    "#         sns.heatmap(neuron_respond_stimulus_change, ax=ggs[0])\n",
    "#         ggs[0].set_title(\"Stimulus Period\")\n",
    "#         sns.heatmap(neuron_respond_delay_change, ax=ggs[1])\n",
    "#         ggs[1].set_title(\"Delay Period\")\n",
    "#         for gg in ggs:\n",
    "#             gg.set_xlabel(\"Neuron\")\n",
    "#             gg.set_ylabel(\"Training Stage\")\n",
    "#         ffs.savefig(\"./results/modulation_stagechange.png\")\n",
    "        \n",
    "#     stimulus_change_neuron_overtime = np.array(stimulus_change_neuron_overtime)\n",
    "#     delay_change_neuron_overtime = np.array(delay_change_neuron_overtime)\n",
    "#     stimulus_change_cts_neuron_overtime = np.array(stimulus_change_cts_neuron_overtime)\n",
    "#     # print(stimulus_change_neuron_overtime.shape)\n",
    "#     # print(stimulus_change_cts_neuron_overtime.shape)\n",
    "    \n",
    "#     over_stimulus_info_reserved.append(stimulus_information_reserved_overtime)\n",
    "    \n",
    "#     if batch_iter == batch_select:\n",
    "#         fig.suptitle(f\"Illustration with batch {batch_select}\")\n",
    "#         fig.tight_layout()\n",
    "#         fig.savefig(\"./results/illustration.png\")\n",
    "    \n",
    "#         fig, ax = plt.subplots(figsize=(4,4))\n",
    "#         for neuron in range(stimulus_change_neuron_overtime.shape[1]):\n",
    "#             if neuron in not_all_zero_neuron_stimulus:\n",
    "#                 ax.plot(stimulus_change_neuron_overtime[:,neuron,0], alpha=0.2, c=colors[neuron])\n",
    "#         ax.plot(np.mean(stimulus_change_neuron_overtime[:,not_all_zero_neuron_stimulus,0], axis=1), alpha=0.2, linestyle=\"--\",linewidth=3)\n",
    "#         ax.set_xlabel(\"Training Stage\")\n",
    "#         ax.set_ylabel(\"Change of Modulation, Neuron-wise\")\n",
    "#         fig.savefig(\"./results/change_modulation.png\")\n",
    "\n",
    "\n",
    "#     stage_corr_change = []\n",
    "#     for rec in range(stages_num):\n",
    "#         modulation_change_stim_corr = np.zeros(shape=(input_nums, input_nums))\n",
    "#         modulation_change_delay_corr = np.zeros(shape=(input_nums, input_nums))\n",
    "#         for ind1 in range(input_nums):\n",
    "#             for ind2 in range(input_nums):\n",
    "#                 input1 = stimulus_change_neuron_overtime[rec,:,ind1].flatten()\n",
    "#                 input2 = stimulus_change_neuron_overtime[rec,:,ind2].flatten()\n",
    "#                 modulation_change_stim_corr[ind1, ind2] = pearsonr(input1, input2)[0]\n",
    "                \n",
    "#                 # responsive_neurons = neuron_respond_stimulus_change[rec]\n",
    "#                 # responsive_neurons_index = np.where(responsive_neurons == 1)[0]\n",
    "#                 # input1 = stimulus_change_cts_neuron_overtime[rec, :, :, ind1]\n",
    "#                 # input2 = stimulus_change_cts_neuron_overtime[rec, :, :, ind2]\n",
    "#                 # input1 = input1[:, responsive_neurons_index]\n",
    "#                 # input2 = input2[:, responsive_neurons_index]\n",
    "                \n",
    "#                 # corr_matrix = np.corrcoef(input1, input2, rowvar=False)\n",
    "#                 # num_neurons = input1.shape[1]\n",
    "#                 # simlst2 = corr_matrix[:num_neurons, num_neurons:].diagonal()\n",
    "#                 # sim = np.mean(simlst2)  \n",
    "\n",
    "#                 input1 = delay_change_neuron_overtime[rec,:,ind1].flatten()\n",
    "#                 input2 = delay_change_neuron_overtime[rec,:,ind2].flatten()\n",
    "#                 modulation_change_delay_corr[ind1, ind2] = pearsonr(input1, input2)[0]\n",
    "\n",
    "#         # how fixon interact with stimulus information\n",
    "#         # notice that since the validation set has randomized fix seed\n",
    "#         # some stimulus input may have all zero in specific run\n",
    "#         mean_corr_stim = np.nanmean([modulation_change_stim_corr[0, stimulus_cc] for stimulus_cc in range(1,5)])\n",
    "#         mean_corr_delay = np.nanmean([modulation_change_delay_corr[0, stimulus_cc] for stimulus_cc in range(1,5)])\n",
    "#         mean_corr_task2stim = np.nanmean([modulation_change_stim_corr[5+task_selection, stimulus_cc] for stimulus_cc in range(1,5)])\n",
    "#         mean_corr_fixondelay = modulation_change_stim_corr[0, 5+task_selection]\n",
    "        \n",
    "#         stage_corr_change.append([mean_corr_stim, mean_corr_delay, mean_corr_task2stim, mean_corr_fixondelay])\n",
    "\n",
    "#     overall_corr_change.append(stage_corr_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e19c42f-2c46-4fb7-89a4-ee9aa5ea0ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stage_range = [i for i in range(stages_num)]\n",
    "\n",
    "# overall_corr_change = np.array(overall_corr_change)\n",
    "# figcorr, axcorr = plt.subplots(1,4,figsize=(4*4,4))\n",
    "# useylabels = [\"Fixon to Stimulus during Stimulus Period\", \"Fixon to Stimulus during Delay Period\", \\\n",
    "#               \"Task to Stimulus\", \"Fixon to Task\"]\n",
    "# for i in range(4):\n",
    "#     mean_corr, std_corr = np.mean(overall_corr_change[:,:,i], axis=0), np.std(overall_corr_change[:,:,i], axis=0)\n",
    "#     axcorr[i].plot(stage_range, mean_corr, \"-o\", color=c_vals[0])\n",
    "#     axcorr[i].fill_between(stage_range, mean_corr-std_corr, mean_corr+std_corr, color=c_vals_l[0], alpha=0.2)\n",
    "#     axcorr[i].set_xlabel(\"Training Stage\")\n",
    "#     axcorr[i].set_ylabel(useylabels[i])\n",
    "# figcorr.tight_layout()\n",
    "# figcorr.savefig(\"./results/fixon_stimulus.png\")\n",
    "\n",
    "# over_stimulus_info_reserved = np.array(over_stimulus_info_reserved)\n",
    "# figres, axres = plt.subplots(figsize=(4,4))\n",
    "# mean_res, std_res = np.mean(over_stimulus_info_reserved, axis=0), np.std(over_stimulus_info_reserved, axis=0)\n",
    "# axres.plot(stage_range, mean_res, \"-o\", color=c_vals[0])\n",
    "# axres.fill_between(stage_range, mean_res-std_res, mean_res+std_res, color=c_vals_l[0], alpha=0.2)\n",
    "# axres.set_xlabel(\"Training Stage\")\n",
    "# axres.set_ylabel(\"Modulation information of stimulus preserved after delay\", fontsize=12)\n",
    "# figres.savefig(\"./results/norm_preserve.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec30e08f-8431-4054-abb3-ddd2c28aedc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for session_iter in range(0,len(session_breakdown)):\n",
    "#     session_part = session_breakdown[session_iter]\n",
    "\n",
    "#     for pca_target in pca_target_lst:\n",
    "#         print(f\"session_part: {session_part}; pca_target: {pca_target}\")\n",
    "\n",
    "#         if net.hidden_cell_types is not None:\n",
    "#             cell_types = net.hidden_cell_types.detach().cpu().numpy()[0] # Remove batch idx\n",
    "#         cell_type_names = ('Inh.', 'Exc.')\n",
    "#         pca_sort_type = 'ratios' # How to sort cell type PCA; vars, ratios\n",
    "\n",
    "#         if pca_target in ('hs',):\n",
    "#             n_activity = hs.shape[-1]\n",
    "#             # Truncate into session specifically\n",
    "#             hs_cut = hs[:,session_part[0]:session_part[1],:]\n",
    "#             as_shape = hs_cut.shape \n",
    "#             as_flat = hs_cut.reshape((-1, n_activity,))\n",
    "#         elif pca_target in ('Ms',):\n",
    "#             n_activity = Ms.shape[-1]\n",
    "#             # Truncate into session specifically\n",
    "#             Ms_cut = Ms[:,session_part[0]:session_part[1],:]\n",
    "#             # as_shape = (Ms.shape[0], Ms.shape[1], n_activity) \n",
    "#             as_shape = Ms_cut.shape\n",
    "#             as_flat = Ms_cut.reshape((-1, n_activity))\n",
    "\n",
    "#         print(f\"as_shape: {as_shape}\")\n",
    "#         as_shape_saver = as_shape\n",
    "\n",
    "#         total_vars = np.var(as_flat, axis=0) # Var per activity dimension\n",
    "#         activity_zero = np.zeros((1, n_activity,))\n",
    "\n",
    "#         if pca_type in ('full',):\n",
    "#             activity_pca = PCA()\n",
    "\n",
    "#             activity_pca.fit(as_flat)\n",
    "            \n",
    "#             # Corrects signs of components so that their mean/average is positive\n",
    "#             activity_pca.components_ = np.sign(np.sum(activity_pca.components_, axis=-1))[:, np.newaxis] * activity_pca.components_\n",
    "            \n",
    "#             as_pca = activity_pca.transform(as_flat)\n",
    "#             # as_pca = as_pca.reshape(as_shape) # Separates back into batches and sequences\n",
    "#             as_pca = as_pca.reshape(as_shape_saver[0],as_shape_saver[1],-1)\n",
    "\n",
    "#             zeros_pca = activity_pca.transform(activity_zero)\n",
    "#             if pca_target in ('hs',): # Some transformations only make sense for certain activities\n",
    "#                 W_output_pca = activity_pca.transform(W_output)\n",
    "\n",
    "#             # ZIHAN: only do for all period trial (last)\n",
    "#             #\n",
    "#             # Should we use PCA result or the original result?\n",
    "#             #\n",
    "#             if session_iter == len(session_breakdown)-1 and pca_target in ('hs', ):\n",
    "#                 output_num = W_output.shape[0]\n",
    "#                 assert output_num == 3 # low_dim\n",
    "#                 figtest, axstest = plt.subplots(2,1,figsize=(3*1,3*2))\n",
    "#                 # ? Original or PCA\n",
    "#                 fixation_pca = W_output[0,:].reshape(1,-1)\n",
    "#                 # fixation_pca = fixation_pca / np.linalg.norm(fixation_pca, axis=1, keepdims=True)\n",
    "#                 stimulus_pca = W_output[1:,:]\n",
    "#                 # stimulus_pca = stimulus_pca / np.linalg.norm(stimulus_pca, axis=1, keepdims=True)\n",
    "#                 batch_record_fixate, batch_record_stimulus = [], []\n",
    "                \n",
    "#                 for batchiter in range(hs_cut.shape[0]):\n",
    "#                     # ? Original or PCA\n",
    "#                     batch_activity_pca = hs_cut[batchiter,:,:]\n",
    "                    \n",
    "#                     batch_record1, batch_record2 = [], []\n",
    "#                     for timestamp in range(batch_activity_pca.shape[0]):\n",
    "#                         batch_time_activity_pca = batch_activity_pca[timestamp,:].reshape(1,-1)\n",
    "#                         #\n",
    "#                         # batch_time_activity_pca = batch_time_activity_pca / np.linalg.norm(batch_time_activity_pca, axis=1, keepdims=True)\n",
    "\n",
    "#                         projection_magnitude_fix = helper.magnitude_of_projection(batch_time_activity_pca, fixation_pca)\n",
    "#                         batch_record1.append(projection_magnitude_fix)\n",
    "#                         projection_magnitude_stimulus = helper.magnitude_of_projection(batch_time_activity_pca, stimulus_pca)\n",
    "#                         batch_record2.append(projection_magnitude_stimulus)\n",
    "#                         # print(f\"projection_magnitude_fix: {projection_magnitude_fix}; projection_magnitude_stimulus: {projection_magnitude_stimulus}\")\n",
    "#                         # time.sleep(10000)\n",
    "#                     batch_record_fixate.append(batch_record1)\n",
    "#                     batch_record_stimulus.append(batch_record2)\n",
    "\n",
    "#                 batch_record_fixate, batch_record_stimulus = np.array(batch_record_fixate), np.array(batch_record_stimulus)\n",
    "#                 mean_fix, std_fix = np.mean(batch_record_fixate, axis=0), np.std(batch_record_fixate, axis=0)\n",
    "#                 mean_stimulus, std_stimulus = np.mean(batch_record_stimulus, axis=0), np.std(batch_record_stimulus, axis=0)\n",
    "#                 xxx = [i for i in range(mean_fix.shape[0])]\n",
    "#                 axstest[0].plot(xxx, mean_fix)\n",
    "#                 axstest[0].fill_between(xxx, mean_fix-std_fix, mean_fix+std_fix, alpha=0.7, color=\"red\")\n",
    "#                 axstest[0].set_title(\"Projection Magnitude on 1D Subspace of Fixation Period\")\n",
    "#                 axstest[1].plot(xxx, mean_stimulus)\n",
    "#                 axstest[1].fill_between(xxx, mean_stimulus-std_stimulus, mean_stimulus+std_stimulus, alpha=0.7, color=\"red\")\n",
    "#                 axstest[1].set_title(\"Projection Magnitude on 2D Subspace of Fixation Period\")\n",
    "#                 for ax in axstest:\n",
    "#                     for spp in session_breakdown[:-1]:\n",
    "#                         ax.axvline(spp[1], linestyle=\"--\")\n",
    "#                 figtest.tight_layout()\n",
    "#                 figtest.show()\n",
    "#                 # figtest.savefig(f\"./results/zz_test_{pca_target}_{hyp_dict['ruleset']}_{hyp_dict['chosen_network']}_{hyp_dict['addon_name']}_test.png\")\n",
    "            \n",
    "#             print('PR: {:.2f}'.format(\n",
    "#                 helper.participation_ratio_vector(activity_pca.explained_variance_ratio_)\n",
    "#             ))\n",
    "#             print('PCA component PRs - PC1: {:.1f}, PC2: {:.1f}, PC3: {:.1f}'.format(\n",
    "#                     helper.participation_ratio_vector(np.abs(activity_pca.components_[0, :])),\n",
    "#                     helper.participation_ratio_vector(np.abs(activity_pca.components_[1, :])),\n",
    "#                     helper.participation_ratio_vector(np.abs(activity_pca.components_[2, :])),\n",
    "#                 ))\n",
    "\n",
    "#         elif pca_type in ('cell_types',):\n",
    "#             raise NotImplementedError('Need to correct this for Ms activity')\n",
    "#             cell_type_vals = np.unique(cell_types) # Gets unique cell type idxs\n",
    "\n",
    "#             n_cell_types = cell_type_vals.shape[0]\n",
    "#             pcas = [PCA() for _ in range(n_cell_types)]\n",
    "\n",
    "#             cell_types_pca = [] # This needs to be diferent from cell_types because may do cell types in a different order\n",
    "#             hs_pca = []\n",
    "#             explained_vars = []\n",
    "#             explained_var_ratios = []\n",
    "\n",
    "#             zeros_pca = []\n",
    "#             W_output_pca = []\n",
    "#             # Fit each PCA individually\n",
    "#             for cell_type_idx, (cell_type_val, cell_type_name) in enumerate(zip(\n",
    "#                 cell_type_vals, cell_type_names\n",
    "#             )):\n",
    "#                 print('Cell type: {}'.format(cell_type_name))\n",
    "#                 cell_type_filter = (cell_types == cell_type_val)\n",
    "#                 n_cells_type = np.sum(cell_type_filter.astype(np.int32))\n",
    "\n",
    "#                 print(' Ratio of population: {:.2f}, variance: {:.2f}'.format(\n",
    "#                     n_cells_type / n_cells,\n",
    "#                     np.sum(total_vars[cell_type_filter]) / np.sum(total_vars)\n",
    "#                 ))\n",
    "\n",
    "#                 pcas[cell_type_idx].fit(hs_flat[:, cell_type_filter])\n",
    "#                 # Corrects signs of components so that their mean/average is positive\n",
    "#                 pcas[cell_type_idx].components_ = np.sign(np.sum(pcas[cell_type_idx].components_, axis=-1))[:, np.newaxis] * pcas[cell_type_idx].components_\n",
    "\n",
    "#                 hs_pca_type = pcas[cell_type_idx].transform(hs_flat[:, cell_type_filter])\n",
    "\n",
    "#                 print(' PR: {:.2f}'.format(\n",
    "#                     helper.participation_ratio_vector(pcas[cell_type_idx].explained_variance_ratio_)\n",
    "#                 ))\n",
    "#                 print(' PCA component PRs - PC1: {:.1f}, PC2: {:.1f}, PC3: {:.1f}'.format(\n",
    "#                     helper.participation_ratio_vector(np.abs(pcas[cell_type_idx].components_[0, :])),\n",
    "#                     helper.participation_ratio_vector(np.abs(pcas[cell_type_idx].components_[1, :])),\n",
    "#                     helepr.participation_ratio_vector(np.abs(pcas[cell_type_idx].components_[2, :])),\n",
    "#                 ))\n",
    "\n",
    "#                 cell_types_pca.append([cell_type_val for _ in range(n_cells_type)])\n",
    "#                 hs_pca.append(hs_pca_type)\n",
    "#                 explained_vars.append(pcas[cell_type_idx].explained_variance_)\n",
    "#                 explained_var_ratios.append(pcas[cell_type_idx].explained_variance_ratio_)\n",
    "\n",
    "#                 zeros_pca.append(pcas[cell_type_idx].transform(hidden_zero[:, cell_type_filter]))\n",
    "#                 W_output_pca.append(pcas[cell_type_idx].transform(W_output[:, cell_type_filter]))\n",
    "\n",
    "#             explained_vars = np.concatenate(explained_vars, axis=-1)\n",
    "#             explained_var_ratios = np.concatenate(explained_var_ratios, axis=-1)\n",
    "#             # Now sort based on explained variances/explained variance ratio\n",
    "#             if pca_sort_type in ('vars',):\n",
    "#                 pca_type_sort = np.argsort(explained_vars)[::-1] # largest to smallest\n",
    "#             elif pca_sort_type in ('ratios',):\n",
    "#                 pca_type_sort = np.argsort(explained_var_ratios)[::-1] # largest to smallest\n",
    "\n",
    "#             cell_types_pca =  np.concatenate(cell_types_pca, axis=-1)[pca_type_sort]\n",
    "#             hs_pca = np.concatenate(hs_pca, axis=-1)[:, pca_type_sort]\n",
    "#             explained_vars = explained_vars[pca_type_sort]\n",
    "#             explained_var_ratios = explained_var_ratios[pca_type_sort]\n",
    "#             zeros_pca = np.concatenate(zeros_pca, axis=-1)[:, pca_type_sort]\n",
    "#             W_output_pca = np.concatenate(W_output_pca, axis=-1)[:, pca_type_sort]\n",
    "#             print('Overall:')\n",
    "#             print(' PR: {:.2f}'.format(\n",
    "#                 participation_ratio_vector(explained_var_ratios)\n",
    "#             ))\n",
    "#             hs_pca = hs_pca.reshape(hs.shape)\n",
    "\n",
    "#         if session_iter == len(session_breakdown)-1:\n",
    "#             fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(3*3, 3))\n",
    "#             n_pcs_plot = 3\n",
    "\n",
    "#             if pca_type in ('full',):\n",
    "#                 ax1.scatter(np.arange(n_activity), activity_pca.explained_variance_ratio_, color=c_vals[2])\n",
    "#                 cutoff = np.sum(activity_pca.explained_variance_ratio_ > 0.1) # PC with > 0.1 \n",
    "\n",
    "#                 for pc_idx in range(n_pcs_plot):\n",
    "#                     ax2.plot(activity_pca.components_[pc_idx, :], color=c_vals[pc_idx], label='PC{}'.format(pc_idx+1),\n",
    "#                             zorder=5-pc_idx)\n",
    "\n",
    "#             ax1.set_xlabel('PC')\n",
    "#             ax1.set_ylabel('Explained var ratio')\n",
    "#             ax1.set_title(f\"Good PC: {cutoff}\")\n",
    "\n",
    "#             for ax in (ax2, ax3):\n",
    "#                 ax.legend()\n",
    "#                 ax.set_xlabel('Neuron idx')\n",
    "#                 ax.set_ylabel('PC weight')\n",
    "\n",
    "#             fig.show()\n",
    "#             fig.tight_layout()\n",
    "#             # fig.savefig(f\"./results/PC_{pca_target}_{hyp_dict['ruleset']}_{hyp_dict['chosen_network']}_{hyp_dict['addon_name']}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e41c11-be8d-47b6-9bd1-c3b434565657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for session_iter in range(len(session_breakdown)-1,len(session_breakdown)):\n",
    "#     fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(3*3, 3))\n",
    "\n",
    "#     # single_rule: color by labels\n",
    "#     # all_rules: color by rule\n",
    "#     # if multiple testrules are detected, automatically plot based on the rule type\n",
    "#     plot_mode = 'single_rule' if len(task_params[\"rules\"]) == 1 else 'all_rules'\n",
    "\n",
    "#     if plot_mode == 'single_rule':\n",
    "#         rule_idx = 0 # Only used for individual labels\n",
    "#         rule = task_params['rules'][rule_idx]\n",
    "#         rule_batch_idxs = np.arange(n_batch_all)[test_rule_idxs == rule_idx]\n",
    "#     elif plot_mode == 'all_rules':\n",
    "#         rule_batch_idxs = np.arange(n_batch_all)\n",
    "\n",
    "#     if pca_target in ('hs'):\n",
    "#         pc1_idx, pc2_idx, pc3_idx = 0,1,2\n",
    "#     elif pca_target in ('Ms'):\n",
    "#         pc1_idx, pc2_idx, pc3_idx = 0,1,2\n",
    "\n",
    "#     # batch_plot = (0, 1, 2,)\n",
    "#     if task_params['dataset_name'] in ('DelayMatchSample-v0',):\n",
    "#         batch_plot = [False, False, False, False] # 4 distinct paths\n",
    "#     else:\n",
    "#         batch_plot = (0, 1, 2,)\n",
    "    \n",
    "#     # ZIHAN\n",
    "#     # additional analysis\n",
    "#     if session_iter == len(session_breakdown)-1 and pca_target in ('hs', 'Ms'):\n",
    "#         all_normal_vectors = []\n",
    "#         # minus the zero activity pca\n",
    "#         pc_indices_consider = [pc1_idx, pc2_idx, pc3_idx]\n",
    "\n",
    "#         cosine_pc = np.hstack([W_output_pca[1, idx] - zeros_pca[:, idx] for idx in pc_indices_consider])\n",
    "#         # cosine_pc = np.vstack((W_output_pca[1,pc1_idx]-zeros_pca[:,pc1_idx], W_output_pca[1,pc2_idx]-zeros_pca[:,pc2_idx], W_output_pca[1,pc3_idx]-zeros_pca[:,pc3_idx])).reshape(-1)\n",
    "#         cosine_pc = cosine_pc / np.linalg.norm(cosine_pc)\n",
    "\n",
    "#         sine_pc = np.hstack([W_output_pca[2, idx] - zeros_pca[:, idx] for idx in pc_indices_consider])\n",
    "#         # sine_pc = np.vstack((W_output_pca[2,pc1_idx]-zeros_pca[:,pc1_idx], W_output_pca[2,pc2_idx]-zeros_pca[:,pc2_idx], W_output_pca[2,pc3_idx]-zeros_pca[:,pc3_idx])).reshape(-1)\n",
    "#         sine_pc = sine_pc / np.linalg.norm(sine_pc)\n",
    "\n",
    "#         normal_cosine_sine = np.cross(cosine_pc, sine_pc)\n",
    "#         # normalization\n",
    "#         normal_cosine_sine = normal_cosine_sine / np.linalg.norm(normal_cosine_sine)\n",
    "#         all_normal_vectors.append(normal_cosine_sine)\n",
    "\n",
    "#         figscatter = plt.figure(figsize=(3*3, 3))\n",
    "#         ax1_3d = figscatter.add_subplot(131, projection='3d')\n",
    "#         ax2_3d = figscatter.add_subplot(132, projection='3d')\n",
    "#         ax3_3d = figscatter.add_subplot(133)\n",
    "\n",
    "#         init_point_3d = np.stack((zeros_pca[:, pc1_idx], zeros_pca[:, pc2_idx], zeros_pca[:, pc3_idx]), axis=1)\n",
    "#         ax1_3d.scatter(init_point_3d[0,0], init_point_3d[0,1], init_point_3d[0,2], marker=\"s\", color=\"black\")\n",
    "\n",
    "#         ax1_3d.plot([init_point_3d[0,0], init_point_3d[0,0] + cosine_pc[0]],\n",
    "#                     [init_point_3d[0,1], init_point_3d[0,1] + cosine_pc[1]],\n",
    "#                     [init_point_3d[0,2], init_point_3d[0,2] + cosine_pc[2]],\n",
    "#                     color='red', label=\"cosine\")\n",
    "\n",
    "#         ax1_3d.plot([init_point_3d[0,0], init_point_3d[0,0] + sine_pc[0]],\n",
    "#                     [init_point_3d[0,1], init_point_3d[0,1] + sine_pc[1]],\n",
    "#                     [init_point_3d[0,2], init_point_3d[0,2] + sine_pc[2]],\n",
    "#                     color='blue', label=\"sine\")\n",
    "\n",
    "#         ax1_3d.plot([init_point_3d[0,0], init_point_3d[0,0] + normal_cosine_sine[0]],\n",
    "#                     [init_point_3d[0,1], init_point_3d[0,1] + normal_cosine_sine[1]],\n",
    "#                     [init_point_3d[0,2], init_point_3d[0,2] + normal_cosine_sine[2]],\n",
    "#                     color='green', label=\"cosine * sine\")\n",
    "        \n",
    "#         ax1_3d.legend()\n",
    "\n",
    "#         for ii in range(len(session_breakdown[:-1])):\n",
    "#             subsession = session_breakdown[ii]\n",
    "#             clustered_points = as_pca[:, subsession[0]:subsession[1], :]\n",
    "            \n",
    "#             # Extract the principal components for the scatter plot\n",
    "#             pc1_points = clustered_points[:, :, pc1_idx].flatten()\n",
    "#             pc2_points = clustered_points[:, :, pc2_idx].flatten()\n",
    "#             pc3_points = clustered_points[:, :, pc3_idx].flatten()\n",
    "            \n",
    "#             ax1_3d.scatter(pc1_points, pc2_points, pc3_points, c=c_vals[ii], alpha=0.15)\n",
    "\n",
    "#             points_3d = np.vstack((pc1_points, pc2_points, pc3_points)).T\n",
    "#             # points_3d_centered = points_3d - np.mean(points_3d, axis=0)\n",
    "#             scaler = StandardScaler()\n",
    "#             points_3d_standardized = scaler.fit_transform(points_3d)\n",
    "#             pca = PCA(n_components=3)\n",
    "#             pca.fit(points_3d_standardized)\n",
    "#             normal_vector = pca.components_[-1]\n",
    "#             normalized_normal_vector = normal_vector / np.linalg.norm(normal_vector)\n",
    "#             all_normal_vectors.append(normalized_normal_vector)\n",
    "\n",
    "#             point = np.mean(points_3d, axis=0)\n",
    "#             d = -point.dot(normal_vector)\n",
    "#             xx, yy = np.meshgrid(np.linspace(np.min(points_3d[:, 0]), np.max(points_3d[:, 0]), 10), \n",
    "#                                 np.linspace(np.min(points_3d[:, 1]), np.max(points_3d[:, 1]), 10))\n",
    "#             zz = (-normal_vector[0] * xx - normal_vector[1] * yy - d) * 1. / normal_vector[2]\n",
    "            \n",
    "#             ax2_3d.plot_surface(xx, yy, zz, alpha=0.5, color=c_vals_d[ii])\n",
    "\n",
    "#         for thisax in [ax1_3d, ax2_3d]:\n",
    "#             thisax.set_xlabel(f\"PCA {pc1_idx+1}\")\n",
    "#             thisax.set_ylabel(f\"PCA {pc2_idx+1}\")\n",
    "#             thisax.set_zlabel(f\"PCA {pc3_idx+1}\")\n",
    "\n",
    "#         nnn = len(all_normal_vectors)\n",
    "#         normal_align = np.zeros((nnn, nnn))\n",
    "#         for n1 in range(nnn):\n",
    "#             for n2 in range(nnn):\n",
    "#                 dot_product = np.dot(all_normal_vectors[n1], all_normal_vectors[n2])\n",
    "#                 angle_deg = np.degrees(np.arccos(dot_product))\n",
    "#                 # consider both alignment and anti-alignment\n",
    "#                 angle_deg = min(angle_deg, 180.0-angle_deg)\n",
    "#                 normal_align[n1,n2] = angle_deg\n",
    "\n",
    "#         np.fill_diagonal(normal_align, np.nan)\n",
    "#         # mark_labels = [\"response\"] + [f\"period {i+1}\" for i in range(nnn-1)]\n",
    "#         mark_labels = [\"response\"] + recordkyle_nameall[0]\n",
    "\n",
    "#         sns.heatmap(normal_align, cbar=True, annot=True, ax=ax3_3d, cmap='coolwarm', xticklabels=mark_labels, yticklabels=mark_labels)\n",
    "#         ax3_3d.set_xticklabels(ax3_3d.get_xticklabels(), rotation=45)\n",
    "#         ax3_3d.set_yticklabels(ax3_3d.get_yticklabels(), rotation=45)\n",
    "\n",
    "#         figscatter.tight_layout()\n",
    "#         figscatter.savefig(f\"./results/zz_test_{pca_target}_{hyp_dict['ruleset']}_{hyp_dict['chosen_network']}_{hyp_dict['addon_name']}_scatter.png\")\n",
    "\n",
    "#         # what if not based on PC -> Aug 3nd\n",
    "        \n",
    "#         # middle of each session\n",
    "#         breaks_1afterstart = [int((cut[0]+cut[1])/2) for cut in session_breakdown[:-1]]\n",
    "\n",
    "#         # plot more frames \n",
    "#         considerall = 0\n",
    "#         if considerall:\n",
    "#             for cut in session_breakdown[:-1]:\n",
    "#                 breaks_1afterstart.append(cut[0])\n",
    "#                 breaks_1afterstart.append(cut[1]-1)\n",
    "#         breaks_1afterstart = list(np.sort(breaks_1afterstart))\n",
    "#         print(session_breakdown)\n",
    "\n",
    "#         # if pca_target in ('hs',): # in random\n",
    "#         #     figall, axsall = plt.subplots(3,2,figsize=(2*4,3*4))\n",
    "#         #     figavg, axsavg = plt.subplots(3,2,figsize=(2*4,3*4))\n",
    "\n",
    "#         #     assert mpn_depth == 1 # for now\n",
    "#         #     W_ = W_all_[0]\n",
    "\n",
    "#         #     # plot two trials\n",
    "#         #     figW, axsW = plt.subplots(len(breaks_1afterstart)+1,4,figsize=(12*4,4*(len(breaks_1afterstart)+1)))\n",
    "#         #     select_batch = [29,49,69,89]\n",
    "#         #     for pp in select_batch:\n",
    "#         #         sns.heatmap(W_.T, cbar=True, cmap=\"coolwarm\", ax=axsW[0,select_batch.index(pp)])\n",
    "#         #         axsW[0,select_batch.index(pp)].set_title(\"W\")\n",
    "#         #         # take average across batches\n",
    "#         #         # Ms_samplebatch = np.mean(Ms_orig, axis=0) \n",
    "#         #         Ms_samplebatch = Ms_orig[pp,:,:,:]\n",
    "#         #         print(Ms_samplebatch.shape)\n",
    "\n",
    "#         #         for bb in breaks_1afterstart:\n",
    "#         #             sns.heatmap(Ms_samplebatch[bb,:,:].T, cbar=True, cmap=\"coolwarm\", center=0, \\\n",
    "#         #                                 vmin=-1, vmax=1, ax=axsW[breaks_1afterstart.index(bb)+1,select_batch.index(pp)])\n",
    "\n",
    "#         #         for bb in breaks_1afterstart:\n",
    "#         #             axsW[breaks_1afterstart.index(bb)+1,select_batch.index(pp)].set_title(f\"M: Time: {bb}\")\n",
    "#         #             axsW[breaks_1afterstart.index(bb)+1,select_batch.index(pp)].set_xlabel(f\"Neuron\")\n",
    "#         #             axsW[breaks_1afterstart.index(bb)+1,select_batch.index(pp)].set_ylabel(f\"Input Index\")\n",
    "\n",
    "#         #     figW.tight_layout()\n",
    "#         #     figW.savefig(f\"./results/zz_test_{pca_target}_{hyp_dict['ruleset']}_{hyp_dict['chosen_network']}_{hyp_dict['addon_name']}_W.png\")\n",
    "\n",
    "#         #     results = {\n",
    "#         #         'fix_hs_Woutput_all': [],\n",
    "#         #         'response_hs_Woutput_all': [],\n",
    "#         #         'fix_Ms_Woutput_all': [],\n",
    "#         #         'response_Ms_Woutput_all': [],\n",
    "#         #         'fix_MsWs_Woutput_all': [],\n",
    "#         #         'response_MsWs_Woutput_all': [],\n",
    "#         #     }\n",
    "\n",
    "#         #     pltout, axsout = plt.subplots(figsize=(3,3))\n",
    "#         #     sns.heatmap(W_output, ax=axsout, cmap=\"coolwarm\", center=0, cbar=True, square=True)\n",
    "#         #     pltout.savefig(f\"./results/zz_test_{pca_target}_{hyp_dict['ruleset']}_{hyp_dict['chosen_network']}_{hyp_dict['addon_name']}_Woutput.png\")\n",
    "\n",
    "#         #     W_output_fixation = W_output[0,:].reshape(-1,1) # (N,1)\n",
    "#         #     W_output_response = W_output[1:3,:].T # (N,2)\n",
    "\n",
    "\n",
    "#         #     for batch_iter in range(hs.shape[0]):\n",
    "#         #         hs_per_batch = hs[batch_iter,:,:]\n",
    "#         #         Ms_per_batch = Ms_orig[batch_iter,:,:,:]\n",
    "\n",
    "#         #         fix_hs_Woutput, response_hs_Woutput = [], []\n",
    "#         #         fix_Ms_Woutput, response_Ms_Woutput = [], []\n",
    "#         #         fix_MsWs_Woutput, response_MsWs_Woutput = [], []\n",
    "                \n",
    "#         #         for time_cut in range(hs_per_batch.shape[0]):\n",
    "#         #             hs_per_batch_per_time = hs_per_batch[time_cut,:].reshape(-1,1) # (N,1)\n",
    "#         #             Ms_per_batch_per_time = Ms_per_batch[time_cut,:,:] # (N, n_input)\n",
    "\n",
    "#         #             ang_fix_hs_Ws = subspace_angles(hs_per_batch_per_time, W_output_fixation)[0]\n",
    "#         #             fix_hs_Woutput.append(np.degrees(ang_fix_hs_Ws))\n",
    "\n",
    "#         #             ang_response_hs_Ws = subspace_angles(hs_per_batch_per_time, W_output_response)[0]\n",
    "#         #             response_hs_Woutput.append(np.degrees(ang_response_hs_Ws))\n",
    "\n",
    "#         #             try:\n",
    "#         #                 ang_fix_Ms_Ws = subspace_angles(Ms_per_batch_per_time, W_output_fixation)[0]\n",
    "#         #                 fix_Ms_Woutput.append(np.degrees(ang_fix_Ms_Ws))\n",
    "#         #             except:\n",
    "#         #                 fix_Ms_Woutput.append(np.nan)\n",
    "                    \n",
    "#         #             try:\n",
    "#         #                 ang_response_Ms_Ws = subspace_angles(Ms_per_batch_per_time, W_output_response)[0]\n",
    "#         #                 response_Ms_Woutput.append(np.degrees(ang_response_Ms_Ws))\n",
    "#         #             except:\n",
    "#         #                 response_Ms_Woutput.append(np.nan)\n",
    "\n",
    "#         #             try:\n",
    "#         #                 ang_fix_MsWs_Ws = subspace_angles(Ms_per_batch_per_time * W_ + W_, W_output_fixation)[0]\n",
    "#         #                 fix_MsWs_Woutput.append(np.degrees(ang_fix_MsWs_Ws))\n",
    "#         #             except:\n",
    "#         #                 fix_MsWs_Woutput.append(np.nan)\n",
    "                    \n",
    "#         #             try:\n",
    "#         #                 ang_response_MsWs_Ws = subspace_angles(Ms_per_batch_per_time * W_ + W_, W_output_response)[0]\n",
    "#         #                 response_MsWs_Woutput.append(np.degrees(ang_response_MsWs_Ws))\n",
    "#         #             except:\n",
    "#         #                 response_MsWs_Woutput.append(np.nan)\n",
    "\n",
    "\n",
    "#         #         results['fix_hs_Woutput_all'].append(fix_hs_Woutput)\n",
    "#         #         results['response_hs_Woutput_all'].append(response_hs_Woutput)\n",
    "#         #         results['fix_Ms_Woutput_all'].append(fix_Ms_Woutput)\n",
    "#         #         results['response_Ms_Woutput_all'].append(response_Ms_Woutput)\n",
    "#         #         results['fix_MsWs_Woutput_all'].append(fix_MsWs_Woutput)\n",
    "#         #         results['response_MsWs_Woutput_all'].append(response_MsWs_Woutput)\n",
    "\n",
    "#         #     for key in results:\n",
    "#         #         results[key] = np.array(results[key])\n",
    "            \n",
    "#         #     result_key = list(results.keys())\n",
    "#         #     axsall = axsall.flatten()\n",
    "#         #     for batch_iter in range(hs.shape[0]):\n",
    "#         #         for key in result_key:\n",
    "#         #             axsall[result_key.index(key)].plot([x for x in range(results[key].shape[1])], results[key][batch_iter,:], color=c_vals[labels[batch_iter,0]])             \n",
    "#         #             axsall[result_key.index(key)].set_title(key)\n",
    "\n",
    "#         #     for ax in axsall.flatten():\n",
    "#         #         for bb in breaks:\n",
    "#         #             ax.axvline(bb, color='r', linestyle=\"--\")\n",
    "\n",
    "#         #     breaks = [0] + breaks\n",
    "#         #     axsavg = axsavg.flatten()\n",
    "#         #     for key in result_key:\n",
    "#         #         means, stds = [], []\n",
    "#         #         for bb in range(len(breaks)-1):\n",
    "#         #             meanp = np.nanmean(results[key][:,breaks[bb]:breaks[bb+1]])\n",
    "#         #             stdp = np.nanstd(results[key][:,breaks[bb]:breaks[bb+1]])\n",
    "#         #             means.append(meanp)\n",
    "#         #             stds.append(stdp)\n",
    "#         #         means, stds = np.array(means), np.array(stds)\n",
    "#         #         axsavg[result_key.index(key)].plot([x for x in range(len(means))], means, \"-o\")\n",
    "#         #         axsavg[result_key.index(key)].fill_between([x for x in range(len(means))], means-stds, means+stds, alpha=0.3, color=\"red\")\n",
    "#         #         axsavg[result_key.index(key)].set_title(key)\n",
    "\n",
    "#         #     figall.show()\n",
    "#         #     figavg.show()\n",
    "#         #     figall.savefig(f\"./results/zz_test_{hyp_dict['ruleset']}_{hyp_dict['chosen_network']}_{hyp_dict['addon_name']}_all2all.png\")\n",
    "#         #     figavg.savefig(f\"./results/zz_test_{hyp_dict['ruleset']}_{hyp_dict['chosen_network']}_{hyp_dict['addon_name']}_all2allavg.png\")\n",
    "\n",
    "#     for ax, pc_idxs in zip((ax1, ax2, ax3),((pc1_idx, pc2_idx), (pc1_idx, pc3_idx), (pc2_idx, pc3_idx))):\n",
    "#         domain_save = []\n",
    "#         for batch_idx_idx, batch_idx in enumerate(rule_batch_idxs):\n",
    "#             if plot_mode == 'single_rule':\n",
    "#                 batch_color_idx = labels[batch_idx][0]\n",
    "#             elif plot_mode == 'all_rules':\n",
    "#                 batch_color_idx = int(test_rule_idxs[batch_idx])\n",
    "\n",
    "#             # MODIFY BY ZIHAN\n",
    "#             # only plot the categorization at the whole final period\n",
    "#             # where all sections are included in plotting\n",
    "#             if session_iter == len(session_breakdown)-1:\n",
    "#                 cutoff_evidence = recordkyle_all\n",
    "                \n",
    "#                 temp = []\n",
    "#                 for jj in range(len(cutoff_evidence[batch_idx])-1):\n",
    "#                     xx = as_pca[batch_idx, cutoff_evidence[batch_idx][jj]:cutoff_evidence[batch_idx][jj+1], pc_idxs[0]]\n",
    "#                     yy = as_pca[batch_idx, cutoff_evidence[batch_idx][jj]:cutoff_evidence[batch_idx][jj+1], pc_idxs[1]]\n",
    "#                     temp.append([xx, yy])\n",
    "#                     ax.scatter(xx, yy, color=c_vals_l[batch_color_idx], alpha=0.7, marker=markers_vals[jj], s=8)\n",
    "#                 domain_save.append(temp)\n",
    "\n",
    "#                 if batch_idx_idx in batch_plot:\n",
    "#                     for jj in range(len(cutoff_evidence[batch_idx])-1):\n",
    "#                         ax.plot(as_pca[batch_idx, cutoff_evidence[batch_idx][jj]:cutoff_evidence[batch_idx][jj+1], pc_idxs[0]], \n",
    "#                                 as_pca[batch_idx, cutoff_evidence[batch_idx][jj]:cutoff_evidence[batch_idx][jj+1], pc_idxs[1]], \n",
    "#                                 color=c_vals[batch_color_idx], \n",
    "#                                 linestyle=l_vals[jj], alpha=1.0, zorder=10\n",
    "#                         )\n",
    "\n",
    "#             else:\n",
    "#                 ax.scatter(as_pca[batch_idx, :, pc_idxs[0]], as_pca[batch_idx, :, pc_idxs[1]], color=c_vals_l[batch_color_idx], alpha=0.7, marker=\"o\", s=8)\n",
    "\n",
    "#         if pca_type in ('full',):\n",
    "#             ax.set_xlabel('PC {}'.format(pc_idxs[0]+1))\n",
    "#             ax.set_ylabel('PC {}'.format(pc_idxs[1]+1))\n",
    "#         elif pca_type in ('cell_types',):\n",
    "#             ax.set_xlabel('PC {} (cell type: {})'.format(pc_idxs[0]+1, cell_types_pca[pc_idxs[0]]))\n",
    "#             ax.set_ylabel('PC {} (cell type: {})'.format(pc_idxs[1]+1, cell_types_pca[pc_idxs[1]]))\n",
    "#         # Plot zero point\n",
    "#         ax.scatter(zeros_pca[:, pc_idxs[0]], zeros_pca[:, pc_idxs[1]], color='k',\n",
    "#                 marker='s')\n",
    "\n",
    "#         # Plot readouts\n",
    "#         ro_vector_dir_all = []\n",
    "#         init_points = []\n",
    "#         # if pca_target in ('Ms',):\n",
    "#         #     continue\n",
    "#         for out_idx, output_dim_label in enumerate(output_dim_labels):\n",
    "#             if pca_target in ('Ms',):\n",
    "#                 RO_SCALE = 5\n",
    "#             elif pca_target in ('hs',):\n",
    "#                 RO_SCALE = 1\n",
    "\n",
    "#             ro_vector_dir = np.array((\n",
    "#                 W_output_pca[out_idx, pc_idxs[0]] - zeros_pca[:, pc_idxs[0]],\n",
    "#                 W_output_pca[out_idx, pc_idxs[1]] - zeros_pca[:, pc_idxs[1]],\n",
    "#             ))\n",
    "\n",
    "#             norm = np.linalg.norm(ro_vector_dir)\n",
    "\n",
    "#             if norm != 0:\n",
    "#                 ro_vector_dir = ro_vector_dir / norm\n",
    "#             else:\n",
    "#                 ro_vector_dir = ro_vector_dir\n",
    "\n",
    "#             ro_vector_dir = RO_SCALE * ro_vector_dir\n",
    "#             ro_vector_dir_all.append(ro_vector_dir)\n",
    "\n",
    "#             init_point = [zeros_pca[:, pc_idxs[0]], zeros_pca[:, pc_idxs[1]]]\n",
    "\n",
    "#             ax.plot((zeros_pca[:, pc_idxs[0]], zeros_pca[:, pc_idxs[0]] + ro_vector_dir[0]),\n",
    "#                     (zeros_pca[:, pc_idxs[1]], zeros_pca[:, pc_idxs[1]] + ro_vector_dir[1]),\n",
    "#                     color=c_vals[out_idx], label=output_dim_label, zorder=9)\n",
    "\n",
    "#             init_points.append(init_point)\n",
    "            \n",
    "#     fig.show()\n",
    "#     fig.tight_layout()\n",
    "#     # fig.savefig(f\"./results/trajectory_{pca_target}_{hyp_dict['ruleset']}_{hyp_dict['chosen_network']}_{hyp_dict['addon_name']}_period{session_iter}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cde6dfd-3338-4a68-b43a-02679b6d5e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(db.keys())\n",
    "\n",
    "# if len(rules_dict[hyp_dict['ruleset']]) > 1:\n",
    "#     hs_all = []\n",
    "#     if net_params['net_type'] in ('dmpn',):\n",
    "#         layer_idx_lst = [i for i in range(mpn_depth)]\n",
    "#         for layer_idx in layer_idx_lst:\n",
    "#             if layer_idx == 0:\n",
    "#                 hs = db['hidden0'].detach().cpu().numpy()\n",
    "#             elif layer_idx == 1:\n",
    "#                 hs = db['hidden1'].detach().cpu().numpy()\n",
    "#             hs_all.append(hs)\n",
    "#     elif net_params['net_type'] in ('gru',):\n",
    "#         layer_idx_lst = [0]\n",
    "#         hs_all = [db['hidden'].detach().cpu().numpy()]\n",
    "\n",
    "#     for hsiter in range(len(hs_all)):\n",
    "#         hs = hs_all[hsiter]\n",
    "#         layer_name = layer_idx_lst[hsiter]\n",
    "\n",
    "#         cell_vars_tot = np.var(hs, axis=(0, 1)) # Var over batch and sequence\n",
    "#         n_rules = len(task_params['rules'])\n",
    "#         n_cells = hs.shape[-1]\n",
    "\n",
    "#         # cell_vars_dtypes = [('rule{}'.format(rule_idx), np.float) for rule_idx in range(n_rules)]# Useful for sorting later\n",
    "#         cell_vars_rules = np.zeros((n_rules, n_cells,))\n",
    "#         cell_vars_rules_norm = np.zeros_like(cell_vars_rules)\n",
    "\n",
    "#         for rule_idx, rule in enumerate(task_params['rules']):\n",
    "#             print('Rule {} (idx {})'.format(rule, rule_idx))\n",
    "#             rule_hs = hs[test_rule_idxs == rule_idx, :, :]\n",
    "#             cell_vars_rules[rule_idx] = np.var(rule_hs, axis=(0, 1)) # Var over batch and sequence\n",
    "\n",
    "#         # Now normalize everything\n",
    "#         cell_max_var = np.max(cell_vars_rules, axis=0) # Across rules\n",
    "#         for rule_idx, rule in enumerate(task_params['rules']):\n",
    "#             cell_vars_rules_norm[rule_idx] = np.where(\n",
    "#                 cell_max_var > 0., cell_vars_rules[rule_idx] / cell_max_var, 0.\n",
    "#             )\n",
    "\n",
    "#         # Now sort\n",
    "#         if n_rules > 1:\n",
    "#             rule0_vals = cell_vars_rules_norm[0].tolist()\n",
    "#             rule1_vals = cell_vars_rules_norm[1].tolist()\n",
    "\n",
    "#         rule01_vals = np.array(list(zip(rule0_vals, rule1_vals)), dtype=[('rule0', float), ('rule1', float)])\n",
    "#         sort_idxs = np.argsort(rule01_vals, order=['rule0', 'rule1'])[::-1]\n",
    "\n",
    "#         # sort_idxs = np.argsort(cell_vars_rules_norm[0])[::-1]\n",
    "#         cell_vars_rules_sorted_norm = cell_vars_rules_norm[:, sort_idxs]\n",
    "\n",
    "#         fig, ax = plt.subplots(2, 1, figsize=(12, 3*2))\n",
    "\n",
    "#         for rule_idx, rule in enumerate(task_params['rules']):\n",
    "#             ax[0].plot(cell_vars_rules_sorted_norm[rule_idx], color=c_vals[rule_idx],\n",
    "#                     label=task_params['rules'][rule_idx])\n",
    "\n",
    "#         ax[0].legend()\n",
    "\n",
    "#         ax[0].set_xlabel('Cell_idx')\n",
    "#         ax[0].set_ylabel('Norm. task var.')\n",
    "\n",
    "\n",
    "#         ax[1].matshow(cell_vars_rules_sorted_norm, aspect='auto', vmin=0.0, vmax=1.0,)\n",
    "#         ax[1].set_yticks(np.arange(n_rules))\n",
    "#         ax[1].set_yticklabels(task_params['rules'])\n",
    "#         ax[1].set_xlabel('Cell idx')\n",
    "#         fig.show()\n",
    "#         # fig.savefig(f\"./results/categorization_{hyp_dict['ruleset']}_{hyp_dict['chosen_network']}_layer_{layer_name}_{hyp_dict['addon_name']}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55f9607-c862-45eb-8828-17308b9eff92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002cb63f-4e2e-45b3-b5b1-f43c1475896b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc627c95-873e-4617-b943-f7d997ea5e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def analyze_trajectory(save_trajectory, save_trajectory_random):\n",
    "#     \"\"\"\n",
    "#     Analyze trajectories by calculating mean absolute values for fixations and tasks.\n",
    "#     \"\"\"\n",
    "#     def process_trajectory(trajectory):\n",
    "#         results = []\n",
    "#         for batch in trajectory:\n",
    "#             stim1_fixon = batch[0, 1][stimulus_start:response_start]\n",
    "#             stim1_task = batch[3, 1][stimulus_start:response_start]\n",
    "#             results.append([np.mean(np.abs(stim1_fixon + stim1_task)), np.mean(np.abs(stim1_fixon)), np.mean(np.abs(stim1_task))])\n",
    "#         return np.array(results)\n",
    "    \n",
    "#     # Process both trajectories\n",
    "#     result = process_trajectory(save_trajectory)\n",
    "#     result_random = process_trajectory(save_trajectory_random)\n",
    "    \n",
    "#     # Return the mean of the computed values\n",
    "#     return np.mean(result[:, 0]), np.mean(result[:, 1]), np.mean(result[:,2]), np.mean(result_random[:, 0]), np.mean(result_random[:, 1]), np.mean(result_random[:, 2])\n",
    "\n",
    "# fixon_task_diff = np.array([analyze_trajectory(all_trajectory[i], all_trajectory_random[i]) for i in range(len(all_trajectory_random))])\n",
    "\n",
    "# figfixontaskcancel, axsfixontaskcancel = plt.subplots(1,2,figsize=(3*2,3))\n",
    "# axsfixontaskcancel[0].plot(fixon_task_diff[:,0], \"-o\", c=c_vals[0], label=\"abs(fixon-task)\")\n",
    "# axsfixontaskcancel[0].plot(fixon_task_diff[:,1], \"-o\", c=c_vals[1], label=\"abs(fixon)\")\n",
    "# axsfixontaskcancel[0].plot(fixon_task_diff[:,2], \"-o\", c=c_vals[2], label=\"abs(task)\")\n",
    "# axsfixontaskcancel[0].plot(fixon_task_diff[:,3], \"-o\", c=c_vals_l[0], label=\"abs(fixon-task) random\")\n",
    "# axsfixontaskcancel[0].plot(fixon_task_diff[:,4], \"-o\", c=c_vals_l[1], label=\"abs(fixon) random\")\n",
    "# axsfixontaskcancel[0].plot(fixon_task_diff[:,5], \"-o\", c=c_vals_l[2], label=\"abs(task) random\")\n",
    "# axsfixontaskcancel[0].legend()\n",
    "# axsfixontaskcancel[0].set_xlabel(\"Learning Stage\")\n",
    "# axsfixontaskcancel[0].set_ylabel(\"Projection Magnitude\")\n",
    "# axsfixontaskcancel[1].plot(loss_lst, \"-o\", c=c_vals[2])\n",
    "# axsfixontaskcancel[1].set_xlabel(\"Learning Stage\")\n",
    "# axsfixontaskcancel[1].set_ylabel(\"Loss\")\n",
    "# axsfixontaskcancel[1].set_yscale(\"log\")\n",
    "\n",
    "# figfixontaskcancel.tight_layout()\n",
    "# figfixontaskcancel.savefig(\"./paper/cancel.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
