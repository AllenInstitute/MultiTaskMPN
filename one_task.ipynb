{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "374fb8f2-9b83-44ce-821b-8917a114c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import os \n",
    "\n",
    "# PyTorch Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Data Handling and Image Processing\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Style for Matplotlib\n",
    "import scienceplots\n",
    "plt.style.use('science')\n",
    "plt.style.use(['no-latex'])\n",
    "\n",
    "# Scientific Computing and Machine Learning\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.linalg import subspace_angles\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Custom Modules and Extensions\n",
    "sys.path.append(\"../netrep/\")\n",
    "sys.path.append(\"../svcca/\")\n",
    "\n",
    "import networks as nets  # Contains RNNs\n",
    "import net_helpers\n",
    "import mpn_tasks\n",
    "import helper\n",
    "import mpn\n",
    "\n",
    "import scienceplots\n",
    "plt.style.use('science')\n",
    "plt.style.use(['no-latex'])\n",
    "\n",
    "# Memory Optimization\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd1556c8-a4b6-434b-a60f-37035980bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 Red, 1 blue, 2 green, 3 purple, 4 orange, 5 teal, 6 gray, 7 pink, 8 yellow\n",
    "c_vals = ['#e53e3e', '#3182ce', '#38a169', '#805ad5','#dd6b20', '#319795', '#718096', '#d53f8c', '#d69e2e',]\n",
    "c_vals_l = ['#feb2b2', '#90cdf4', '#9ae6b4', '#d6bcfa', '#fbd38d', '#81e6d9', '#e2e8f0', '#fbb6ce', '#faf089',]\n",
    "c_vals_d = ['#9b2c2c', '#2c5282', '#276749', '#553c9a', '#9c4221', '#285e61', '#2d3748', '#97266d', '#975a16',]\n",
    "l_vals = ['solid', 'dashed', 'dotted', 'dashdot', '-', '--', '-.', ':', (0, (3, 1, 1, 1)), (0, (5, 10))]\n",
    "markers_vals = ['o', 'v', '^', '<', '>', '1', '2', '3', '4', 's', 'p', '*', 'h', 'H', '+', 'x', 'D', 'd', '|', '_']\n",
    "hyp_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34978bf6-67b8-41bd-a022-a7b46a320686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set seed 627\n",
      "Fixation_off: True; Task_info: True\n",
      "Rules: ['delaydm1']\n",
      "  Input size 7, Output size 3\n",
      "Using CUDA...\n"
     ]
    }
   ],
   "source": [
    "# Reload modules if changes have been made to them\n",
    "from importlib import reload\n",
    "\n",
    "reload(nets)\n",
    "reload(net_helpers)\n",
    "\n",
    "fixseed = False # randomize setting the seed may lead to not perfectly solved results\n",
    "seed = random.randint(1,1000) if not fixseed else 8 # random set the seed to test robustness by default\n",
    "print(f\"Set seed {seed}\")\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "hyp_dict['task_type'] = 'multitask' # int, NeuroGym, multitask\n",
    "hyp_dict['mode_for_all'] = \"random_batch\"\n",
    "hyp_dict['ruleset'] = 'delaydm1' # low_dim, all, test\n",
    "\n",
    "accept_rules = ('fdgo', 'fdanti', 'delaygo', 'delayanti', 'reactgo', 'reactanti', \n",
    "                'delaydm1', 'delaydm2', 'dmsgo', 'dmcgo', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm', 'dm1')\n",
    "\n",
    "\n",
    "rules_dict = \\\n",
    "    {'all' : ['fdgo', 'reactgo', 'delaygo', 'fdanti', 'reactanti', 'delayanti',\n",
    "              'dm1', 'dm2', 'contextdm1', 'contextdm2', 'multidm',\n",
    "              'delaydm1', 'delaydm2', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm',\n",
    "              'dmsgo', 'dmsnogo', 'dmcgo', 'dmcnogo'],\n",
    "     'low_dim' : ['fdgo', 'reactgo', 'delaygo', 'fdanti', 'reactanti', 'delayanti',\n",
    "                 'delaydm1', 'delaydm2', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm',\n",
    "                 'dmsgo', 'dmsnogo', 'dmcgo', 'dmcnogo'],\n",
    "\n",
    "     'gofamily': ['fdgo', 'fdanti', 'reactgo', 'reactanti', 'delaygo', 'delayanti'],\n",
    "\n",
    "     'delaygo': ['delaygo'],\n",
    "     'delaygofamily': ['delaygo', 'delayanti'],\n",
    "     'fdgo': ['fdgo'],\n",
    "     'fdfamily': ['fdgo', 'fdanti'],\n",
    "     'reactgo': ['reactgo'],\n",
    "     'reactfamily': ['reactgo', 'reactanti'],\n",
    "     \n",
    "     'delaydm1': ['delaydm1'],\n",
    "     'delaydmfamily': ['delaydm1', 'delaydm2'],\n",
    "     \n",
    "     'dmsgofamily': ['dmsgo', 'dmsnogo'],\n",
    "     'dmsgo': ['dmsgo'],\n",
    "     'dmcgo': ['dmcgo'],\n",
    "     'contextdelaydm1': ['contextdelaydm1'], \n",
    "     'contextdelayfamily': ['contextdelaydm1', 'contextdelaydm2'],\n",
    "    }\n",
    "    \n",
    "\n",
    "# This can either be used to set parameters OR set parameters and train\n",
    "train = True # whether or not to train the network\n",
    "verbose = True\n",
    "hyp_dict['run_mode'] = 'minimal' # minimal, debug\n",
    "hyp_dict['chosen_network'] = \"dmpn\"\n",
    "\n",
    "# suffix for saving images\n",
    "hyp_dict['addon_name'] = \"\"\n",
    "\n",
    "mpn_depth = 1\n",
    "\n",
    "# for coding \n",
    "if hyp_dict['chosen_network'] in (\"gru\", \"vanilla\"):\n",
    "    mpn_depth = 1\n",
    "\n",
    "def current_basic_params():\n",
    "    task_params = {\n",
    "        'task_type': hyp_dict['task_type'],\n",
    "        'rules': rules_dict[hyp_dict['ruleset']],\n",
    "        'dt': 40, # ms, directly influence sequence lengths,\n",
    "        'ruleset': hyp_dict['ruleset'],\n",
    "        'n_eachring': 8, # Number of distinct possible inputs on each ring\n",
    "        'in_out_mode': 'low_dim',  # high_dim or low_dim or low_dim_pos (Robert vs. Laura's onetask, resp)\n",
    "        'sigma_x': 0.00, # Laura raised to 0.1 to prevent overfitting (Robert uses 0.01)\n",
    "        'mask_type': 'cost', # 'cost', None\n",
    "        'fixate_off': True, # Second fixation signal goes on when first is off\n",
    "        'task_info': True, \n",
    "        'randomize_inputs': False,\n",
    "        'n_input': 20, # Only used if inputs are randomized,\n",
    "        'modality_diff': False, # if two stimulus are included in the task, put them into different modality,\n",
    "        'label_strength': False,\n",
    "        'long_fixation': 'normal', \n",
    "        'long_stimulus': 'normal',\n",
    "        'long_delay': 'normal',\n",
    "        'long_response': 'normal',\n",
    "        'adjust_task_prop': True,\n",
    "        'adjust_task_decay': 0.9, \n",
    "    }\n",
    "\n",
    "    print(f\"Fixation_off: {task_params['fixate_off']}; Task_info: {task_params['task_info']}\")\n",
    "\n",
    "    train_params = {\n",
    "        'lr': 1e-3,\n",
    "        'n_batches': 128,\n",
    "        'batch_size': 128,\n",
    "        'gradient_clip': 10,\n",
    "        'valid_n_batch': 500,\n",
    "        'n_datasets': 5000, # Number of distinct batches\n",
    "        'valid_check': None, \n",
    "        'n_epochs_per_set': 1, # longer/shorter training\n",
    "        'task_mask': None, # None, task\n",
    "        'weight_reg': 'L2',\n",
    "        'reg_lambda': 1e-4,\n",
    "    }\n",
    "\n",
    "    if not train: # some \n",
    "        assert train_params['n_epochs_per_set'] == 0\n",
    "\n",
    "    n_hidden = 200\n",
    "\n",
    "    net_params = {\n",
    "        'net_type': hyp_dict['chosen_network'], # mpn1, dmpn, vanilla\n",
    "        'n_neurons': [1] + [n_hidden] * mpn_depth + [1],\n",
    "        'output_bias': False, # Turn off biases for easier interpretation\n",
    "        'loss_type': 'MSE', # XE, MSE\n",
    "        'activation': 'tanh', # linear, ReLU, sigmoid, tanh, tanh_re, tukey, heaviside\n",
    "        'cuda': True,\n",
    "        'monitor_freq': train_params[\"n_epochs_per_set\"],\n",
    "        'monitor_valid_out': True, # Whether or not to save validation output throughout training\n",
    "        'output_matrix': '',# \"\" (default); \"untrained\", or \"orthogonal\"\n",
    "        'input_layer_add': False, # for one-task, probably no need to have input layer added\n",
    "        'input_layer_add_trainable': False, # revise this is effectively to [randomize_inputs], tune this\n",
    "        'input_layer_bias': False, \n",
    "        'input_layer': \"trainable\", # for RNN only\n",
    "        \"tbptt\": True, # for local learning? \n",
    "        'acc_measure': 'stimulus', \n",
    "        \n",
    "        'ml_params0': {\n",
    "            'bias': True, # Bias of layer\n",
    "            'mp_type': 'mult',\n",
    "            'm_update_type': 'hebb_assoc', # hebb_assoc, hebb_pre\n",
    "            'eta_type': 'scalar', # scalar, pre_vector, post_vector, matrix\n",
    "            'eta_train': True,\n",
    "            # 'eta_init': 'gaussian', \n",
    "            'lam_type': 'scalar', # scalar, pre_vector, post_vector, matrix\n",
    "            'm_time_scale': 400, # ms, sets lambda\n",
    "            'lam_train': False,\n",
    "            'W_freeze': False  \n",
    "        },\n",
    "\n",
    "        # Vanilla RNN params\n",
    "        'leaky': True,\n",
    "        'alpha': 0.2,\n",
    "    }\n",
    "\n",
    "    # actually I don't think it is needed\n",
    "    # putting here to warn the parameter checking every time \n",
    "    # when switching network\n",
    "    if hyp_dict['chosen_network'] in (\"gru\", \"vanilla\"):\n",
    "        net_params[\"ml_params\"] = net_params[\"ml_params0\"]\n",
    "        del net_params[\"ml_params0\"]\n",
    "        \n",
    "    return task_params, train_params, net_params\n",
    "\n",
    "task_params, train_params, net_params = current_basic_params()\n",
    "\n",
    "shift_index = 1 if not task_params['fixate_off'] else 0\n",
    "\n",
    "if hyp_dict['task_type'] in ('multitask',):\n",
    "    task_params, train_params, net_params = mpn_tasks.convert_and_init_multitask_params(\n",
    "        (task_params, train_params, net_params)\n",
    "    )\n",
    "\n",
    "    net_params['prefs'] = mpn_tasks.get_prefs(task_params['hp'])\n",
    "\n",
    "    print('Rules: {}'.format(task_params['rules']))\n",
    "    print('  Input size {}, Output size {}'.format(\n",
    "        task_params['n_input'], task_params['n_output'],\n",
    "    ))\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "if net_params['cuda']:\n",
    "    print('Using CUDA...')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('Using CPU...')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a341b36-dc7a-42b0-bffb-cb17d380041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_dict[\"mess_with_training\"] = False\n",
    "\n",
    "if hyp_dict['mess_with_training']:\n",
    "    hyp_dict['addon_name'] += \"messwithtraining\"\n",
    "\n",
    "params = task_params, train_params, net_params\n",
    "\n",
    "# random matrix\n",
    "random_matrix = None\n",
    "if task_params[\"randomize_inputs\"]:\n",
    "    random_matrix = task_params[\"randomize_matrix\"]\n",
    "\n",
    "if net_params['net_type'] == 'mpn1':\n",
    "    netFunction = mpn.MultiPlasticNet\n",
    "elif net_params['net_type'] == 'dmpn':\n",
    "    netFunction = mpn.DeepMultiPlasticNet\n",
    "elif net_params['net_type'] == 'vanilla':\n",
    "    netFunction = nets.VanillaRNN\n",
    "elif net_params['net_type'] == 'gru':\n",
    "    netFunction = nets.GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07e4fe48-2af6-4741-8d42-01803ba0abf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Align ['delaydm1'] With Same Time\n",
      "delaydm1\n",
      "torch.Size([500, 85, 7])\n"
     ]
    }
   ],
   "source": [
    "test_n_batch = train_params[\"valid_n_batch\"]\n",
    "color_by = \"stim\"\n",
    "\n",
    "task_random_fix = True\n",
    "if task_random_fix:\n",
    "    print(f\"Align {task_params['rules']} With Same Time\")\n",
    "\n",
    "if task_params['task_type'] in ('multitask',): # Test batch consists of all the rules\n",
    "    task_params['hp']['batch_size_train'] = test_n_batch\n",
    "    # using homogeneous cutting off\n",
    "    test_mode_for_all = \"random\"\n",
    "    # ZIHAN\n",
    "    # generate test data using \"random\"\n",
    "    test_data, test_trials_extra = mpn_tasks.generate_trials_wrap(task_params, test_n_batch, \\\n",
    "                rules=task_params['rules'], mode_input=test_mode_for_all, fix=task_random_fix)\n",
    "    _, test_trials, test_rule_idxs = test_trials_extra\n",
    "\n",
    "    task_params['dataset_name'] = 'multitask'\n",
    "\n",
    "    if task_params['in_out_mode'] in ('low_dim_pos',):\n",
    "        output_dim_labels = ('Fixate', 'Cos', '-Cos', 'Sin', '-Sin')\n",
    "    elif task_params['in_out_mode'] in ('low_dim',):\n",
    "        output_dim_labels = ('Fixate', 'Cos', 'Sin')\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    labels = []\n",
    "    for rule_idx, rule in enumerate(task_params['rules']):\n",
    "        print(rule)\n",
    "        if rule in accept_rules:\n",
    "            if hyp_dict['ruleset'] in ('dmsgo', 'dmcgo'):\n",
    "                labels.append(test_trials[rule_idx].meta['matches'])\n",
    "            else:\n",
    "                labels.append(test_trials[rule_idx].meta['resp1' if color_by == \"resp\" else 'stim1'])\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "    labels = np.concatenate(labels, axis=0).reshape(-1,1)\n",
    "\n",
    "test_input, test_output, test_mask = test_data\n",
    "\n",
    "permutation = np.random.permutation(test_input.shape[0])\n",
    "test_input = test_input[permutation]\n",
    "test_output = test_output[permutation]\n",
    "test_mask = test_mask[permutation]\n",
    "labels = labels[permutation]\n",
    "\n",
    "test_input_np = test_input.detach().cpu().numpy()\n",
    "test_output_np = test_output.detach().cpu().numpy()\n",
    "\n",
    "n_batch_all = test_input_np.shape[0] # Total number of batches, might be different than test_n_batch\n",
    "max_seq_len = test_input_np.shape[1]\n",
    "\n",
    "if task_params[\"randomize_inputs\"]: \n",
    "    rmat = task_params[\"randomize_matrix\"]\n",
    "    test_input_np = np.matmul(test_input_np, np.linalg.pinv(rmat))\n",
    "\n",
    "print(test_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e78b44e9-b6e3-4c1c-a5c7-5a608a6d090b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla RNN:\n",
      "  n_neurons - input: 7, hidden: 200, output: 3\n",
      "  input layer: trainable // rec layer: trainable // TBPTT: True\n",
      " output layer: trainable\n",
      "  Act: tanh\n",
      "  Leaky updates, timescale 200 ms (old: 0.8 new: 0.2)\n",
      "Trainable parameters: 42,200\n",
      "W_input: (200, 7)\n",
      "W_rec: (200, 200)\n",
      "Parameter containing:\n",
      "tensor([[-0.0907,  0.0386,  0.1004,  ..., -0.0297,  0.1109, -0.0080],\n",
      "        [ 0.0431,  0.0440, -0.0225,  ..., -0.0978,  0.0192, -0.1177],\n",
      "        [ 0.1010, -0.0912,  0.0727,  ...,  0.0453,  0.0191, -0.1052],\n",
      "        ...,\n",
      "        [-0.0237,  0.0833,  0.0660,  ..., -0.0567,  0.0560,  0.0973],\n",
      "        [ 0.0150, -0.1000, -0.0031,  ..., -0.0620,  0.1083,  0.0902],\n",
      "        [-0.0898, -0.0494,  0.0960,  ..., -0.1124, -0.0268, -0.0007]],\n",
      "       requires_grad=True)\n",
      "W_output: (3, 200)\n",
      "b_hidden: (200,)\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 124, max_seq_len 124\n",
      "inputs_all paddled: (128, 124, 7)\n",
      "inputs_all: torch.Size([128, 124, 7])\n",
      "========== Setup Parameters ==========\n",
      "Train parameters:\n",
      "  Loss: MSE // LR: 1.00e-03 // Optim: adam\n",
      "  Grad type: backprop // Gradient clip: 1.0e+01\n",
      "Weight reg: L2, coef: 1.0e-04\n",
      "Activity reg: None\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 100, LR: 1.000e-03 - train_loss:1.237e-01, rounded train_acc:0.000, valid_loss:1.269e-01, rounded valid_acc:0.002\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 132, max_seq_len 132\n",
      "inputs_all paddled: (128, 132, 7)\n",
      "inputs_all: torch.Size([128, 132, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 200, LR: 1.000e-03 - train_loss:1.165e-01, rounded train_acc:0.000, valid_loss:1.230e-01, rounded valid_acc:0.002\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 133, max_seq_len 133\n",
      "inputs_all paddled: (128, 133, 7)\n",
      "inputs_all: torch.Size([128, 133, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 300, LR: 1.000e-03 - train_loss:1.239e-01, rounded train_acc:0.003, valid_loss:1.218e-01, rounded valid_acc:0.008\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 147, max_seq_len 147\n",
      "inputs_all paddled: (128, 147, 7)\n",
      "inputs_all: torch.Size([128, 147, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 400, LR: 1.000e-03 - train_loss:1.313e-01, rounded train_acc:0.000, valid_loss:1.209e-01, rounded valid_acc:0.002\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 132, max_seq_len 132\n",
      "inputs_all paddled: (128, 132, 7)\n",
      "inputs_all: torch.Size([128, 132, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 500, LR: 1.000e-03 - train_loss:1.114e-01, rounded train_acc:0.014, valid_loss:1.198e-01, rounded valid_acc:0.003\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 136, max_seq_len 136\n",
      "inputs_all paddled: (128, 136, 7)\n",
      "inputs_all: torch.Size([128, 136, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 600, LR: 1.000e-03 - train_loss:1.302e-01, rounded train_acc:0.000, valid_loss:1.255e-01, rounded valid_acc:0.002\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 135, max_seq_len 135\n",
      "inputs_all paddled: (128, 135, 7)\n",
      "inputs_all: torch.Size([128, 135, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 700, LR: 1.000e-03 - train_loss:1.187e-01, rounded train_acc:0.000, valid_loss:1.193e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 133, max_seq_len 133\n",
      "inputs_all paddled: (128, 133, 7)\n",
      "inputs_all: torch.Size([128, 133, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 800, LR: 1.000e-03 - train_loss:1.180e-01, rounded train_acc:0.002, valid_loss:1.209e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 137, max_seq_len 137\n",
      "inputs_all paddled: (128, 137, 7)\n",
      "inputs_all: torch.Size([128, 137, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 900, LR: 1.000e-03 - train_loss:1.227e-01, rounded train_acc:0.000, valid_loss:1.227e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 141, max_seq_len 141\n",
      "inputs_all paddled: (128, 141, 7)\n",
      "inputs_all: torch.Size([128, 141, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 1000, LR: 1.000e-03 - train_loss:1.214e-01, rounded train_acc:0.000, valid_loss:1.193e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 134, max_seq_len 134\n",
      "inputs_all paddled: (128, 134, 7)\n",
      "inputs_all: torch.Size([128, 134, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 1100, LR: 1.000e-03 - train_loss:1.293e-01, rounded train_acc:0.000, valid_loss:1.201e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 143, max_seq_len 143\n",
      "inputs_all paddled: (128, 143, 7)\n",
      "inputs_all: torch.Size([128, 143, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 1200, LR: 1.000e-03 - train_loss:1.164e-01, rounded train_acc:0.000, valid_loss:1.194e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 136, max_seq_len 136\n",
      "inputs_all paddled: (128, 136, 7)\n",
      "inputs_all: torch.Size([128, 136, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 1300, LR: 1.000e-03 - train_loss:1.235e-01, rounded train_acc:0.000, valid_loss:1.230e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 133, max_seq_len 133\n",
      "inputs_all paddled: (128, 133, 7)\n",
      "inputs_all: torch.Size([128, 133, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 1400, LR: 1.000e-03 - train_loss:1.331e-01, rounded train_acc:0.000, valid_loss:1.219e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 131, max_seq_len 131\n",
      "inputs_all paddled: (128, 131, 7)\n",
      "inputs_all: torch.Size([128, 131, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 1500, LR: 1.000e-03 - train_loss:1.270e-01, rounded train_acc:0.000, valid_loss:1.258e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 135, max_seq_len 135\n",
      "inputs_all paddled: (128, 135, 7)\n",
      "inputs_all: torch.Size([128, 135, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 1600, LR: 1.000e-03 - train_loss:1.336e-01, rounded train_acc:0.000, valid_loss:1.265e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 139, max_seq_len 139\n",
      "inputs_all paddled: (128, 139, 7)\n",
      "inputs_all: torch.Size([128, 139, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 1700, LR: 1.000e-03 - train_loss:1.266e-01, rounded train_acc:0.000, valid_loss:1.260e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 135, max_seq_len 135\n",
      "inputs_all paddled: (128, 135, 7)\n",
      "inputs_all: torch.Size([128, 135, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 1800, LR: 1.000e-03 - train_loss:1.314e-01, rounded train_acc:0.000, valid_loss:1.227e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 139, max_seq_len 139\n",
      "inputs_all paddled: (128, 139, 7)\n",
      "inputs_all: torch.Size([128, 139, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 1900, LR: 1.000e-03 - train_loss:1.336e-01, rounded train_acc:0.000, valid_loss:1.241e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 134, max_seq_len 134\n",
      "inputs_all paddled: (128, 134, 7)\n",
      "inputs_all: torch.Size([128, 134, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 2000, LR: 1.000e-03 - train_loss:1.224e-01, rounded train_acc:0.000, valid_loss:1.236e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 141, max_seq_len 141\n",
      "inputs_all paddled: (128, 141, 7)\n",
      "inputs_all: torch.Size([128, 141, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 2100, LR: 1.000e-03 - train_loss:1.303e-01, rounded train_acc:0.000, valid_loss:1.226e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 143, max_seq_len 143\n",
      "inputs_all paddled: (128, 143, 7)\n",
      "inputs_all: torch.Size([128, 143, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 2200, LR: 1.000e-03 - train_loss:1.293e-01, rounded train_acc:0.000, valid_loss:1.226e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 139, max_seq_len 139\n",
      "inputs_all paddled: (128, 139, 7)\n",
      "inputs_all: torch.Size([128, 139, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 2300, LR: 1.000e-03 - train_loss:1.224e-01, rounded train_acc:0.000, valid_loss:1.226e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 139, max_seq_len 139\n",
      "inputs_all paddled: (128, 139, 7)\n",
      "inputs_all: torch.Size([128, 139, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 2400, LR: 1.000e-03 - train_loss:1.336e-01, rounded train_acc:0.000, valid_loss:1.221e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 122, max_seq_len 122\n",
      "inputs_all paddled: (128, 122, 7)\n",
      "inputs_all: torch.Size([128, 122, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 2500, LR: 1.000e-03 - train_loss:1.255e-01, rounded train_acc:0.000, valid_loss:1.224e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 140, max_seq_len 140\n",
      "inputs_all paddled: (128, 140, 7)\n",
      "inputs_all: torch.Size([128, 140, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 2600, LR: 1.000e-03 - train_loss:1.260e-01, rounded train_acc:0.000, valid_loss:1.218e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 135, max_seq_len 135\n",
      "inputs_all paddled: (128, 135, 7)\n",
      "inputs_all: torch.Size([128, 135, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 2700, LR: 1.000e-03 - train_loss:1.212e-01, rounded train_acc:0.000, valid_loss:1.219e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 141, max_seq_len 141\n",
      "inputs_all paddled: (128, 141, 7)\n",
      "inputs_all: torch.Size([128, 141, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 2800, LR: 1.000e-03 - train_loss:1.312e-01, rounded train_acc:0.000, valid_loss:1.221e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 137, max_seq_len 137\n",
      "inputs_all paddled: (128, 137, 7)\n",
      "inputs_all: torch.Size([128, 137, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 2900, LR: 1.000e-03 - train_loss:1.246e-01, rounded train_acc:0.000, valid_loss:1.215e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 131, max_seq_len 131\n",
      "inputs_all paddled: (128, 131, 7)\n",
      "inputs_all: torch.Size([128, 131, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 3000, LR: 1.000e-03 - train_loss:1.144e-01, rounded train_acc:0.000, valid_loss:1.216e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 136, max_seq_len 136\n",
      "inputs_all paddled: (128, 136, 7)\n",
      "inputs_all: torch.Size([128, 136, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 3100, LR: 1.000e-03 - train_loss:1.245e-01, rounded train_acc:0.000, valid_loss:1.212e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 142, max_seq_len 142\n",
      "inputs_all paddled: (128, 142, 7)\n",
      "inputs_all: torch.Size([128, 142, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 3200, LR: 1.000e-03 - train_loss:1.216e-01, rounded train_acc:0.000, valid_loss:1.213e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 137, max_seq_len 137\n",
      "inputs_all paddled: (128, 137, 7)\n",
      "inputs_all: torch.Size([128, 137, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 3300, LR: 1.000e-03 - train_loss:1.279e-01, rounded train_acc:0.000, valid_loss:1.211e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 135, max_seq_len 135\n",
      "inputs_all paddled: (128, 135, 7)\n",
      "inputs_all: torch.Size([128, 135, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 3400, LR: 1.000e-03 - train_loss:1.320e-01, rounded train_acc:0.000, valid_loss:1.214e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 144, max_seq_len 144\n",
      "inputs_all paddled: (128, 144, 7)\n",
      "inputs_all: torch.Size([128, 144, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 3500, LR: 1.000e-03 - train_loss:1.340e-01, rounded train_acc:0.000, valid_loss:1.210e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 134, max_seq_len 134\n",
      "inputs_all paddled: (128, 134, 7)\n",
      "inputs_all: torch.Size([128, 134, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 3600, LR: 1.000e-03 - train_loss:1.262e-01, rounded train_acc:0.000, valid_loss:1.214e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 127, max_seq_len 127\n",
      "inputs_all paddled: (128, 127, 7)\n",
      "inputs_all: torch.Size([128, 127, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 3700, LR: 1.000e-03 - train_loss:1.290e-01, rounded train_acc:0.000, valid_loss:1.216e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 140, max_seq_len 140\n",
      "inputs_all paddled: (128, 140, 7)\n",
      "inputs_all: torch.Size([128, 140, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 3800, LR: 1.000e-03 - train_loss:1.278e-01, rounded train_acc:0.000, valid_loss:1.215e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 136, max_seq_len 136\n",
      "inputs_all paddled: (128, 136, 7)\n",
      "inputs_all: torch.Size([128, 136, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 3900, LR: 1.000e-03 - train_loss:1.268e-01, rounded train_acc:0.000, valid_loss:1.206e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 140, max_seq_len 140\n",
      "inputs_all paddled: (128, 140, 7)\n",
      "inputs_all: torch.Size([128, 140, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 4000, LR: 1.000e-03 - train_loss:1.164e-01, rounded train_acc:0.000, valid_loss:1.207e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 129, max_seq_len 129\n",
      "inputs_all paddled: (128, 129, 7)\n",
      "inputs_all: torch.Size([128, 129, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 4100, LR: 1.000e-03 - train_loss:1.287e-01, rounded train_acc:0.000, valid_loss:1.208e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 130, max_seq_len 130\n",
      "inputs_all paddled: (128, 130, 7)\n",
      "inputs_all: torch.Size([128, 130, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 4200, LR: 1.000e-03 - train_loss:1.172e-01, rounded train_acc:0.000, valid_loss:1.203e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 135, max_seq_len 135\n",
      "inputs_all paddled: (128, 135, 7)\n",
      "inputs_all: torch.Size([128, 135, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 4300, LR: 1.000e-03 - train_loss:1.276e-01, rounded train_acc:0.000, valid_loss:1.203e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 138, max_seq_len 138\n",
      "inputs_all paddled: (128, 138, 7)\n",
      "inputs_all: torch.Size([128, 138, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 4400, LR: 1.000e-03 - train_loss:1.161e-01, rounded train_acc:0.000, valid_loss:1.209e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 133, max_seq_len 133\n",
      "inputs_all paddled: (128, 133, 7)\n",
      "inputs_all: torch.Size([128, 133, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 4500, LR: 1.000e-03 - train_loss:1.210e-01, rounded train_acc:0.000, valid_loss:1.216e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 132, max_seq_len 132\n",
      "inputs_all paddled: (128, 132, 7)\n",
      "inputs_all: torch.Size([128, 132, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 4600, LR: 1.000e-03 - train_loss:1.269e-01, rounded train_acc:0.000, valid_loss:1.211e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 128, max_seq_len 128\n",
      "inputs_all paddled: (128, 128, 7)\n",
      "inputs_all: torch.Size([128, 128, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 4700, LR: 1.000e-03 - train_loss:1.250e-01, rounded train_acc:0.000, valid_loss:1.205e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 132, max_seq_len 132\n",
      "inputs_all paddled: (128, 132, 7)\n",
      "inputs_all: torch.Size([128, 132, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 4800, LR: 1.000e-03 - train_loss:1.240e-01, rounded train_acc:0.000, valid_loss:1.211e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 139, max_seq_len 139\n",
      "inputs_all paddled: (128, 139, 7)\n",
      "inputs_all: torch.Size([128, 139, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 4900, LR: 1.000e-03 - train_loss:1.176e-01, rounded train_acc:0.000, valid_loss:1.201e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [1.]\n",
      "Rule: delaydm1\n",
      "Rule delaydm1 seq_len 133, max_seq_len 133\n",
      "inputs_all paddled: (128, 133, 7)\n",
      "inputs_all: torch.Size([128, 133, 7])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 5000, LR: 1.000e-03 - train_loss:1.123e-01, rounded train_acc:0.000, valid_loss:1.208e-01, rounded valid_acc:0.000\n"
     ]
    }
   ],
   "source": [
    "# we use net at different training stage on the same test_input\n",
    "net, _, (counter_lst, netout_lst, db_lst, Winput_lst, Winputbias_lst, \\\n",
    "         Woutput_lst, Wall_lst, marker_lst, loss_lst, acc_lst), _ = net_helpers.train_network(params, \n",
    "                                                                                           device=device, \n",
    "                                                                                           verbose=verbose, \n",
    "                                                                                           train=train, \n",
    "                                                                                           hyp_dict=hyp_dict, \n",
    "                                                                                           netFunction=netFunction, \n",
    "                                                                                           test_input=[test_input],\n",
    "                                                                                           print_frequency=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b6856d2-8051-4bd0-87db-17457f116e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['W_input', 'W_rec', 'W_output', 'b_hidden']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trainable_parameters(model):\n",
    "    \"\"\"Return a list of (name, parameter) for trainable parameters.\"\"\"\n",
    "    return [(name) for name, p in model.named_parameters() if p.requires_grad]\n",
    "\n",
    "trainable_parameters(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47256480-5b47-496e-887e-b8de35dcc8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zihan.zhang/.conda/envs/mpn/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3558: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if train:\n",
    "    fig, ax = plt.subplots(1,1,figsize=(4,4))\n",
    "    ax.plot(net.hist['iters_monitor'][1:], net.hist['train_acc'][1:], color=c_vals[0], label='Full train accuracy')\n",
    "    ax.plot(net.hist['iters_monitor'][1:], net.hist['valid_acc'][1:], color=c_vals[1], label='Full valid accuracy')\n",
    "    if net.weight_reg is not None:\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['train_loss_output_label'], color=c_vals_l[0], zorder=-1, label='Output label')\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['train_loss_reg_term'], color=c_vals_l[0], zorder=-1, label='Reg term', linestyle='dashed')\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['valid_loss_output_label'], color=c_vals_l[1], zorder=-1, label='Output valid label')\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['valid_loss_reg_term'], color=c_vals_l[1], zorder=-1, label='Reg valid term', linestyle='dashed')\n",
    "    \n",
    "    ax.legend()\n",
    "    ax.set_ylabel('Accuracy', fontsize=15)\n",
    "    ax.set_xlabel('# Batches', fontsize=15)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"./onetask/loss_{hyp_dict['ruleset']}_{task_params['fixate_off']}.png\")\n",
    "    \n",
    "print('Done!')\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e544aca8-271f-49d3-9ed3-ab7023f23600",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    net_helpers.net_eta_lambda_analysis(net, net_params, hyp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b385cd9-3f03-4c33-a943-ef80ca4df69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyp_dict['chosen_network'] == \"dmpn\":\n",
    "    if net_params[\"input_layer_add\"]:\n",
    "        input_matrix = net.W_initial_linear.weight.data.detach().cpu().numpy()\n",
    "        figinp, axsinp = plt.subplots(1,1,figsize=(4,4))\n",
    "        sns.heatmap(input_matrix, ax=axsinp, square=True, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c10ab17-bab6-4228-9096-af042e9ac385",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "use_finalstage = False\n",
    "if use_finalstage:\n",
    "    # plotting output in the validation set\n",
    "    net_out, db = net.iterate_sequence_batch(test_input, run_mode='track_states')\n",
    "    W_output = net.W_output.detach().cpu().numpy()\n",
    "\n",
    "    W_all_ = []\n",
    "    for i in range(len(net.mp_layers)):\n",
    "        W_all_.append(net.mp_layers[i].W.detach().cpu().numpy())\n",
    "    W_ = W_all_[0]\n",
    "    \n",
    "else:\n",
    "    ind = len(marker_lst)-1\n",
    "    # ind = 0\n",
    "    network_at_percent = (marker_lst[ind]+1)/train_params['n_datasets']*100\n",
    "    print(f\"Using network at {network_at_percent}%\")\n",
    "    net_out = netout_lst[0][ind]\n",
    "    db = db_lst[0][ind]\n",
    "    W_output = Woutput_lst[ind]\n",
    "    if net_params[\"net_type\"] == \"dmpn\":\n",
    "        W_ = Wall_lst[ind][0]\n",
    "\n",
    "if net_params['loss_type'] in ('MSE',):\n",
    "    fig, axs = plt.subplots(5, 1, figsize=(6, 3*5))\n",
    "    figin, axsin = plt.subplots(5, 1, figsize=(6, 3*5))\n",
    "\n",
    "    if test_output_np.shape[-1] == 1:\n",
    "        for batch_idx, ax in enumerate(axs):\n",
    "            ax.plot(net_out[batch_idx, :, 0], color=c_vals[batch_idx])\n",
    "            ax.plot(test_output_np[batch_idx, :, 0], color=c_vals_l[batch_idx])\n",
    "\n",
    "    else:\n",
    "        for batch_idx, ax in enumerate(axs):\n",
    "            task_label = test_input_np[batch_idx, 0, 6-shift_index:]\n",
    "            output_batch = net_out[batch_idx,:,:]\n",
    "            task_label_index = np.where(np.isclose(task_label, 1, atol=0.1))[0][0]\n",
    "            for out_idx in range(test_output_np.shape[-1]):\n",
    "                axs[batch_idx].plot(net_out[batch_idx,:,out_idx], color=c_vals[out_idx+3])\n",
    "                axs[batch_idx].plot(test_output_np[batch_idx,:,out_idx], color=c_vals_l[out_idx+3], \n",
    "                                    linewidth=5, alpha=0.5)\n",
    "            axs[batch_idx].set_ylim([-1.2, 1.2])\n",
    "            axs[batch_idx].set_xlabel(\"Time Steps\", fontsize=15)\n",
    "            axs[batch_idx].set_ylabel(\"Output Magnitude\", fontsize=15)\n",
    "\n",
    "            input_batch = test_input[batch_idx,:,:].cpu().numpy()\n",
    "            for inp_idx in range(input_batch.shape[-1]):\n",
    "                axsin[batch_idx].plot(input_batch[:,inp_idx], color=c_vals[inp_idx], label=inp_idx)\n",
    "            # axsin[batch_idx].legend()\n",
    "            axsin[batch_idx].set_ylim([-1.2, 1.2])\n",
    "            axsin[batch_idx].set_xlabel(\"Time Steps\", fontsize=15)\n",
    "            axsin[batch_idx].set_ylabel(\"Input Magnitude\", fontsize=15)\n",
    "\n",
    "    fig.suptitle(f\"Validation Set Output Comparison using Network at Percentage {network_at_percent}%\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"./onetask/lowD_{hyp_dict['ruleset']}_{hyp_dict['chosen_network']}_{hyp_dict['addon_name']}_seed{seed}.png\", dpi=300)\n",
    "\n",
    "    figin.suptitle(f\"Validation Set Output Comparison using Network at Percentage {network_at_percent}%\")\n",
    "    figin.tight_layout()\n",
    "    figin.savefig(f\"./onetask/lowD_{hyp_dict['ruleset']}_{hyp_dict['chosen_network']}_{hyp_dict['addon_name']}_input_seed{seed}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7719a0a5-ac01-4064-bcc3-6296a240056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,1,figsize=(8,4*2))\n",
    "sns.heatmap(input_batch.T, ax=axs[0], cmap=\"coolwarm\")\n",
    "sns.heatmap(output_batch.T, ax=axs[1], cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9efbd2f-a48d-4f8e-ba55-b0b102463546",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_index = 0 # 1 layer MPN \n",
    "if net_params[\"input_layer_add\"]:\n",
    "    layer_index += 1 \n",
    "\n",
    "# here db is selected based on learning stage selection \n",
    "def modulation_extraction(db, max_seq_len, layer_index):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    n_batch_all_ = test_input.shape[0]\n",
    "    \n",
    "    Ms = np.concatenate((\n",
    "        db[f'M{layer_index}'].reshape(n_batch_all_, max_seq_len, -1),\n",
    "    ), axis=-1)\n",
    "\n",
    "    Ms_orig = np.concatenate((\n",
    "        db[f'M{layer_index}'],\n",
    "    ), axis=-1)\n",
    "\n",
    "    bs = np.concatenate((\n",
    "        db[f'b{layer_index}'],\n",
    "    ), axis=-1) \n",
    "\n",
    "    hs = np.concatenate((\n",
    "        db[f'hidden{layer_index}'].reshape(n_batch_all_, max_seq_len, -1),\n",
    "    ), axis=-1)\n",
    "\n",
    "    xs = np.concatenate((\n",
    "        db[f'input{layer_index}'].reshape(n_batch_all_, max_seq_len, -1),\n",
    "    ), axis=-1)\n",
    "\n",
    "    return Ms, Ms_orig, hs, bs, xs\n",
    "    \n",
    "if net_params[\"net_type\"] in (\"dmpn\", ):\n",
    "    if mpn_depth == 1:\n",
    "        Ms, Ms_orig, hs, bs, xs = modulation_extraction(db_lst[0][-1], max_seq_len, layer_index)\n",
    "        print(Ms_orig.shape)\n",
    "    else:\n",
    "        modulations, hiddens = [], []\n",
    "        for i in range(mpn_depth):\n",
    "            modulations.append(db[f'M{i}'].detach().cpu().numpy().reshape(n_batch_all, max_seq_len, -1))\n",
    "            hiddens.append(db[f'hidden{i}'].detach().cpu().numpy().reshape(n_batch_all, max_seq_len, -1),)\n",
    "\n",
    "        Ms = modulations[0]\n",
    "        hs = hiddens[0]\n",
    "        \n",
    "elif net_params[\"net_type\"] in (\"vanilla\", \"gru\"):\n",
    "    hs = db['hidden'].detach().cpu().numpy()\n",
    "\n",
    "pca_type = 'full' # full, cell_types\n",
    "pca_target_lst = ['hs', 'Ms'] # hs, 'Ms' \n",
    "if net_params[\"net_type\"] in (\"vanilla\", \"gru\"):\n",
    "    pca_target_lst = ['hs'] # if not dmpn, no M information effectively\n",
    "\n",
    "# using recorded information\n",
    "recordkyle_all, recordkyle_nameall = [], []\n",
    "for test_subtrial in test_trials:\n",
    "    metaepoch = test_subtrial.epochs\n",
    "    periodname = list(metaepoch.keys())\n",
    "    recordkyle, recordkyle_name = [], []\n",
    "    for keyiter in range(len(periodname)):\n",
    "        try:\n",
    "            recordkyle_name.append(periodname[keyiter])\n",
    "            if test_mode_for_all == \"random\":\n",
    "                recordkyle.append(metaepoch[periodname[keyiter]][1])\n",
    "            elif test_mode_for_all == \"random_batch\":\n",
    "                recordkyle.append(list(metaepoch[periodname[keyiter]][1]))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    if test_mode_for_all in (\"random\",):\n",
    "        fillrecordkyle = []\n",
    "        for timestamp in recordkyle:\n",
    "            fillrecordkyle.append([timestamp for _ in range(hs.shape[0])])\n",
    "        recordkyle = fillrecordkyle\n",
    "\n",
    "    recordkyle.insert(0, [0 for _ in range(len(recordkyle[1]))])\n",
    "    recordkyle = np.array(recordkyle).T.tolist()\n",
    "    recordkyle_all.extend(recordkyle)\n",
    "    recordkyle_nameall.append(recordkyle_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f41fdc5-88a2-459a-8ea5-8177aa1a9393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sep 30th\n",
    "# This part of code should be adaptive for multitask, which may have different breaks and periods for each task\n",
    "unique_lists = set(tuple(lst) for lst in recordkyle_all)\n",
    "# here select task specific information\n",
    "# which maximally should have length of number of tasks\n",
    "unique_recordkyle_all = [list(lst) for lst in unique_lists]\n",
    "if not task_random_fix:\n",
    "    assert len(unique_recordkyle_all) >= len(rules_dict[hyp_dict['ruleset']])\n",
    "else:\n",
    "    print(\"Test DataSet Random Seed Is Fixed\")\n",
    "\n",
    "all_session_breakdown = []\n",
    "for task_specific_time in unique_recordkyle_all:\n",
    "    session_breakdown = []\n",
    "    for sindex in range(0,len(task_specific_time)-1):\n",
    "        # all sessions should be the same for each task\n",
    "        # but different across tasks\n",
    "        # though the time of when response period starts might be similar across\n",
    "        session_breakdown.append([task_specific_time[sindex], task_specific_time[sindex+1]]) \n",
    "    session_breakdown.append([task_specific_time[0], task_specific_time[-1]])\n",
    "    all_session_breakdown.append(session_breakdown)\n",
    "\n",
    "# break down time\n",
    "all_breaks = []\n",
    "for session_breakdown in all_session_breakdown:\n",
    "    breaks = [cut[1] for cut in session_breakdown[:-1]]\n",
    "    print(f\"Task {all_session_breakdown.index(session_breakdown)}; breaks: {breaks}\")\n",
    "    all_breaks.append(breaks)\n",
    "\n",
    "# for delay-task\n",
    "assert len(all_breaks)\n",
    "response_start = all_breaks[0][-2]\n",
    "stimulus_start = all_breaks[0][0]\n",
    "stimulus_end = all_breaks[0][1]\n",
    "print(f\"response_start: {response_start}\")\n",
    "print(f\"stimulus_start: {stimulus_start}\")\n",
    "print(f\"stimulus_end: {stimulus_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5adc633-8a73-4e8b-8733-b4ce98482ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages_num = len(Wall_lst) # how many recorded neurons in total\n",
    "break_info = all_breaks[0]\n",
    "\n",
    "input_nums = Ms_orig.shape[-1]\n",
    "batch_nums = Ms_orig.shape[0]\n",
    "neuron_nums = Ms_orig.shape[2]\n",
    "colors = helper.generate_rainbow_colors(Ms_orig.shape[2])\n",
    "\n",
    "def generate_random_orthonormal_matrix(N, num_columns=3):\n",
    "    \"\"\"\n",
    "    generates an N x num_columns random matrix with orthonormal columns.\n",
    "    \"\"\"\n",
    "    random_matrix = np.random.randn(N, num_columns)    \n",
    "    Q, R = np.linalg.qr(random_matrix)    \n",
    "    return Q[:, :num_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6166f5-5166-4a76-bd2c-fa675fc6e0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e212e03-6fb4-439b-9fd6-5aefd8a09a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check from equation 2-7\n",
    "def plot_trajectory_by_index(label_index, stage_iter, verbose=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    W_ = Wall_lst[stage_iter][0]\n",
    "    W_output = Woutput_lst[stage_iter]\n",
    "    _, Ms_orig, _, bs, _ = modulation_extraction(db_lst[0][stage_iter], max_seq_len, layer_index) # batch * seq_len * hidden_neuron * input_neuron\n",
    "\n",
    "    if verbose:\n",
    "        figsize1, figsize2 = 3, 6\n",
    "        figexh1, axsexh1 = plt.subplots(3,3,figsize=(figsize2*3,figsize1*3))  \n",
    "        figexh2, axsexh2 = plt.subplots(4,3,figsize=(figsize2*3,figsize1*4))  \n",
    "        figdiff, axsdiff = plt.subplots(1,2,figsize=(4*2,2))\n",
    "\n",
    "    task_middle_dict = {}\n",
    "    task_labels_across_batch = []\n",
    "\n",
    "    saver_shape1 = (3,3)\n",
    "    saver1 = np.empty((test_input.shape[0], saver_shape1[0], saver_shape1[1]), dtype=object)\n",
    "    saver_shape2 = (4,3)\n",
    "    saver2 = np.empty((test_input.shape[0], saver_shape2[0]+1, saver_shape2[1]), dtype=object)\n",
    "    saver2_random = np.empty((test_input.shape[0], saver_shape2[0]+1, saver_shape2[1]), dtype=object) # projection to random space\n",
    "\n",
    "    random_output_Y_lst = [generate_random_orthonormal_matrix(W_output.shape[1]) for _ in range(10)]\n",
    "\n",
    "    for batch_iter in range(test_input.shape[0]):\n",
    "        writeon = 0\n",
    "        labels_for_batch = labels[batch_iter,0]\n",
    "        \n",
    "        if labels_for_batch in label_index: # >=0: for all label; ==0, say, for specific label on the ring (regardless on which task is using)\n",
    "            xx = test_input[batch_iter, :, :].cpu().numpy()[0,6-shift_index:]\n",
    "            which_task = np.where(xx)[0][0] # extract here, will repeat later at different time slices\n",
    "            \n",
    "            if labels_for_batch not in task_middle_dict.keys():\n",
    "                task_middle_dict[which_task] = []\n",
    "                writeon = 1\n",
    "                \n",
    "            x_batch_taskinfo = test_input[batch_iter, :, :][:,6-shift_index:].cpu().numpy()[0,:]\n",
    "            task_specific = np.where(x_batch_taskinfo == 1)[0]\n",
    "            assert len(task_specific) == 1\n",
    "            task_specific = task_specific[0]\n",
    "            \n",
    "            task_labels_across_batch.append(task_specific) # load task information (which task) across batches\n",
    "        \n",
    "            res_eq26, res_eq8, res_eq11 = [], [], []\n",
    "            res_meta = []\n",
    "\n",
    "            for i in range(saver_shape1[0]):\n",
    "                for j in range(saver_shape1[1]):\n",
    "                    saver1[batch_iter, i, j] = np.array([])\n",
    "        \n",
    "            for i in range(saver_shape2[0]+1):\n",
    "                for j in range(saver_shape2[1]):\n",
    "                    saver2[batch_iter, i, j] = np.array([])\n",
    "                    saver2_random[batch_iter, i, j] = np.array([])\n",
    "        \n",
    "            for time_iter in range(test_input.shape[1]):\n",
    "                x = test_input[batch_iter, time_iter, :].cpu().numpy().reshape(-1,1)\n",
    "                \n",
    "                input_length = len(x)\n",
    "        \n",
    "                x_fixon, x_fixoff, x_stimulus, x_task = [np.zeros((input_length, 1)) for _ in range(4)]\n",
    "                # one-hot encoded vector for fixation\n",
    "                x_fixon[0,0] = x[0,0] \n",
    "                # one-hot encoded vector for fixation off (set to dummy if not presented)\n",
    "                x_fixoff[1,0] = x[1,0] if task_params['fixate_off'] else 0\n",
    "                # one-hot encoded vector for stimulus\n",
    "                x_stimulus[2-shift_index:6-shift_index,0] = x[2-shift_index:6-shift_index,0]\n",
    "                # one-hot encoded vector for task\n",
    "                # task (dynamically setting for all element after the 6th elements)\n",
    "                tasks_info = x[6-shift_index:,0]\n",
    "                x_task[6-shift_index:,0] = tasks_info\n",
    "\n",
    "                which_task = np.where(tasks_info)[0][0]\n",
    "                \n",
    "                Mt = Ms_orig[batch_iter, time_iter, :, :] \n",
    "                bt = bs[batch_iter, time_iter, :].reshape(-1,1) # hidden_neuron * 1\n",
    "                \n",
    "                middle =  W_ + W_ * Mt\n",
    "\n",
    "                if time_iter >= response_start + 1 and len(label_index) == 1:\n",
    "                    if writeon:\n",
    "                        task_middle_dict[which_task].append(middle)\n",
    "                \n",
    "                y_fix = W_output[0,:].reshape(1,-1)\n",
    "                Y_resp1 = W_output[1,:].reshape(1,-1)\n",
    "                Y_resp2 = W_output[2,:].reshape(1,-1)\n",
    "\n",
    "                if task_params['fixate_off']:\n",
    "                    allX1 = [x_fixon+x_task, x_fixoff+x_task, x_stimulus+x_fixon+x_task]\n",
    "                else:\n",
    "                    allX1 = [x_fixon+x_task, x_task, x_stimulus+x_fixon+x_task]\n",
    "                allX1name = [\"x_fixon+x_task\", \"x_fixoff+x_task\", \"x_stimulus+x_fixon+x_task\"]\n",
    "                allX2 = [x_fixon, x_fixoff, x_stimulus, x_task]\n",
    "                allX2name = [\"x_fixon\", \"x_fixoff\", \"x_stimulus\", \"x_task\"]\n",
    "                allY = [y_fix, Y_resp1, Y_resp2]\n",
    "                allYname = [\"y_fix\", \"Y_resp1\", \"Y_resp2\"]\n",
    "        \n",
    "                for yiter in range(len(allY)):\n",
    "                    for xiter in range(len(allX1)):\n",
    "                        # res1 = helper.to_unit_vector(allY[yiter]) @ helper.to_unit_vector(middle @ allX1[xiter])\n",
    "                        # print(middle.shape, allX1[xiter].shape)\n",
    "                        step1 = middle @ allX1[xiter] + bt # adjust according to specific bias \n",
    "                        res1 = allY[yiter] @ step1 \n",
    "                        saver1[batch_iter, xiter, yiter] = np.append(saver1[batch_iter, xiter, yiter], res1[0,0])\n",
    "        \n",
    "                for y1 in range(len(allY)):\n",
    "                    for x1 in range(len(allX2)):\n",
    "                        # res2 = helper.to_unit_vector(allY[yiter]) @ helper.to_unit_vector(middle @ allX2[xiter])\n",
    "                        step1 = middle @ allX2[x1]\n",
    "                        res2 = allY[y1] @ step1\n",
    "                        res2_random = [((random_output_Y[:,y1].reshape(1,-1)) @ middle @ allX2[x1])[0,0] \n",
    "                                       for random_output_Y in random_output_Y_lst]\n",
    "                        \n",
    "                        saver2[batch_iter, x1, y1] = np.append(saver2[batch_iter, x1, y1], res2[0,0])\n",
    "                        saver2_random[batch_iter, x1, y1] = np.append(saver2_random[batch_iter, x1, y1], np.mean(res2_random))\n",
    "\n",
    "                # how about bias projection to output\n",
    "                for y_iter2 in range(len(allY)):\n",
    "                    step1 = bt \n",
    "                    res2 = allY[y_iter2] @ step1\n",
    "                    saver2[batch_iter, len(allX2), y_iter2] = np.append(saver2[batch_iter, len(allX2), y_iter2], res2[0,0])\n",
    "\n",
    "            if verbose:\n",
    "                for i in range(saver_shape1[0]):\n",
    "                    for j in range(saver_shape1[1]):\n",
    "                        axsexh1[i,j].plot(saver1[batch_iter,i,j], color=c_vals[labels_for_batch], linestyle=l_vals[task_specific])\n",
    "            \n",
    "                for i in range(saver_shape2[0]):\n",
    "                    for j in range(saver_shape2[1]):            \n",
    "                        axsexh2[i,j].plot(saver2[batch_iter,i,j], color=c_vals[labels_for_batch], linestyle=l_vals[task_specific])\n",
    "        \n",
    "                # # extract fixon-task information explicitly\n",
    "                axsdiff[0].plot(saver2[batch_iter,0,1] + saver2[batch_iter,3,1], color=c_vals[labels_for_batch], linestyle=l_vals[task_specific])\n",
    "                axsdiff[0].plot(saver2_random[batch_iter,0,1] + saver2_random[batch_iter,3,1], color=c_vals_l[labels_for_batch], linestyle=l_vals[task_specific])\n",
    "                \n",
    "                axsdiff[1].plot(saver2[batch_iter,0,2] + saver2[batch_iter,3,2], color=c_vals[labels_for_batch], linestyle=l_vals[task_specific])\n",
    "                axsdiff[1].plot(saver2_random[batch_iter,0,2] + saver2_random[batch_iter,3,2], color=c_vals_l[labels_for_batch], linestyle=l_vals[task_specific])\n",
    "\n",
    "    if verbose:\n",
    "        # plot fixon/task information for one specific stimulus on one figure\n",
    "        # show perfect cancellation until fixon info goes away (during response period)\n",
    "        figpaper, axspaper = plt.subplots(8,1,figsize=(6,figsize1*8))\n",
    "\n",
    "        temp_saver = []\n",
    "\n",
    "        for batch_iter in range(test_input.shape[0]):\n",
    "            labels_for_batch = labels[batch_iter,0]\n",
    "            if labels_for_batch in label_index and labels_for_batch not in temp_saver:\n",
    "                f_fixon, f_task, f_bias = saver2[batch_iter, 0, 1], saver2[batch_iter, 3, 1], saver2[batch_iter, -1, 1]\n",
    "                axspaper[len(temp_saver)].plot(f_fixon, color=c_vals[0], linestyle=l_vals[0], label=\"Fixon\")\n",
    "                axspaper[len(temp_saver)].plot(f_task+f_bias, color=c_vals[1], linestyle=l_vals[1], label=\"Task\")\n",
    "                axspaper[len(temp_saver)].plot(f_fixon+f_task+f_bias, color=c_vals[2], linestyle=l_vals[3], linewidth=3, \\\n",
    "                                                       label=\"Combine\")\n",
    "                axspaper[len(temp_saver)].axhline(0, color=c_vals[3])\n",
    "                axspaper[len(temp_saver)].set_xlabel(\"Timestep\", fontsize=15)\n",
    "                axspaper[len(temp_saver)].set_ylabel(\"Modulation Component\", fontsize=15)\n",
    "\n",
    "                temp_saver.append(labels_for_batch)\n",
    "\n",
    "        for axsp in axspaper:\n",
    "            axsp.legend(loc=\"best\", frameon=True, fontsize=15)\n",
    "            axsp.set_ylim([-2.0, 2.0])\n",
    "        figpaper.tight_layout()   \n",
    "        figpaper.savefig(f\"./onetask/show_seed{seed}.png\", dpi=300)\n",
    "        \n",
    "        for i in range(saver_shape1[0]):\n",
    "            for j in range(saver_shape1[1]):\n",
    "                axsexh1[i,j].set_ylim([-1.2, 1.2])\n",
    "                axsexh1[i,j].set_title(f\"{allX1name[i]} & {allYname[j]}\")\n",
    "        \n",
    "        for i in range(saver_shape2[0]):\n",
    "            for j in range(saver_shape2[1]):\n",
    "                axsexh2[i,j].set_ylim([-1.2, 1.2])\n",
    "                axsexh2[i,j].set_title(f\"{allX2name[i]} & {allYname[j]}\")\n",
    "        \n",
    "        for ax in np.concatenate((axsexh1.flatten(), axsexh2.flatten())):\n",
    "            for breaks in all_breaks:\n",
    "                for bb in breaks:\n",
    "                    ax.axvline(bb, linestyle=\"--\", c=c_vals[all_breaks.index(breaks)])\n",
    "    \n",
    "        label_index_name = \"all\" if len(label_index) == 8 else label_index\n",
    "        \n",
    "        figexh1.suptitle(f\"Exhaustive Search 1 {color_by} at Stage {stage_iter}\")\n",
    "        figexh1.tight_layout()\n",
    "        figexh1.savefig(f\"./onetask/es1_{task_params['fixate_off']}_{network_at_percent}_{label_index_name}_seed{seed}.png\", dpi=300)\n",
    "        \n",
    "        figexh2.suptitle(f\"Exhaustive Search 2 {color_by} Stage {stage_iter}\")\n",
    "        figexh2.tight_layout()\n",
    "        figexh2.savefig(f\"./onetask/es2_{task_params['fixate_off']}_{network_at_percent}_{label_index_name}_seed{seed}.png\", dpi=300)\n",
    "\n",
    "        axsdiff[0].set_title(\"Stimulus 1\")\n",
    "        axsdiff[1].set_title(\"Stimulus 2\")\n",
    "        figdiff.suptitle(f\"Fixon-Task at Stage {stage_iter}\")\n",
    "        figdiff.tight_layout()\n",
    "        figdiff.savefig(f\"./onetask/diff_{task_params['fixate_off']}_{network_at_percent}_{label_index_name}_seed{seed}.png\", dpi=300)\n",
    "\n",
    "    return task_middle_dict if len(label_index) == 1 else {}, task_labels_across_batch, saver2, saver2_random # only do it for single task learnig for clarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954cbe71-7953-48eb-9f2f-af7d5b3be088",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trajectory, all_trajectory_random = [], []\n",
    "for stage_iter in range(stages_num):\n",
    "    task_middle_dict, task_labels_across_batch, save_trajectory, save_trajectory_random = plot_trajectory_by_index(np.unique(labels), \n",
    "                                                                                                                   stage_iter, \n",
    "                                                                                                                   verbose=(stage_iter==stages_num-1))\n",
    "    all_trajectory.append(save_trajectory)\n",
    "    all_trajectory_random.append(save_trajectory_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d7d96a-195a-4b05-adda-ef5af48cc49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_trajectory(save_trajectory, save_trajectory_random):\n",
    "    \"\"\"\n",
    "    Analyze trajectories by calculating mean absolute values for fixations and tasks.\n",
    "    \"\"\"\n",
    "    def process_trajectory(trajectory, ind=False):\n",
    "        results = []\n",
    "        for batch in trajectory:\n",
    "            stim1_fixon = batch[0, 1][stimulus_start:response_start]\n",
    "            stim1_task = batch[3, 1][stimulus_start:response_start]\n",
    "            if ind: \n",
    "                bias = batch[4, 1][stimulus_start:response_start]\n",
    "            else:\n",
    "                bias = np.zeros_like(stim1_fixon.shape)\n",
    "                \n",
    "            results.append([np.mean(np.abs(stim1_fixon + stim1_task + bias)), np.mean(np.abs(stim1_fixon)), np.mean(np.abs(stim1_task))])\n",
    "        return np.array(results)\n",
    "    \n",
    "    # Process both trajectories\n",
    "    result = process_trajectory(save_trajectory, True)\n",
    "    result_random = process_trajectory(save_trajectory_random)\n",
    "    \n",
    "    # Return the mean of the computed values\n",
    "    return np.mean(result[:, 0]), np.mean(result[:, 1]), np.mean(result[:,2]), np.mean(result_random[:, 0]), np.mean(result_random[:, 1]), np.mean(result_random[:, 2])\n",
    "\n",
    "fixon_task_diff = np.array([analyze_trajectory(all_trajectory[i], all_trajectory_random[i]) for i in range(len(all_trajectory_random))])\n",
    "\n",
    "figfixontaskcancel, axsfixontaskcancel = plt.subplots(figsize=(6,3))\n",
    "axsfixontaskcancel.plot(counter_lst, fixon_task_diff[:,0], \"-o\", c=c_vals[0], \n",
    "                        label=r\"$\\text{|Fix - Task|}$\")\n",
    "axsfixontaskcancel.plot(counter_lst, fixon_task_diff[:,1], \"-o\", c=c_vals[1], \n",
    "                        label=r\"$\\text{|Fix|}$\")\n",
    "axsfixontaskcancel.plot(counter_lst, fixon_task_diff[:,2], \"-o\", c=c_vals[2], \n",
    "                        label=r\"$\\text{|Task|}$\")\n",
    "# axsfixontaskcancel[0].plot(fixon_task_diff[:,3], \"-o\", c=c_vals_l[0], label=\"abs(fixon-task) random\")\n",
    "# axsfixontaskcancel[0].plot(fixon_task_diff[:,4], \"-o\", c=c_vals_l[1], label=\"abs(fixon) random\")\n",
    "# axsfixontaskcancel[0].plot(fixon_task_diff[:,5], \"-o\", c=c_vals_l[2], label=\"abs(task) random\")\n",
    "axsfixontaskcancel.legend(loc=\"best\", fontsize=15, frameon=True)\n",
    "axsfixontaskcancel.set_ylabel(\"Magnitude Projection \\nMagnitude\", fontsize=15)\n",
    "# axsfixontaskcancel.set_title(\"Average Cancellation Effect Before Response Period\")\n",
    "\n",
    "axsfixontaskcancel.set_xlabel(\"# Dataset\", fontsize=15)\n",
    "axsfixontaskcancel.set_xscale(\"log\")\n",
    "axsfixontaskcancel.tick_params(axis='y', labelsize=12)\n",
    "axsfixontaskcancel.tick_params(axis='x', labelsize=12)\n",
    "figfixontaskcancel.tight_layout()\n",
    "figfixontaskcancel.savefig(f\"./onetask/cancel_seed{seed}.png\", dpi=300)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(6,3))  # Create a new figure\n",
    "\n",
    "ax1.plot(counter_lst, loss_lst, \"-o\", c=c_vals[0])\n",
    "ax1.set_ylabel(\"MSE Loss\", color=c_vals[0], fontsize=15)\n",
    "ax1.tick_params(axis='y', colors=c_vals[0], labelsize=12)\n",
    "ax1.set_yscale(\"log\")  # Keep log scale for loss\n",
    "ax1.set_xlabel(\"Counter\")\n",
    "\n",
    "# Create a second y-axis for accuracy (right)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(counter_lst, acc_lst, \"-o\", c=c_vals[1])\n",
    "ax2.axhline(y=1/8, linestyle=\"--\", label=\"By Chance\")\n",
    "ax2.set_ylabel(\"Accuracy\", color=c_vals[1], fontsize=15)\n",
    "ax2.tick_params(axis='y', colors=c_vals[1], labelsize=12)\n",
    "ax2.legend(loc='best', frameon=True, fontsize=15)\n",
    "\n",
    "ax1.set_xlabel(\"# Dataset\", fontsize=15)\n",
    "ax1.tick_params(axis='x', labelsize=12)\n",
    "ax1.set_xscale(\"log\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"./onetask/loss_acc_seed{seed}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80c1aa8-307f-4c84-9255-4e9d7cdc75db",
   "metadata": {},
   "outputs": [],
   "source": [
    "modulation_dict_diff_lst = []\n",
    "modulation_dict_lst = []\n",
    "hidden_output_dict_lst = []\n",
    "hidden_dict_lst = []\n",
    "hidden_all_dict_lst = []\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=stages_num,\n",
    "    specs=[[{'type': 'scatter3d'}] * stages_num],\n",
    "    subplot_titles=[f\"Stage {i+1}\" for i in range(stages_num)]  # Add titles for each subplot\n",
    ")\n",
    "\n",
    "camera = dict(\n",
    "    eye=dict(x=1.25, y=1.25, z=1.25),  # Position of the camera\n",
    "    up=dict(x=0, y=0, z=1),            # Orientation of the camera\n",
    "    center=dict(x=0, y=0, z=0),        # Focal point of the camera\n",
    ")\n",
    "\n",
    "for stage_iter in range(stages_num):\n",
    "    Woutput = Woutput_lst[stage_iter]\n",
    "    _, Ms_orig, hs, bs, _ = modulation_extraction(db_lst[0][stage_iter], max_seq_len, layer_index)\n",
    "    \n",
    "    hs_stimulus = hs[:,stimulus_start:stimulus_end,:]\n",
    "\n",
    "    Ms_fix = Ms_orig[:,:stimulus_start,:,:]\n",
    "    Ms_stimulus = Ms_orig[:,stimulus_start:stimulus_end,:,:]\n",
    "    Ms_delay = Ms_orig[:,stimulus_end:response_start,:,:]\n",
    "    Ms_response = Ms_orig[:,response_start:,:,:]\n",
    "    \n",
    "    Ms_all = [Ms_fix, Ms_stimulus, Ms_delay, Ms_response]\n",
    "\n",
    "    modulation_diff_dict, modulation_dict, hidden_output_dict, hidden_dict, hidden_all_dict = {}, {}, {}, {}, {}\n",
    "    \n",
    "    for batch_iter in range(batch_nums):\n",
    "        hs_stimulus_batch = hs_stimulus[batch_iter,:,:]\n",
    "        hs_stimulus_batch_output = hs_stimulus_batch @ Woutput.T\n",
    "\n",
    "        trace = go.Scatter3d(\n",
    "            x=hs_stimulus_batch_output[:, 0],\n",
    "            y=hs_stimulus_batch_output[:, 1],\n",
    "            z=hs_stimulus_batch_output[:, 2],\n",
    "            mode='lines+markers', \n",
    "            line=dict(color=c_vals[labels[batch_iter][0]], width=2),  \n",
    "            marker=dict(size=5, symbol='circle'), \n",
    "            name=f\"Batch {batch_iter} - Stage {stage_iter}\",\n",
    "            showlegend=False \n",
    "        )\n",
    "        fig.add_trace(trace, row=1, col=stage_iter + 1)\n",
    "        \n",
    "        # modulation change for different periods (end - start)\n",
    "        Ms_fixon = [Ms_[batch_iter,-1,:,0] - Ms_[batch_iter,0,:,0] for Ms_ in Ms_all]\n",
    "        Ms_stimulus_task = Ms_stimulus[batch_iter,-1,:,-1] - Ms_stimulus[batch_iter,0,:,-1]\n",
    "        \n",
    "        modulation_diff_dict[labels[batch_iter,0]] = Ms_fixon # change of modulation on fixon during stimulus period\n",
    "        modulation_dict[labels[batch_iter,0]] =  Ms_stimulus[batch_iter,-1,:,0]\n",
    "        hidden_output_dict[labels[batch_iter,0]] = hs_stimulus_batch_output\n",
    "        hidden_dict[labels[batch_iter,0]] = hs_stimulus_batch[-1,:]\n",
    "        hidden_all_dict[labels[batch_iter,0]] = hs_stimulus_batch\n",
    "\n",
    "    modulation_dict_diff_lst.append(modulation_diff_dict)\n",
    "    modulation_dict_lst.append(modulation_dict)\n",
    "    hidden_output_dict_lst.append(hidden_output_dict)\n",
    "    hidden_dict_lst.append(hidden_dict)\n",
    "    hidden_all_dict_lst.append(hidden_all_dict)\n",
    "\n",
    "for stage_iter in range(stages_num):\n",
    "    fig.update_layout(\n",
    "        **{\n",
    "            f\"scene{stage_iter + 1}\": dict(\n",
    "                xaxis=dict(range=[-1.3, 1.3], title=\"X\"),\n",
    "                yaxis=dict(range=[-1.3, 1.3], title=\"Y\"),\n",
    "                zaxis=dict(range=[-1.3, 1.3], title=\"Z\"),\n",
    "                aspectmode='cube',  \n",
    "                camera=camera,       \n",
    "                domain=dict(\n",
    "                    x=[stage_iter / stages_num, (stage_iter + 1) / stages_num - 0.02],  \n",
    "                    y=[0, 1]  \n",
    "                )\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"3D Interactive Plot for Different Stages\",\n",
    "    height=600, \n",
    "    width=600 * stages_num, \n",
    "    margin=dict(l=10, r=10, t=50, b=10),  \n",
    ")\n",
    "\n",
    "output_file = \"./save/3d_interactive_plot_compact.html\"\n",
    "# fig.write_html(output_file)\n",
    "\n",
    "print(f\"Plot saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb427a04-1279-4f63-867b-87dee94a7745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "modulation_change_stage, m_corr_stage, h_corr_stage = [[],[],[],[]], [], []\n",
    "fig_hc, axs_hc = plt.subplots(2,1,figsize=(6,3*2))\n",
    "\n",
    "def binarize(arr, threshold):\n",
    "    \"\"\"\"\"\"\n",
    "    return (np.abs(arr) > threshold).astype(int)\n",
    "\n",
    "def normalized_participation_ratio(cov_matrix):\n",
    "    \"\"\"\"\"\"\n",
    "    eigenvalues = np.linalg.eigvalsh(cov_matrix)  \n",
    "    sum_eigen = np.sum(eigenvalues)\n",
    "    sum_eigen_sq = np.sum(eigenvalues ** 2)    \n",
    "    N = len(eigenvalues)\n",
    "    npr = (sum_eigen ** 2) / (N * sum_eigen_sq)\n",
    "    return npr\n",
    "    \n",
    "for i in range(stages_num):\n",
    "    def analyze_hm_change(lst, index=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        md = lst[i]\n",
    "        if index is None: \n",
    "            md_m = [np.array(value) for value in md.values()]\n",
    "        else:\n",
    "            md_m = [np.array(value[index]) for value in md.values()]\n",
    "        md_m = np.column_stack(md_m)\n",
    "        md_m = md_m.T # num_stimulus * hidden_size\n",
    "        mc_stage = list(np.mean(np.abs(md_m), axis=1))\n",
    "        synptic_corr = cosine_similarity(md_m)     \n",
    "        mean_corr = np.nanmean(np.triu(synptic_corr))\n",
    "        return mean_corr, mc_stage, md_m\n",
    "        \n",
    "    m_mean_corr, _, md_m = analyze_hm_change(modulation_dict_lst)\n",
    "    h_mean_corr, _, md_h = analyze_hm_change(hidden_dict_lst)\n",
    "        \n",
    "    m_corr_stage.append(m_mean_corr)\n",
    "    h_corr_stage.append(h_mean_corr)\n",
    "    md_m_diff_stim = None\n",
    "    for p in range(4):\n",
    "        _, mc_stage, md_m_diff = analyze_hm_change(modulation_dict_diff_lst, p)\n",
    "        if p == 1:\n",
    "            md_m_diff_stim = md_m_diff\n",
    "        elif p == 3:\n",
    "            md_m_diff_response = md_m_diff\n",
    "        modulation_change_stage[p].append(mc_stage)\n",
    "\n",
    "    if i == stages_num - 1:\n",
    "        sns.heatmap(md_m_diff_stim, ax=axs_hc[0], cmap=\"coolwarm\")\n",
    "        sns.heatmap(md_m_diff_response, ax=axs_hc[1], cmap=\"coolwarm\")\n",
    "        for ax_hc in axs_hc: \n",
    "            ax_hc.set_xticks([])\n",
    "            ax_hc.set_yticks([])\n",
    "            ax_hc.set_xlabel(\"Hidden\", fontsize=15)\n",
    "            ax_hc.set_ylabel(\"Stimuli\", fontsize=15)\n",
    "        fig_hc.tight_layout()\n",
    "        fig_hc.savefig(f\"./onetask/modulation_heatmap_seed{seed}.png\", dpi=300)\n",
    "\n",
    "modulation_change_stage = np.array(modulation_change_stage)\n",
    "period_names = [\"Fixation\", \"Stimulus\", \"Delay\", \"Response\"]\n",
    "\n",
    "figmc,axsmc = plt.subplots(3,1,figsize=(6,3*3))\n",
    "for i in range(4):\n",
    "    mcs = modulation_change_stage[i]\n",
    "    print(mcs.shape)\n",
    "    axsmc[0].plot(counter_lst, np.mean(mcs, axis=1), \"-o\", c=c_vals[i], label=period_names[i])\n",
    "    axsmc[0].fill_between(counter_lst, np.mean(mcs, axis=1) - np.std(mcs, axis=1), \\\n",
    "                                       np.mean(mcs, axis=1) + np.std(mcs, axis=1), color=c_vals_l[i])\n",
    "axsmc[0].set_ylabel(\"Change of Modulation\", fontsize=15)\n",
    "axsmc[0].legend(loc=\"best\", frameon=True, fontsize=15)\n",
    "axsmc[1].plot(counter_lst, m_corr_stage/m_corr_stage[0], \"-o\")\n",
    "axsmc[1].set_ylabel(\"Average Synaptic Correlation \\nbetween Stimulus\", fontsize=15)\n",
    "axsmc[2].plot(counter_lst, h_corr_stage/h_corr_stage[0], \"-o\")\n",
    "axsmc[2].set_ylabel(\"Postsynaptic Activity Correlation \\nbetween Stimulus\", fontsize=15)\n",
    "\n",
    "\n",
    "def save_dict_with_count_npz(directory, data_dict, it=\"\"):\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    # Count existing files in the directory\n",
    "    file_count = len([f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))])\n",
    "\n",
    "    # Generate filename based on file count\n",
    "    filename = os.path.join(directory, f\"{it}{file_count}.npz\")\n",
    "\n",
    "    # Save dictionary as an NPZ file\n",
    "    np.savez(filename, **data_dict)\n",
    "\n",
    "    print(f\"Dictionary saved as: {filename}\")\n",
    "\n",
    "\n",
    "data_json = {\"counter_lst\": counter_lst, \"m_corr_stage\": m_corr_stage/m_corr_stage[0], \"h_corr_stage\": h_corr_stage/h_corr_stage[0]}\n",
    "save_dict_with_count_npz(\"./onetask_data\", data_json, it=\"corr\")\n",
    "\n",
    "for ax in axsmc:\n",
    "    ax.set_xlabel(\"# Dataset\", fontsize=15)\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.tick_params(axis=\"y\", labelsize=12)\n",
    "    ax.tick_params(axis=\"x\", labelsize=12)\n",
    "    \n",
    "figmc.tight_layout()\n",
    "figmc.savefig(f\"./onetask/modulation_change_seed{seed}.png\", dpi=300)\n",
    "\n",
    "def traj_length(array):\n",
    "    \"\"\"\"\"\"\n",
    "    diffs = np.diff(array, axis=0)  \n",
    "    return np.sum(np.linalg.norm(diffs, axis=1))\n",
    "\n",
    "hidden_length_all = []\n",
    "for stage_iter in range(stages_num):\n",
    "    hidden_stage = hidden_output_dict_lst[stage_iter] \n",
    "    hidden_stage = {k: hidden_stage[k] for k in sorted(hidden_stage.keys())}\n",
    "    hidden_length = [traj_length(arr) for arr in hidden_stage.values()]    \n",
    "    hidden_length_all.append(hidden_length)\n",
    "    \n",
    "hidden_length_all = np.array(hidden_length_all)\n",
    "figt, axst = plt.subplots(figsize=(6,3))\n",
    "for i in range(hidden_length_all.shape[1]):\n",
    "    axst.plot(counter_lst, hidden_length_all[:,i], \"-o\", c=c_vals[i])\n",
    "axst.set_xlabel(\"# Dataset\", fontsize=15)\n",
    "axst.set_xscale(\"log\")\n",
    "axst.set_ylabel(\"Length of Hidden \\nState Trajectory\", fontsize=15)\n",
    "axst.tick_params(axis=\"y\", labelsize=12)\n",
    "axst.tick_params(axis=\"x\", labelsize=12)\n",
    "figt.tight_layout()\n",
    "figt.savefig(f\"./onetask/length_hidden_state_seed{seed}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904b7833-7825-487f-9a02-f5938cbecbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os \n",
    "\n",
    "def load_all_npz_files(directory):\n",
    "    npz_files = sorted(glob.glob(os.path.join(directory, \"*.npz\")))\n",
    "\n",
    "    data_list = []\n",
    "    for file in npz_files:\n",
    "        data = np.load(file)  \n",
    "        data_list.append({key: data[key] for key in data.files})  \n",
    "\n",
    "        print(f\"Loaded: {file}\") \n",
    "\n",
    "    return data_list\n",
    "\n",
    "plotall = True \n",
    "if plotall:\n",
    "    directory = \"./onetask_data\"\n",
    "    all_data = load_all_npz_files(directory)\n",
    "    counter_lst_all, m_corr_all, h_corr_all = [], [], []\n",
    "    for i, data in enumerate(all_data):\n",
    "        counter_lst_all.append(data[\"counter_lst\"])\n",
    "        m_corr_all.append(data[\"m_corr_stage\"])\n",
    "        h_corr_all.append(data[\"h_corr_stage\"])\n",
    "\n",
    "    counter_lst_all = np.array(counter_lst_all)\n",
    "    m_corr_all = np.array(m_corr_all)\n",
    "    h_corr_all = np.array(h_corr_all)\n",
    "\n",
    "    mean_counter = np.mean(counter_lst_all, axis=0)\n",
    "    mean_m_corr = np.mean(m_corr_all, axis=0)\n",
    "    std_m_corr = np.std(m_corr_all, axis=0)\n",
    "    \n",
    "    mean_h_corr = np.mean(h_corr_all, axis=0)\n",
    "    std_h_corr = np.std(h_corr_all, axis=0)\n",
    "    \n",
    "    figmcall, axsmcall = plt.subplots(2,1,figsize=(6,3*2))\n",
    "    axsmcall[0].plot(mean_counter, mean_m_corr, \"-o\", label=\"Mean m_corr\", color=c_vals[0])\n",
    "    axsmcall[0].fill_between(mean_counter, mean_m_corr - std_m_corr, mean_m_corr + std_m_corr, \n",
    "                             color=c_vals_l[0], alpha=0.2)\n",
    "    axsmcall[0].set_ylabel(\"Cos of Modulation\", fontsize=15)\n",
    "    axsmcall[1].plot(mean_counter, mean_h_corr, \"-o\", label=\"Mean h_corr\", color=c_vals[0])\n",
    "    axsmcall[1].fill_between(mean_counter, mean_h_corr - std_h_corr, mean_h_corr + std_h_corr, \n",
    "                             color=c_vals_l[0], alpha=0.2)\n",
    "    axsmcall[1].set_ylabel(\"Cos of Hidden Activity\", fontsize=15)\n",
    "    for ax in axsmcall:\n",
    "        ax.set_xlabel(\"# Dataset\", fontsize=15)\n",
    "        ax.set_xscale(\"log\")\n",
    "        ax.tick_params(axis=\"y\", labelsize=12)\n",
    "        ax.tick_params(axis=\"x\", labelsize=12)\n",
    "    figmcall.tight_layout()\n",
    "    figmcall.savefig(f\"./onetask/modulation_analysis_during_learning_seed{seed}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c697459a-40b7-4410-b4de-e651462f45df",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = Wall_lst[-1][0]\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ffac89-2d1b-460f-a42d-a782c3940f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixon_task_projoutput = []\n",
    "for stage_iter in range(stages_num):\n",
    "    W = Wall_lst[stage_iter][0]\n",
    "    W_output = Woutput_lst[stage_iter]\n",
    "\n",
    "    _, Ms_orig, _, bs, _ = modulation_extraction(db_lst[0][stage_iter], max_seq_len, layer_index) # batch * seq_len * hidden_neuron * input_neuron\n",
    "    bias = np.mean(bs, axis=0)\n",
    "\n",
    "    W_output_random_lst = [generate_random_orthonormal_matrix(W_output.shape[1]) for _ in range(10)]\n",
    "    W_fixon = W[:,0].reshape(-1,1)\n",
    "    W_task = W[:,6-shift_index].reshape(-1,1)\n",
    "    \n",
    "    fixon_output, task_output = W_output[1:,:] @ W_fixon, W_output[1:,:] @ W_task\n",
    "    bias_output = np.mean(bias @ (W_output[1:,:].T), axis=0)\n",
    "    fixon_proj_output_norm1 = fixon_output[0] + bias_output[0]\n",
    "    task_proj_output_norm1 = task_output[0] \n",
    "    fixon_proj_output_norm2 = fixon_output[1] + bias_output[1]\n",
    "    task_proj_output_norm2 = task_output[1] \n",
    "\n",
    "    fixon_output_random, task_output_random = [(W_output_random.T @ W_fixon) for W_output_random in W_output_random_lst], [(W_output_random.T @ W_task) for W_output_random in W_output_random_lst]\n",
    "    fixon_proj_output_norm_random = np.mean([np.sum(fixon_output_r) for fixon_output_r in fixon_output_random])\n",
    "    task_proj_output_norm_random = np.mean([np.sum(task_output_r) for task_output_r in task_output_random])\n",
    "    \n",
    "    fixon_task_projoutput.append([fixon_proj_output_norm1[0], task_proj_output_norm1[0], \n",
    "                                  fixon_proj_output_norm2[0], task_proj_output_norm2[0], \n",
    "                                  fixon_proj_output_norm_random, task_proj_output_norm_random]\n",
    "    )\n",
    "\n",
    "fixon_task_projoutput = np.array(fixon_task_projoutput)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,3))\n",
    "ax.plot(counter_lst, fixon_task_projoutput[:, 0], marker=\"o\", color=c_vals[0], \n",
    "        linestyle=l_vals[0], label=\"fixon proj output1\")\n",
    "ax.plot(counter_lst, fixon_task_projoutput[:, 1], marker=\"o\", color=c_vals[0], \n",
    "        linestyle=l_vals[1], label=\"task proj output1\")\n",
    "ax.plot(counter_lst, fixon_task_projoutput[:, 0]+fixon_task_projoutput[:, 1], marker=\"o\", color=c_vals[1], \n",
    "        linestyle=l_vals[2], linewidth=1, label=\"fixon/task sum output1\")\n",
    "ax.axhline(0, color=c_vals[1], linestyle=l_vals[2])\n",
    "\n",
    "# ax.plot(counter_lst, fixon_task_projoutput[:, 2], \"-o\", color=c_vals[1], linestyle=l_vals[0], label=\"fixon proj output2\")\n",
    "# ax.plot(counter_lst, fixon_task_projoutput[:, 3], \"-o\", color=c_vals[1], linestyle=l_vals[1], label=\"task proj output2\")\n",
    "# ax.plot(counter_lst, fixon_task_projoutput[:, 2]+fixon_task_projoutput[:, 3], \"-o\", color=c_vals[1], linestyle=l_vals[2], linewidth=3, label=\"fixon/task sum output2\")\n",
    "# ax.plot(counter_lst, fixon_task_projoutput[:,2], \"-o\", c=c_vals_l[0], label=\"fixon proj output random\")\n",
    "# ax.plot(counter_lst, fixon_task_projoutput[:,3], \"-o\",c=c_vals_l[1], label=\"task proj output random\")\n",
    "ax.legend(loc=\"lower left\", fontsize=15, frameon=True)    \n",
    "ax.set_xlabel(\"# Dataset\", fontsize=15)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_ylabel(\"Weight Component \\nProjection\", fontsize=15)\n",
    "ax.tick_params(axis=\"y\", labelsize=12)\n",
    "ax.tick_params(axis=\"x\", labelsize=12)\n",
    "fig.savefig(f\"./onetask/w_to_output_seed{seed}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb3d781-4921-407a-a0e6-570f200cd040",
   "metadata": {},
   "outputs": [],
   "source": [
    "fighs, axshs = plt.subplots(3,1,figsize=(6*1,3*3),squeeze=False)\n",
    "\n",
    "start_stage = stages_num-1\n",
    "for stage_iter in range(start_stage, stages_num):\n",
    "    PCA_downsample = 3\n",
    "    _, Ms_orig, hs, bs, _ = modulation_extraction(db_lst[0][stage_iter], max_seq_len, layer_index)\n",
    "\n",
    "    pca = PCA(n_components = PCA_downsample)\n",
    "\n",
    "    Ms_end_of_stimulus = Ms_orig[:,stimulus_end:stimulus_end+1,:,:]\n",
    "    Ms_end_of_stimulus_flattened = Ms_end_of_stimulus.reshape(Ms_end_of_stimulus.shape[0] * Ms_end_of_stimulus.shape[1] * Ms_end_of_stimulus.shape[3], \n",
    "                                                              Ms_end_of_stimulus.shape[2])\n",
    "    pca.fit(Ms_end_of_stimulus_flattened)\n",
    "    Ms_flattened = Ms_orig.reshape(Ms_orig.shape[0] * Ms_orig.shape[1] * Ms_orig.shape[3], Ms_orig.shape[2])\n",
    "    projected_data = pca.transform(Ms_flattened)\n",
    "    Ms_reconstructed = projected_data.reshape(Ms_orig.shape[0], Ms_orig.shape[1], Ms_orig.shape[3], PCA_downsample)\n",
    "\n",
    "    lowd_data_lst = [Ms_reconstructed[:,:,0,:]] # for instance, modulation of fixon\n",
    "    \n",
    "    for i in range(hs.shape[0]):\n",
    "        for dd in range(len(lowd_data_lst)):\n",
    "            lowd_data = lowd_data_lst[dd]\n",
    "            data_batch = lowd_data[i,:,:]\n",
    "\n",
    "            col = stage_iter - start_stage\n",
    "            axs = axshs[:, col]\n",
    "            \n",
    "            # (row, xPC, yPC) with 0-based PC indices: PC1->0, PC2->1, PC3->2\n",
    "            pairs = [\n",
    "                (0, 0, 1),  # (PC1, PC2)\n",
    "                (1, 0, 2),  # (PC1, PC3)\n",
    "                (2, 1, 2),  # (PC2, PC3)\n",
    "            ]\n",
    "            \n",
    "            color = c_vals[labels[i, 0]]\n",
    "            \n",
    "            # Plot (PC1,PC2) in 3 segments (as you had)\n",
    "            segments_12 = [\n",
    "                (slice(stimulus_start, stimulus_end), markers_vals[dd],      \"-\"),\n",
    "                (slice(stimulus_end, response_start), markers_vals[dd + 1],  \"-.\"),\n",
    "                (slice(response_start, None),         markers_vals[dd + 2],  \"--\"),\n",
    "            ]\n",
    "            \n",
    "            row, xpc, ypc = pairs[0]\n",
    "            for sl, mk, ml in segments_12:\n",
    "                axs[row].plot(data_batch[sl, xpc], data_batch[sl, ypc],\n",
    "                              marker=mk, markersize=4, c=color, alpha=0.5, linestyle=ml)\n",
    "            \n",
    "            # Plot the other two pairs in the stimulus segment only (as you had)\n",
    "            for row, xpc, ypc in pairs[1:]:\n",
    "                axs[row].plot(data_batch[stimulus_start:stimulus_end, xpc],\n",
    "                              data_batch[stimulus_start:stimulus_end, ypc],\n",
    "                              marker=markers_vals[dd], markersize=3, c=color, alpha=0.5)\n",
    "            \n",
    "            # Labels\n",
    "            for row, xpc, ypc in pairs:\n",
    "                axs[row].set_xlabel(f\"PC {xpc+1}\", fontsize=15)\n",
    "                axs[row].set_ylabel(f\"PC {ypc+1}\", fontsize=15)\n",
    "\n",
    "\n",
    "fighs.tight_layout()\n",
    "fighs.savefig(f\"./onetask/m_pca_seed{seed}.png\", dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d992cd9-2415-4322-8351-2da5056d05af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mpn)",
   "language": "python",
   "name": "mpn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
