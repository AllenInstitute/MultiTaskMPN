{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "374fb8f2-9b83-44ce-821b-8917a114c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# PyTorch Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Data Handling and Image Processing\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Style for Matplotlib\n",
    "import scienceplots\n",
    "plt.style.use('science')\n",
    "plt.style.use(['no-latex'])\n",
    "\n",
    "# Scientific Computing and Machine Learning\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.linalg import subspace_angles\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Custom Modules and Extensions\n",
    "sys.path.append(\"../netrep/\")\n",
    "sys.path.append(\"../svcca/\")\n",
    "import cca_core\n",
    "from netrep.metrics import LinearMetric\n",
    "import networks as nets  # Contains RNNs\n",
    "import net_helpers\n",
    "import tasks\n",
    "import helper\n",
    "import mpn\n",
    "\n",
    "import scienceplots\n",
    "plt.style.use('science')\n",
    "plt.style.use(['no-latex'])\n",
    "\n",
    "# Memory Optimization\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd1556c8-a4b6-434b-a60f-37035980bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 Red, 1 blue, 2 green, 3 purple, 4 orange, 5 teal, 6 gray, 7 pink, 8 yellow\n",
    "c_vals = ['#e53e3e', '#3182ce', '#38a169', '#805ad5','#dd6b20', '#319795', '#718096', '#d53f8c', '#d69e2e',]\n",
    "c_vals_l = ['#feb2b2', '#90cdf4', '#9ae6b4', '#d6bcfa', '#fbd38d', '#81e6d9', '#e2e8f0', '#fbb6ce', '#faf089',]\n",
    "c_vals_d = ['#9b2c2c', '#2c5282', '#276749', '#553c9a', '#9c4221', '#285e61', '#2d3748', '#97266d', '#975a16',]\n",
    "l_vals = ['solid', 'dashed', 'dotted', 'dashdot', '-', '--', '-.', ':', (0, (3, 1, 1, 1)), (0, (5, 10))]\n",
    "markers_vals = ['o', 'v', '^', '<', '>', '1', '2', '3', '4', 's', 'p', '*', 'h', 'H', '+', 'x', 'D', 'd', '|', '_']\n",
    "hyp_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34978bf6-67b8-41bd-a022-a7b46a320686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set seed 893\n",
      "Fixation_off: True; Task_info: True\n",
      "Rules: ['delaydm1']\n",
      "  Input size 7, Output size 3\n",
      "Using CUDA...\n"
     ]
    }
   ],
   "source": [
    "# Reload modules if changes have been made to them\n",
    "from importlib import reload\n",
    "\n",
    "reload(nets)\n",
    "reload(net_helpers)\n",
    "\n",
    "fixseed = False # randomize setting the seed may lead to not perfectly solved results\n",
    "seed = random.randint(1,1000) if not fixseed else 8 # random set the seed to test robustness by default\n",
    "print(f\"Set seed {seed}\")\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "hyp_dict['task_type'] = 'multitask' # int, NeuroGym, multitask\n",
    "hyp_dict['mode_for_all'] = \"random_batch\"\n",
    "hyp_dict['ruleset'] = 'delaydm1' # low_dim, all, test\n",
    "\n",
    "accept_rules = ('fdgo', 'fdanti', 'delaygo', 'delayanti', 'reactgo', 'reactanti', \n",
    "                'delaydm1', 'delaydm2', 'dmsgo', 'dmcgo', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm', 'dm1')\n",
    "\n",
    "\n",
    "rules_dict = \\\n",
    "    {'all' : ['fdgo', 'reactgo', 'delaygo', 'fdanti', 'reactanti', 'delayanti',\n",
    "              'dm1', 'dm2', 'contextdm1', 'contextdm2', 'multidm',\n",
    "              'delaydm1', 'delaydm2', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm',\n",
    "              'dmsgo', 'dmsnogo', 'dmcgo', 'dmcnogo'],\n",
    "     'low_dim' : ['fdgo', 'reactgo', 'delaygo', 'fdanti', 'reactanti', 'delayanti',\n",
    "                 'delaydm1', 'delaydm2', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm',\n",
    "                 'dmsgo', 'dmsnogo', 'dmcgo', 'dmcnogo'],\n",
    "\n",
    "     'gofamily': ['fdgo', 'fdanti', 'reactgo', 'reactanti', 'delaygo', 'delayanti'],\n",
    "\n",
    "     'delaygo': ['delaygo'],\n",
    "     'delaygofamily': ['delaygo', 'delayanti'],\n",
    "     'fdgo': ['fdgo'],\n",
    "     'fdfamily': ['fdgo', 'fdanti'],\n",
    "     'reactgo': ['reactgo'],\n",
    "     'reactfamily': ['reactgo', 'reactanti'],\n",
    "     \n",
    "     'delaydm1': ['delaydm1'],\n",
    "     'delaydmfamily': ['delaydm1', 'delaydm2'],\n",
    "     \n",
    "     'dmsgofamily': ['dmsgo', 'dmsnogo'],\n",
    "     'dmsgo': ['dmsgo'],\n",
    "     'dmcgo': ['dmcgo'],\n",
    "     'contextdelaydm1': ['contextdelaydm1'], \n",
    "     'contextdelayfamily': ['contextdelaydm1', 'contextdelaydm2'],\n",
    "     'dm1': ['dm1']\n",
    "    }\n",
    "    \n",
    "\n",
    "# This can either be used to set parameters OR set parameters and train\n",
    "train = True # whether or not to train the network\n",
    "verbose = True\n",
    "hyp_dict['run_mode'] = 'minimal' # minimal, debug\n",
    "hyp_dict['chosen_network'] = \"dmpn\"\n",
    "\n",
    "# suffix for saving images\n",
    "hyp_dict['addon_name'] = \"\"\n",
    "\n",
    "mpn_depth = 1\n",
    "\n",
    "# for coding \n",
    "if hyp_dict['chosen_network'] in (\"gru\", \"vanilla\"):\n",
    "    mpn_depth = 1\n",
    "\n",
    "def current_basic_params():\n",
    "    task_params = {\n",
    "        'task_type': hyp_dict['task_type'],\n",
    "        'rules': rules_dict[hyp_dict['ruleset']],\n",
    "        'dt': 40, # ms, directly influence sequence lengths,\n",
    "        'ruleset': hyp_dict['ruleset'],\n",
    "        'n_eachring': 8, # Number of distinct possible inputs on each ring\n",
    "        'in_out_mode': 'low_dim',  # high_dim or low_dim or low_dim_pos (Robert vs. Laura's paper, resp)\n",
    "        'sigma_x': 0.00, # Laura raised to 0.1 to prevent overfitting (Robert uses 0.01)\n",
    "        'mask_type': 'cost', # 'cost', None\n",
    "        'fixate_off': True, # Second fixation signal goes on when first is off\n",
    "        'task_info': True, \n",
    "        'randomize_inputs': False,\n",
    "        'n_input': 20, # Only used if inputs are randomized,\n",
    "        'modality_diff': True # if two stimulus are included in the task, put them into different modality \n",
    "    }\n",
    "\n",
    "    print(f\"Fixation_off: {task_params['fixate_off']}; Task_info: {task_params['task_info']}\")\n",
    "\n",
    "    train_params = {\n",
    "        'lr': 1e-3,\n",
    "        'n_batches': 640,\n",
    "        'batch_size': 640,\n",
    "        # 'gradient_clip': 10,\n",
    "        'valid_n_batch': 200,\n",
    "        'n_datasets': 100, # Number of distinct batches\n",
    "        'n_epochs_per_set': 400, # longer/shorter training\n",
    "        # 'task_mask': None, # None, task\n",
    "        # 'weight_reg': 'L2',\n",
    "        # 'reg_lambda': 1e-4,\n",
    "    }\n",
    "\n",
    "    if not train: # some \n",
    "        assert train_params['n_epochs_per_set'] == 0\n",
    "\n",
    "    n_hidden = 100\n",
    "\n",
    "    net_params = {\n",
    "        'net_type': hyp_dict['chosen_network'], # mpn1, dmpn, vanilla\n",
    "        'n_neurons': [1] + [n_hidden] * mpn_depth + [1],\n",
    "        'output_bias': False, # Turn off biases for easier interpretation\n",
    "        'loss_type': 'MSE', # XE, MSE\n",
    "        'activation': 'tanh', # linear, ReLU, sigmoid, tanh, tanh_re, tukey, heaviside\n",
    "        'cuda': True,\n",
    "        'monitor_freq': 100,\n",
    "        'monitor_valid_out': True, # Whether or not to save validation output throughout training\n",
    "        'output_matrix': '',# \"\" (default); \"untrained\", or \"orthogonal\"\n",
    "        'input_layer_add': True, \n",
    "        'input_layer_bias': True,\n",
    "        \n",
    "        # for one-layer MPN, GRU or Vanilla\n",
    "        'ml_params': {\n",
    "            'bias': True, # Bias of layer\n",
    "            'mp_type': 'mult',\n",
    "            'm_update_type': 'hebb_assoc', # hebb_assoc, hebb_pre\n",
    "            'eta_type': 'scalar', # scalar, pre_vector, post_vector, matrix\n",
    "            'eta_train': False,\n",
    "            # 'eta_init': 'gaussian', \n",
    "            'lam_type': 'scalar', # scalar, pre_vector, post_vector, matrix\n",
    "            'm_time_scale': 400, # ms, sets lambda\n",
    "            'lam_train': False,\n",
    "        },\n",
    "\n",
    "        # Vanilla RNN params\n",
    "        'leaky': True,\n",
    "        'alpha': 0.2,\n",
    "    }\n",
    "\n",
    "    # for multiple MPN layers, assert \n",
    "    if mpn_depth > 1:\n",
    "        for mpl_idx in range(mpn_depth - 1):\n",
    "            assert f'ml_params{mpl_idx}' in net_params.keys()\n",
    "\n",
    "    # actually I don't think it is needed\n",
    "    # putting here to warn the parameter checking every time \n",
    "    # when switching network\n",
    "    if hyp_dict['chosen_network'] in (\"gru\", \"vanilla\"):\n",
    "        assert f'ml_params' in net_params.keys()\n",
    "\n",
    "    return task_params, train_params, net_params\n",
    "\n",
    "task_params, train_params, net_params = current_basic_params()\n",
    "\n",
    "shift_index = 1 if not task_params['fixate_off'] else 0\n",
    "\n",
    "if hyp_dict['task_type'] in ('multitask',):\n",
    "    task_params, train_params, net_params = tasks.convert_and_init_multitask_params(\n",
    "        (task_params, train_params, net_params)\n",
    "    )\n",
    "\n",
    "    net_params['prefs'] = tasks.get_prefs(task_params['hp'])\n",
    "\n",
    "    print('Rules: {}'.format(task_params['rules']))\n",
    "    print('  Input size {}, Output size {}'.format(\n",
    "        task_params['n_input'], task_params['n_output'],\n",
    "    ))\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "if net_params['cuda']:\n",
    "    print('Using CUDA...')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('Using CPU...')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a341b36-dc7a-42b0-bffb-cb17d380041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_dict[\"mess_with_training\"] = False\n",
    "\n",
    "if hyp_dict['mess_with_training']:\n",
    "    hyp_dict['addon_name'] += \"messwithtraining\"\n",
    "\n",
    "params = task_params, train_params, net_params\n",
    "\n",
    "# random matrix\n",
    "random_matrix = None\n",
    "if task_params[\"randomize_inputs\"]:\n",
    "    random_matrix = task_params[\"randomize_matrix\"]\n",
    "\n",
    "if net_params['net_type'] == 'mpn1':\n",
    "    netFunction = mpn.MultiPlasticNet\n",
    "elif net_params['net_type'] == 'dmpn':\n",
    "    netFunction = mpn.DeepMultiPlasticNet\n",
    "elif net_params['net_type'] == 'vanilla':\n",
    "    netFunction = nets.VanillaRNN\n",
    "elif net_params['net_type'] == 'gru':\n",
    "    netFunction = nets.GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07e4fe48-2af6-4741-8d42-01803ba0abf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Align ['delaydm1'] With Same Time\n",
      "delaydm1\n"
     ]
    }
   ],
   "source": [
    "test_n_batch = train_params[\"valid_n_batch\"]\n",
    "color_by = \"stim\"\n",
    "\n",
    "task_random_fix = True\n",
    "if task_random_fix:\n",
    "    print(f\"Align {task_params['rules']} With Same Time\")\n",
    "\n",
    "if task_params['task_type'] in ('multitask',): # Test batch consists of all the rules\n",
    "    task_params['hp']['batch_size_train'] = test_n_batch\n",
    "    # using homogeneous cutting off\n",
    "    test_mode_for_all = \"random\"\n",
    "    # ZIHAN\n",
    "    # generate test data using \"random\"\n",
    "    test_data, test_trials_extra = tasks.generate_trials_wrap(task_params, test_n_batch, \\\n",
    "                rules=task_params['rules'], mode_input=test_mode_for_all, fix=task_random_fix)\n",
    "    _, test_trials, test_rule_idxs = test_trials_extra\n",
    "\n",
    "    task_params['dataset_name'] = 'multitask'\n",
    "\n",
    "    if task_params['in_out_mode'] in ('low_dim_pos',):\n",
    "        output_dim_labels = ('Fixate', 'Cos', '-Cos', 'Sin', '-Sin')\n",
    "    elif task_params['in_out_mode'] in ('low_dim',):\n",
    "        output_dim_labels = ('Fixate', 'Cos', 'Sin')\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    labels = []\n",
    "    for rule_idx, rule in enumerate(task_params['rules']):\n",
    "        print(rule)\n",
    "        if rule in accept_rules:\n",
    "            if hyp_dict['ruleset'] in ('dmsgo', 'dmcgo'):\n",
    "                labels.append(test_trials[rule_idx].meta['matches'])\n",
    "            else:\n",
    "                labels.append(test_trials[rule_idx].meta['resp1' if color_by == \"resp\" else 'stim1'])\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "    labels = np.concatenate(labels, axis=0).reshape(-1,1)\n",
    "\n",
    "test_input, test_output, test_mask = test_data\n",
    "\n",
    "permutation = np.random.permutation(test_input.shape[0])\n",
    "test_input = test_input[permutation]\n",
    "test_output = test_output[permutation]\n",
    "test_mask = test_mask[permutation]\n",
    "labels = labels[permutation]\n",
    "\n",
    "test_input_np = test_input.detach().cpu().numpy()\n",
    "test_output_np = test_output.detach().cpu().numpy()\n",
    "\n",
    "n_batch_all = test_input_np.shape[0] # Total number of batches, might be different than test_n_batch\n",
    "max_seq_len = test_input_np.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e78b44e9-b6e3-4c1c-a5c7-5a608a6d090b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 100, 100, 3]\n",
      "MultiPlastic Net:\n",
      "  output neurons: 3\n",
      "  Act: tanh\n",
      "\n",
      "=== Layer Universal Setup ===\n",
      "  MP Layer1 parameters:\n",
      "    n_neurons - input: 100, output: 100\n",
      "    M matrix parameters:    update bounds - Max mult: 1.0, Min mult: -1.0\n",
      "      type: mult // Update - type: hebb_assoc // Act fn: linear\n",
      "      Eta: scalar (fixed) // Lambda: scalar (fixed) // Lambda_max: 0.90 (tau: 4.0e+02)\n",
      "How about Test Data at dataset 0\n",
      "Train parameters:\n",
      "  Loss: MSE // LR: 1.00e-03 // Optim: adam\n",
      "  Grad type: backprop // Gradient clip: None\n",
      "Weight reg: None\n",
      "Iter: 0, LR: 1.000e-03 - train_loss:2.705e-01, rounded train_acc:0.236, valid_loss:2.742e-01, rounded valid_acc:0.233\n",
      "Iter: 100, LR: 1.000e-03 - train_loss:7.839e-02, rounded train_acc:0.447, valid_loss:9.321e-02, rounded valid_acc:0.502\n",
      "Iter: 200, LR: 1.000e-03 - train_loss:6.704e-02, rounded train_acc:0.582, valid_loss:8.682e-02, rounded valid_acc:0.514\n",
      "Iter: 300, LR: 1.000e-03 - train_loss:6.351e-02, rounded train_acc:0.593, valid_loss:8.377e-02, rounded valid_acc:0.559\n",
      "Iter: 400, LR: 1.000e-03 - train_loss:6.651e-02, rounded train_acc:0.557, valid_loss:7.746e-02, rounded valid_acc:0.556\n",
      "How about Test Data at dataset 1\n",
      "Iter: 500, LR: 1.000e-03 - train_loss:5.502e-02, rounded train_acc:0.551, valid_loss:6.442e-02, rounded valid_acc:0.540\n",
      "Iter: 600, LR: 1.000e-03 - train_loss:4.935e-02, rounded train_acc:0.574, valid_loss:6.396e-02, rounded valid_acc:0.551\n",
      "Iter: 700, LR: 1.000e-03 - train_loss:4.294e-02, rounded train_acc:0.638, valid_loss:5.596e-02, rounded valid_acc:0.601\n",
      "Iter: 800, LR: 1.000e-03 - train_loss:3.867e-02, rounded train_acc:0.668, valid_loss:5.460e-02, rounded valid_acc:0.607\n",
      "How about Test Data at dataset 2\n",
      "Iter: 900, LR: 1.000e-03 - train_loss:4.320e-02, rounded train_acc:0.631, valid_loss:5.521e-02, rounded valid_acc:0.610\n",
      "Iter: 1000, LR: 1.000e-03 - train_loss:4.021e-02, rounded train_acc:0.663, valid_loss:5.459e-02, rounded valid_acc:0.601\n",
      "Iter: 1100, LR: 1.000e-03 - train_loss:3.327e-02, rounded train_acc:0.688, valid_loss:5.077e-02, rounded valid_acc:0.642\n",
      "Iter: 1200, LR: 1.000e-03 - train_loss:3.039e-02, rounded train_acc:0.710, valid_loss:5.053e-02, rounded valid_acc:0.645\n",
      "Iter: 1300, LR: 1.000e-03 - train_loss:3.652e-02, rounded train_acc:0.700, valid_loss:4.655e-02, rounded valid_acc:0.617\n",
      "Iter: 1400, LR: 1.000e-03 - train_loss:3.207e-02, rounded train_acc:0.714, valid_loss:4.517e-02, rounded valid_acc:0.633\n",
      "Iter: 1500, LR: 1.000e-03 - train_loss:2.866e-02, rounded train_acc:0.730, valid_loss:4.202e-02, rounded valid_acc:0.640\n",
      "Iter: 1600, LR: 1.000e-03 - train_loss:2.665e-02, rounded train_acc:0.760, valid_loss:3.945e-02, rounded valid_acc:0.658\n",
      "How about Test Data at dataset 4\n",
      "Iter: 1700, LR: 1.000e-03 - train_loss:2.982e-02, rounded train_acc:0.747, valid_loss:3.857e-02, rounded valid_acc:0.688\n",
      "Iter: 1800, LR: 1.000e-03 - train_loss:2.914e-02, rounded train_acc:0.768, valid_loss:3.748e-02, rounded valid_acc:0.701\n",
      "Iter: 1900, LR: 1.000e-03 - train_loss:2.678e-02, rounded train_acc:0.766, valid_loss:3.706e-02, rounded valid_acc:0.689\n",
      "Iter: 2000, LR: 1.000e-03 - train_loss:2.324e-02, rounded train_acc:0.772, valid_loss:3.504e-02, rounded valid_acc:0.695\n",
      "Iter: 2100, LR: 1.000e-03 - train_loss:2.876e-02, rounded train_acc:0.765, valid_loss:3.737e-02, rounded valid_acc:0.731\n",
      "Iter: 2200, LR: 1.000e-03 - train_loss:2.607e-02, rounded train_acc:0.787, valid_loss:3.829e-02, rounded valid_acc:0.711\n",
      "Iter: 2300, LR: 1.000e-03 - train_loss:2.598e-02, rounded train_acc:0.764, valid_loss:3.719e-02, rounded valid_acc:0.707\n",
      "Iter: 2400, LR: 1.000e-03 - train_loss:2.240e-02, rounded train_acc:0.799, valid_loss:3.436e-02, rounded valid_acc:0.731\n",
      "Iter: 2500, LR: 1.000e-03 - train_loss:2.515e-02, rounded train_acc:0.798, valid_loss:3.842e-02, rounded valid_acc:0.704\n",
      "Iter: 2600, LR: 1.000e-03 - train_loss:2.048e-02, rounded train_acc:0.814, valid_loss:3.603e-02, rounded valid_acc:0.716\n",
      "Iter: 2700, LR: 1.000e-03 - train_loss:2.101e-02, rounded train_acc:0.810, valid_loss:3.570e-02, rounded valid_acc:0.724\n",
      "Iter: 2800, LR: 1.000e-03 - train_loss:2.116e-02, rounded train_acc:0.826, valid_loss:4.017e-02, rounded valid_acc:0.725\n",
      "Iter: 2900, LR: 1.000e-03 - train_loss:3.002e-02, rounded train_acc:0.746, valid_loss:3.940e-02, rounded valid_acc:0.704\n",
      "Iter: 3000, LR: 1.000e-03 - train_loss:2.490e-02, rounded train_acc:0.773, valid_loss:3.825e-02, rounded valid_acc:0.703\n",
      "Iter: 3100, LR: 1.000e-03 - train_loss:2.763e-02, rounded train_acc:0.721, valid_loss:3.979e-02, rounded valid_acc:0.645\n",
      "Iter: 3200, LR: 1.000e-03 - train_loss:2.724e-02, rounded train_acc:0.741, valid_loss:3.984e-02, rounded valid_acc:0.698\n",
      "How about Test Data at dataset 8\n",
      "Iter: 3300, LR: 1.000e-03 - train_loss:2.915e-02, rounded train_acc:0.750, valid_loss:3.815e-02, rounded valid_acc:0.693\n",
      "Iter: 3400, LR: 1.000e-03 - train_loss:2.813e-02, rounded train_acc:0.767, valid_loss:4.308e-02, rounded valid_acc:0.685\n",
      "Iter: 3500, LR: 1.000e-03 - train_loss:2.773e-02, rounded train_acc:0.758, valid_loss:4.196e-02, rounded valid_acc:0.704\n",
      "Iter: 3600, LR: 1.000e-03 - train_loss:2.458e-02, rounded train_acc:0.749, valid_loss:3.837e-02, rounded valid_acc:0.698\n",
      "Iter: 3700, LR: 1.000e-03 - train_loss:2.892e-02, rounded train_acc:0.721, valid_loss:4.192e-02, rounded valid_acc:0.655\n",
      "Iter: 3800, LR: 1.000e-03 - train_loss:2.659e-02, rounded train_acc:0.760, valid_loss:4.037e-02, rounded valid_acc:0.671\n",
      "Iter: 3900, LR: 1.000e-03 - train_loss:2.627e-02, rounded train_acc:0.762, valid_loss:4.428e-02, rounded valid_acc:0.690\n",
      "Iter: 4000, LR: 1.000e-03 - train_loss:2.590e-02, rounded train_acc:0.725, valid_loss:4.246e-02, rounded valid_acc:0.634\n",
      "Iter: 4100, LR: 1.000e-03 - train_loss:2.705e-02, rounded train_acc:0.761, valid_loss:3.774e-02, rounded valid_acc:0.676\n",
      "Iter: 4200, LR: 1.000e-03 - train_loss:2.585e-02, rounded train_acc:0.750, valid_loss:3.719e-02, rounded valid_acc:0.704\n",
      "Iter: 4300, LR: 1.000e-03 - train_loss:2.310e-02, rounded train_acc:0.748, valid_loss:3.863e-02, rounded valid_acc:0.657\n",
      "Iter: 4400, LR: 1.000e-03 - train_loss:2.331e-02, rounded train_acc:0.750, valid_loss:3.767e-02, rounded valid_acc:0.642\n",
      "Iter: 4500, LR: 1.000e-03 - train_loss:2.375e-02, rounded train_acc:0.787, valid_loss:3.150e-02, rounded valid_acc:0.695\n",
      "Iter: 4600, LR: 1.000e-03 - train_loss:2.075e-02, rounded train_acc:0.809, valid_loss:3.054e-02, rounded valid_acc:0.697\n",
      "Iter: 4700, LR: 1.000e-03 - train_loss:1.918e-02, rounded train_acc:0.813, valid_loss:2.981e-02, rounded valid_acc:0.690\n",
      "Iter: 4800, LR: 1.000e-03 - train_loss:1.879e-02, rounded train_acc:0.830, valid_loss:2.890e-02, rounded valid_acc:0.705\n",
      "Iter: 4900, LR: 1.000e-03 - train_loss:2.227e-02, rounded train_acc:0.763, valid_loss:3.164e-02, rounded valid_acc:0.737\n",
      "Iter: 5000, LR: 1.000e-03 - train_loss:1.712e-02, rounded train_acc:0.823, valid_loss:2.993e-02, rounded valid_acc:0.732\n",
      "Iter: 5100, LR: 1.000e-03 - train_loss:1.534e-02, rounded train_acc:0.843, valid_loss:2.879e-02, rounded valid_acc:0.751\n",
      "Iter: 5200, LR: 1.000e-03 - train_loss:1.510e-02, rounded train_acc:0.831, valid_loss:2.826e-02, rounded valid_acc:0.754\n",
      "Iter: 5300, LR: 1.000e-03 - train_loss:1.946e-02, rounded train_acc:0.794, valid_loss:2.703e-02, rounded valid_acc:0.774\n",
      "Iter: 5400, LR: 1.000e-03 - train_loss:1.783e-02, rounded train_acc:0.814, valid_loss:2.625e-02, rounded valid_acc:0.762\n",
      "Iter: 5500, LR: 1.000e-03 - train_loss:1.628e-02, rounded train_acc:0.826, valid_loss:2.424e-02, rounded valid_acc:0.784\n",
      "Iter: 5600, LR: 1.000e-03 - train_loss:1.516e-02, rounded train_acc:0.833, valid_loss:2.433e-02, rounded valid_acc:0.778\n",
      "Iter: 5700, LR: 1.000e-03 - train_loss:1.799e-02, rounded train_acc:0.828, valid_loss:2.404e-02, rounded valid_acc:0.795\n",
      "Iter: 5800, LR: 1.000e-03 - train_loss:1.644e-02, rounded train_acc:0.840, valid_loss:2.479e-02, rounded valid_acc:0.764\n",
      "Iter: 5900, LR: 1.000e-03 - train_loss:1.364e-02, rounded train_acc:0.851, valid_loss:2.347e-02, rounded valid_acc:0.790\n",
      "Iter: 6000, LR: 1.000e-03 - train_loss:1.561e-02, rounded train_acc:0.852, valid_loss:2.595e-02, rounded valid_acc:0.810\n",
      "Iter: 6100, LR: 1.000e-03 - train_loss:1.722e-02, rounded train_acc:0.841, valid_loss:2.139e-02, rounded valid_acc:0.774\n",
      "Iter: 6200, LR: 1.000e-03 - train_loss:1.399e-02, rounded train_acc:0.861, valid_loss:2.172e-02, rounded valid_acc:0.792\n",
      "Iter: 6300, LR: 1.000e-03 - train_loss:1.313e-02, rounded train_acc:0.875, valid_loss:2.152e-02, rounded valid_acc:0.788\n",
      "Iter: 6400, LR: 1.000e-03 - train_loss:1.717e-02, rounded train_acc:0.810, valid_loss:2.447e-02, rounded valid_acc:0.812\n",
      "How about Test Data at dataset 16\n",
      "Iter: 6500, LR: 1.000e-03 - train_loss:1.604e-02, rounded train_acc:0.840, valid_loss:2.353e-02, rounded valid_acc:0.782\n",
      "Iter: 6600, LR: 1.000e-03 - train_loss:1.380e-02, rounded train_acc:0.857, valid_loss:2.252e-02, rounded valid_acc:0.796\n",
      "Iter: 6700, LR: 1.000e-03 - train_loss:1.301e-02, rounded train_acc:0.872, valid_loss:2.302e-02, rounded valid_acc:0.795\n",
      "Iter: 6800, LR: 1.000e-03 - train_loss:1.306e-02, rounded train_acc:0.868, valid_loss:2.355e-02, rounded valid_acc:0.784\n",
      "Iter: 6900, LR: 1.000e-03 - train_loss:1.273e-02, rounded train_acc:0.864, valid_loss:2.148e-02, rounded valid_acc:0.851\n",
      "Iter: 7000, LR: 1.000e-03 - train_loss:1.098e-02, rounded train_acc:0.894, valid_loss:2.162e-02, rounded valid_acc:0.852\n",
      "Iter: 7100, LR: 1.000e-03 - train_loss:1.027e-02, rounded train_acc:0.896, valid_loss:2.391e-02, rounded valid_acc:0.851\n",
      "Iter: 7200, LR: 1.000e-03 - train_loss:9.879e-03, rounded train_acc:0.893, valid_loss:2.093e-02, rounded valid_acc:0.859\n",
      "Iter: 7300, LR: 1.000e-03 - train_loss:1.295e-02, rounded train_acc:0.882, valid_loss:1.901e-02, rounded valid_acc:0.844\n",
      "Iter: 7400, LR: 1.000e-03 - train_loss:1.224e-02, rounded train_acc:0.877, valid_loss:1.990e-02, rounded valid_acc:0.847\n",
      "Iter: 7500, LR: 1.000e-03 - train_loss:1.241e-02, rounded train_acc:0.878, valid_loss:1.849e-02, rounded valid_acc:0.830\n",
      "Iter: 7600, LR: 1.000e-03 - train_loss:1.215e-02, rounded train_acc:0.859, valid_loss:2.178e-02, rounded valid_acc:0.789\n",
      "Iter: 7700, LR: 1.000e-03 - train_loss:1.532e-02, rounded train_acc:0.876, valid_loss:2.342e-02, rounded valid_acc:0.825\n",
      "Iter: 7800, LR: 1.000e-03 - train_loss:1.603e-02, rounded train_acc:0.857, valid_loss:2.065e-02, rounded valid_acc:0.826\n",
      "Iter: 7900, LR: 1.000e-03 - train_loss:1.901e-02, rounded train_acc:0.837, valid_loss:3.625e-02, rounded valid_acc:0.694\n",
      "Iter: 8000, LR: 1.000e-03 - train_loss:1.255e-02, rounded train_acc:0.884, valid_loss:2.161e-02, rounded valid_acc:0.835\n",
      "Iter: 8100, LR: 1.000e-03 - train_loss:1.574e-02, rounded train_acc:0.873, valid_loss:2.309e-02, rounded valid_acc:0.815\n",
      "Iter: 8200, LR: 1.000e-03 - train_loss:1.288e-02, rounded train_acc:0.866, valid_loss:2.332e-02, rounded valid_acc:0.831\n",
      "Iter: 8300, LR: 1.000e-03 - train_loss:1.214e-02, rounded train_acc:0.884, valid_loss:2.324e-02, rounded valid_acc:0.814\n",
      "Iter: 8400, LR: 1.000e-03 - train_loss:1.225e-02, rounded train_acc:0.877, valid_loss:2.523e-02, rounded valid_acc:0.794\n",
      "Iter: 8500, LR: 1.000e-03 - train_loss:1.362e-02, rounded train_acc:0.878, valid_loss:2.052e-02, rounded valid_acc:0.822\n",
      "Iter: 8600, LR: 1.000e-03 - train_loss:1.145e-02, rounded train_acc:0.890, valid_loss:2.127e-02, rounded valid_acc:0.814\n",
      "Iter: 8700, LR: 1.000e-03 - train_loss:1.497e-02, rounded train_acc:0.844, valid_loss:2.036e-02, rounded valid_acc:0.809\n",
      "Iter: 8800, LR: 1.000e-03 - train_loss:1.352e-02, rounded train_acc:0.889, valid_loss:1.983e-02, rounded valid_acc:0.822\n",
      "Iter: 8900, LR: 1.000e-03 - train_loss:1.191e-02, rounded train_acc:0.888, valid_loss:1.976e-02, rounded valid_acc:0.855\n",
      "Iter: 9000, LR: 1.000e-03 - train_loss:1.788e-02, rounded train_acc:0.847, valid_loss:2.614e-02, rounded valid_acc:0.804\n",
      "Iter: 9100, LR: 1.000e-03 - train_loss:1.160e-02, rounded train_acc:0.890, valid_loss:1.958e-02, rounded valid_acc:0.853\n",
      "Iter: 9200, LR: 1.000e-03 - train_loss:1.206e-02, rounded train_acc:0.879, valid_loss:1.782e-02, rounded valid_acc:0.858\n",
      "Iter: 9300, LR: 1.000e-03 - train_loss:1.191e-02, rounded train_acc:0.870, valid_loss:1.629e-02, rounded valid_acc:0.863\n",
      "Iter: 9400, LR: 1.000e-03 - train_loss:1.145e-02, rounded train_acc:0.867, valid_loss:2.018e-02, rounded valid_acc:0.849\n",
      "Iter: 9500, LR: 1.000e-03 - train_loss:1.069e-02, rounded train_acc:0.874, valid_loss:1.880e-02, rounded valid_acc:0.848\n",
      "Iter: 9600, LR: 1.000e-03 - train_loss:1.235e-02, rounded train_acc:0.868, valid_loss:1.732e-02, rounded valid_acc:0.857\n",
      "Iter: 9700, LR: 1.000e-03 - train_loss:1.187e-02, rounded train_acc:0.899, valid_loss:2.007e-02, rounded valid_acc:0.837\n",
      "Iter: 9800, LR: 1.000e-03 - train_loss:1.056e-02, rounded train_acc:0.902, valid_loss:2.080e-02, rounded valid_acc:0.847\n",
      "Iter: 9900, LR: 1.000e-03 - train_loss:1.155e-02, rounded train_acc:0.896, valid_loss:2.004e-02, rounded valid_acc:0.848\n",
      "Iter: 10000, LR: 1.000e-03 - train_loss:9.676e-03, rounded train_acc:0.901, valid_loss:2.216e-02, rounded valid_acc:0.849\n",
      "Iter: 10100, LR: 1.000e-03 - train_loss:1.468e-02, rounded train_acc:0.863, valid_loss:2.205e-02, rounded valid_acc:0.853\n",
      "Iter: 10200, LR: 1.000e-03 - train_loss:1.300e-02, rounded train_acc:0.878, valid_loss:2.080e-02, rounded valid_acc:0.870\n",
      "Iter: 10300, LR: 1.000e-03 - train_loss:1.174e-02, rounded train_acc:0.892, valid_loss:2.085e-02, rounded valid_acc:0.867\n",
      "Iter: 10400, LR: 1.000e-03 - train_loss:1.099e-02, rounded train_acc:0.889, valid_loss:2.036e-02, rounded valid_acc:0.876\n",
      "Iter: 10500, LR: 1.000e-03 - train_loss:1.175e-02, rounded train_acc:0.910, valid_loss:2.011e-02, rounded valid_acc:0.860\n",
      "Iter: 10600, LR: 1.000e-03 - train_loss:1.322e-02, rounded train_acc:0.875, valid_loss:1.688e-02, rounded valid_acc:0.882\n",
      "Iter: 10700, LR: 1.000e-03 - train_loss:1.105e-02, rounded train_acc:0.909, valid_loss:1.865e-02, rounded valid_acc:0.866\n",
      "Iter: 10800, LR: 1.000e-03 - train_loss:1.063e-02, rounded train_acc:0.926, valid_loss:1.940e-02, rounded valid_acc:0.878\n",
      "Iter: 10900, LR: 1.000e-03 - train_loss:1.232e-02, rounded train_acc:0.910, valid_loss:2.178e-02, rounded valid_acc:0.868\n",
      "Iter: 11000, LR: 1.000e-03 - train_loss:1.194e-02, rounded train_acc:0.921, valid_loss:1.617e-02, rounded valid_acc:0.890\n",
      "Iter: 11100, LR: 1.000e-03 - train_loss:1.073e-02, rounded train_acc:0.913, valid_loss:1.711e-02, rounded valid_acc:0.885\n",
      "Iter: 11200, LR: 1.000e-03 - train_loss:1.099e-02, rounded train_acc:0.905, valid_loss:1.658e-02, rounded valid_acc:0.901\n",
      "Iter: 11300, LR: 1.000e-03 - train_loss:1.704e-02, rounded train_acc:0.861, valid_loss:3.038e-02, rounded valid_acc:0.750\n",
      "Iter: 11400, LR: 1.000e-03 - train_loss:2.155e-02, rounded train_acc:0.886, valid_loss:2.171e-02, rounded valid_acc:0.835\n",
      "Iter: 11500, LR: 1.000e-03 - train_loss:1.189e-02, rounded train_acc:0.910, valid_loss:1.780e-02, rounded valid_acc:0.867\n",
      "Iter: 11600, LR: 1.000e-03 - train_loss:9.976e-03, rounded train_acc:0.922, valid_loss:1.822e-02, rounded valid_acc:0.877\n",
      "Iter: 11700, LR: 1.000e-03 - train_loss:1.047e-02, rounded train_acc:0.909, valid_loss:1.637e-02, rounded valid_acc:0.879\n",
      "Iter: 11800, LR: 1.000e-03 - train_loss:1.021e-02, rounded train_acc:0.928, valid_loss:1.646e-02, rounded valid_acc:0.868\n",
      "Iter: 11900, LR: 1.000e-03 - train_loss:8.637e-03, rounded train_acc:0.932, valid_loss:1.676e-02, rounded valid_acc:0.884\n",
      "Iter: 12000, LR: 1.000e-03 - train_loss:1.011e-02, rounded train_acc:0.921, valid_loss:1.849e-02, rounded valid_acc:0.870\n",
      "Iter: 12100, LR: 1.000e-03 - train_loss:1.131e-02, rounded train_acc:0.894, valid_loss:1.689e-02, rounded valid_acc:0.881\n",
      "Iter: 12200, LR: 1.000e-03 - train_loss:1.136e-02, rounded train_acc:0.901, valid_loss:1.534e-02, rounded valid_acc:0.889\n",
      "Iter: 12300, LR: 1.000e-03 - train_loss:9.447e-03, rounded train_acc:0.915, valid_loss:1.519e-02, rounded valid_acc:0.884\n",
      "Iter: 12400, LR: 1.000e-03 - train_loss:1.470e-02, rounded train_acc:0.923, valid_loss:1.575e-02, rounded valid_acc:0.876\n",
      "Iter: 12500, LR: 1.000e-03 - train_loss:1.235e-02, rounded train_acc:0.906, valid_loss:1.533e-02, rounded valid_acc:0.896\n",
      "Iter: 12600, LR: 1.000e-03 - train_loss:1.131e-02, rounded train_acc:0.913, valid_loss:1.467e-02, rounded valid_acc:0.882\n",
      "Iter: 12700, LR: 1.000e-03 - train_loss:1.212e-02, rounded train_acc:0.915, valid_loss:1.557e-02, rounded valid_acc:0.896\n",
      "Iter: 12800, LR: 1.000e-03 - train_loss:1.021e-02, rounded train_acc:0.921, valid_loss:1.593e-02, rounded valid_acc:0.882\n",
      "How about Test Data at dataset 32\n",
      "Iter: 12900, LR: 1.000e-03 - train_loss:1.029e-02, rounded train_acc:0.929, valid_loss:1.632e-02, rounded valid_acc:0.879\n",
      "Iter: 13000, LR: 1.000e-03 - train_loss:1.111e-02, rounded train_acc:0.937, valid_loss:1.548e-02, rounded valid_acc:0.894\n",
      "Iter: 13100, LR: 1.000e-03 - train_loss:1.067e-02, rounded train_acc:0.915, valid_loss:1.791e-02, rounded valid_acc:0.868\n",
      "Iter: 13200, LR: 1.000e-03 - train_loss:1.200e-02, rounded train_acc:0.913, valid_loss:2.072e-02, rounded valid_acc:0.883\n",
      "Iter: 13300, LR: 1.000e-03 - train_loss:1.311e-02, rounded train_acc:0.897, valid_loss:1.881e-02, rounded valid_acc:0.840\n",
      "Iter: 13400, LR: 1.000e-03 - train_loss:1.276e-02, rounded train_acc:0.893, valid_loss:2.416e-02, rounded valid_acc:0.879\n",
      "Iter: 13500, LR: 1.000e-03 - train_loss:1.088e-02, rounded train_acc:0.913, valid_loss:2.118e-02, rounded valid_acc:0.872\n",
      "Iter: 13600, LR: 1.000e-03 - train_loss:1.200e-02, rounded train_acc:0.912, valid_loss:2.023e-02, rounded valid_acc:0.875\n",
      "Iter: 13700, LR: 1.000e-03 - train_loss:1.442e-02, rounded train_acc:0.885, valid_loss:1.794e-02, rounded valid_acc:0.886\n",
      "Iter: 13800, LR: 1.000e-03 - train_loss:1.421e-02, rounded train_acc:0.884, valid_loss:2.270e-02, rounded valid_acc:0.879\n",
      "Iter: 13900, LR: 1.000e-03 - train_loss:1.063e-02, rounded train_acc:0.918, valid_loss:1.716e-02, rounded valid_acc:0.869\n",
      "Iter: 14000, LR: 1.000e-03 - train_loss:1.139e-02, rounded train_acc:0.913, valid_loss:1.917e-02, rounded valid_acc:0.881\n",
      "Iter: 14100, LR: 1.000e-03 - train_loss:1.351e-02, rounded train_acc:0.914, valid_loss:2.005e-02, rounded valid_acc:0.886\n",
      "Iter: 14200, LR: 1.000e-03 - train_loss:1.231e-02, rounded train_acc:0.911, valid_loss:1.840e-02, rounded valid_acc:0.881\n",
      "Iter: 14300, LR: 1.000e-03 - train_loss:9.378e-03, rounded train_acc:0.931, valid_loss:1.636e-02, rounded valid_acc:0.896\n",
      "Iter: 14400, LR: 1.000e-03 - train_loss:1.017e-02, rounded train_acc:0.936, valid_loss:1.981e-02, rounded valid_acc:0.877\n",
      "Iter: 14500, LR: 1.000e-03 - train_loss:1.363e-02, rounded train_acc:0.899, valid_loss:1.794e-02, rounded valid_acc:0.886\n",
      "Iter: 14600, LR: 1.000e-03 - train_loss:1.544e-02, rounded train_acc:0.905, valid_loss:1.465e-02, rounded valid_acc:0.890\n",
      "Iter: 14700, LR: 1.000e-03 - train_loss:1.313e-02, rounded train_acc:0.923, valid_loss:1.564e-02, rounded valid_acc:0.893\n",
      "Iter: 14800, LR: 1.000e-03 - train_loss:1.422e-02, rounded train_acc:0.910, valid_loss:1.348e-02, rounded valid_acc:0.882\n",
      "Iter: 14900, LR: 1.000e-03 - train_loss:1.216e-02, rounded train_acc:0.934, valid_loss:1.423e-02, rounded valid_acc:0.925\n",
      "Iter: 15000, LR: 1.000e-03 - train_loss:1.378e-02, rounded train_acc:0.926, valid_loss:1.390e-02, rounded valid_acc:0.925\n",
      "Iter: 15100, LR: 1.000e-03 - train_loss:8.686e-03, rounded train_acc:0.943, valid_loss:1.492e-02, rounded valid_acc:0.905\n",
      "Iter: 15200, LR: 1.000e-03 - train_loss:1.128e-02, rounded train_acc:0.934, valid_loss:1.610e-02, rounded valid_acc:0.915\n",
      "Iter: 15300, LR: 1.000e-03 - train_loss:1.555e-02, rounded train_acc:0.903, valid_loss:1.993e-02, rounded valid_acc:0.895\n",
      "Iter: 15400, LR: 1.000e-03 - train_loss:1.036e-02, rounded train_acc:0.938, valid_loss:1.512e-02, rounded valid_acc:0.893\n",
      "Iter: 15500, LR: 1.000e-03 - train_loss:1.540e-02, rounded train_acc:0.926, valid_loss:2.037e-02, rounded valid_acc:0.873\n",
      "Iter: 15600, LR: 1.000e-03 - train_loss:9.663e-03, rounded train_acc:0.939, valid_loss:1.659e-02, rounded valid_acc:0.904\n",
      "Iter: 15700, LR: 1.000e-03 - train_loss:2.039e-02, rounded train_acc:0.899, valid_loss:2.219e-02, rounded valid_acc:0.874\n",
      "Iter: 15800, LR: 1.000e-03 - train_loss:1.136e-02, rounded train_acc:0.936, valid_loss:1.828e-02, rounded valid_acc:0.891\n",
      "Iter: 15900, LR: 1.000e-03 - train_loss:1.522e-02, rounded train_acc:0.905, valid_loss:1.738e-02, rounded valid_acc:0.882\n",
      "Iter: 16000, LR: 1.000e-03 - train_loss:1.177e-02, rounded train_acc:0.933, valid_loss:1.976e-02, rounded valid_acc:0.880\n",
      "Iter: 16100, LR: 1.000e-03 - train_loss:1.172e-02, rounded train_acc:0.938, valid_loss:1.657e-02, rounded valid_acc:0.909\n",
      "Iter: 16200, LR: 1.000e-03 - train_loss:9.017e-03, rounded train_acc:0.954, valid_loss:1.333e-02, rounded valid_acc:0.914\n",
      "Iter: 16300, LR: 1.000e-03 - train_loss:9.670e-03, rounded train_acc:0.947, valid_loss:1.348e-02, rounded valid_acc:0.907\n",
      "Iter: 16400, LR: 1.000e-03 - train_loss:9.129e-03, rounded train_acc:0.958, valid_loss:1.799e-02, rounded valid_acc:0.899\n",
      "Iter: 16500, LR: 1.000e-03 - train_loss:1.978e-02, rounded train_acc:0.926, valid_loss:2.737e-02, rounded valid_acc:0.876\n",
      "Iter: 16600, LR: 1.000e-03 - train_loss:1.156e-02, rounded train_acc:0.928, valid_loss:1.551e-02, rounded valid_acc:0.886\n",
      "Iter: 16700, LR: 1.000e-03 - train_loss:8.230e-03, rounded train_acc:0.946, valid_loss:1.556e-02, rounded valid_acc:0.891\n",
      "Iter: 16800, LR: 1.000e-03 - train_loss:9.948e-03, rounded train_acc:0.941, valid_loss:1.499e-02, rounded valid_acc:0.890\n",
      "Iter: 16900, LR: 1.000e-03 - train_loss:1.462e-02, rounded train_acc:0.917, valid_loss:2.322e-02, rounded valid_acc:0.891\n",
      "Iter: 17000, LR: 1.000e-03 - train_loss:2.486e-02, rounded train_acc:0.920, valid_loss:2.761e-02, rounded valid_acc:0.892\n",
      "Iter: 17100, LR: 1.000e-03 - train_loss:1.805e-02, rounded train_acc:0.928, valid_loss:2.471e-02, rounded valid_acc:0.866\n",
      "Iter: 17200, LR: 1.000e-03 - train_loss:1.171e-02, rounded train_acc:0.944, valid_loss:2.174e-02, rounded valid_acc:0.898\n",
      "Iter: 17300, LR: 1.000e-03 - train_loss:1.207e-02, rounded train_acc:0.933, valid_loss:1.452e-02, rounded valid_acc:0.906\n",
      "Iter: 17400, LR: 1.000e-03 - train_loss:1.261e-02, rounded train_acc:0.935, valid_loss:1.907e-02, rounded valid_acc:0.911\n",
      "Iter: 17500, LR: 1.000e-03 - train_loss:2.589e-02, rounded train_acc:0.849, valid_loss:2.390e-02, rounded valid_acc:0.893\n",
      "Iter: 17600, LR: 1.000e-03 - train_loss:1.257e-02, rounded train_acc:0.928, valid_loss:1.328e-02, rounded valid_acc:0.941\n",
      "Iter: 17700, LR: 1.000e-03 - train_loss:1.207e-02, rounded train_acc:0.954, valid_loss:1.549e-02, rounded valid_acc:0.923\n",
      "Iter: 17800, LR: 1.000e-03 - train_loss:1.447e-02, rounded train_acc:0.938, valid_loss:2.094e-02, rounded valid_acc:0.903\n",
      "Iter: 17900, LR: 1.000e-03 - train_loss:1.253e-02, rounded train_acc:0.942, valid_loss:1.688e-02, rounded valid_acc:0.922\n",
      "Iter: 18000, LR: 1.000e-03 - train_loss:1.506e-02, rounded train_acc:0.900, valid_loss:2.424e-02, rounded valid_acc:0.877\n",
      "Iter: 18100, LR: 1.000e-03 - train_loss:1.652e-02, rounded train_acc:0.933, valid_loss:2.077e-02, rounded valid_acc:0.904\n",
      "Iter: 18200, LR: 1.000e-03 - train_loss:1.265e-02, rounded train_acc:0.936, valid_loss:1.803e-02, rounded valid_acc:0.898\n",
      "Iter: 18300, LR: 1.000e-03 - train_loss:1.431e-02, rounded train_acc:0.934, valid_loss:2.523e-02, rounded valid_acc:0.879\n",
      "Iter: 18400, LR: 1.000e-03 - train_loss:1.257e-02, rounded train_acc:0.926, valid_loss:1.541e-02, rounded valid_acc:0.904\n",
      "Iter: 18500, LR: 1.000e-03 - train_loss:1.391e-02, rounded train_acc:0.930, valid_loss:1.824e-02, rounded valid_acc:0.903\n",
      "Iter: 18600, LR: 1.000e-03 - train_loss:1.733e-02, rounded train_acc:0.932, valid_loss:2.411e-02, rounded valid_acc:0.889\n",
      "Iter: 18700, LR: 1.000e-03 - train_loss:1.535e-02, rounded train_acc:0.937, valid_loss:1.899e-02, rounded valid_acc:0.905\n",
      "Iter: 18800, LR: 1.000e-03 - train_loss:1.201e-02, rounded train_acc:0.934, valid_loss:1.923e-02, rounded valid_acc:0.902\n",
      "Iter: 18900, LR: 1.000e-03 - train_loss:1.184e-02, rounded train_acc:0.939, valid_loss:1.369e-02, rounded valid_acc:0.914\n",
      "Iter: 19000, LR: 1.000e-03 - train_loss:1.236e-02, rounded train_acc:0.937, valid_loss:2.121e-02, rounded valid_acc:0.889\n",
      "Iter: 19100, LR: 1.000e-03 - train_loss:1.085e-02, rounded train_acc:0.949, valid_loss:1.891e-02, rounded valid_acc:0.916\n",
      "Iter: 19200, LR: 1.000e-03 - train_loss:1.080e-02, rounded train_acc:0.942, valid_loss:1.306e-02, rounded valid_acc:0.912\n",
      "Iter: 19300, LR: 1.000e-03 - train_loss:1.480e-02, rounded train_acc:0.918, valid_loss:1.660e-02, rounded valid_acc:0.899\n",
      "Iter: 19400, LR: 1.000e-03 - train_loss:1.116e-02, rounded train_acc:0.936, valid_loss:1.247e-02, rounded valid_acc:0.910\n",
      "Iter: 19500, LR: 1.000e-03 - train_loss:1.018e-02, rounded train_acc:0.945, valid_loss:1.351e-02, rounded valid_acc:0.922\n",
      "Iter: 19600, LR: 1.000e-03 - train_loss:9.628e-03, rounded train_acc:0.940, valid_loss:1.319e-02, rounded valid_acc:0.907\n",
      "Iter: 19700, LR: 1.000e-03 - train_loss:2.579e-02, rounded train_acc:0.910, valid_loss:2.495e-02, rounded valid_acc:0.870\n",
      "Iter: 19800, LR: 1.000e-03 - train_loss:1.386e-02, rounded train_acc:0.916, valid_loss:1.713e-02, rounded valid_acc:0.904\n",
      "Iter: 19900, LR: 1.000e-03 - train_loss:8.321e-03, rounded train_acc:0.957, valid_loss:1.482e-02, rounded valid_acc:0.916\n",
      "Iter: 20000, LR: 1.000e-03 - train_loss:1.343e-02, rounded train_acc:0.943, valid_loss:1.823e-02, rounded valid_acc:0.898\n",
      "Iter: 20100, LR: 1.000e-03 - train_loss:1.899e-02, rounded train_acc:0.875, valid_loss:2.105e-02, rounded valid_acc:0.880\n",
      "Iter: 20200, LR: 1.000e-03 - train_loss:1.237e-02, rounded train_acc:0.921, valid_loss:1.567e-02, rounded valid_acc:0.913\n",
      "Iter: 20300, LR: 1.000e-03 - train_loss:2.974e-02, rounded train_acc:0.679, valid_loss:4.322e-02, rounded valid_acc:0.623\n",
      "Iter: 20400, LR: 1.000e-03 - train_loss:2.415e-02, rounded train_acc:0.742, valid_loss:3.650e-02, rounded valid_acc:0.660\n",
      "Iter: 20500, LR: 1.000e-03 - train_loss:2.375e-02, rounded train_acc:0.757, valid_loss:3.797e-02, rounded valid_acc:0.633\n",
      "Iter: 20600, LR: 1.000e-03 - train_loss:1.707e-02, rounded train_acc:0.783, valid_loss:3.148e-02, rounded valid_acc:0.669\n",
      "Iter: 20700, LR: 1.000e-03 - train_loss:1.702e-02, rounded train_acc:0.793, valid_loss:3.442e-02, rounded valid_acc:0.666\n",
      "Iter: 20800, LR: 1.000e-03 - train_loss:2.046e-02, rounded train_acc:0.787, valid_loss:3.576e-02, rounded valid_acc:0.626\n",
      "Iter: 20900, LR: 1.000e-03 - train_loss:2.211e-02, rounded train_acc:0.737, valid_loss:2.944e-02, rounded valid_acc:0.649\n",
      "Iter: 21000, LR: 1.000e-03 - train_loss:1.906e-02, rounded train_acc:0.772, valid_loss:2.738e-02, rounded valid_acc:0.667\n",
      "Iter: 21100, LR: 1.000e-03 - train_loss:1.678e-02, rounded train_acc:0.762, valid_loss:2.588e-02, rounded valid_acc:0.671\n",
      "Iter: 21200, LR: 1.000e-03 - train_loss:1.500e-02, rounded train_acc:0.794, valid_loss:2.567e-02, rounded valid_acc:0.694\n",
      "Iter: 21300, LR: 1.000e-03 - train_loss:1.950e-02, rounded train_acc:0.754, valid_loss:2.991e-02, rounded valid_acc:0.687\n",
      "Iter: 21400, LR: 1.000e-03 - train_loss:3.380e-02, rounded train_acc:0.746, valid_loss:4.372e-02, rounded valid_acc:0.721\n",
      "Iter: 21500, LR: 1.000e-03 - train_loss:2.688e-02, rounded train_acc:0.748, valid_loss:3.322e-02, rounded valid_acc:0.672\n",
      "Iter: 21600, LR: 1.000e-03 - train_loss:2.105e-02, rounded train_acc:0.773, valid_loss:3.422e-02, rounded valid_acc:0.711\n",
      "Iter: 21700, LR: 1.000e-03 - train_loss:2.454e-02, rounded train_acc:0.759, valid_loss:3.391e-02, rounded valid_acc:0.712\n",
      "Iter: 21800, LR: 1.000e-03 - train_loss:4.427e-02, rounded train_acc:0.575, valid_loss:5.125e-02, rounded valid_acc:0.458\n",
      "Iter: 21900, LR: 1.000e-03 - train_loss:3.625e-02, rounded train_acc:0.653, valid_loss:4.278e-02, rounded valid_acc:0.578\n",
      "Iter: 22000, LR: 1.000e-03 - train_loss:2.895e-02, rounded train_acc:0.660, valid_loss:3.281e-02, rounded valid_acc:0.604\n",
      "Iter: 22100, LR: 1.000e-03 - train_loss:3.384e-02, rounded train_acc:0.704, valid_loss:3.804e-02, rounded valid_acc:0.639\n",
      "Iter: 22200, LR: 1.000e-03 - train_loss:3.626e-02, rounded train_acc:0.748, valid_loss:4.430e-02, rounded valid_acc:0.649\n",
      "Iter: 22300, LR: 1.000e-03 - train_loss:2.840e-02, rounded train_acc:0.755, valid_loss:3.794e-02, rounded valid_acc:0.651\n",
      "Iter: 22400, LR: 1.000e-03 - train_loss:2.383e-02, rounded train_acc:0.780, valid_loss:3.576e-02, rounded valid_acc:0.686\n",
      "Iter: 22500, LR: 1.000e-03 - train_loss:5.824e-02, rounded train_acc:0.602, valid_loss:6.400e-02, rounded valid_acc:0.597\n",
      "Iter: 22600, LR: 1.000e-03 - train_loss:3.675e-02, rounded train_acc:0.725, valid_loss:4.362e-02, rounded valid_acc:0.658\n",
      "Iter: 22700, LR: 1.000e-03 - train_loss:4.925e-02, rounded train_acc:0.700, valid_loss:6.708e-02, rounded valid_acc:0.638\n",
      "Iter: 22800, LR: 1.000e-03 - train_loss:4.184e-02, rounded train_acc:0.753, valid_loss:6.551e-02, rounded valid_acc:0.692\n",
      "Iter: 22900, LR: 1.000e-03 - train_loss:4.287e-02, rounded train_acc:0.735, valid_loss:6.510e-02, rounded valid_acc:0.651\n",
      "Iter: 23000, LR: 1.000e-03 - train_loss:4.181e-02, rounded train_acc:0.732, valid_loss:6.391e-02, rounded valid_acc:0.645\n",
      "Iter: 23100, LR: 1.000e-03 - train_loss:4.262e-02, rounded train_acc:0.752, valid_loss:6.370e-02, rounded valid_acc:0.659\n",
      "Iter: 23200, LR: 1.000e-03 - train_loss:5.122e-02, rounded train_acc:0.697, valid_loss:7.260e-02, rounded valid_acc:0.657\n",
      "Iter: 23300, LR: 1.000e-03 - train_loss:4.939e-02, rounded train_acc:0.702, valid_loss:6.611e-02, rounded valid_acc:0.635\n",
      "Iter: 23400, LR: 1.000e-03 - train_loss:4.828e-02, rounded train_acc:0.675, valid_loss:5.883e-02, rounded valid_acc:0.585\n",
      "Iter: 23500, LR: 1.000e-03 - train_loss:3.955e-02, rounded train_acc:0.696, valid_loss:5.584e-02, rounded valid_acc:0.614\n",
      "Iter: 23600, LR: 1.000e-03 - train_loss:4.175e-02, rounded train_acc:0.647, valid_loss:5.536e-02, rounded valid_acc:0.594\n",
      "Iter: 23700, LR: 1.000e-03 - train_loss:4.804e-02, rounded train_acc:0.643, valid_loss:5.481e-02, rounded valid_acc:0.634\n",
      "Iter: 23800, LR: 1.000e-03 - train_loss:4.542e-02, rounded train_acc:0.656, valid_loss:5.311e-02, rounded valid_acc:0.653\n",
      "Iter: 23900, LR: 1.000e-03 - train_loss:4.147e-02, rounded train_acc:0.666, valid_loss:4.920e-02, rounded valid_acc:0.644\n",
      "Iter: 24000, LR: 1.000e-03 - train_loss:3.980e-02, rounded train_acc:0.679, valid_loss:5.230e-02, rounded valid_acc:0.664\n",
      "Iter: 24100, LR: 1.000e-03 - train_loss:4.367e-02, rounded train_acc:0.695, valid_loss:6.194e-02, rounded valid_acc:0.629\n",
      "Iter: 24200, LR: 1.000e-03 - train_loss:3.699e-02, rounded train_acc:0.718, valid_loss:5.901e-02, rounded valid_acc:0.651\n",
      "Iter: 24300, LR: 1.000e-03 - train_loss:4.187e-02, rounded train_acc:0.671, valid_loss:6.246e-02, rounded valid_acc:0.617\n",
      "Iter: 24400, LR: 1.000e-03 - train_loss:4.006e-02, rounded train_acc:0.726, valid_loss:6.234e-02, rounded valid_acc:0.661\n",
      "Iter: 24500, LR: 1.000e-03 - train_loss:3.633e-02, rounded train_acc:0.732, valid_loss:5.614e-02, rounded valid_acc:0.688\n",
      "Iter: 24600, LR: 1.000e-03 - train_loss:3.476e-02, rounded train_acc:0.717, valid_loss:5.456e-02, rounded valid_acc:0.685\n",
      "Iter: 24700, LR: 1.000e-03 - train_loss:3.244e-02, rounded train_acc:0.726, valid_loss:4.984e-02, rounded valid_acc:0.660\n",
      "Iter: 24800, LR: 1.000e-03 - train_loss:4.175e-02, rounded train_acc:0.662, valid_loss:5.548e-02, rounded valid_acc:0.650\n",
      "Iter: 24900, LR: 1.000e-03 - train_loss:5.968e-02, rounded train_acc:0.575, valid_loss:6.818e-02, rounded valid_acc:0.525\n",
      "Iter: 25000, LR: 1.000e-03 - train_loss:5.851e-02, rounded train_acc:0.620, valid_loss:7.010e-02, rounded valid_acc:0.569\n",
      "Iter: 25100, LR: 1.000e-03 - train_loss:5.730e-02, rounded train_acc:0.626, valid_loss:7.696e-02, rounded valid_acc:0.588\n",
      "Iter: 25200, LR: 1.000e-03 - train_loss:5.592e-02, rounded train_acc:0.628, valid_loss:7.137e-02, rounded valid_acc:0.610\n",
      "Iter: 25300, LR: 1.000e-03 - train_loss:6.202e-02, rounded train_acc:0.583, valid_loss:6.998e-02, rounded valid_acc:0.588\n",
      "Iter: 25400, LR: 1.000e-03 - train_loss:5.577e-02, rounded train_acc:0.608, valid_loss:7.316e-02, rounded valid_acc:0.635\n",
      "Iter: 25500, LR: 1.000e-03 - train_loss:4.988e-02, rounded train_acc:0.624, valid_loss:6.460e-02, rounded valid_acc:0.632\n",
      "Iter: 25600, LR: 1.000e-03 - train_loss:4.749e-02, rounded train_acc:0.644, valid_loss:6.724e-02, rounded valid_acc:0.623\n",
      "How about Test Data at dataset 64\n",
      "Iter: 25700, LR: 1.000e-03 - train_loss:4.648e-02, rounded train_acc:0.658, valid_loss:6.371e-02, rounded valid_acc:0.608\n",
      "Iter: 25800, LR: 1.000e-03 - train_loss:5.077e-02, rounded train_acc:0.636, valid_loss:6.449e-02, rounded valid_acc:0.556\n",
      "Iter: 25900, LR: 1.000e-03 - train_loss:5.449e-02, rounded train_acc:0.602, valid_loss:7.518e-02, rounded valid_acc:0.528\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# we use net at different training stage on the same test_input\u001b[39;00m\n\u001b[1;32m      2\u001b[0m net, _, (counter_lst, netout_lst, db_lst, Winput_lst, Winputbias_lst, \\\n\u001b[0;32m----> 3\u001b[0m          Woutput_lst, Wall_lst, marker_lst, loss_lst, acc_lst) \u001b[38;5;241m=\u001b[39m \u001b[43mnet_helpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyp_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyp_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetFunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetFunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mmfs1/gscratch/amath/zihan-zhang/mpn/net_helpers.py:101\u001b[0m, in \u001b[0;36mtrain_network\u001b[0;34m(params, net, device, verbose, train, hyp_dict, netFunction, test_input)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Wall_lst\u001b[38;5;241m.\u001b[39mappend(W_all_)\n\u001b[1;32m     99\u001b[0m     marker_lst\u001b[38;5;241m.\u001b[39mappend(dataset_idx)\n\u001b[0;32m--> 101\u001b[0m _, monitor_loss, monitor_acc \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_trails\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_trails\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_trails\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_thresh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyp_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrun_mode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (is_power_of_2_or_zero(dataset_idx) \u001b[38;5;129;01mor\u001b[39;00m dataset_idx \u001b[38;5;241m==\u001b[39m train_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_datasets\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# print(f\"monitor_loss: {monitor_loss}\")\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     loss_lst\u001b[38;5;241m.\u001b[39mappend(monitor_loss)\n",
      "File \u001b[0;32m/mmfs1/gscratch/amath/zihan-zhang/mpn/net_helpers.py:631\u001b[0m, in \u001b[0;36mBaseNetwork.fit\u001b[0;34m(self, train_params, train_data, train_trails, valid_batch, valid_trails, new_thresh, run_mode)\u001b[0m\n\u001b[1;32m    627\u001b[0m \t\t\u001b[38;5;28mprint\u001b[39m(init_string) \n\u001b[1;32m    630\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain() \u001b[38;5;66;03m# put in train mode (doesn't really do anything unless we are using dropout/batch norm)\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m db, monitor_loss, monitor_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_trails\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_trails\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_trails\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m\t\t\t  \u001b[49m\u001b[43mnew_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_thresh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval() \u001b[38;5;66;03m# return to eval mode\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m db, monitor_loss, monitor_acc\n",
      "File \u001b[0;32m/mmfs1/gscratch/amath/zihan-zhang/mpn/net_helpers.py:741\u001b[0m, in \u001b[0;36mBaseNetwork.train_epochs\u001b[0;34m(self, train_params, train_data, train_trails, valid_batch, valid_trails, new_thresh, monitor, run_mode)\u001b[0m\n\u001b[1;32m    738\u001b[0m \tt0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;66;03m# Backward start\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_type \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackprop\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_debug\u001b[39m\u001b[38;5;124m'\u001b[39m,): \u001b[38;5;66;03m# Standard supervised training procedures to compute gradient\u001b[39;00m\n\u001b[0;32m--> 741\u001b[0m \t\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    742\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_clip \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    743\u001b[0m \t\ttorch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_clip)\n",
      "File \u001b[0;32m/gscratch/deepthought/zihan/miniconda3/envs/playground/lib/python3.8/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gscratch/deepthought/zihan/miniconda3/envs/playground/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# we use net at different training stage on the same test_input\n",
    "net, _, (counter_lst, netout_lst, db_lst, Winput_lst, Winputbias_lst, \\\n",
    "         Woutput_lst, Wall_lst, marker_lst, loss_lst, acc_lst) = net_helpers.train_network(params, device=device, verbose=verbose, train=train, hyp_dict=hyp_dict, netFunction=netFunction, test_input=test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47256480-5b47-496e-887e-b8de35dcc8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    fig, ax = plt.subplots(1,1,figsize=(3,3))\n",
    "    ax.plot(net.hist['iters_monitor'][1:], net.hist['train_acc'][1:], color=c_vals[0], label='Full train accuracy')\n",
    "    ax.plot(net.hist['iters_monitor'][1:], net.hist['valid_acc'][1:], color=c_vals[1], label='Full valid accuracy')\n",
    "    if net.weight_reg is not None:\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['train_loss_output_label'], color=c_vals_l[0], zorder=-1, label='Output label')\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['train_loss_reg_term'], color=c_vals_l[0], zorder=-1, label='Reg term', linestyle='dashed')\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['valid_loss_output_label'], color=c_vals_l[1], zorder=-1, label='Output valid label')\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['valid_loss_reg_term'], color=c_vals_l[1], zorder=-1, label='Reg valid term', linestyle='dashed')\n",
    "    \n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    # ax.set_ylabel('Loss ({})'.format(net.loss_type))\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_xlabel('# Batches')\n",
    "    plt.savefig(f\"./paper/loss_{hyp_dict['ruleset']}_{task_params['fixate_off']}.png\")\n",
    "    \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e544aca8-271f-49d3-9ed3-ab7023f23600",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    net_helpers.net_eta_lambda_analysis(net, net_params, hyp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b385cd9-3f03-4c33-a943-ef80ca4df69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyp_dict['chosen_network'] == \"dmpn\":\n",
    "    if net_params[\"input_layer_add\"]:\n",
    "        input_matrix = net.W_initial_linear.weight.data.detach().cpu().numpy()\n",
    "        figinp, axsinp = plt.subplots(1,1,figsize=(4,4))\n",
    "        sns.heatmap(input_matrix, ax=axsinp, square=True, cmap='coolwarm')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c10ab17-bab6-4228-9096-af042e9ac385",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "use_finalstage = False\n",
    "if use_finalstage:\n",
    "    # plotting output in the validation set\n",
    "    net_out, db = net.iterate_sequence_batch(test_input, run_mode='track_states')\n",
    "    W_output = net.W_output.detach().cpu().numpy()\n",
    "\n",
    "    W_all_ = []\n",
    "    for i in range(len(net.mp_layers)):\n",
    "        W_all_.append(net.mp_layers[i].W.detach().cpu().numpy())\n",
    "    W_ = W_all_[0]\n",
    "    \n",
    "else:\n",
    "    ind = len(marker_lst)-1 \n",
    "    # ind = 0\n",
    "    network_at_percent = (marker_lst[ind]+1)/train_params['n_datasets']*100\n",
    "    print(f\"Using network at {network_at_percent}%\")\n",
    "    net_out = netout_lst[ind]\n",
    "    db = db_lst[ind]\n",
    "    W_output = Woutput_lst[ind]\n",
    "    if net_params[\"net_type\"] == \"dmpn\":\n",
    "        W_ = Wall_lst[ind][0]\n",
    "\n",
    "if net_params['loss_type'] in ('MSE',):\n",
    "    fig, axs = plt.subplots(5, 1, figsize=(4, 5*2))\n",
    "    figin, axsin = plt.subplots(5, 1, figsize=(4, 5*2))\n",
    "\n",
    "    if test_output_np.shape[-1] == 1:\n",
    "        for batch_idx, ax in enumerate(axs):\n",
    "            ax.plot(net_out[batch_idx, :, 0], color=c_vals[batch_idx])\n",
    "            ax.plot(test_output_np[batch_idx, :, 0], color=c_vals_l[batch_idx])\n",
    "\n",
    "    else:\n",
    "        for batch_idx, ax in enumerate(axs):\n",
    "            task_label = test_input_np[batch_idx, 0, 6-shift_index:]\n",
    "            task_label_index = np.where(task_label == 1)[0][0]\n",
    "            for out_idx in range(test_output_np.shape[-1]):\n",
    "                axs[batch_idx].plot(net_out[batch_idx, :, out_idx], color=c_vals[out_idx])\n",
    "                axs[batch_idx].plot(test_output_np[batch_idx, :, out_idx], color=c_vals_l[out_idx], linewidth=5, alpha=0.5)\n",
    "\n",
    "            input_batch = test_input[batch_idx,:,:].cpu().numpy()\n",
    "            for inp_idx in range(input_batch.shape[-1]):\n",
    "                axsin[batch_idx].plot(input_batch[:,inp_idx], color=c_vals[inp_idx], label=inp_idx)\n",
    "            axsin[batch_idx].legend()\n",
    "\n",
    "    fig.suptitle(f\"Validation Set Output Comparison using Network at Percentage {network_at_percent}%\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"./paper/lowD_{hyp_dict['ruleset']}_{hyp_dict['chosen_network']}_{hyp_dict['addon_name']}.png\")\n",
    "\n",
    "\n",
    "    figin.suptitle(f\"Validation Set Output Comparison using Network at Percentage {network_at_percent}%\")\n",
    "    figin.tight_layout()\n",
    "    figin.savefig(f\"./paper/lowD_{hyp_dict['ruleset']}_{hyp_dict['chosen_network']}_{hyp_dict['addon_name']}_input.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9efbd2f-a48d-4f8e-ba55-b0b102463546",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_marker = 1 if net_params[\"input_layer_add\"] else 0\n",
    "\n",
    "# here db is selected based on learning stage selection \n",
    "def modulation_extraction(db):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print(db.keys())\n",
    "    Ms = np.concatenate((\n",
    "        db[f'M{ll_marker}'].detach().cpu().numpy().reshape(n_batch_all, max_seq_len, -1),\n",
    "    ), axis=-1)\n",
    "\n",
    "    Ms_orig = np.concatenate((\n",
    "        db[f'M{ll_marker}'].detach().cpu().numpy(),\n",
    "    ), axis=-1)\n",
    "\n",
    "    bs = np.concatenate((\n",
    "        db[f'b{ll_marker}'].detach().cpu().numpy(),\n",
    "    ), axis=-1) \n",
    "\n",
    "    hs = np.concatenate((\n",
    "        db[f'hidden{ll_marker}'].detach().cpu().numpy().reshape(n_batch_all, max_seq_len, -1),\n",
    "    ), axis=-1)\n",
    "\n",
    "    return Ms, Ms_orig, hs, bs\n",
    "    \n",
    "if net_params[\"net_type\"] in (\"dmpn\", ):\n",
    "    if mpn_depth == 1:\n",
    "        Ms, Ms_orig, hs, bs = modulation_extraction(db)\n",
    "    else:\n",
    "        modulations, hiddens = [], []\n",
    "        for i in range(mpn_depth):\n",
    "            modulations.append(db[f'M{i}'].detach().cpu().numpy().reshape(n_batch_all, max_seq_len, -1))\n",
    "            hiddens.append(db[f'hidden{i}'].detach().cpu().numpy().reshape(n_batch_all, max_seq_len, -1),)\n",
    "\n",
    "        Ms = modulations[0]\n",
    "        hs = hiddens[0]\n",
    "        \n",
    "elif net_params[\"net_type\"] in (\"vanilla\", \"gru\"):\n",
    "    hs = db['hidden'].detach().cpu().numpy()\n",
    "\n",
    "pca_type = 'full' # full, cell_types\n",
    "pca_target_lst = ['hs', 'Ms'] # hs, 'Ms' \n",
    "if net_params[\"net_type\"] in (\"vanilla\", \"gru\"):\n",
    "    pca_target_lst = ['hs'] # if not dmpn, no M information effectively\n",
    "\n",
    "# using recorded information\n",
    "recordkyle_all, recordkyle_nameall = [], []\n",
    "for test_subtrial in test_trials:\n",
    "    metaepoch = test_subtrial.epochs\n",
    "    periodname = list(metaepoch.keys())\n",
    "    recordkyle, recordkyle_name = [], []\n",
    "    for keyiter in range(len(periodname)):\n",
    "        try:\n",
    "            recordkyle_name.append(periodname[keyiter])\n",
    "            if test_mode_for_all == \"random\":\n",
    "                recordkyle.append(metaepoch[periodname[keyiter]][1])\n",
    "            elif test_mode_for_all == \"random_batch\":\n",
    "                recordkyle.append(list(metaepoch[periodname[keyiter]][1]))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    if test_mode_for_all in (\"random\",):\n",
    "        fillrecordkyle = []\n",
    "        for timestamp in recordkyle:\n",
    "            fillrecordkyle.append([timestamp for _ in range(hs.shape[0])])\n",
    "        recordkyle = fillrecordkyle\n",
    "\n",
    "    recordkyle.insert(0, [0 for _ in range(len(recordkyle[1]))])\n",
    "    recordkyle = np.array(recordkyle).T.tolist()\n",
    "    recordkyle_all.extend(recordkyle)\n",
    "    recordkyle_nameall.append(recordkyle_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f41fdc5-88a2-459a-8ea5-8177aa1a9393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sep 30th\n",
    "# This part of code should be adaptive for multitask, which may have different breaks and periods for each task\n",
    "unique_lists = set(tuple(lst) for lst in recordkyle_all)\n",
    "# here select task specific information\n",
    "# which maximally should have length of number of tasks\n",
    "unique_recordkyle_all = [list(lst) for lst in unique_lists]\n",
    "if not task_random_fix:\n",
    "    assert len(unique_recordkyle_all) >= len(rules_dict[hyp_dict['ruleset']])\n",
    "else:\n",
    "    print(\"Test DataSet Random Seed Is Fixed\")\n",
    "\n",
    "all_session_breakdown = []\n",
    "for task_specific_time in unique_recordkyle_all:\n",
    "    session_breakdown = []\n",
    "    for sindex in range(0,len(task_specific_time)-1):\n",
    "        # all sessions should be the same for each task\n",
    "        # but different across tasks\n",
    "        # though the time of when response period starts might be similar across\n",
    "        session_breakdown.append([task_specific_time[sindex], task_specific_time[sindex+1]]) \n",
    "    session_breakdown.append([task_specific_time[0], task_specific_time[-1]])\n",
    "    all_session_breakdown.append(session_breakdown)\n",
    "\n",
    "# break down time\n",
    "all_breaks = []\n",
    "for session_breakdown in all_session_breakdown:\n",
    "    breaks = [cut[1] for cut in session_breakdown[:-1]]\n",
    "    print(f\"Task {all_session_breakdown.index(session_breakdown)}; breaks: {breaks}\")\n",
    "    all_breaks.append(breaks)\n",
    "\n",
    "# for delay-task\n",
    "assert len(all_breaks)\n",
    "response_start = all_breaks[0][-2]\n",
    "stimulus_start = all_breaks[0][0]\n",
    "stimulus_end = all_breaks[0][1]\n",
    "print(f\"response_start: {response_start}\")\n",
    "print(f\"stimulus_start: {stimulus_start}\")\n",
    "print(f\"stimulus_end: {stimulus_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5adc633-8a73-4e8b-8733-b4ce98482ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages_num = len(Wall_lst) # how many recorded neurons in total\n",
    "break_info = all_breaks[0]\n",
    "\n",
    "input_nums = Ms_orig.shape[-1]\n",
    "batch_nums = Ms_orig.shape[0]\n",
    "neuron_nums = Ms_orig.shape[2]\n",
    "colors = helper.generate_rainbow_colors(Ms_orig.shape[2])\n",
    "\n",
    "def generate_random_orthonormal_matrix(N, num_columns=3):\n",
    "    \"\"\"\n",
    "    generates an N x num_columns random matrix with orthonormal columns.\n",
    "    \"\"\"\n",
    "    random_matrix = np.random.randn(N, num_columns)    \n",
    "    Q, R = np.linalg.qr(random_matrix)    \n",
    "    return Q[:, :num_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e212e03-6fb4-439b-9fd6-5aefd8a09a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check from equation 2-7\n",
    "def plot_trajectory_by_index(label_index, stage_iter, verbose=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    W_ = Wall_lst[stage_iter][0]\n",
    "    W_output = Woutput_lst[stage_iter]\n",
    "    _, Ms_orig, _, bs = modulation_extraction(db_lst[stage_iter]) # batch * seq_len * hidden_neuron * input_neuron\n",
    "\n",
    "    if verbose:\n",
    "        figsize1, figsize2 = 2, 6\n",
    "        figexh1, axsexh1 = plt.subplots(3,3,figsize=(figsize2*3,figsize1*3))  \n",
    "        figexh2, axsexh2 = plt.subplots(4,3,figsize=(figsize2*3,figsize1*4))  \n",
    "        figdiff, axsdiff = plt.subplots(1,2,figsize=(4*2,2))\n",
    "\n",
    "    task_middle_dict = {}\n",
    "    task_labels_across_batch = []\n",
    "\n",
    "    saver_shape1 = (3,3)\n",
    "    saver1 = np.empty((test_input.shape[0], saver_shape1[0], saver_shape1[1]), dtype=object)\n",
    "    saver_shape2 = (4,3)\n",
    "    saver2 = np.empty((test_input.shape[0], saver_shape2[0]+1, saver_shape2[1]), dtype=object)\n",
    "    saver2_random = np.empty((test_input.shape[0], saver_shape2[0]+1, saver_shape2[1]), dtype=object) # projection to random space\n",
    "\n",
    "    random_output_Y_lst = [generate_random_orthonormal_matrix(W_output.shape[1]) for _ in range(10)]\n",
    "\n",
    "    for batch_iter in range(test_input.shape[0]):\n",
    "        writeon = 0\n",
    "        labels_for_batch = labels[batch_iter,0]\n",
    "        \n",
    "        if labels_for_batch in label_index: # >=0: for all label; ==0, say, for specific label on the ring (regardless on which task is using)\n",
    "            xx = test_input[batch_iter, :, :].cpu().numpy()[0,6-shift_index:]\n",
    "            which_task = np.where(xx)[0][0] # extract here, will repeat later at different time slices\n",
    "            \n",
    "            if labels_for_batch not in task_middle_dict.keys():\n",
    "                task_middle_dict[which_task] = []\n",
    "                writeon = 1\n",
    "                \n",
    "            x_batch_taskinfo = test_input[batch_iter, :, :][:,6-shift_index:].cpu().numpy()[0,:]\n",
    "            task_specific = np.where(x_batch_taskinfo == 1)[0]\n",
    "            assert len(task_specific) == 1\n",
    "            task_specific = task_specific[0]\n",
    "            \n",
    "            task_labels_across_batch.append(task_specific) # load task information (which task) across batches\n",
    "        \n",
    "            res_eq26, res_eq8, res_eq11 = [], [], []\n",
    "            res_meta = []\n",
    "\n",
    "            for i in range(saver_shape1[0]):\n",
    "                for j in range(saver_shape1[1]):\n",
    "                    saver1[batch_iter, i, j] = np.array([])\n",
    "        \n",
    "            for i in range(saver_shape2[0]+1):\n",
    "                for j in range(saver_shape2[1]):\n",
    "                    saver2[batch_iter, i, j] = np.array([])\n",
    "                    saver2_random[batch_iter, i, j] = np.array([])\n",
    "        \n",
    "            for time_iter in range(test_input.shape[1]):\n",
    "                x = test_input[batch_iter, time_iter, :].cpu().numpy().reshape(-1,1)\n",
    "                \n",
    "                input_length = len(x)\n",
    "        \n",
    "                x_fixon, x_fixoff, x_stimulus, x_task = [np.zeros((input_length, 1)) for _ in range(4)]\n",
    "                # one-hot encoded vector for fixation\n",
    "                x_fixon[0,0] = x[0,0] \n",
    "                # one-hot encoded vector for fixation off (set to dummy if not presented)\n",
    "                x_fixoff[1,0] = x[1,0] if task_params['fixate_off'] else 0\n",
    "                # one-hot encoded vector for stimulus\n",
    "                x_stimulus[2-shift_index:6-shift_index,0] = x[2-shift_index:6-shift_index,0]\n",
    "                # one-hot encoded vector for task\n",
    "                # task (dynamically setting for all element after the 6th elements)\n",
    "                tasks_info = x[6-shift_index:,0]\n",
    "                x_task[6-shift_index:,0] = tasks_info\n",
    "\n",
    "                which_task = np.where(tasks_info)[0][0]\n",
    "                \n",
    "                Mt = Ms_orig[batch_iter, time_iter, :, :] \n",
    "                bt = bs[batch_iter, time_iter, :].reshape(-1,1) # hidden_neuron * 1\n",
    "                \n",
    "                middle =  W_ + W_ * Mt\n",
    "\n",
    "                if time_iter >= response_start + 1 and len(label_index) == 1:\n",
    "                    if writeon:\n",
    "                        task_middle_dict[which_task].append(middle)\n",
    "                \n",
    "                y_fix = W_output[0,:].reshape(1,-1)\n",
    "                Y_resp1 = W_output[1,:].reshape(1,-1)\n",
    "                Y_resp2 = W_output[2,:].reshape(1,-1)\n",
    "\n",
    "                if task_params['fixate_off']:\n",
    "                    allX1 = [x_fixon+x_task, x_fixoff+x_task, x_stimulus+x_fixon+x_task]\n",
    "                else:\n",
    "                    allX1 = [x_fixon+x_task, x_task, x_stimulus+x_fixon+x_task]\n",
    "                allX1name = [\"x_fixon+x_task\", \"x_fixoff+x_task\", \"x_stimulus+x_fixon+x_task\"]\n",
    "                allX2 = [x_fixon, x_fixoff, x_stimulus, x_task]\n",
    "                allX2name = [\"x_fixon\", \"x_fixoff\", \"x_stimulus\", \"x_task\"]\n",
    "                allY = [y_fix, Y_resp1, Y_resp2]\n",
    "                allYname = [\"y_fix\", \"Y_resp1\", \"Y_resp2\"]\n",
    "        \n",
    "                for yiter in range(len(allY)):\n",
    "                    for xiter in range(len(allX1)):\n",
    "                        # res1 = helper.to_unit_vector(allY[yiter]) @ helper.to_unit_vector(middle @ allX1[xiter])\n",
    "                        step1 = middle @ allX1[xiter] + bt # adjust according to specific bias \n",
    "                        res1 = allY[yiter] @ step1 \n",
    "                        saver1[batch_iter, xiter, yiter] = np.append(saver1[batch_iter, xiter, yiter], res1[0,0])\n",
    "        \n",
    "                for y1 in range(len(allY)):\n",
    "                    for x1 in range(len(allX2)):\n",
    "                        # res2 = helper.to_unit_vector(allY[yiter]) @ helper.to_unit_vector(middle @ allX2[xiter])\n",
    "                        step1 = middle @ allX2[x1]\n",
    "                        res2 = allY[y1] @ step1\n",
    "                        res2_random = [((random_output_Y[:,y1].reshape(1,-1)) @ middle @ allX2[x1])[0,0] for random_output_Y in random_output_Y_lst]\n",
    "                        \n",
    "                        saver2[batch_iter, x1, y1] = np.append(saver2[batch_iter, x1, y1], res2[0,0])\n",
    "                        saver2_random[batch_iter, x1, y1] = np.append(saver2_random[batch_iter, x1, y1], np.mean(res2_random))\n",
    "\n",
    "                # how about bias projection to output\n",
    "                for y_iter2 in range(len(allY)):\n",
    "                    step1 = bt \n",
    "                    res2 = allY[y_iter2] @ step1\n",
    "                    saver2[batch_iter, len(allX2), y_iter2] = np.append(saver2[batch_iter, len(allX2), y_iter2], res2[0,0])\n",
    "\n",
    "            if verbose:\n",
    "                for i in range(saver_shape1[0]):\n",
    "                    for j in range(saver_shape1[1]):\n",
    "                        axsexh1[i,j].plot(saver1[batch_iter,i,j], color=c_vals[labels_for_batch], linestyle=l_vals[task_specific])\n",
    "            \n",
    "                for i in range(saver_shape2[0]):\n",
    "                    for j in range(saver_shape2[1]):            \n",
    "                        axsexh2[i,j].plot(saver2[batch_iter,i,j], color=c_vals[labels_for_batch], linestyle=l_vals[task_specific])\n",
    "        \n",
    "                # # extract fixon-task information explicitly\n",
    "                axsdiff[0].plot(saver2[batch_iter,0,1] + saver2[batch_iter,3,1], color=c_vals[labels_for_batch], linestyle=l_vals[task_specific])\n",
    "                axsdiff[0].plot(saver2_random[batch_iter,0,1] + saver2_random[batch_iter,3,1], color=c_vals_l[labels_for_batch], linestyle=l_vals[task_specific])\n",
    "                \n",
    "                axsdiff[1].plot(saver2[batch_iter,0,2] + saver2[batch_iter,3,2], color=c_vals[labels_for_batch], linestyle=l_vals[task_specific])\n",
    "                axsdiff[1].plot(saver2_random[batch_iter,0,2] + saver2_random[batch_iter,3,2], color=c_vals_l[labels_for_batch], linestyle=l_vals[task_specific])\n",
    "\n",
    "    if verbose:\n",
    "        # plot fixon/task information for one specific stimulus on one figure\n",
    "        # show perfect cancellation until fixon info goes away (during response period)\n",
    "        figpaper, axspaper = plt.subplots(8,1,figsize=(4, figsize1*8))\n",
    "\n",
    "        temp_saver = []\n",
    "\n",
    "        for batch_iter in range(test_input.shape[0]):\n",
    "            labels_for_batch = labels[batch_iter,0]\n",
    "            if labels_for_batch in label_index and labels_for_batch not in temp_saver:\n",
    "                f_fixon, f_task, f_bias = saver2[batch_iter, 0, 1], saver2[batch_iter, 3, 1], saver2[batch_iter, -1, 1]\n",
    "                axspaper[len(temp_saver)].plot(f_fixon, color=c_vals[0], linestyle=l_vals[0], label=\"Fixon\")\n",
    "                axspaper[len(temp_saver)].plot(f_task+f_bias, color=c_vals[1], linestyle=l_vals[1], label=\"Task+Bias\")\n",
    "                axspaper[len(temp_saver)].plot(f_fixon+f_task+f_bias, color=c_vals[2], linestyle=l_vals[3], linewidth=3, \\\n",
    "                                                       label=\"Combine\")\n",
    "                axspaper[len(temp_saver)].axhline(0, color=c_vals[3])\n",
    "\n",
    "                temp_saver.append(labels_for_batch)\n",
    "\n",
    "        for axsp in axspaper:\n",
    "            axsp.legend()\n",
    "            axsp.set_ylim([-2.0, 2.0])\n",
    "        figpaper.tight_layout()   \n",
    "        figpaper.savefig(\"./paper/show.png\")\n",
    "        \n",
    "        for i in range(saver_shape1[0]):\n",
    "            for j in range(saver_shape1[1]):\n",
    "                axsexh1[i,j].set_ylim([-2.0, 2.0])\n",
    "                axsexh1[i,j].set_title(f\"{allX1name[i]} & {allYname[j]}\")\n",
    "        \n",
    "        for i in range(saver_shape2[0]):\n",
    "            for j in range(saver_shape2[1]):\n",
    "                axsexh2[i,j].set_ylim([-2.0, 2.0])\n",
    "                axsexh2[i,j].set_title(f\"{allX2name[i]} & {allYname[j]}\")\n",
    "        \n",
    "        for ax in np.concatenate((axsexh1.flatten(), axsexh2.flatten())):\n",
    "            for breaks in all_breaks:\n",
    "                for bb in breaks:\n",
    "                    ax.axvline(bb, linestyle=\"--\", c=c_vals[all_breaks.index(breaks)])\n",
    "    \n",
    "        label_index_name = \"all\" if len(label_index) == 8 else label_index\n",
    "        \n",
    "        figexh1.suptitle(f\"Exhaustive Search 1 {color_by} at Stage {stage_iter}\")\n",
    "        figexh1.tight_layout()\n",
    "        figexh1.savefig(f\"./paper/es1_{task_params['fixate_off']}_{network_at_percent}_{label_index_name}.png\")\n",
    "        \n",
    "        figexh2.suptitle(f\"Exhaustive Search 2 {color_by} Stage {stage_iter}\")\n",
    "        figexh2.tight_layout()\n",
    "        figexh2.savefig(f\"./results/es2_{task_params['fixate_off']}_{network_at_percent}_{label_index_name}.png\")\n",
    "\n",
    "        axsdiff[0].set_title(\"Stimulus 1\")\n",
    "        axsdiff[1].set_title(\"Stimulus 2\")\n",
    "        figdiff.suptitle(f\"Fixon-Task at Stage {stage_iter}\")\n",
    "        figdiff.tight_layout()\n",
    "        figdiff.savefig(f\"./paper/diff_{task_params['fixate_off']}_{network_at_percent}_{label_index_name}.png\")\n",
    "\n",
    "    return task_middle_dict if len(label_index) == 1 else {}, task_labels_across_batch, saver2, saver2_random # only do it for single task learnig for clarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954cbe71-7953-48eb-9f2f-af7d5b3be088",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trajectory, all_trajectory_random = [], []\n",
    "for stage_iter in range(stages_num):\n",
    "    task_middle_dict, task_labels_across_batch, save_trajectory, save_trajectory_random = plot_trajectory_by_index(np.unique(labels), \\\n",
    "                                                                                                                    stage_iter, \\\n",
    "                                                                                                                    verbose=(stage_iter==stages_num-1)\n",
    "                                                                                          )\n",
    "    all_trajectory.append(save_trajectory)\n",
    "    all_trajectory_random.append(save_trajectory_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d7d96a-195a-4b05-adda-ef5af48cc49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_trajectory(save_trajectory, save_trajectory_random):\n",
    "    \"\"\"\n",
    "    Analyze trajectories by calculating mean absolute values for fixations and tasks.\n",
    "    \"\"\"\n",
    "    def process_trajectory(trajectory, ind=False):\n",
    "        results = []\n",
    "        for batch in trajectory:\n",
    "            stim1_fixon = batch[0, 1][stimulus_start:response_start]\n",
    "            stim1_task = batch[3, 1][stimulus_start:response_start]\n",
    "            if ind: \n",
    "                bias = batch[4, 1][stimulus_start:response_start]\n",
    "            else:\n",
    "                bias = np.zeros_like(stim1_fixon.shape)\n",
    "                \n",
    "            results.append([np.mean(np.abs(stim1_fixon + stim1_task + bias)), np.mean(np.abs(stim1_fixon)), np.mean(np.abs(stim1_task))])\n",
    "        return np.array(results)\n",
    "    \n",
    "    # Process both trajectories\n",
    "    result = process_trajectory(save_trajectory, True)\n",
    "    result_random = process_trajectory(save_trajectory_random)\n",
    "    \n",
    "    # Return the mean of the computed values\n",
    "    return np.mean(result[:, 0]), np.mean(result[:, 1]), np.mean(result[:,2]), np.mean(result_random[:, 0]), np.mean(result_random[:, 1]), np.mean(result_random[:, 2])\n",
    "\n",
    "fixon_task_diff = np.array([analyze_trajectory(all_trajectory[i], all_trajectory_random[i]) for i in range(len(all_trajectory_random))])\n",
    "\n",
    "figfixontaskcancel, axsfixontaskcancel = plt.subplots(figsize=(4,2))\n",
    "axsfixontaskcancel.plot(counter_lst, fixon_task_diff[:,0], \"-o\", c=c_vals[0], label=\"abs(fixon-task)\")\n",
    "axsfixontaskcancel.plot(counter_lst, fixon_task_diff[:,1], \"-o\", c=c_vals[1], label=\"abs(fixon)\")\n",
    "axsfixontaskcancel.plot(counter_lst, fixon_task_diff[:,2], \"-o\", c=c_vals[2], label=\"abs(task)\")\n",
    "# axsfixontaskcancel[0].plot(fixon_task_diff[:,3], \"-o\", c=c_vals_l[0], label=\"abs(fixon-task) random\")\n",
    "# axsfixontaskcancel[0].plot(fixon_task_diff[:,4], \"-o\", c=c_vals_l[1], label=\"abs(fixon) random\")\n",
    "# axsfixontaskcancel[0].plot(fixon_task_diff[:,5], \"-o\", c=c_vals_l[2], label=\"abs(task) random\")\n",
    "axsfixontaskcancel.legend()\n",
    "axsfixontaskcancel.set_ylabel(\"Projection Magnitude\")\n",
    "# axsfixontaskcancel.set_title(\"Average Cancellation Effect Before Response Period\")\n",
    "\n",
    "axsfixontaskcancel.set_xlabel(\"# Dataset\")\n",
    "axsfixontaskcancel.set_xscale(\"log\")\n",
    "figfixontaskcancel.tight_layout()\n",
    "figfixontaskcancel.savefig(\"./paper/cancel.png\")\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(4,2))  # Create a new figure\n",
    "\n",
    "ax1.plot(counter_lst, loss_lst, \"-o\", c=c_vals[0], label=\"Loss\")\n",
    "ax1.set_ylabel(\"Loss\", color=c_vals[0])\n",
    "ax1.tick_params(axis='y', colors=c_vals[0])\n",
    "ax1.set_yscale(\"log\")  # Keep log scale for loss\n",
    "ax1.set_xlabel(\"Counter\")\n",
    "\n",
    "# Create a second y-axis for accuracy (right)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(counter_lst, acc_lst, \"-o\", c=c_vals[1], label=\"Accuracy\")\n",
    "ax2.set_ylabel(\"Accuracy\", color=c_vals[1])\n",
    "ax2.tick_params(axis='y', colors=c_vals[1])\n",
    "\n",
    "ax1.set_xlabel(\"# Dataset\")\n",
    "ax1.set_xscale(\"log\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"./paper/loss_acc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80c1aa8-307f-4c84-9255-4e9d7cdc75db",
   "metadata": {},
   "outputs": [],
   "source": [
    "modulation_dict_diff_lst = []\n",
    "modulation_dict_lst = []\n",
    "hidden_output_dict_lst = []\n",
    "hidden_dict_lst = []\n",
    "hidden_all_dict_lst = []\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=stages_num,\n",
    "    specs=[[{'type': 'scatter3d'}] * stages_num],\n",
    "    subplot_titles=[f\"Stage {i+1}\" for i in range(stages_num)]  # Add titles for each subplot\n",
    ")\n",
    "\n",
    "camera = dict(\n",
    "    eye=dict(x=1.25, y=1.25, z=1.25),  # Position of the camera\n",
    "    up=dict(x=0, y=0, z=1),            # Orientation of the camera\n",
    "    center=dict(x=0, y=0, z=0),        # Focal point of the camera\n",
    ")\n",
    "\n",
    "for stage_iter in range(stages_num):\n",
    "    Woutput = Woutput_lst[stage_iter]\n",
    "    _, Ms_orig, hs, bs = modulation_extraction(db_lst[stage_iter])\n",
    "    \n",
    "    hs_stimulus = hs[:,stimulus_start-1:stimulus_end,:]\n",
    "    Ms_stimulus = Ms_orig[:,stimulus_start-1:stimulus_end,:,:]\n",
    "\n",
    "    modulation_diff_dict, modulation_dict, hidden_output_dict, hidden_dict, hidden_all_dict = {}, {}, {}, {}, {}\n",
    "    \n",
    "    for batch_iter in range(batch_nums):\n",
    "        hs_stimulus_batch = hs_stimulus[batch_iter,:,:]\n",
    "        hs_stimulus_batch_output = hs_stimulus_batch @ Woutput.T\n",
    "\n",
    "        trace = go.Scatter3d(\n",
    "            x=hs_stimulus_batch_output[:, 0],\n",
    "            y=hs_stimulus_batch_output[:, 1],\n",
    "            z=hs_stimulus_batch_output[:, 2],\n",
    "            mode='lines+markers', \n",
    "            line=dict(color=c_vals[labels[batch_iter][0]], width=2),  \n",
    "            marker=dict(size=5, symbol='circle'), \n",
    "            name=f\"Batch {batch_iter} - Stage {stage_iter}\",\n",
    "            showlegend=False \n",
    "        )\n",
    "        fig.add_trace(trace, row=1, col=stage_iter + 1)\n",
    "        \n",
    "        # modulation \n",
    "        Ms_stimulus_fixon = Ms_stimulus[batch_iter,-1,:,0] - Ms_stimulus[batch_iter,0,:,0]\n",
    "        Ms_stimulus_task = Ms_stimulus[batch_iter,-1,:,-1] - Ms_stimulus[batch_iter,0,:,-1]\n",
    "        \n",
    "        modulation_diff_dict[labels[batch_iter,0]] = Ms_stimulus_fixon # change of modulation on fixon during stimulus period\n",
    "        modulation_dict[labels[batch_iter,0]] =  Ms_stimulus[batch_iter,-1,:,0]\n",
    "        hidden_output_dict[labels[batch_iter,0]] = hs_stimulus_batch_output\n",
    "        hidden_dict[labels[batch_iter,0]] = hs_stimulus_batch[-1,:]\n",
    "        hidden_all_dict[labels[batch_iter,0]] = hs_stimulus_batch\n",
    "\n",
    "    modulation_dict_diff_lst.append(modulation_diff_dict)\n",
    "    modulation_dict_lst.append(modulation_dict)\n",
    "    hidden_output_dict_lst.append(hidden_output_dict)\n",
    "    hidden_dict_lst.append(hidden_dict)\n",
    "    hidden_all_dict_lst.append(hidden_all_dict)\n",
    "\n",
    "for stage_iter in range(stages_num):\n",
    "    fig.update_layout(\n",
    "        **{\n",
    "            f\"scene{stage_iter + 1}\": dict(\n",
    "                xaxis=dict(range=[-1.3, 1.3], title=\"X\"),\n",
    "                yaxis=dict(range=[-1.3, 1.3], title=\"Y\"),\n",
    "                zaxis=dict(range=[-1.3, 1.3], title=\"Z\"),\n",
    "                aspectmode='cube',  \n",
    "                camera=camera,       \n",
    "                domain=dict(\n",
    "                    x=[stage_iter / stages_num, (stage_iter + 1) / stages_num - 0.02],  \n",
    "                    y=[0, 1]  \n",
    "                )\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"3D Interactive Plot for Different Stages\",\n",
    "    height=600, \n",
    "    width=600 * stages_num, \n",
    "    margin=dict(l=10, r=10, t=50, b=10),  \n",
    ")\n",
    "\n",
    "output_file = \"./save/3d_interactive_plot_compact.html\"\n",
    "fig.write_html(output_file)\n",
    "\n",
    "print(f\"Plot saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb427a04-1279-4f63-867b-87dee94a7745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "modulation_change_stage, m_corr_stage, h_corr_stage = [], [], []\n",
    "fig_hc, axs_hc = plt.subplots(figsize=(6,2))\n",
    "\n",
    "def binarize(arr, threshold):\n",
    "    \"\"\"\"\"\"\n",
    "    return (np.abs(arr) > threshold).astype(int)\n",
    "\n",
    "def normalized_participation_ratio(cov_matrix):\n",
    "    \"\"\"\"\"\"\n",
    "    eigenvalues = np.linalg.eigvalsh(cov_matrix)  \n",
    "    sum_eigen = np.sum(eigenvalues)\n",
    "    sum_eigen_sq = np.sum(eigenvalues ** 2)    \n",
    "    N = len(eigenvalues)\n",
    "    npr = (sum_eigen ** 2) / (N * sum_eigen_sq)\n",
    "    return npr\n",
    "    \n",
    "for i in range(stages_num):\n",
    "    def analyze_hm_change(lst):\n",
    "        md = lst[i]\n",
    "        md_m = [np.array(value) for value in md.values()]\n",
    "        md_m = np.column_stack(md_m)\n",
    "        md_m = md_m.T # num_stimulus * hidden_size\n",
    "        mc_stage = list(np.sum(np.abs(md_m), axis=1))\n",
    "        # synptic_corr = np.corrcoef(md_m, rowvar=True)\n",
    "        synptic_corr = cosine_similarity(md_m)\n",
    "                \n",
    "        mean_corr = np.nanmean(np.triu(synptic_corr))\n",
    "        return mean_corr, mc_stage, md_m\n",
    "        \n",
    "    m_mean_corr, _, md_m = analyze_hm_change(modulation_dict_lst)\n",
    "    m_diff_mean_corr, mc_stage, md_m_diff = analyze_hm_change(modulation_dict_diff_lst)\n",
    "    h_mean_corr, _, md_h = analyze_hm_change(hidden_dict_lst)\n",
    "        \n",
    "    m_corr_stage.append(m_mean_corr)\n",
    "    h_corr_stage.append(h_mean_corr)\n",
    "    modulation_change_stage.append(mc_stage)\n",
    "\n",
    "    if i == stages_num - 1:\n",
    "        sns.heatmap(md_m_diff, ax=axs_hc, cmap=\"coolwarm\")\n",
    "        fig_hc.savefig(\"./paper/modulation_heatmap.png\")\n",
    "\n",
    "modulation_change_stage = np.array(modulation_change_stage)\n",
    "\n",
    "figmc,axsmc = plt.subplots(1, 3, figsize=(4*3,2))\n",
    "# for i in range(modulation_change_stage.shape[1]):\n",
    "#     axsmc[0].plot(counter_lst, modulation_change_stage[:,i], \"-o\", c=c_vals[i])\n",
    "axsmc[0].plot(counter_lst, np.mean(modulation_change_stage, axis=1), \"-o\", c=c_vals[0])\n",
    "axsmc[0].fill_between(counter_lst, np.mean(modulation_change_stage, axis=1) - np.std(modulation_change_stage, axis=1), \\\n",
    "                                   np.mean(modulation_change_stage, axis=1) + np.std(modulation_change_stage, axis=1), color=c_vals_l[0])\n",
    "axsmc[0].set_ylabel(\"Change of Modulation\")\n",
    "# normalized? \n",
    "axsmc[1].plot(counter_lst, m_corr_stage/m_corr_stage[0], \"-o\")\n",
    "axsmc[1].set_ylabel(\"Average Synaptic Correlation between Stimulus\", fontsize=10)\n",
    "axsmc[2].plot(counter_lst, h_corr_stage/h_corr_stage[0], \"-o\")\n",
    "axsmc[2].set_ylabel(\"Postsynaptic Activity Correlation between Stimulus\", fontsize=10)\n",
    "\n",
    "import os \n",
    "\n",
    "def save_dict_with_count_npz(directory, data_dict, it=\"\"):\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    # Count existing files in the directory\n",
    "    file_count = len([f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))])\n",
    "\n",
    "    # Generate filename based on file count\n",
    "    filename = os.path.join(directory, f\"{it}{file_count}.npz\")\n",
    "\n",
    "    # Save dictionary as an NPZ file\n",
    "    np.savez(filename, **data_dict)\n",
    "\n",
    "    print(f\"Dictionary saved as: {filename}\")\n",
    "\n",
    "\n",
    "data_json = {\"counter_lst\": counter_lst, \"m_corr_stage\": m_corr_stage/m_corr_stage[0], \"h_corr_stage\": h_corr_stage/h_corr_stage[0]}\n",
    "save_dict_with_count_npz(\"./paper_data\", data_json, it=\"corr\")\n",
    "\n",
    "for ax in axsmc:\n",
    "    ax.set_xlabel(\"# Dataset\")\n",
    "    ax.set_xscale(\"log\")\n",
    "    \n",
    "figmc.tight_layout()\n",
    "figmc.savefig(\"./paper/modulation_change.png\")\n",
    "\n",
    "def traj_length(array):\n",
    "    \"\"\"\"\"\"\n",
    "    diffs = np.diff(array, axis=0)  \n",
    "    return np.sum(np.linalg.norm(diffs, axis=1))\n",
    "\n",
    "hidden_length_all = []\n",
    "for stage_iter in range(stages_num):\n",
    "    hidden_stage = hidden_output_dict_lst[stage_iter] \n",
    "    hidden_stage = {k: hidden_stage[k] for k in sorted(hidden_stage.keys())}\n",
    "    hidden_length = [traj_length(arr) for arr in hidden_stage.values()]    \n",
    "    hidden_length_all.append(hidden_length)\n",
    "    \n",
    "hidden_length_all = np.array(hidden_length_all)\n",
    "figt, axst = plt.subplots(figsize=(3,3))\n",
    "for i in range(hidden_length_all.shape[1]):\n",
    "    axst.plot(counter_lst, hidden_length_all[:,i], \"-o\", c=c_vals[i])\n",
    "axst.set_xlabel(\"# Dataset\")\n",
    "axst.set_xscale(\"log\")\n",
    "axst.set_ylabel(\"Length of Hidden State Trajectory\")\n",
    "figt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904b7833-7825-487f-9a02-f5938cbecbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def load_all_npz_files(directory):\n",
    "    npz_files = sorted(glob.glob(os.path.join(directory, \"*.npz\")))\n",
    "\n",
    "    data_list = []\n",
    "    for file in npz_files:\n",
    "        data = np.load(file)  \n",
    "        data_list.append({key: data[key] for key in data.files})  \n",
    "\n",
    "        print(f\"Loaded: {file}\") \n",
    "\n",
    "    return data_list\n",
    "\n",
    "plotall = True \n",
    "if plotall:\n",
    "    directory = \"./paper_data\"\n",
    "    all_data = load_all_npz_files(directory)\n",
    "    counter_lst_all, m_corr_all, h_corr_all = [], [], []\n",
    "    for i, data in enumerate(all_data):\n",
    "        counter_lst_all.append(data[\"counter_lst\"])\n",
    "        m_corr_all.append(data[\"m_corr_stage\"])\n",
    "        h_corr_all.append(data[\"h_corr_stage\"])\n",
    "\n",
    "    counter_lst_all = np.array(counter_lst_all)\n",
    "    m_corr_all = np.array(m_corr_all)\n",
    "    h_corr_all = np.array(h_corr_all)\n",
    "\n",
    "    mean_counter = np.mean(counter_lst_all, axis=0)\n",
    "    mean_m_corr = np.mean(m_corr_all, axis=0)\n",
    "    std_m_corr = np.std(m_corr_all, axis=0)\n",
    "    \n",
    "    mean_h_corr = np.mean(h_corr_all, axis=0)\n",
    "    std_h_corr = np.std(h_corr_all, axis=0)\n",
    "    \n",
    "    figmcall, axsmcall = plt.subplots(1,2,figsize=(4*2,2))\n",
    "    axsmcall[0].plot(mean_counter, mean_m_corr, \"-o\", label=\"Mean m_corr\", color=c_vals[0])\n",
    "    axsmcall[0].fill_between(mean_counter, mean_m_corr - std_m_corr, mean_m_corr + std_m_corr, color=c_vals_l[0], alpha=0.2)\n",
    "    axsmcall[0].set_ylabel(\"Cos of Modulation\", fontsize=10)\n",
    "    axsmcall[1].plot(mean_counter, mean_h_corr, \"-o\", label=\"Mean h_corr\", color=c_vals[0])\n",
    "    axsmcall[1].fill_between(mean_counter, mean_h_corr - std_h_corr, mean_h_corr + std_h_corr, color=c_vals_l[0], alpha=0.2)\n",
    "    axsmcall[1].set_ylabel(\"Cos of Hidden Activity\", fontsize=10)\n",
    "    for ax in axsmcall:\n",
    "        ax.set_xlabel(\"# Dataset\")\n",
    "        ax.set_xscale(\"log\")\n",
    "    figmcall.tight_layout()\n",
    "    figmcall.savefig(\"./paper/modulation_analysis_during_learning.png\")\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ffac89-2d1b-460f-a42d-a782c3940f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixon_task_projoutput = []\n",
    "for stage_iter in range(stages_num):\n",
    "    W = Wall_lst[stage_iter][0]\n",
    "    W_output = Woutput_lst[stage_iter]\n",
    "\n",
    "    _, Ms_orig, _, bs = modulation_extraction(db_lst[stage_iter]) # batch * seq_len * hidden_neuron * input_neuron\n",
    "    bias = np.mean(bs, axis=0)\n",
    "\n",
    "    W_output_random_lst = [generate_random_orthonormal_matrix(W_output.shape[1]) for _ in range(10)]\n",
    "    W_fixon = W[:,0].reshape(-1,1)\n",
    "    W_task = W[:,5].reshape(-1,1)\n",
    "    \n",
    "    fixon_output, task_output = W_output[1:,:] @ W_fixon, W_output[1:,:] @ W_task\n",
    "    bias_output = np.mean(bias @ (W_output[1:,:].T), axis=0)\n",
    "    fixon_proj_output_norm1 = fixon_output[0] + bias_output[0]\n",
    "    task_proj_output_norm1 = task_output[0] \n",
    "    fixon_proj_output_norm2 = fixon_output[1] + bias_output[1]\n",
    "    task_proj_output_norm2 = task_output[1] \n",
    "\n",
    "    fixon_output_random, task_output_random = [(W_output_random.T @ W_fixon) for W_output_random in W_output_random_lst], [(W_output_random.T @ W_task) for W_output_random in W_output_random_lst]\n",
    "    fixon_proj_output_norm_random = np.mean([np.sum(fixon_output_r) for fixon_output_r in fixon_output_random])\n",
    "    task_proj_output_norm_random = np.mean([np.sum(task_output_r) for task_output_r in task_output_random])\n",
    "    \n",
    "    fixon_task_projoutput.append([fixon_proj_output_norm1, task_proj_output_norm1, fixon_proj_output_norm2, task_proj_output_norm2, fixon_proj_output_norm_random, task_proj_output_norm_random])\n",
    "\n",
    "fixon_task_projoutput = np.array(fixon_task_projoutput)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,2))\n",
    "ax.plot(counter_lst, fixon_task_projoutput[:, 0], \"-o\", color=c_vals[0], linestyle=l_vals[0], label=\"fixon proj output1\")\n",
    "ax.plot(counter_lst, fixon_task_projoutput[:, 1], \"-o\", color=c_vals[0], linestyle=l_vals[1], label=\"task proj output1\")\n",
    "ax.plot(counter_lst, fixon_task_projoutput[:, 0]+fixon_task_projoutput[:, 1], \"-o\", color=c_vals[1], linestyle=l_vals[2], linewidth=1, label=\"fixon/task sum output1\")\n",
    "ax.axhline(0, color=c_vals[1], linestyle=l_vals[2])\n",
    "\n",
    "# ax.plot(counter_lst, fixon_task_projoutput[:, 2], \"-o\", color=c_vals[1], linestyle=l_vals[0], label=\"fixon proj output2\")\n",
    "# ax.plot(counter_lst, fixon_task_projoutput[:, 3], \"-o\", color=c_vals[1], linestyle=l_vals[1], label=\"task proj output2\")\n",
    "# ax.plot(counter_lst, fixon_task_projoutput[:, 2]+fixon_task_projoutput[:, 3], \"-o\", color=c_vals[1], linestyle=l_vals[2], linewidth=3, label=\"fixon/task sum output2\")\n",
    "# ax.plot(counter_lst, fixon_task_projoutput[:,2], \"-o\", c=c_vals_l[0], label=\"fixon proj output random\")\n",
    "# ax.plot(counter_lst, fixon_task_projoutput[:,3], \"-o\",c=c_vals_l[1], label=\"task proj output random\")\n",
    "ax.legend()    \n",
    "ax.set_xlabel(\"# Dataset\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_ylabel(\"Projection\")\n",
    "fig.savefig(\"./paper/w_to_output.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb3d781-4921-407a-a0e6-570f200cd040",
   "metadata": {},
   "outputs": [],
   "source": [
    "fighs, axshs = plt.subplots(3,2,figsize=(3*2,3*3))\n",
    "pca_er_stage = []\n",
    "\n",
    "start_stage = stages_num-2\n",
    "for stage_iter in range(start_stage, stages_num):\n",
    "    PCA_downsample = 3\n",
    "    _, Ms_orig, hs, bs = modulation_extraction(db_lst[stage_iter])\n",
    "\n",
    "    prlst = [normalized_participation_ratio(np.cov(hs[i,:,:], rowvar=False)) for i in range(hs.shape[0])]\n",
    "\n",
    "    pca = PCA(n_components = PCA_downsample)\n",
    "    # # use hs \n",
    "    # hs_flattened = hs.reshape(hs.shape[0]*hs.shape[1], hs.shape[2])\n",
    "    # hs_pca = pca.fit_transform(hs_flattened)\n",
    "    # hs_reconstructed = hs_pca.reshape(hs.shape[0], hs.shape[1], PCA_downsample)\n",
    "    # lowd_data = hs_reconstructed\n",
    "    # # use Ms\n",
    "    # Ms_flattened = Ms_orig.reshape(Ms_orig.shape[0] * Ms_orig.shape[1] * Ms_orig.shape[3], Ms_orig.shape[2])\n",
    "    # ms_pca = pca.fit_transform(Ms_flattened)\n",
    "    # Ms_reconstructed = ms_pca.reshape(Ms_orig.shape[0], Ms_orig.shape[1], Ms_orig.shape[3], PCA_downsample)\n",
    "    # lowd_data_lst = [Ms_reconstructed[:,:,0,:]] # for instance, modulation of fixon\n",
    "    Ms_end_of_stimulus = Ms_orig[:,stimulus_end:stimulus_end+1,:,:]\n",
    "    Ms_end_of_stimulus_flattened = Ms_end_of_stimulus.reshape(Ms_end_of_stimulus.shape[0] * Ms_end_of_stimulus.shape[1] * Ms_end_of_stimulus.shape[3], Ms_end_of_stimulus.shape[2])\n",
    "    pca.fit(Ms_end_of_stimulus_flattened)\n",
    "    Ms_flattened = Ms_orig.reshape(Ms_orig.shape[0] * Ms_orig.shape[1] * Ms_orig.shape[3], Ms_orig.shape[2])\n",
    "    projected_data = pca.transform(Ms_flattened)\n",
    "    Ms_reconstructed = projected_data.reshape(Ms_orig.shape[0], Ms_orig.shape[1], Ms_orig.shape[3], PCA_downsample)\n",
    "\n",
    "    lowd_data_lst = [Ms_reconstructed[:,:,0,:]] # for instance, modulation of fixon\n",
    "\n",
    "    # pca_er_stage.append(pca.explained_variance_ratio_)\n",
    "    \n",
    "    for i in range(hs.shape[0]):\n",
    "        for dd in range(len(lowd_data_lst)):\n",
    "            lowd_data = lowd_data_lst[dd]\n",
    "            data_batch = lowd_data[i,:,:]\n",
    "\n",
    "            axshs[0,stage_iter-start_stage].scatter(data_batch[stimulus_start:stimulus_end,0], data_batch[stimulus_start:stimulus_end,1], marker=markers_vals[dd], c=c_vals[labels[i,0]], alpha=alphas)\n",
    "            axshs[0,stage_iter-start_stage].set_xlabel(\"PC 1\")\n",
    "            axshs[0,stage_iter-start_stage].set_ylabel(\"PC 2\")\n",
    "            axshs[1,stage_iter-start_stage].scatter(data_batch[stimulus_start:stimulus_end,0], data_batch[stimulus_start:stimulus_end,2], marker=markers_vals[dd], c=c_vals[labels[i,0]], alpha=alphas)\n",
    "            axshs[2,stage_iter-start_stage].scatter(data_batch[stimulus_start:stimulus_end,1], data_batch[stimulus_start:stimulus_end,2], marker=markers_vals[dd], c=c_vals[labels[i,0]], alpha=alphas)\n",
    "\n",
    "fighs.tight_layout()\n",
    "fighs.savefig(\"./paper/m_pca.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b13ffb-b00b-40c0-87a3-8c7e19d096c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_select = 10\n",
    "\n",
    "# overall_corr_change, over_stimulus_info_reserved = [], []\n",
    "\n",
    "# useseed = None\n",
    "\n",
    "# for batch_iter in range(batch_nums):\n",
    "#     if batch_iter % 20 == 0:\n",
    "#         print(f\"batch_iter: {batch_iter}\")\n",
    "        \n",
    "#     task_selection = task_labels_across_batch[batch_iter]\n",
    "    \n",
    "#     stimulus_change_neuron_overtime = []\n",
    "#     stimulus_change_cts_neuron_overtime = []\n",
    "#     delay_change_neuron_overtime = []\n",
    "#     stimulus_information_reserved_overtime = []\n",
    "\n",
    "#     if batch_iter == batch_select:\n",
    "#         fig, axs = plt.subplots(stages_num,6,figsize=(8*2,2*stages_num))\n",
    "#         for ax in axs.flatten():\n",
    "#             for breaktime in break_info:\n",
    "#                 ax.axvline(breaktime, linestyle=\"--\")\n",
    "\n",
    "#     neuron_respond_stimulus_change, neuron_respond_delay_change = [], []\n",
    "#     for stage_iter in range(stages_num):\n",
    "#         W_ = Wall_lst[stage_iter][0]\n",
    "#         _, Ms_orig, _, bs = modulation_extraction(db_lst[stage_iter]) # batch * seq_len * hidden_neuron * input_neuron\n",
    "#         middle = W_[np.newaxis, np.newaxis, :, :] * (Ms_orig + 1)\n",
    "\n",
    "#         useful_info = Ms_orig\n",
    "    \n",
    "#         # how modulation signal is changed during the stimulus period (last timestep minus first timestep)\n",
    "#         stimulus_change_neuron = np.abs(useful_info[batch_iter, stimulus_end-1, :, :] - useful_info[batch_iter, stimulus_start, :, :]) # discrete case (end-before)\n",
    "#         stimulus_change_neuron_cts = np.abs(useful_info[batch_iter, stimulus_start:stimulus_end, :, :] - useful_info[batch_iter, stimulus_start, :, :]) # continuous case (whole trace)\n",
    "#         delay_change_neuron = np.abs(useful_info[batch_iter, response_start-1, :, :] - useful_info[batch_iter, stimulus_end, :, :])\n",
    "\n",
    "#         # which neuron has significant change of modulation signal\n",
    "#         neuron_to_stimulus = (np.abs(stimulus_change_neuron[:, 0]) > 1e-2).astype(int) # N*1 array\n",
    "#         neuron_respond_stimulus_change.append(neuron_to_stimulus)\n",
    "#         neuron_to_delay = (np.abs(delay_change_neuron[:, 0]) > 1e-2).astype(int) # N*1 array\n",
    "#         neuron_respond_delay_change.append(neuron_to_delay)\n",
    "\n",
    "#         unmatched_neuron = sum(1 for l1, l2 in zip(neuron_to_stimulus, neuron_to_delay) if l2 == 1 and l1 != 1)\n",
    "        \n",
    "#         # to match up dimension only\n",
    "#         assert task_params['fixate_off'] is False\n",
    "#         # fixon and task (even in multiple, but matched one) will have modulation change oppositely\n",
    "#         # assert np.sum(np.abs(stimulus_change_neuron[:,0] - stimulus_change_neuron[:,5+task_selection])) < 1e-5 \n",
    "        \n",
    "#         stimulus_change_neuron_overtime.append(stimulus_change_neuron)\n",
    "#         stimulus_change_cts_neuron_overtime.append(stimulus_change_neuron_cts)\n",
    "#         delay_change_neuron_overtime.append(delay_change_neuron)\n",
    "        \n",
    "#         info_reserved = np.abs(useful_info[batch_iter,break_info[2]-1,:,5+task_selection] - useful_info[batch_iter,break_info[0],:,5+task_selection])\n",
    "#         stimulus_information_reserved_overtime.append(np.mean(info_reserved))\n",
    "\n",
    "#         # plot_neuron = [1]\n",
    "#         plot_neuron = [i for i in range(neuron_nums)]\n",
    "        \n",
    "#         if batch_iter == batch_select:\n",
    "#             for neuron, color in enumerate(colors[:useful_info.shape[2]]):\n",
    "#                 if neuron_to_stimulus[neuron] and neuron in plot_neuron:\n",
    "#                     for j in range(6):\n",
    "#                         axs[stage_iter,j].plot(useful_info[batch_iter, :, neuron, j], alpha=0.2, c=color)\n",
    "#                         axs[stage_iter,j].set_title(f\"Input {j}\")\n",
    "\n",
    "#     neuron_respond_stimulus_change = np.array(neuron_respond_stimulus_change)\n",
    "#     not_all_zero_neuron_stimulus = np.where(np.any(neuron_respond_stimulus_change != 0, axis=0))[0]\n",
    "#     neuron_respond_delay_change = np.array(neuron_respond_delay_change)\n",
    "#     not_all_zero_neuron_delay = np.where(np.any(neuron_respond_delay_change != 0, axis=0))[0]\n",
    "\n",
    "#     if batch_iter == batch_select:\n",
    "#         ffs, ggs = plt.subplots(1,2,figsize=(10*2,4))\n",
    "#         sns.heatmap(neuron_respond_stimulus_change, ax=ggs[0])\n",
    "#         ggs[0].set_title(\"Stimulus Period\")\n",
    "#         sns.heatmap(neuron_respond_delay_change, ax=ggs[1])\n",
    "#         ggs[1].set_title(\"Delay Period\")\n",
    "#         for gg in ggs:\n",
    "#             gg.set_xlabel(\"Neuron\")\n",
    "#             gg.set_ylabel(\"Training Stage\")\n",
    "#         ffs.savefig(\"./results/modulation_stagechange.png\")\n",
    "        \n",
    "#     stimulus_change_neuron_overtime = np.array(stimulus_change_neuron_overtime)\n",
    "#     delay_change_neuron_overtime = np.array(delay_change_neuron_overtime)\n",
    "#     stimulus_change_cts_neuron_overtime = np.array(stimulus_change_cts_neuron_overtime)\n",
    "#     # print(stimulus_change_neuron_overtime.shape)\n",
    "#     # print(stimulus_change_cts_neuron_overtime.shape)\n",
    "    \n",
    "#     over_stimulus_info_reserved.append(stimulus_information_reserved_overtime)\n",
    "    \n",
    "#     if batch_iter == batch_select:\n",
    "#         fig.suptitle(f\"Illustration with batch {batch_select}\")\n",
    "#         fig.tight_layout()\n",
    "#         fig.savefig(\"./results/illustration.png\")\n",
    "    \n",
    "#         fig, ax = plt.subplots(figsize=(4,4))\n",
    "#         for neuron in range(stimulus_change_neuron_overtime.shape[1]):\n",
    "#             if neuron in not_all_zero_neuron_stimulus:\n",
    "#                 ax.plot(stimulus_change_neuron_overtime[:,neuron,0], alpha=0.2, c=colors[neuron])\n",
    "#         ax.plot(np.mean(stimulus_change_neuron_overtime[:,not_all_zero_neuron_stimulus,0], axis=1), alpha=0.2, linestyle=\"--\",linewidth=3)\n",
    "#         ax.set_xlabel(\"Training Stage\")\n",
    "#         ax.set_ylabel(\"Change of Modulation, Neuron-wise\")\n",
    "#         fig.savefig(\"./results/change_modulation.png\")\n",
    "\n",
    "\n",
    "#     stage_corr_change = []\n",
    "#     for rec in range(stages_num):\n",
    "#         modulation_change_stim_corr = np.zeros(shape=(input_nums, input_nums))\n",
    "#         modulation_change_delay_corr = np.zeros(shape=(input_nums, input_nums))\n",
    "#         for ind1 in range(input_nums):\n",
    "#             for ind2 in range(input_nums):\n",
    "#                 input1 = stimulus_change_neuron_overtime[rec,:,ind1].flatten()\n",
    "#                 input2 = stimulus_change_neuron_overtime[rec,:,ind2].flatten()\n",
    "#                 modulation_change_stim_corr[ind1, ind2] = pearsonr(input1, input2)[0]\n",
    "                \n",
    "#                 # responsive_neurons = neuron_respond_stimulus_change[rec]\n",
    "#                 # responsive_neurons_index = np.where(responsive_neurons == 1)[0]\n",
    "#                 # input1 = stimulus_change_cts_neuron_overtime[rec, :, :, ind1]\n",
    "#                 # input2 = stimulus_change_cts_neuron_overtime[rec, :, :, ind2]\n",
    "#                 # input1 = input1[:, responsive_neurons_index]\n",
    "#                 # input2 = input2[:, responsive_neurons_index]\n",
    "                \n",
    "#                 # corr_matrix = np.corrcoef(input1, input2, rowvar=False)\n",
    "#                 # num_neurons = input1.shape[1]\n",
    "#                 # simlst2 = corr_matrix[:num_neurons, num_neurons:].diagonal()\n",
    "#                 # sim = np.mean(simlst2)  \n",
    "\n",
    "#                 input1 = delay_change_neuron_overtime[rec,:,ind1].flatten()\n",
    "#                 input2 = delay_change_neuron_overtime[rec,:,ind2].flatten()\n",
    "#                 modulation_change_delay_corr[ind1, ind2] = pearsonr(input1, input2)[0]\n",
    "\n",
    "#         # how fixon interact with stimulus information\n",
    "#         # notice that since the validation set has randomized fix seed\n",
    "#         # some stimulus input may have all zero in specific run\n",
    "#         mean_corr_stim = np.nanmean([modulation_change_stim_corr[0, stimulus_cc] for stimulus_cc in range(1,5)])\n",
    "#         mean_corr_delay = np.nanmean([modulation_change_delay_corr[0, stimulus_cc] for stimulus_cc in range(1,5)])\n",
    "#         mean_corr_task2stim = np.nanmean([modulation_change_stim_corr[5+task_selection, stimulus_cc] for stimulus_cc in range(1,5)])\n",
    "#         mean_corr_fixondelay = modulation_change_stim_corr[0, 5+task_selection]\n",
    "        \n",
    "#         stage_corr_change.append([mean_corr_stim, mean_corr_delay, mean_corr_task2stim, mean_corr_fixondelay])\n",
    "\n",
    "#     overall_corr_change.append(stage_corr_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e19c42f-2c46-4fb7-89a4-ee9aa5ea0ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stage_range = [i for i in range(stages_num)]\n",
    "\n",
    "# overall_corr_change = np.array(overall_corr_change)\n",
    "# figcorr, axcorr = plt.subplots(1,4,figsize=(4*4,4))\n",
    "# useylabels = [\"Fixon to Stimulus during Stimulus Period\", \"Fixon to Stimulus during Delay Period\", \\\n",
    "#               \"Task to Stimulus\", \"Fixon to Task\"]\n",
    "# for i in range(4):\n",
    "#     mean_corr, std_corr = np.mean(overall_corr_change[:,:,i], axis=0), np.std(overall_corr_change[:,:,i], axis=0)\n",
    "#     axcorr[i].plot(stage_range, mean_corr, \"-o\", color=c_vals[0])\n",
    "#     axcorr[i].fill_between(stage_range, mean_corr-std_corr, mean_corr+std_corr, color=c_vals_l[0], alpha=0.2)\n",
    "#     axcorr[i].set_xlabel(\"Training Stage\")\n",
    "#     axcorr[i].set_ylabel(useylabels[i])\n",
    "# figcorr.tight_layout()\n",
    "# figcorr.savefig(\"./results/fixon_stimulus.png\")\n",
    "\n",
    "# over_stimulus_info_reserved = np.array(over_stimulus_info_reserved)\n",
    "# figres, axres = plt.subplots(figsize=(4,4))\n",
    "# mean_res, std_res = np.mean(over_stimulus_info_reserved, axis=0), np.std(over_stimulus_info_reserved, axis=0)\n",
    "# axres.plot(stage_range, mean_res, \"-o\", color=c_vals[0])\n",
    "# axres.fill_between(stage_range, mean_res-std_res, mean_res+std_res, color=c_vals_l[0], alpha=0.2)\n",
    "# axres.set_xlabel(\"Training Stage\")\n",
    "# axres.set_ylabel(\"Modulation information of stimulus preserved after delay\", fontsize=12)\n",
    "# figres.savefig(\"./results/norm_preserve.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec30e08f-8431-4054-abb3-ddd2c28aedc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for session_iter in range(0,len(session_breakdown)):\n",
    "#     session_part = session_breakdown[session_iter]\n",
    "\n",
    "#     for pca_target in pca_target_lst:\n",
    "#         print(f\"session_part: {session_part}; pca_target: {pca_target}\")\n",
    "\n",
    "#         if net.hidden_cell_types is not None:\n",
    "#             cell_types = net.hidden_cell_types.detach().cpu().numpy()[0] # Remove batch idx\n",
    "#         cell_type_names = ('Inh.', 'Exc.')\n",
    "#         pca_sort_type = 'ratios' # How to sort cell type PCA; vars, ratios\n",
    "\n",
    "#         if pca_target in ('hs',):\n",
    "#             n_activity = hs.shape[-1]\n",
    "#             # Truncate into session specifically\n",
    "#             hs_cut = hs[:,session_part[0]:session_part[1],:]\n",
    "#             as_shape = hs_cut.shape \n",
    "#             as_flat = hs_cut.reshape((-1, n_activity,))\n",
    "#         elif pca_target in ('Ms',):\n",
    "#             n_activity = Ms.shape[-1]\n",
    "#             # Truncate into session specifically\n",
    "#             Ms_cut = Ms[:,session_part[0]:session_part[1],:]\n",
    "#             # as_shape = (Ms.shape[0], Ms.shape[1], n_activity) \n",
    "#             as_shape = Ms_cut.shape\n",
    "#             as_flat = Ms_cut.reshape((-1, n_activity))\n",
    "\n",
    "#         print(f\"as_shape: {as_shape}\")\n",
    "#         as_shape_saver = as_shape\n",
    "\n",
    "#         total_vars = np.var(as_flat, axis=0) # Var per activity dimension\n",
    "#         activity_zero = np.zeros((1, n_activity,))\n",
    "\n",
    "#         if pca_type in ('full',):\n",
    "#             activity_pca = PCA()\n",
    "\n",
    "#             activity_pca.fit(as_flat)\n",
    "            \n",
    "#             # Corrects signs of components so that their mean/average is positive\n",
    "#             activity_pca.components_ = np.sign(np.sum(activity_pca.components_, axis=-1))[:, np.newaxis] * activity_pca.components_\n",
    "            \n",
    "#             as_pca = activity_pca.transform(as_flat)\n",
    "#             # as_pca = as_pca.reshape(as_shape) # Separates back into batches and sequences\n",
    "#             as_pca = as_pca.reshape(as_shape_saver[0],as_shape_saver[1],-1)\n",
    "\n",
    "#             zeros_pca = activity_pca.transform(activity_zero)\n",
    "#             if pca_target in ('hs',): # Some transformations only make sense for certain activities\n",
    "#                 W_output_pca = activity_pca.transform(W_output)\n",
    "\n",
    "#             # ZIHAN: only do for all period trial (last)\n",
    "#             #\n",
    "#             # Should we use PCA result or the original result?\n",
    "#             #\n",
    "#             if session_iter == len(session_breakdown)-1 and pca_target in ('hs', ):\n",
    "#                 output_num = W_output.shape[0]\n",
    "#                 assert output_num == 3 # low_dim\n",
    "#                 figtest, axstest = plt.subplots(2,1,figsize=(3*1,3*2))\n",
    "#                 # ? Original or PCA\n",
    "#                 fixation_pca = W_output[0,:].reshape(1,-1)\n",
    "#                 # fixation_pca = fixation_pca / np.linalg.norm(fixation_pca, axis=1, keepdims=True)\n",
    "#                 stimulus_pca = W_output[1:,:]\n",
    "#                 # stimulus_pca = stimulus_pca / np.linalg.norm(stimulus_pca, axis=1, keepdims=True)\n",
    "#                 batch_record_fixate, batch_record_stimulus = [], []\n",
    "                \n",
    "#                 for batchiter in range(hs_cut.shape[0]):\n",
    "#                     # ? Original or PCA\n",
    "#                     batch_activity_pca = hs_cut[batchiter,:,:]\n",
    "                    \n",
    "#                     batch_record1, batch_record2 = [], []\n",
    "#                     for timestamp in range(batch_activity_pca.shape[0]):\n",
    "#                         batch_time_activity_pca = batch_activity_pca[timestamp,:].reshape(1,-1)\n",
    "#                         #\n",
    "#                         # batch_time_activity_pca = batch_time_activity_pca / np.linalg.norm(batch_time_activity_pca, axis=1, keepdims=True)\n",
    "\n",
    "#                         projection_magnitude_fix = helper.magnitude_of_projection(batch_time_activity_pca, fixation_pca)\n",
    "#                         batch_record1.append(projection_magnitude_fix)\n",
    "#                         projection_magnitude_stimulus = helper.magnitude_of_projection(batch_time_activity_pca, stimulus_pca)\n",
    "#                         batch_record2.append(projection_magnitude_stimulus)\n",
    "#                         # print(f\"projection_magnitude_fix: {projection_magnitude_fix}; projection_magnitude_stimulus: {projection_magnitude_stimulus}\")\n",
    "#                         # time.sleep(10000)\n",
    "#                     batch_record_fixate.append(batch_record1)\n",
    "#                     batch_record_stimulus.append(batch_record2)\n",
    "\n",
    "#                 batch_record_fixate, batch_record_stimulus = np.array(batch_record_fixate), np.array(batch_record_stimulus)\n",
    "#                 mean_fix, std_fix = np.mean(batch_record_fixate, axis=0), np.std(batch_record_fixate, axis=0)\n",
    "#                 mean_stimulus, std_stimulus = np.mean(batch_record_stimulus, axis=0), np.std(batch_record_stimulus, axis=0)\n",
    "#                 xxx = [i for i in range(mean_fix.shape[0])]\n",
    "#                 axstest[0].plot(xxx, mean_fix)\n",
    "#                 axstest[0].fill_between(xxx, mean_fix-std_fix, mean_fix+std_fix, alpha=0.7, color=\"red\")\n",
    "#                 axstest[0].set_title(\"Projection Magnitude on 1D Subspace of Fixation Period\")\n",
    "#                 axstest[1].plot(xxx, mean_stimulus)\n",
    "#                 axstest[1].fill_between(xxx, mean_stimulus-std_stimulus, mean_stimulus+std_stimulus, alpha=0.7, color=\"red\")\n",
    "#                 axstest[1].set_title(\"Projection Magnitude on 2D Subspace of Fixation Period\")\n",
    "#                 for ax in axstest:\n",
    "#                     for spp in session_breakdown[:-1]:\n",
    "#                         ax.axvline(spp[1], linestyle=\"--\")\n",
    "#                 figtest.tight_layout()\n",
    "#                 figtest.show()\n",
    "#                 # figtest.savefig(f\"./results/zz_test_{pca_target}_{hyp_dict['ruleset']}_{hyp_dict['chosen_network']}_{hyp_dict['addon_name']}_test.png\")\n",
    "            \n",
    "#             print('PR: {:.2f}'.format(\n",
    "#                 helper.participation_ratio_vector(activity_pca.explained_variance_ratio_)\n",
    "#             ))\n",
    "#             print('PCA component PRs - PC1: {:.1f}, PC2: {:.1f}, PC3: {:.1f}'.format(\n",
    "#                     helper.participation_ratio_vector(np.abs(activity_pca.components_[0, :])),\n",
    "#                     helper.participation_ratio_vector(np.abs(activity_pca.components_[1, :])),\n",
    "#                     helper.participation_ratio_vector(np.abs(activity_pca.components_[2, :])),\n",
    "#                 ))\n",
    "\n",
    "#         elif pca_type in ('cell_types',):\n",
    "#             raise NotImplementedError('Need to correct this for Ms activity')\n",
    "#             cell_type_vals = np.unique(cell_types) # Gets unique cell type idxs\n",
    "\n",
    "#             n_cell_types = cell_type_vals.shape[0]\n",
    "#             pcas = [PCA() for _ in range(n_cell_types)]\n",
    "\n",
    "#             cell_types_pca = [] # This needs to be diferent from cell_types because may do cell types in a different order\n",
    "#             hs_pca = []\n",
    "#             explained_vars = []\n",
    "#             explained_var_ratios = []\n",
    "\n",
    "#             zeros_pca = []\n",
    "#             W_output_pca = []\n",
    "#             # Fit each PCA individually\n",
    "#             for cell_type_idx, (cell_type_val, cell_type_name) in enumerate(zip(\n",
    "#                 cell_type_vals, cell_type_names\n",
    "#             )):\n",
    "#                 print('Cell type: {}'.format(cell_type_name))\n",
    "#                 cell_type_filter = (cell_types == cell_type_val)\n",
    "#                 n_cells_type = np.sum(cell_type_filter.astype(np.int32))\n",
    "\n",
    "#                 print(' Ratio of population: {:.2f}, variance: {:.2f}'.format(\n",
    "#                     n_cells_type / n_cells,\n",
    "#                     np.sum(total_vars[cell_type_filter]) / np.sum(total_vars)\n",
    "#                 ))\n",
    "\n",
    "#                 pcas[cell_type_idx].fit(hs_flat[:, cell_type_filter])\n",
    "#                 # Corrects signs of components so that their mean/average is positive\n",
    "#                 pcas[cell_type_idx].components_ = np.sign(np.sum(pcas[cell_type_idx].components_, axis=-1))[:, np.newaxis] * pcas[cell_type_idx].components_\n",
    "\n",
    "#                 hs_pca_type = pcas[cell_type_idx].transform(hs_flat[:, cell_type_filter])\n",
    "\n",
    "#                 print(' PR: {:.2f}'.format(\n",
    "#                     helper.participation_ratio_vector(pcas[cell_type_idx].explained_variance_ratio_)\n",
    "#                 ))\n",
    "#                 print(' PCA component PRs - PC1: {:.1f}, PC2: {:.1f}, PC3: {:.1f}'.format(\n",
    "#                     helper.participation_ratio_vector(np.abs(pcas[cell_type_idx].components_[0, :])),\n",
    "#                     helper.participation_ratio_vector(np.abs(pcas[cell_type_idx].components_[1, :])),\n",
    "#                     helepr.participation_ratio_vector(np.abs(pcas[cell_type_idx].components_[2, :])),\n",
    "#                 ))\n",
    "\n",
    "#                 cell_types_pca.append([cell_type_val for _ in range(n_cells_type)])\n",
    "#                 hs_pca.append(hs_pca_type)\n",
    "#                 explained_vars.append(pcas[cell_type_idx].explained_variance_)\n",
    "#                 explained_var_ratios.append(pcas[cell_type_idx].explained_variance_ratio_)\n",
    "\n",
    "#                 zeros_pca.append(pcas[cell_type_idx].transform(hidden_zero[:, cell_type_filter]))\n",
    "#                 W_output_pca.append(pcas[cell_type_idx].transform(W_output[:, cell_type_filter]))\n",
    "\n",
    "#             explained_vars = np.concatenate(explained_vars, axis=-1)\n",
    "#             explained_var_ratios = np.concatenate(explained_var_ratios, axis=-1)\n",
    "#             # Now sort based on explained variances/explained variance ratio\n",
    "#             if pca_sort_type in ('vars',):\n",
    "#                 pca_type_sort = np.argsort(explained_vars)[::-1] # largest to smallest\n",
    "#             elif pca_sort_type in ('ratios',):\n",
    "#                 pca_type_sort = np.argsort(explained_var_ratios)[::-1] # largest to smallest\n",
    "\n",
    "#             cell_types_pca =  np.concatenate(cell_types_pca, axis=-1)[pca_type_sort]\n",
    "#             hs_pca = np.concatenate(hs_pca, axis=-1)[:, pca_type_sort]\n",
    "#             explained_vars = explained_vars[pca_type_sort]\n",
    "#             explained_var_ratios = explained_var_ratios[pca_type_sort]\n",
    "#             zeros_pca = np.concatenate(zeros_pca, axis=-1)[:, pca_type_sort]\n",
    "#             W_output_pca = np.concatenate(W_output_pca, axis=-1)[:, pca_type_sort]\n",
    "#             print('Overall:')\n",
    "#             print(' PR: {:.2f}'.format(\n",
    "#                 participation_ratio_vector(explained_var_ratios)\n",
    "#             ))\n",
    "#             hs_pca = hs_pca.reshape(hs.shape)\n",
    "\n",
    "#         if session_iter == len(session_breakdown)-1:\n",
    "#             fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(3*3, 3))\n",
    "#             n_pcs_plot = 3\n",
    "\n",
    "#             if pca_type in ('full',):\n",
    "#                 ax1.scatter(np.arange(n_activity), activity_pca.explained_variance_ratio_, color=c_vals[2])\n",
    "#                 cutoff = np.sum(activity_pca.explained_variance_ratio_ > 0.1) # PC with > 0.1 \n",
    "\n",
    "#                 for pc_idx in range(n_pcs_plot):\n",
    "#                     ax2.plot(activity_pca.components_[pc_idx, :], color=c_vals[pc_idx], label='PC{}'.format(pc_idx+1),\n",
    "#                             zorder=5-pc_idx)\n",
    "\n",
    "#             ax1.set_xlabel('PC')\n",
    "#             ax1.set_ylabel('Explained var ratio')\n",
    "#             ax1.set_title(f\"Good PC: {cutoff}\")\n",
    "\n",
    "#             for ax in (ax2, ax3):\n",
    "#                 ax.legend()\n",
    "#                 ax.set_xlabel('Neuron idx')\n",
    "#                 ax.set_ylabel('PC weight')\n",
    "\n",
    "#             fig.show()\n",
    "#             fig.tight_layout()\n",
    "#             # fig.savefig(f\"./results/PC_{pca_target}_{hyp_dict['ruleset']}_{hyp_dict['chosen_network']}_{hyp_dict['addon_name']}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e41c11-be8d-47b6-9bd1-c3b434565657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for session_iter in range(len(session_breakdown)-1,len(session_breakdown)):\n",
    "#     fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(3*3, 3))\n",
    "\n",
    "#     # single_rule: color by labels\n",
    "#     # all_rules: color by rule\n",
    "#     # if multiple testrules are detected, automatically plot based on the rule type\n",
    "#     plot_mode = 'single_rule' if len(task_params[\"rules\"]) == 1 else 'all_rules'\n",
    "\n",
    "#     if plot_mode == 'single_rule':\n",
    "#         rule_idx = 0 # Only used for individual labels\n",
    "#         rule = task_params['rules'][rule_idx]\n",
    "#         rule_batch_idxs = np.arange(n_batch_all)[test_rule_idxs == rule_idx]\n",
    "#     elif plot_mode == 'all_rules':\n",
    "#         rule_batch_idxs = np.arange(n_batch_all)\n",
    "\n",
    "#     if pca_target in ('hs'):\n",
    "#         pc1_idx, pc2_idx, pc3_idx = 0,1,2\n",
    "#     elif pca_target in ('Ms'):\n",
    "#         pc1_idx, pc2_idx, pc3_idx = 0,1,2\n",
    "\n",
    "#     # batch_plot = (0, 1, 2,)\n",
    "#     if task_params['dataset_name'] in ('DelayMatchSample-v0',):\n",
    "#         batch_plot = [False, False, False, False] # 4 distinct paths\n",
    "#     else:\n",
    "#         batch_plot = (0, 1, 2,)\n",
    "    \n",
    "#     # ZIHAN\n",
    "#     # additional analysis\n",
    "#     if session_iter == len(session_breakdown)-1 and pca_target in ('hs', 'Ms'):\n",
    "#         all_normal_vectors = []\n",
    "#         # minus the zero activity pca\n",
    "#         pc_indices_consider = [pc1_idx, pc2_idx, pc3_idx]\n",
    "\n",
    "#         cosine_pc = np.hstack([W_output_pca[1, idx] - zeros_pca[:, idx] for idx in pc_indices_consider])\n",
    "#         # cosine_pc = np.vstack((W_output_pca[1,pc1_idx]-zeros_pca[:,pc1_idx], W_output_pca[1,pc2_idx]-zeros_pca[:,pc2_idx], W_output_pca[1,pc3_idx]-zeros_pca[:,pc3_idx])).reshape(-1)\n",
    "#         cosine_pc = cosine_pc / np.linalg.norm(cosine_pc)\n",
    "\n",
    "#         sine_pc = np.hstack([W_output_pca[2, idx] - zeros_pca[:, idx] for idx in pc_indices_consider])\n",
    "#         # sine_pc = np.vstack((W_output_pca[2,pc1_idx]-zeros_pca[:,pc1_idx], W_output_pca[2,pc2_idx]-zeros_pca[:,pc2_idx], W_output_pca[2,pc3_idx]-zeros_pca[:,pc3_idx])).reshape(-1)\n",
    "#         sine_pc = sine_pc / np.linalg.norm(sine_pc)\n",
    "\n",
    "#         normal_cosine_sine = np.cross(cosine_pc, sine_pc)\n",
    "#         # normalization\n",
    "#         normal_cosine_sine = normal_cosine_sine / np.linalg.norm(normal_cosine_sine)\n",
    "#         all_normal_vectors.append(normal_cosine_sine)\n",
    "\n",
    "#         figscatter = plt.figure(figsize=(3*3, 3))\n",
    "#         ax1_3d = figscatter.add_subplot(131, projection='3d')\n",
    "#         ax2_3d = figscatter.add_subplot(132, projection='3d')\n",
    "#         ax3_3d = figscatter.add_subplot(133)\n",
    "\n",
    "#         init_point_3d = np.stack((zeros_pca[:, pc1_idx], zeros_pca[:, pc2_idx], zeros_pca[:, pc3_idx]), axis=1)\n",
    "#         ax1_3d.scatter(init_point_3d[0,0], init_point_3d[0,1], init_point_3d[0,2], marker=\"s\", color=\"black\")\n",
    "\n",
    "#         ax1_3d.plot([init_point_3d[0,0], init_point_3d[0,0] + cosine_pc[0]],\n",
    "#                     [init_point_3d[0,1], init_point_3d[0,1] + cosine_pc[1]],\n",
    "#                     [init_point_3d[0,2], init_point_3d[0,2] + cosine_pc[2]],\n",
    "#                     color='red', label=\"cosine\")\n",
    "\n",
    "#         ax1_3d.plot([init_point_3d[0,0], init_point_3d[0,0] + sine_pc[0]],\n",
    "#                     [init_point_3d[0,1], init_point_3d[0,1] + sine_pc[1]],\n",
    "#                     [init_point_3d[0,2], init_point_3d[0,2] + sine_pc[2]],\n",
    "#                     color='blue', label=\"sine\")\n",
    "\n",
    "#         ax1_3d.plot([init_point_3d[0,0], init_point_3d[0,0] + normal_cosine_sine[0]],\n",
    "#                     [init_point_3d[0,1], init_point_3d[0,1] + normal_cosine_sine[1]],\n",
    "#                     [init_point_3d[0,2], init_point_3d[0,2] + normal_cosine_sine[2]],\n",
    "#                     color='green', label=\"cosine * sine\")\n",
    "        \n",
    "#         ax1_3d.legend()\n",
    "\n",
    "#         for ii in range(len(session_breakdown[:-1])):\n",
    "#             subsession = session_breakdown[ii]\n",
    "#             clustered_points = as_pca[:, subsession[0]:subsession[1], :]\n",
    "            \n",
    "#             # Extract the principal components for the scatter plot\n",
    "#             pc1_points = clustered_points[:, :, pc1_idx].flatten()\n",
    "#             pc2_points = clustered_points[:, :, pc2_idx].flatten()\n",
    "#             pc3_points = clustered_points[:, :, pc3_idx].flatten()\n",
    "            \n",
    "#             ax1_3d.scatter(pc1_points, pc2_points, pc3_points, c=c_vals[ii], alpha=0.15)\n",
    "\n",
    "#             points_3d = np.vstack((pc1_points, pc2_points, pc3_points)).T\n",
    "#             # points_3d_centered = points_3d - np.mean(points_3d, axis=0)\n",
    "#             scaler = StandardScaler()\n",
    "#             points_3d_standardized = scaler.fit_transform(points_3d)\n",
    "#             pca = PCA(n_components=3)\n",
    "#             pca.fit(points_3d_standardized)\n",
    "#             normal_vector = pca.components_[-1]\n",
    "#             normalized_normal_vector = normal_vector / np.linalg.norm(normal_vector)\n",
    "#             all_normal_vectors.append(normalized_normal_vector)\n",
    "\n",
    "#             point = np.mean(points_3d, axis=0)\n",
    "#             d = -point.dot(normal_vector)\n",
    "#             xx, yy = np.meshgrid(np.linspace(np.min(points_3d[:, 0]), np.max(points_3d[:, 0]), 10), \n",
    "#                                 np.linspace(np.min(points_3d[:, 1]), np.max(points_3d[:, 1]), 10))\n",
    "#             zz = (-normal_vector[0] * xx - normal_vector[1] * yy - d) * 1. / normal_vector[2]\n",
    "            \n",
    "#             ax2_3d.plot_surface(xx, yy, zz, alpha=0.5, color=c_vals_d[ii])\n",
    "\n",
    "#         for thisax in [ax1_3d, ax2_3d]:\n",
    "#             thisax.set_xlabel(f\"PCA {pc1_idx+1}\")\n",
    "#             thisax.set_ylabel(f\"PCA {pc2_idx+1}\")\n",
    "#             thisax.set_zlabel(f\"PCA {pc3_idx+1}\")\n",
    "\n",
    "#         nnn = len(all_normal_vectors)\n",
    "#         normal_align = np.zeros((nnn, nnn))\n",
    "#         for n1 in range(nnn):\n",
    "#             for n2 in range(nnn):\n",
    "#                 dot_product = np.dot(all_normal_vectors[n1], all_normal_vectors[n2])\n",
    "#                 angle_deg = np.degrees(np.arccos(dot_product))\n",
    "#                 # consider both alignment and anti-alignment\n",
    "#                 angle_deg = min(angle_deg, 180.0-angle_deg)\n",
    "#                 normal_align[n1,n2] = angle_deg\n",
    "\n",
    "#         np.fill_diagonal(normal_align, np.nan)\n",
    "#         # mark_labels = [\"response\"] + [f\"period {i+1}\" for i in range(nnn-1)]\n",
    "#         mark_labels = [\"response\"] + recordkyle_nameall[0]\n",
    "\n",
    "#         sns.heatmap(normal_align, cbar=True, annot=True, ax=ax3_3d, cmap='coolwarm', xticklabels=mark_labels, yticklabels=mark_labels)\n",
    "#         ax3_3d.set_xticklabels(ax3_3d.get_xticklabels(), rotation=45)\n",
    "#         ax3_3d.set_yticklabels(ax3_3d.get_yticklabels(), rotation=45)\n",
    "\n",
    "#         figscatter.tight_layout()\n",
    "#         figscatter.savefig(f\"./results/zz_test_{pca_target}_{hyp_dict['ruleset']}_{hyp_dict['chosen_network']}_{hyp_dict['addon_name']}_scatter.png\")\n",
    "\n",
    "#         # what if not based on PC -> Aug 3nd\n",
    "        \n",
    "#         # middle of each session\n",
    "#         breaks_1afterstart = [int((cut[0]+cut[1])/2) for cut in session_breakdown[:-1]]\n",
    "\n",
    "#         # plot more frames \n",
    "#         considerall = 0\n",
    "#         if considerall:\n",
    "#             for cut in session_breakdown[:-1]:\n",
    "#                 breaks_1afterstart.append(cut[0])\n",
    "#                 breaks_1afterstart.append(cut[1]-1)\n",
    "#         breaks_1afterstart = list(np.sort(breaks_1afterstart))\n",
    "#         print(session_breakdown)\n",
    "\n",
    "#         # if pca_target in ('hs',): # in random\n",
    "#         #     figall, axsall = plt.subplots(3,2,figsize=(2*4,3*4))\n",
    "#         #     figavg, axsavg = plt.subplots(3,2,figsize=(2*4,3*4))\n",
    "\n",
    "#         #     assert mpn_depth == 1 # for now\n",
    "#         #     W_ = W_all_[0]\n",
    "\n",
    "#         #     # plot two trials\n",
    "#         #     figW, axsW = plt.subplots(len(breaks_1afterstart)+1,4,figsize=(12*4,4*(len(breaks_1afterstart)+1)))\n",
    "#         #     select_batch = [29,49,69,89]\n",
    "#         #     for pp in select_batch:\n",
    "#         #         sns.heatmap(W_.T, cbar=True, cmap=\"coolwarm\", ax=axsW[0,select_batch.index(pp)])\n",
    "#         #         axsW[0,select_batch.index(pp)].set_title(\"W\")\n",
    "#         #         # take average across batches\n",
    "#         #         # Ms_samplebatch = np.mean(Ms_orig, axis=0) \n",
    "#         #         Ms_samplebatch = Ms_orig[pp,:,:,:]\n",
    "#         #         print(Ms_samplebatch.shape)\n",
    "\n",
    "#         #         for bb in breaks_1afterstart:\n",
    "#         #             sns.heatmap(Ms_samplebatch[bb,:,:].T, cbar=True, cmap=\"coolwarm\", center=0, \\\n",
    "#         #                                 vmin=-1, vmax=1, ax=axsW[breaks_1afterstart.index(bb)+1,select_batch.index(pp)])\n",
    "\n",
    "#         #         for bb in breaks_1afterstart:\n",
    "#         #             axsW[breaks_1afterstart.index(bb)+1,select_batch.index(pp)].set_title(f\"M: Time: {bb}\")\n",
    "#         #             axsW[breaks_1afterstart.index(bb)+1,select_batch.index(pp)].set_xlabel(f\"Neuron\")\n",
    "#         #             axsW[breaks_1afterstart.index(bb)+1,select_batch.index(pp)].set_ylabel(f\"Input Index\")\n",
    "\n",
    "#         #     figW.tight_layout()\n",
    "#         #     figW.savefig(f\"./results/zz_test_{pca_target}_{hyp_dict['ruleset']}_{hyp_dict['chosen_network']}_{hyp_dict['addon_name']}_W.png\")\n",
    "\n",
    "#         #     results = {\n",
    "#         #         'fix_hs_Woutput_all': [],\n",
    "#         #         'response_hs_Woutput_all': [],\n",
    "#         #         'fix_Ms_Woutput_all': [],\n",
    "#         #         'response_Ms_Woutput_all': [],\n",
    "#         #         'fix_MsWs_Woutput_all': [],\n",
    "#         #         'response_MsWs_Woutput_all': [],\n",
    "#         #     }\n",
    "\n",
    "#         #     pltout, axsout = plt.subplots(figsize=(3,3))\n",
    "#         #     sns.heatmap(W_output, ax=axsout, cmap=\"coolwarm\", center=0, cbar=True, square=True)\n",
    "#         #     pltout.savefig(f\"./results/zz_test_{pca_target}_{hyp_dict['ruleset']}_{hyp_dict['chosen_network']}_{hyp_dict['addon_name']}_Woutput.png\")\n",
    "\n",
    "#         #     W_output_fixation = W_output[0,:].reshape(-1,1) # (N,1)\n",
    "#         #     W_output_response = W_output[1:3,:].T # (N,2)\n",
    "\n",
    "\n",
    "#         #     for batch_iter in range(hs.shape[0]):\n",
    "#         #         hs_per_batch = hs[batch_iter,:,:]\n",
    "#         #         Ms_per_batch = Ms_orig[batch_iter,:,:,:]\n",
    "\n",
    "#         #         fix_hs_Woutput, response_hs_Woutput = [], []\n",
    "#         #         fix_Ms_Woutput, response_Ms_Woutput = [], []\n",
    "#         #         fix_MsWs_Woutput, response_MsWs_Woutput = [], []\n",
    "                \n",
    "#         #         for time_cut in range(hs_per_batch.shape[0]):\n",
    "#         #             hs_per_batch_per_time = hs_per_batch[time_cut,:].reshape(-1,1) # (N,1)\n",
    "#         #             Ms_per_batch_per_time = Ms_per_batch[time_cut,:,:] # (N, n_input)\n",
    "\n",
    "#         #             ang_fix_hs_Ws = subspace_angles(hs_per_batch_per_time, W_output_fixation)[0]\n",
    "#         #             fix_hs_Woutput.append(np.degrees(ang_fix_hs_Ws))\n",
    "\n",
    "#         #             ang_response_hs_Ws = subspace_angles(hs_per_batch_per_time, W_output_response)[0]\n",
    "#         #             response_hs_Woutput.append(np.degrees(ang_response_hs_Ws))\n",
    "\n",
    "#         #             try:\n",
    "#         #                 ang_fix_Ms_Ws = subspace_angles(Ms_per_batch_per_time, W_output_fixation)[0]\n",
    "#         #                 fix_Ms_Woutput.append(np.degrees(ang_fix_Ms_Ws))\n",
    "#         #             except:\n",
    "#         #                 fix_Ms_Woutput.append(np.nan)\n",
    "                    \n",
    "#         #             try:\n",
    "#         #                 ang_response_Ms_Ws = subspace_angles(Ms_per_batch_per_time, W_output_response)[0]\n",
    "#         #                 response_Ms_Woutput.append(np.degrees(ang_response_Ms_Ws))\n",
    "#         #             except:\n",
    "#         #                 response_Ms_Woutput.append(np.nan)\n",
    "\n",
    "#         #             try:\n",
    "#         #                 ang_fix_MsWs_Ws = subspace_angles(Ms_per_batch_per_time * W_ + W_, W_output_fixation)[0]\n",
    "#         #                 fix_MsWs_Woutput.append(np.degrees(ang_fix_MsWs_Ws))\n",
    "#         #             except:\n",
    "#         #                 fix_MsWs_Woutput.append(np.nan)\n",
    "                    \n",
    "#         #             try:\n",
    "#         #                 ang_response_MsWs_Ws = subspace_angles(Ms_per_batch_per_time * W_ + W_, W_output_response)[0]\n",
    "#         #                 response_MsWs_Woutput.append(np.degrees(ang_response_MsWs_Ws))\n",
    "#         #             except:\n",
    "#         #                 response_MsWs_Woutput.append(np.nan)\n",
    "\n",
    "\n",
    "#         #         results['fix_hs_Woutput_all'].append(fix_hs_Woutput)\n",
    "#         #         results['response_hs_Woutput_all'].append(response_hs_Woutput)\n",
    "#         #         results['fix_Ms_Woutput_all'].append(fix_Ms_Woutput)\n",
    "#         #         results['response_Ms_Woutput_all'].append(response_Ms_Woutput)\n",
    "#         #         results['fix_MsWs_Woutput_all'].append(fix_MsWs_Woutput)\n",
    "#         #         results['response_MsWs_Woutput_all'].append(response_MsWs_Woutput)\n",
    "\n",
    "#         #     for key in results:\n",
    "#         #         results[key] = np.array(results[key])\n",
    "            \n",
    "#         #     result_key = list(results.keys())\n",
    "#         #     axsall = axsall.flatten()\n",
    "#         #     for batch_iter in range(hs.shape[0]):\n",
    "#         #         for key in result_key:\n",
    "#         #             axsall[result_key.index(key)].plot([x for x in range(results[key].shape[1])], results[key][batch_iter,:], color=c_vals[labels[batch_iter,0]])             \n",
    "#         #             axsall[result_key.index(key)].set_title(key)\n",
    "\n",
    "#         #     for ax in axsall.flatten():\n",
    "#         #         for bb in breaks:\n",
    "#         #             ax.axvline(bb, color='r', linestyle=\"--\")\n",
    "\n",
    "#         #     breaks = [0] + breaks\n",
    "#         #     axsavg = axsavg.flatten()\n",
    "#         #     for key in result_key:\n",
    "#         #         means, stds = [], []\n",
    "#         #         for bb in range(len(breaks)-1):\n",
    "#         #             meanp = np.nanmean(results[key][:,breaks[bb]:breaks[bb+1]])\n",
    "#         #             stdp = np.nanstd(results[key][:,breaks[bb]:breaks[bb+1]])\n",
    "#         #             means.append(meanp)\n",
    "#         #             stds.append(stdp)\n",
    "#         #         means, stds = np.array(means), np.array(stds)\n",
    "#         #         axsavg[result_key.index(key)].plot([x for x in range(len(means))], means, \"-o\")\n",
    "#         #         axsavg[result_key.index(key)].fill_between([x for x in range(len(means))], means-stds, means+stds, alpha=0.3, color=\"red\")\n",
    "#         #         axsavg[result_key.index(key)].set_title(key)\n",
    "\n",
    "#         #     figall.show()\n",
    "#         #     figavg.show()\n",
    "#         #     figall.savefig(f\"./results/zz_test_{hyp_dict['ruleset']}_{hyp_dict['chosen_network']}_{hyp_dict['addon_name']}_all2all.png\")\n",
    "#         #     figavg.savefig(f\"./results/zz_test_{hyp_dict['ruleset']}_{hyp_dict['chosen_network']}_{hyp_dict['addon_name']}_all2allavg.png\")\n",
    "\n",
    "#     for ax, pc_idxs in zip((ax1, ax2, ax3),((pc1_idx, pc2_idx), (pc1_idx, pc3_idx), (pc2_idx, pc3_idx))):\n",
    "#         domain_save = []\n",
    "#         for batch_idx_idx, batch_idx in enumerate(rule_batch_idxs):\n",
    "#             if plot_mode == 'single_rule':\n",
    "#                 batch_color_idx = labels[batch_idx][0]\n",
    "#             elif plot_mode == 'all_rules':\n",
    "#                 batch_color_idx = int(test_rule_idxs[batch_idx])\n",
    "\n",
    "#             # MODIFY BY ZIHAN\n",
    "#             # only plot the categorization at the whole final period\n",
    "#             # where all sections are included in plotting\n",
    "#             if session_iter == len(session_breakdown)-1:\n",
    "#                 cutoff_evidence = recordkyle_all\n",
    "                \n",
    "#                 temp = []\n",
    "#                 for jj in range(len(cutoff_evidence[batch_idx])-1):\n",
    "#                     xx = as_pca[batch_idx, cutoff_evidence[batch_idx][jj]:cutoff_evidence[batch_idx][jj+1], pc_idxs[0]]\n",
    "#                     yy = as_pca[batch_idx, cutoff_evidence[batch_idx][jj]:cutoff_evidence[batch_idx][jj+1], pc_idxs[1]]\n",
    "#                     temp.append([xx, yy])\n",
    "#                     ax.scatter(xx, yy, color=c_vals_l[batch_color_idx], alpha=0.7, marker=markers_vals[jj], s=8)\n",
    "#                 domain_save.append(temp)\n",
    "\n",
    "#                 if batch_idx_idx in batch_plot:\n",
    "#                     for jj in range(len(cutoff_evidence[batch_idx])-1):\n",
    "#                         ax.plot(as_pca[batch_idx, cutoff_evidence[batch_idx][jj]:cutoff_evidence[batch_idx][jj+1], pc_idxs[0]], \n",
    "#                                 as_pca[batch_idx, cutoff_evidence[batch_idx][jj]:cutoff_evidence[batch_idx][jj+1], pc_idxs[1]], \n",
    "#                                 color=c_vals[batch_color_idx], \n",
    "#                                 linestyle=l_vals[jj], alpha=1.0, zorder=10\n",
    "#                         )\n",
    "\n",
    "#             else:\n",
    "#                 ax.scatter(as_pca[batch_idx, :, pc_idxs[0]], as_pca[batch_idx, :, pc_idxs[1]], color=c_vals_l[batch_color_idx], alpha=0.7, marker=\"o\", s=8)\n",
    "\n",
    "#         if pca_type in ('full',):\n",
    "#             ax.set_xlabel('PC {}'.format(pc_idxs[0]+1))\n",
    "#             ax.set_ylabel('PC {}'.format(pc_idxs[1]+1))\n",
    "#         elif pca_type in ('cell_types',):\n",
    "#             ax.set_xlabel('PC {} (cell type: {})'.format(pc_idxs[0]+1, cell_types_pca[pc_idxs[0]]))\n",
    "#             ax.set_ylabel('PC {} (cell type: {})'.format(pc_idxs[1]+1, cell_types_pca[pc_idxs[1]]))\n",
    "#         # Plot zero point\n",
    "#         ax.scatter(zeros_pca[:, pc_idxs[0]], zeros_pca[:, pc_idxs[1]], color='k',\n",
    "#                 marker='s')\n",
    "\n",
    "#         # Plot readouts\n",
    "#         ro_vector_dir_all = []\n",
    "#         init_points = []\n",
    "#         # if pca_target in ('Ms',):\n",
    "#         #     continue\n",
    "#         for out_idx, output_dim_label in enumerate(output_dim_labels):\n",
    "#             if pca_target in ('Ms',):\n",
    "#                 RO_SCALE = 5\n",
    "#             elif pca_target in ('hs',):\n",
    "#                 RO_SCALE = 1\n",
    "\n",
    "#             ro_vector_dir = np.array((\n",
    "#                 W_output_pca[out_idx, pc_idxs[0]] - zeros_pca[:, pc_idxs[0]],\n",
    "#                 W_output_pca[out_idx, pc_idxs[1]] - zeros_pca[:, pc_idxs[1]],\n",
    "#             ))\n",
    "\n",
    "#             norm = np.linalg.norm(ro_vector_dir)\n",
    "\n",
    "#             if norm != 0:\n",
    "#                 ro_vector_dir = ro_vector_dir / norm\n",
    "#             else:\n",
    "#                 ro_vector_dir = ro_vector_dir\n",
    "\n",
    "#             ro_vector_dir = RO_SCALE * ro_vector_dir\n",
    "#             ro_vector_dir_all.append(ro_vector_dir)\n",
    "\n",
    "#             init_point = [zeros_pca[:, pc_idxs[0]], zeros_pca[:, pc_idxs[1]]]\n",
    "\n",
    "#             ax.plot((zeros_pca[:, pc_idxs[0]], zeros_pca[:, pc_idxs[0]] + ro_vector_dir[0]),\n",
    "#                     (zeros_pca[:, pc_idxs[1]], zeros_pca[:, pc_idxs[1]] + ro_vector_dir[1]),\n",
    "#                     color=c_vals[out_idx], label=output_dim_label, zorder=9)\n",
    "\n",
    "#             init_points.append(init_point)\n",
    "            \n",
    "#     fig.show()\n",
    "#     fig.tight_layout()\n",
    "#     # fig.savefig(f\"./results/trajectory_{pca_target}_{hyp_dict['ruleset']}_{hyp_dict['chosen_network']}_{hyp_dict['addon_name']}_period{session_iter}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cde6dfd-3338-4a68-b43a-02679b6d5e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(db.keys())\n",
    "\n",
    "# if len(rules_dict[hyp_dict['ruleset']]) > 1:\n",
    "#     hs_all = []\n",
    "#     if net_params['net_type'] in ('dmpn',):\n",
    "#         layer_idx_lst = [i for i in range(mpn_depth)]\n",
    "#         for layer_idx in layer_idx_lst:\n",
    "#             if layer_idx == 0:\n",
    "#                 hs = db['hidden0'].detach().cpu().numpy()\n",
    "#             elif layer_idx == 1:\n",
    "#                 hs = db['hidden1'].detach().cpu().numpy()\n",
    "#             hs_all.append(hs)\n",
    "#     elif net_params['net_type'] in ('gru',):\n",
    "#         layer_idx_lst = [0]\n",
    "#         hs_all = [db['hidden'].detach().cpu().numpy()]\n",
    "\n",
    "#     for hsiter in range(len(hs_all)):\n",
    "#         hs = hs_all[hsiter]\n",
    "#         layer_name = layer_idx_lst[hsiter]\n",
    "\n",
    "#         cell_vars_tot = np.var(hs, axis=(0, 1)) # Var over batch and sequence\n",
    "#         n_rules = len(task_params['rules'])\n",
    "#         n_cells = hs.shape[-1]\n",
    "\n",
    "#         # cell_vars_dtypes = [('rule{}'.format(rule_idx), np.float) for rule_idx in range(n_rules)]# Useful for sorting later\n",
    "#         cell_vars_rules = np.zeros((n_rules, n_cells,))\n",
    "#         cell_vars_rules_norm = np.zeros_like(cell_vars_rules)\n",
    "\n",
    "#         for rule_idx, rule in enumerate(task_params['rules']):\n",
    "#             print('Rule {} (idx {})'.format(rule, rule_idx))\n",
    "#             rule_hs = hs[test_rule_idxs == rule_idx, :, :]\n",
    "#             cell_vars_rules[rule_idx] = np.var(rule_hs, axis=(0, 1)) # Var over batch and sequence\n",
    "\n",
    "#         # Now normalize everything\n",
    "#         cell_max_var = np.max(cell_vars_rules, axis=0) # Across rules\n",
    "#         for rule_idx, rule in enumerate(task_params['rules']):\n",
    "#             cell_vars_rules_norm[rule_idx] = np.where(\n",
    "#                 cell_max_var > 0., cell_vars_rules[rule_idx] / cell_max_var, 0.\n",
    "#             )\n",
    "\n",
    "#         # Now sort\n",
    "#         if n_rules > 1:\n",
    "#             rule0_vals = cell_vars_rules_norm[0].tolist()\n",
    "#             rule1_vals = cell_vars_rules_norm[1].tolist()\n",
    "\n",
    "#         rule01_vals = np.array(list(zip(rule0_vals, rule1_vals)), dtype=[('rule0', float), ('rule1', float)])\n",
    "#         sort_idxs = np.argsort(rule01_vals, order=['rule0', 'rule1'])[::-1]\n",
    "\n",
    "#         # sort_idxs = np.argsort(cell_vars_rules_norm[0])[::-1]\n",
    "#         cell_vars_rules_sorted_norm = cell_vars_rules_norm[:, sort_idxs]\n",
    "\n",
    "#         fig, ax = plt.subplots(2, 1, figsize=(12, 3*2))\n",
    "\n",
    "#         for rule_idx, rule in enumerate(task_params['rules']):\n",
    "#             ax[0].plot(cell_vars_rules_sorted_norm[rule_idx], color=c_vals[rule_idx],\n",
    "#                     label=task_params['rules'][rule_idx])\n",
    "\n",
    "#         ax[0].legend()\n",
    "\n",
    "#         ax[0].set_xlabel('Cell_idx')\n",
    "#         ax[0].set_ylabel('Norm. task var.')\n",
    "\n",
    "\n",
    "#         ax[1].matshow(cell_vars_rules_sorted_norm, aspect='auto', vmin=0.0, vmax=1.0,)\n",
    "#         ax[1].set_yticks(np.arange(n_rules))\n",
    "#         ax[1].set_yticklabels(task_params['rules'])\n",
    "#         ax[1].set_xlabel('Cell idx')\n",
    "#         fig.show()\n",
    "#         # fig.savefig(f\"./results/categorization_{hyp_dict['ruleset']}_{hyp_dict['chosen_network']}_layer_{layer_name}_{hyp_dict['addon_name']}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc627c95-873e-4617-b943-f7d997ea5e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def analyze_trajectory(save_trajectory, save_trajectory_random):\n",
    "#     \"\"\"\n",
    "#     Analyze trajectories by calculating mean absolute values for fixations and tasks.\n",
    "#     \"\"\"\n",
    "#     def process_trajectory(trajectory):\n",
    "#         results = []\n",
    "#         for batch in trajectory:\n",
    "#             stim1_fixon = batch[0, 1][stimulus_start:response_start]\n",
    "#             stim1_task = batch[3, 1][stimulus_start:response_start]\n",
    "#             results.append([np.mean(np.abs(stim1_fixon + stim1_task)), np.mean(np.abs(stim1_fixon)), np.mean(np.abs(stim1_task))])\n",
    "#         return np.array(results)\n",
    "    \n",
    "#     # Process both trajectories\n",
    "#     result = process_trajectory(save_trajectory)\n",
    "#     result_random = process_trajectory(save_trajectory_random)\n",
    "    \n",
    "#     # Return the mean of the computed values\n",
    "#     return np.mean(result[:, 0]), np.mean(result[:, 1]), np.mean(result[:,2]), np.mean(result_random[:, 0]), np.mean(result_random[:, 1]), np.mean(result_random[:, 2])\n",
    "\n",
    "# fixon_task_diff = np.array([analyze_trajectory(all_trajectory[i], all_trajectory_random[i]) for i in range(len(all_trajectory_random))])\n",
    "\n",
    "# figfixontaskcancel, axsfixontaskcancel = plt.subplots(1,2,figsize=(3*2,3))\n",
    "# axsfixontaskcancel[0].plot(fixon_task_diff[:,0], \"-o\", c=c_vals[0], label=\"abs(fixon-task)\")\n",
    "# axsfixontaskcancel[0].plot(fixon_task_diff[:,1], \"-o\", c=c_vals[1], label=\"abs(fixon)\")\n",
    "# axsfixontaskcancel[0].plot(fixon_task_diff[:,2], \"-o\", c=c_vals[2], label=\"abs(task)\")\n",
    "# axsfixontaskcancel[0].plot(fixon_task_diff[:,3], \"-o\", c=c_vals_l[0], label=\"abs(fixon-task) random\")\n",
    "# axsfixontaskcancel[0].plot(fixon_task_diff[:,4], \"-o\", c=c_vals_l[1], label=\"abs(fixon) random\")\n",
    "# axsfixontaskcancel[0].plot(fixon_task_diff[:,5], \"-o\", c=c_vals_l[2], label=\"abs(task) random\")\n",
    "# axsfixontaskcancel[0].legend()\n",
    "# axsfixontaskcancel[0].set_xlabel(\"Learning Stage\")\n",
    "# axsfixontaskcancel[0].set_ylabel(\"Projection Magnitude\")\n",
    "# axsfixontaskcancel[1].plot(loss_lst, \"-o\", c=c_vals[2])\n",
    "# axsfixontaskcancel[1].set_xlabel(\"Learning Stage\")\n",
    "# axsfixontaskcancel[1].set_ylabel(\"Loss\")\n",
    "# axsfixontaskcancel[1].set_yscale(\"log\")\n",
    "\n",
    "# figfixontaskcancel.tight_layout()\n",
    "# figfixontaskcancel.savefig(\"./paper/cancel.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
