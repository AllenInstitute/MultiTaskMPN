{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "374fb8f2-9b83-44ce-821b-8917a114c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# PyTorch Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import gc \n",
    "\n",
    "# Data Handling and Image Processing\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Style for Matplotlib\n",
    "import scienceplots\n",
    "plt.style.use('science')\n",
    "plt.style.use(['no-latex'])\n",
    "\n",
    "# Scientific Computing and Machine Learning\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.linalg import subspace_angles\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Custom Modules and Extensions\n",
    "sys.path.append(\"../netrep/\")\n",
    "sys.path.append(\"../svcca/\")\n",
    "import cca_core\n",
    "from netrep.metrics import LinearMetric\n",
    "import networks as nets  # Contains RNNs\n",
    "import net_helpers\n",
    "import mpn_tasks\n",
    "import helper\n",
    "import mpn\n",
    "\n",
    "import scienceplots\n",
    "plt.style.use('science')\n",
    "plt.style.use(['no-latex'])\n",
    "\n",
    "# Memory Optimization\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd1556c8-a4b6-434b-a60f-37035980bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 Red, 1 blue, 2 green, 3 purple, 4 orange, 5 teal, 6 gray, 7 pink, 8 yellow\n",
    "c_vals = ['#e53e3e', '#3182ce', '#38a169', '#805ad5','#dd6b20', '#319795', '#718096', '#d53f8c', '#d69e2e',]\n",
    "c_vals_l = ['#feb2b2', '#90cdf4', '#9ae6b4', '#d6bcfa', '#fbd38d', '#81e6d9', '#e2e8f0', '#fbb6ce', '#faf089',]\n",
    "c_vals_d = ['#9b2c2c', '#2c5282', '#276749', '#553c9a', '#9c4221', '#285e61', '#2d3748', '#97266d', '#975a16',]\n",
    "l_vals = ['solid', 'dashed', 'dotted', 'dashdot', '-', '--', '-.', ':', (0, (3, 1, 1, 1)), (0, (5, 10))]\n",
    "markers_vals = ['o', 'v', '^', '<', '>', '1', '2', '3', '4', 's', 'p', '*', 'h', 'H', '+', 'x', 'D', 'd', '|', '_']\n",
    "hyp_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34978bf6-67b8-41bd-a022-a7b46a320686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set seed 241\n",
      "Fixation_off: True; Task_info: True\n",
      "Rules: ['delaydm1']\n",
      "  Input size 7, Output size 3\n",
      "Using CUDA...\n",
      "Align ['delaydm1'] With Same Time\n",
      "delaydm1\n",
      "[7, 100, 100, 3]\n",
      "MultiPlastic Net:\n",
      "  output neurons: 3\n",
      "  Act: tanh\n",
      "\n",
      "Input Layer Frozen\n",
      "=== Layer Specific Setup ===\n",
      "100\n",
      "  MP Layer1 parameters:\n",
      "    n_neurons - input: 100, output: 100\n",
      "    M matrix parameters:    update bounds - Max mult: 1.0, Min mult: -1.0\n",
      "      type: mult // Update - type: hebb_assoc // Act fn: linear\n",
      "      Eta: scalar (train) // Lambda: scalar (fixed) // Lambda_max: 0.99 (tau: 4.0e+03)\n",
      "How about Test Data at dataset 0\n",
      "Train parameters:\n",
      "  Loss: MSE // LR: 1.00e-03 // Optim: adam\n",
      "  Grad type: backprop // Gradient clip: 1.0e+01\n",
      "Weight reg: None\n",
      "Activity reg: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gscratch/deepthought/zihan/miniconda3/envs/playground/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, LR: 1.000e-03 - train_loss:5.684e-01, rounded train_acc:0.146, valid_loss:6.021e-01, rounded valid_acc:0.170\n",
      "Iter: 100, LR: 1.000e-03 - train_loss:1.522e-01, rounded train_acc:0.327, valid_loss:1.765e-01, rounded valid_acc:0.266\n",
      "How about Test Data at dataset 1\n",
      "Iter: 200, LR: 1.000e-03 - train_loss:1.180e-01, rounded train_acc:0.442, valid_loss:1.530e-01, rounded valid_acc:0.330\n",
      "Iter: 300, LR: 1.000e-03 - train_loss:9.683e-02, rounded train_acc:0.591, valid_loss:1.264e-01, rounded valid_acc:0.539\n",
      "Iter: 400, LR: 1.000e-03 - train_loss:6.356e-02, rounded train_acc:0.694, valid_loss:9.799e-02, rounded valid_acc:0.591\n",
      "How about Test Data at dataset 4\n",
      "Iter: 500, LR: 1.000e-03 - train_loss:4.898e-02, rounded train_acc:0.775, valid_loss:8.019e-02, rounded valid_acc:0.667\n",
      "Iter: 600, LR: 1.000e-03 - train_loss:4.375e-02, rounded train_acc:0.793, valid_loss:6.956e-02, rounded valid_acc:0.672\n",
      "Iter: 700, LR: 1.000e-03 - train_loss:3.540e-02, rounded train_acc:0.833, valid_loss:6.226e-02, rounded valid_acc:0.703\n",
      "Iter: 800, LR: 1.000e-03 - train_loss:3.631e-02, rounded train_acc:0.835, valid_loss:5.979e-02, rounded valid_acc:0.712\n",
      "Iter: 900, LR: 1.000e-03 - train_loss:3.302e-02, rounded train_acc:0.838, valid_loss:5.950e-02, rounded valid_acc:0.697\n",
      "Iter: 1000, LR: 1.000e-03 - train_loss:2.953e-02, rounded train_acc:0.864, valid_loss:5.423e-02, rounded valid_acc:0.753\n",
      "Iter: 1100, LR: 1.000e-03 - train_loss:2.869e-02, rounded train_acc:0.875, valid_loss:5.110e-02, rounded valid_acc:0.761\n",
      "Iter: 1200, LR: 1.000e-03 - train_loss:2.738e-02, rounded train_acc:0.875, valid_loss:4.841e-02, rounded valid_acc:0.791\n",
      "Iter: 1300, LR: 1.000e-03 - train_loss:2.588e-02, rounded train_acc:0.898, valid_loss:4.889e-02, rounded valid_acc:0.751\n",
      "Iter: 1400, LR: 1.000e-03 - train_loss:2.114e-02, rounded train_acc:0.922, valid_loss:4.537e-02, rounded valid_acc:0.774\n",
      "Iter: 1500, LR: 1.000e-03 - train_loss:2.205e-02, rounded train_acc:0.920, valid_loss:4.546e-02, rounded valid_acc:0.776\n",
      "Iter: 1600, LR: 1.000e-03 - train_loss:2.031e-02, rounded train_acc:0.926, valid_loss:3.982e-02, rounded valid_acc:0.801\n",
      "How about Test Data at dataset 16\n",
      "Iter: 1700, LR: 1.000e-03 - train_loss:1.795e-02, rounded train_acc:0.933, valid_loss:4.110e-02, rounded valid_acc:0.807\n",
      "Iter: 1800, LR: 1.000e-03 - train_loss:2.101e-02, rounded train_acc:0.918, valid_loss:4.090e-02, rounded valid_acc:0.808\n",
      "Iter: 1900, LR: 1.000e-03 - train_loss:1.833e-02, rounded train_acc:0.935, valid_loss:3.644e-02, rounded valid_acc:0.847\n",
      "Iter: 2000, LR: 1.000e-03 - train_loss:1.743e-02, rounded train_acc:0.944, valid_loss:3.262e-02, rounded valid_acc:0.854\n",
      "Iter: 2100, LR: 1.000e-03 - train_loss:1.479e-02, rounded train_acc:0.950, valid_loss:3.397e-02, rounded valid_acc:0.833\n",
      "Iter: 2200, LR: 1.000e-03 - train_loss:1.543e-02, rounded train_acc:0.946, valid_loss:3.102e-02, rounded valid_acc:0.851\n",
      "Iter: 2300, LR: 1.000e-03 - train_loss:1.578e-02, rounded train_acc:0.951, valid_loss:3.646e-02, rounded valid_acc:0.824\n",
      "Iter: 2400, LR: 1.000e-03 - train_loss:1.463e-02, rounded train_acc:0.957, valid_loss:2.947e-02, rounded valid_acc:0.857\n",
      "Iter: 2500, LR: 1.000e-03 - train_loss:1.629e-02, rounded train_acc:0.939, valid_loss:3.052e-02, rounded valid_acc:0.877\n",
      "Iter: 2600, LR: 1.000e-03 - train_loss:1.454e-02, rounded train_acc:0.960, valid_loss:3.194e-02, rounded valid_acc:0.843\n",
      "Iter: 2700, LR: 1.000e-03 - train_loss:1.367e-02, rounded train_acc:0.958, valid_loss:2.986e-02, rounded valid_acc:0.855\n",
      "Iter: 2800, LR: 1.000e-03 - train_loss:1.288e-02, rounded train_acc:0.965, valid_loss:3.239e-02, rounded valid_acc:0.862\n",
      "Iter: 2900, LR: 1.000e-03 - train_loss:1.442e-02, rounded train_acc:0.949, valid_loss:2.936e-02, rounded valid_acc:0.849\n",
      "Iter: 3000, LR: 1.000e-03 - train_loss:1.319e-02, rounded train_acc:0.964, valid_loss:3.138e-02, rounded valid_acc:0.831\n",
      "Iter: 3100, LR: 1.000e-03 - train_loss:1.316e-02, rounded train_acc:0.952, valid_loss:2.844e-02, rounded valid_acc:0.857\n",
      "Iter: 3200, LR: 1.000e-03 - train_loss:1.275e-02, rounded train_acc:0.966, valid_loss:2.756e-02, rounded valid_acc:0.853\n",
      "Iter: 3300, LR: 1.000e-03 - train_loss:1.255e-02, rounded train_acc:0.968, valid_loss:2.879e-02, rounded valid_acc:0.880\n",
      "Iter: 3400, LR: 1.000e-03 - train_loss:1.229e-02, rounded train_acc:0.962, valid_loss:2.802e-02, rounded valid_acc:0.850\n",
      "Iter: 3500, LR: 1.000e-03 - train_loss:1.158e-02, rounded train_acc:0.970, valid_loss:2.753e-02, rounded valid_acc:0.848\n",
      "Iter: 3600, LR: 1.000e-03 - train_loss:1.218e-02, rounded train_acc:0.958, valid_loss:2.703e-02, rounded valid_acc:0.887\n",
      "Iter: 3700, LR: 1.000e-03 - train_loss:1.122e-02, rounded train_acc:0.975, valid_loss:2.935e-02, rounded valid_acc:0.875\n",
      "Iter: 3800, LR: 1.000e-03 - train_loss:1.249e-02, rounded train_acc:0.963, valid_loss:2.713e-02, rounded valid_acc:0.896\n",
      "Iter: 3900, LR: 1.000e-03 - train_loss:1.276e-02, rounded train_acc:0.959, valid_loss:2.806e-02, rounded valid_acc:0.862\n",
      "Iter: 4000, LR: 1.000e-03 - train_loss:1.096e-02, rounded train_acc:0.973, valid_loss:2.646e-02, rounded valid_acc:0.867\n",
      "Iter: 4100, LR: 1.000e-03 - train_loss:1.007e-02, rounded train_acc:0.979, valid_loss:2.358e-02, rounded valid_acc:0.899\n",
      "Iter: 4200, LR: 1.000e-03 - train_loss:1.222e-02, rounded train_acc:0.954, valid_loss:2.696e-02, rounded valid_acc:0.884\n",
      "Iter: 4300, LR: 1.000e-03 - train_loss:1.086e-02, rounded train_acc:0.971, valid_loss:2.414e-02, rounded valid_acc:0.887\n",
      "Iter: 4400, LR: 1.000e-03 - train_loss:1.115e-02, rounded train_acc:0.969, valid_loss:2.378e-02, rounded valid_acc:0.893\n",
      "Iter: 4500, LR: 1.000e-03 - train_loss:1.156e-02, rounded train_acc:0.968, valid_loss:2.530e-02, rounded valid_acc:0.878\n",
      "Iter: 4600, LR: 1.000e-03 - train_loss:1.111e-02, rounded train_acc:0.972, valid_loss:2.258e-02, rounded valid_acc:0.872\n",
      "Iter: 4700, LR: 1.000e-03 - train_loss:9.650e-03, rounded train_acc:0.977, valid_loss:2.104e-02, rounded valid_acc:0.907\n",
      "Iter: 4800, LR: 1.000e-03 - train_loss:9.235e-03, rounded train_acc:0.974, valid_loss:2.514e-02, rounded valid_acc:0.883\n",
      "Iter: 4900, LR: 1.000e-03 - train_loss:9.301e-03, rounded train_acc:0.979, valid_loss:2.255e-02, rounded valid_acc:0.893\n",
      "Iter: 5000, LR: 1.000e-03 - train_loss:9.968e-03, rounded train_acc:0.972, valid_loss:2.171e-02, rounded valid_acc:0.903\n",
      "Iter: 5100, LR: 1.000e-03 - train_loss:9.399e-03, rounded train_acc:0.978, valid_loss:2.244e-02, rounded valid_acc:0.904\n",
      "Iter: 5200, LR: 1.000e-03 - train_loss:8.752e-03, rounded train_acc:0.984, valid_loss:2.080e-02, rounded valid_acc:0.899\n",
      "Iter: 5300, LR: 1.000e-03 - train_loss:8.925e-03, rounded train_acc:0.981, valid_loss:1.842e-02, rounded valid_acc:0.927\n",
      "Iter: 5400, LR: 1.000e-03 - train_loss:8.284e-03, rounded train_acc:0.987, valid_loss:1.993e-02, rounded valid_acc:0.920\n",
      "Iter: 5500, LR: 1.000e-03 - train_loss:1.021e-02, rounded train_acc:0.975, valid_loss:1.938e-02, rounded valid_acc:0.933\n",
      "Iter: 5600, LR: 1.000e-03 - train_loss:8.140e-03, rounded train_acc:0.987, valid_loss:1.796e-02, rounded valid_acc:0.942\n",
      "Iter: 5700, LR: 1.000e-03 - train_loss:8.868e-03, rounded train_acc:0.988, valid_loss:1.885e-02, rounded valid_acc:0.938\n",
      "Iter: 5800, LR: 1.000e-03 - train_loss:8.666e-03, rounded train_acc:0.980, valid_loss:2.107e-02, rounded valid_acc:0.925\n",
      "Iter: 5900, LR: 1.000e-03 - train_loss:8.726e-03, rounded train_acc:0.984, valid_loss:2.054e-02, rounded valid_acc:0.917\n",
      "Iter: 6000, LR: 1.000e-03 - train_loss:7.853e-03, rounded train_acc:0.986, valid_loss:1.761e-02, rounded valid_acc:0.942\n",
      "Iter: 6100, LR: 1.000e-03 - train_loss:7.509e-03, rounded train_acc:0.986, valid_loss:1.858e-02, rounded valid_acc:0.939\n",
      "Iter: 6200, LR: 1.000e-03 - train_loss:8.781e-03, rounded train_acc:0.982, valid_loss:1.914e-02, rounded valid_acc:0.938\n",
      "Iter: 6300, LR: 1.000e-03 - train_loss:8.694e-03, rounded train_acc:0.984, valid_loss:1.842e-02, rounded valid_acc:0.916\n",
      "Iter: 6400, LR: 1.000e-03 - train_loss:7.279e-03, rounded train_acc:0.989, valid_loss:1.532e-02, rounded valid_acc:0.958\n",
      "How about Test Data at dataset 64\n",
      "Iter: 6500, LR: 1.000e-03 - train_loss:7.730e-03, rounded train_acc:0.984, valid_loss:1.792e-02, rounded valid_acc:0.926\n",
      "Iter: 6600, LR: 1.000e-03 - train_loss:7.079e-03, rounded train_acc:0.994, valid_loss:1.638e-02, rounded valid_acc:0.935\n",
      "Iter: 6700, LR: 1.000e-03 - train_loss:7.493e-03, rounded train_acc:0.991, valid_loss:1.676e-02, rounded valid_acc:0.940\n",
      "Iter: 6800, LR: 1.000e-03 - train_loss:7.517e-03, rounded train_acc:0.987, valid_loss:1.679e-02, rounded valid_acc:0.951\n",
      "Iter: 6900, LR: 1.000e-03 - train_loss:6.759e-03, rounded train_acc:0.991, valid_loss:1.612e-02, rounded valid_acc:0.949\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 268\u001b[0m\n\u001b[1;32m    262\u001b[0m     test_input_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmatmul(test_input_np, np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mpinv(rmat))\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m##############\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m##############\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# we use net at different training stage on the same test_input\u001b[39;00m\n\u001b[1;32m    267\u001b[0m net, _, (counter_lst, netout_lst, db_lst, Winput_lst, Winputbias_lst, \\\n\u001b[0;32m--> 268\u001b[0m          Woutput_lst, Wall_lst, marker_lst, loss_lst, acc_lst) \u001b[38;5;241m=\u001b[39m \u001b[43mnet_helpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyp_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyp_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetFunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetFunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m##############\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m##############\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train:\n",
      "File \u001b[0;32m/mmfs1/gscratch/amath/zihan-zhang/mpn/net_helpers.py:117\u001b[0m, in \u001b[0;36mtrain_network\u001b[0;34m(params, net, device, verbose, train, hyp_dict, netFunction, test_input)\u001b[0m\n\u001b[1;32m    113\u001b[0m     Wall_lst\u001b[38;5;241m.\u001b[39mappend(W_all_)\n\u001b[1;32m    115\u001b[0m     marker_lst\u001b[38;5;241m.\u001b[39mappend(dataset_idx)\n\u001b[0;32m--> 117\u001b[0m _, monitor_loss, monitor_acc \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_trails\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_trails\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_trails\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_thresh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyp_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrun_mode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (is_power_of_4_or_zero(dataset_idx) \u001b[38;5;129;01mor\u001b[39;00m dataset_idx \u001b[38;5;241m==\u001b[39m train_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_datasets\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    120\u001b[0m     loss_lst\u001b[38;5;241m.\u001b[39mappend(monitor_loss)\n",
      "File \u001b[0;32m/mmfs1/gscratch/amath/zihan-zhang/mpn/net_helpers.py:680\u001b[0m, in \u001b[0;36mBaseNetwork.fit\u001b[0;34m(self, train_params, train_data, train_trails, valid_batch, valid_trails, new_thresh, run_mode)\u001b[0m\n\u001b[1;32m    676\u001b[0m \t\t\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain() \u001b[38;5;66;03m# put in train mode (doesn't really do anything unless we are using dropout/batch norm)\u001b[39;00m\n\u001b[0;32m--> 680\u001b[0m db, monitor_loss, monitor_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_trails\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_trails\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_trails\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m\t\t\t  \u001b[49m\u001b[43mnew_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_thresh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval() \u001b[38;5;66;03m# return to eval mode\u001b[39;00m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m db, monitor_loss, monitor_acc\n",
      "File \u001b[0;32m/mmfs1/gscratch/amath/zihan-zhang/mpn/net_helpers.py:790\u001b[0m, in \u001b[0;36mBaseNetwork.train_epochs\u001b[0;34m(self, train_params, train_data, train_trails, valid_batch, valid_trails, new_thresh, monitor, run_mode)\u001b[0m\n\u001b[1;32m    787\u001b[0m \tt0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;66;03m# Backward start\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_type \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackprop\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_debug\u001b[39m\u001b[38;5;124m'\u001b[39m,): \u001b[38;5;66;03m# Standard supervised training procedures to compute gradient\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m \t\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    791\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_clip \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    792\u001b[0m \t\ttorch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_clip)\n",
      "File \u001b[0;32m/gscratch/deepthought/zihan/miniconda3/envs/playground/lib/python3.8/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gscratch/deepthought/zihan/miniconda3/envs/playground/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Reload modules if changes have been made to them\n",
    "from importlib import reload\n",
    "\n",
    "reload(nets)\n",
    "reload(net_helpers)\n",
    "\n",
    "for _ in range(5):\n",
    "    fixseed = False # randomize setting the seed may lead to not perfectly solved results\n",
    "    seed = random.randint(1,1000) if not fixseed else 8 # random set the seed to test robustness by default\n",
    "    print(f\"Set seed {seed}\")\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    hyp_dict['task_type'] = 'multitask' # int, NeuroGym, multitask\n",
    "    hyp_dict['mode_for_all'] = \"random_batch\"\n",
    "    hyp_dict['ruleset'] = 'delaydm1' # low_dim, all, test\n",
    "    \n",
    "    accept_rules = ('fdgo', 'fdanti', 'delaygo', 'delayanti', 'reactgo', 'reactanti', \n",
    "                    'delaydm1', 'delaydm2', 'dmsgo', 'dmcgo', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm', 'dm1')\n",
    "    \n",
    "    rules_dict = \\\n",
    "        {'all' : ['fdgo', 'reactgo', 'delaygo', 'fdanti', 'reactanti', 'delayanti',\n",
    "                  'dm1', 'dm2', 'contextdm1', 'contextdm2', 'multidm',\n",
    "                  'delaydm1', 'delaydm2', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm',\n",
    "                  'dmsgo', 'dmsnogo', 'dmcgo', 'dmcnogo'],\n",
    "         'low_dim' : ['fdgo', 'reactgo', 'delaygo', 'fdanti', 'reactanti', 'delayanti',\n",
    "                     'delaydm1', 'delaydm2', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm',\n",
    "                     'dmsgo', 'dmsnogo', 'dmcgo', 'dmcnogo'],\n",
    "    \n",
    "         'gofamily': ['fdgo', 'fdanti', 'reactgo', 'reactanti', 'delaygo', 'delayanti'],\n",
    "    \n",
    "         'delaygo': ['delaygo'],\n",
    "         'delaygofamily': ['delaygo', 'delayanti'],\n",
    "         'fdgo': ['fdgo'],\n",
    "         'fdfamily': ['fdgo', 'fdanti'],\n",
    "         'reactgo': ['reactgo'],\n",
    "         'reactfamily': ['reactgo', 'reactanti'],\n",
    "         \n",
    "         'delaydm1': ['delaydm1'],\n",
    "         'delaydmfamily': ['delaydm1', 'delaydm2'],\n",
    "         \n",
    "         'dmsgofamily': ['dmsgo', 'dmsnogo'],\n",
    "         'dmsgo': ['dmsgo'],\n",
    "         'dmcgo': ['dmcgo'],\n",
    "         'contextdelaydm1': ['contextdelaydm1'], \n",
    "         'contextdelayfamily': ['contextdelaydm1', 'contextdelaydm2'],\n",
    "         'dm1': ['dm1']\n",
    "        }\n",
    "        \n",
    "    \n",
    "    # This can either be used to set parameters OR set parameters and train\n",
    "    train = True # whether or not to train the network\n",
    "    verbose = True\n",
    "    hyp_dict['run_mode'] = 'minimal' # minimal, debug\n",
    "    hyp_dict['chosen_network'] = \"dmpn\"\n",
    "    \n",
    "    # suffix for saving images\n",
    "    hyp_dict['addon_name'] = \"eta_scalar_lambda_scalar_notlearn_samemodality_nostrength\"\n",
    "    \n",
    "    mpn_depth = 1\n",
    "    \n",
    "    # for coding \n",
    "    if hyp_dict['chosen_network'] in (\"gru\", \"vanilla\"):\n",
    "        mpn_depth = 1\n",
    "    \n",
    "    def current_basic_params():\n",
    "        task_params = {\n",
    "            'task_type': hyp_dict['task_type'],\n",
    "            'rules': rules_dict[hyp_dict['ruleset']],\n",
    "            'dt': 40, # ms, directly influence sequence lengths,\n",
    "            'ruleset': hyp_dict['ruleset'],\n",
    "            'n_eachring': 8, # Number of distinct possible inputs on each ring\n",
    "            'in_out_mode': 'low_dim',  # high_dim or low_dim or low_dim_pos (Robert vs. Laura's hardtask, resp)\n",
    "            'sigma_x': 0.00, # Laura raised to 0.1 to prevent overfitting (Robert uses 0.01)\n",
    "            'mask_type': 'cost', # 'cost', None\n",
    "            'fixate_off': True, # Second fixation signal goes on when first is off\n",
    "            'task_info': True, \n",
    "            'randomize_inputs': False,\n",
    "            'n_input': 20, # Only used if inputs are randomized,\n",
    "            'modality_diff': True,  # if two stimulus are included in the task, put them into different modality,\n",
    "            'label_strength': True,\n",
    "            'long_delay': 'normal' \n",
    "        }\n",
    "    \n",
    "        print(f\"Fixation_off: {task_params['fixate_off']}; Task_info: {task_params['task_info']}\")\n",
    "    \n",
    "        train_params = {\n",
    "            'lr': 1e-3,\n",
    "            'n_batches': 640,\n",
    "            'batch_size': 640,\n",
    "            'gradient_clip': 10,\n",
    "            'valid_n_batch': 200,\n",
    "            'n_datasets': 200, # Number of distinct batches\n",
    "            'n_epochs_per_set': 100, # longer/shorter training\n",
    "            # 'task_mask': None, # None, task\n",
    "            # 'weight_reg': 'L2',\n",
    "            # 'reg_lambda': 1e-4,\n",
    "\n",
    "            'scheduler': {\n",
    "                'type': 'ReduceLROnPlateau',  # or 'StepLR'\n",
    "                'mode': 'min',                # for ReduceLROnPlateau\n",
    "                'factor': 0.5,                # factor to reduce LR\n",
    "                'patience': 5,                # epochs to wait before reducing LR\n",
    "                'step_size': 30,              # for StepLR (step every 30 datasets)\n",
    "                'gamma': 0.1                  # for StepLR (multiply LR by 0.1)\n",
    "            },\n",
    "        }\n",
    "    \n",
    "        if not train: # some \n",
    "            assert train_params['n_epochs_per_set'] == 0\n",
    "    \n",
    "        n_hidden = 100\n",
    "    \n",
    "        net_params = {\n",
    "            'net_type': hyp_dict['chosen_network'], # mpn1, dmpn, vanilla\n",
    "            'n_neurons': [1] + [n_hidden] * mpn_depth + [1],\n",
    "            'output_bias': False, # Turn off biases for easier interpretation\n",
    "            'loss_type': 'MSE', # XE, MSE\n",
    "            'activation': 'tanh', # linear, ReLU, sigmoid, tanh, tanh_re, tukey, heaviside\n",
    "            'cuda': True,\n",
    "            'monitor_freq': 100,\n",
    "            'monitor_valid_out': True, # Whether or not to save validation output throughout training\n",
    "            'output_matrix': '',# \"\" (default); \"untrained\", or \"orthogonal\"\n",
    "            'input_layer_add': True, \n",
    "            'input_layer_add_trainable': False,\n",
    "            'input_layer_bias': False,\n",
    "            \n",
    "            # for one-layer MPN, GRU or Vanilla\n",
    "            'ml_params1': {\n",
    "                'bias': True, # Bias of layer\n",
    "                'mp_type': 'mult',\n",
    "                'm_update_type': 'hebb_assoc', # hebb_assoc, hebb_pre\n",
    "                'eta_type': 'scalar', # scalar, pre_vector, post_vector, matrix\n",
    "                'eta_train': True,\n",
    "                # 'eta_init': 'gaussian', \n",
    "                'lam_type': 'scalar', # scalar, pre_vector, post_vector, matrix\n",
    "                'm_time_scale': 4000, # ms, sets lambda\n",
    "                'lam_train': False,\n",
    "                'W_freeze': False  \n",
    "            },\n",
    "    \n",
    "            # Vanilla RNN params\n",
    "            'leaky': True,\n",
    "            'alpha': 0.2,\n",
    "        }\n",
    "    \n",
    "        # actually I don't think it is needed\n",
    "        # putting here to warn the parameter checking every time \n",
    "        # when switching network\n",
    "        if hyp_dict['chosen_network'] in (\"gru\", \"vanilla\"):\n",
    "            assert f'ml_params' in net_params.keys()\n",
    "    \n",
    "        return task_params, train_params, net_params\n",
    "    \n",
    "    task_params, train_params, net_params = current_basic_params()\n",
    "    \n",
    "    shift_index = 1 if not task_params['fixate_off'] else 0\n",
    "    \n",
    "    if hyp_dict['task_type'] in ('multitask',):\n",
    "        task_params, train_params, net_params = mpn_tasks.convert_and_init_multitask_params(\n",
    "            (task_params, train_params, net_params)\n",
    "        )\n",
    "    \n",
    "        net_params['prefs'] = mpn_tasks.get_prefs(task_params['hp'])\n",
    "    \n",
    "        print('Rules: {}'.format(task_params['rules']))\n",
    "        print('  Input size {}, Output size {}'.format(\n",
    "            task_params['n_input'], task_params['n_output'],\n",
    "        ))\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    if net_params['cuda']:\n",
    "        print('Using CUDA...')\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        print('Using CPU...')\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    ##############\n",
    "    ##############\n",
    "    hyp_dict[\"mess_with_training\"] = False\n",
    "    \n",
    "    if hyp_dict['mess_with_training']:\n",
    "        hyp_dict['addon_name'] += \"messwithtraining\"\n",
    "    \n",
    "    params = task_params, train_params, net_params\n",
    "    \n",
    "    # random matrix\n",
    "    random_matrix = None\n",
    "    if task_params[\"randomize_inputs\"]:\n",
    "        random_matrix = task_params[\"randomize_matrix\"]\n",
    "    \n",
    "    if net_params['net_type'] == 'mpn1':\n",
    "        netFunction = mpn.MultiPlasticNet\n",
    "    elif net_params['net_type'] == 'dmpn':\n",
    "        netFunction = mpn.DeepMultiPlasticNet\n",
    "    elif net_params['net_type'] == 'vanilla':\n",
    "        netFunction = nets.VanillaRNN\n",
    "    elif net_params['net_type'] == 'gru':\n",
    "        netFunction = nets.GRU\n",
    "    \n",
    "    ##############\n",
    "    ##############\n",
    "    test_n_batch = train_params[\"valid_n_batch\"]\n",
    "    color_by = \"stim\"\n",
    "    \n",
    "    task_random_fix = True\n",
    "    if task_random_fix:\n",
    "        print(f\"Align {task_params['rules']} With Same Time\")\n",
    "    \n",
    "    if task_params['task_type'] in ('multitask',): # Test batch consists of all the rules\n",
    "        task_params['hp']['batch_size_train'] = test_n_batch\n",
    "        # using homogeneous cutting off\n",
    "        test_mode_for_all = \"random\"\n",
    "        # ZIHAN\n",
    "        # generate test data using \"random\"\n",
    "        # also aligned strength setting as in the training data\n",
    "        test_data, test_trials_extra = mpn_tasks.generate_trials_wrap(task_params, test_n_batch, \\\n",
    "                rules=task_params['rules'], mode_input=test_mode_for_all, fix=task_random_fix\n",
    "        )\n",
    "        _, test_trials, test_rule_idxs = test_trials_extra\n",
    "    \n",
    "        task_params['dataset_name'] = 'multitask'\n",
    "    \n",
    "        if task_params['in_out_mode'] in ('low_dim_pos',):\n",
    "            output_dim_labels = ('Fixate', 'Cos', '-Cos', 'Sin', '-Sin')\n",
    "        elif task_params['in_out_mode'] in ('low_dim',):\n",
    "            output_dim_labels = ('Fixate', 'Cos', 'Sin')\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "    \n",
    "        labels = []\n",
    "        for rule_idx, rule in enumerate(task_params['rules']):\n",
    "            print(rule)\n",
    "            if rule in accept_rules:\n",
    "                if hyp_dict['ruleset'] in ('dmsgo', 'dmcgo'):\n",
    "                    labels.append(test_trials[rule_idx].meta['matches'])\n",
    "                else:\n",
    "                    labels.append(test_trials[rule_idx].meta['resp1' if color_by == \"resp\" else 'stim1'])\n",
    "    \n",
    "            else:\n",
    "                raise NotImplementedError()\n",
    "        labels = np.concatenate(labels, axis=0).reshape(-1,1)\n",
    "    \n",
    "    test_input, test_output, test_mask = test_data\n",
    "    \n",
    "    permutation = np.random.permutation(test_input.shape[0])\n",
    "    test_input = test_input[permutation]\n",
    "    test_output = test_output[permutation]\n",
    "    test_mask = test_mask[permutation]\n",
    "    labels = labels[permutation]\n",
    "    \n",
    "    test_input_np = test_input.detach().cpu().numpy()\n",
    "    test_output_np = test_output.detach().cpu().numpy()\n",
    "    \n",
    "    n_batch_all = test_input_np.shape[0] # Total number of batches, might be different than test_n_batch\n",
    "    max_seq_len = test_input_np.shape[1]\n",
    "    \n",
    "    if task_params[\"randomize_inputs\"]: \n",
    "        rmat = task_params[\"randomize_matrix\"]\n",
    "        test_input_np = np.matmul(test_input_np, np.linalg.pinv(rmat))\n",
    "    \n",
    "    ##############\n",
    "    ##############\n",
    "    # we use net at different training stage on the same test_input\n",
    "    net, _, (counter_lst, netout_lst, db_lst, Winput_lst, Winputbias_lst, \\\n",
    "             Woutput_lst, Wall_lst, marker_lst, loss_lst, acc_lst) = net_helpers.train_network(params, device=device, verbose=verbose, train=train, hyp_dict=hyp_dict, netFunction=netFunction, test_input=[test_input])\n",
    "    \n",
    "    \n",
    "    ##############\n",
    "    ##############\n",
    "    if train:\n",
    "        fig, ax = plt.subplots(1,1,figsize=(3,3))\n",
    "        ax.plot(net.hist['iters_monitor'][1:], net.hist['train_acc'][1:], color=c_vals[0], label='Full train accuracy')\n",
    "        ax.plot(net.hist['iters_monitor'][1:], net.hist['valid_acc'][1:], color=c_vals[1], label='Full valid accuracy')\n",
    "        if net.weight_reg is not None:\n",
    "            ax.plot(net.hist['iters_monitor'], net.hist['train_loss_output_label'], color=c_vals_l[0], zorder=-1, label='Output label')\n",
    "            ax.plot(net.hist['iters_monitor'], net.hist['train_loss_reg_term'], color=c_vals_l[0], zorder=-1, label='Reg term', linestyle='dashed')\n",
    "            ax.plot(net.hist['iters_monitor'], net.hist['valid_loss_output_label'], color=c_vals_l[1], zorder=-1, label='Output valid label')\n",
    "            ax.plot(net.hist['iters_monitor'], net.hist['valid_loss_reg_term'], color=c_vals_l[1], zorder=-1, label='Reg valid term', linestyle='dashed')\n",
    "        \n",
    "        # ax.set_yscale('log')\n",
    "        ax.legend()\n",
    "        # ax.set_ylabel('Loss ({})'.format(net.loss_type))\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.set_xlabel('# Batches')\n",
    "        fig.savefig(f\"./hardtask/loss_{hyp_dict['ruleset']}_mpndepth_{mpn_depth}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "        \n",
    "    print('Done!')\n",
    "    \n",
    "    ##############\n",
    "    ##############\n",
    "    if train:\n",
    "        net_helpers.net_eta_lambda_analysis(net, net_params, hyp_dict)\n",
    "    \n",
    "    ##############\n",
    "    ##############\n",
    "    if hyp_dict['chosen_network'] == \"dmpn\":\n",
    "        if net_params[\"input_layer_add\"]:\n",
    "            input_matrix = net.W_initial_linear.weight.data.detach().cpu().numpy()\n",
    "            figinp, axsinp = plt.subplots(1,1,figsize=(4,4))\n",
    "            sns.heatmap(input_matrix, ax=axsinp, square=True, cmap='coolwarm')\n",
    "    \n",
    "    ##############\n",
    "    ##############\n",
    "    use_finalstage = False\n",
    "    if use_finalstage:\n",
    "        # plotting output in the validation set\n",
    "        net_out, db = net.iterate_sequence_batch(test_input, run_mode='track_states')\n",
    "        W_output = net.W_output.detach().cpu().numpy()\n",
    "    \n",
    "        W_all_ = []\n",
    "        for i in range(len(net.mp_layers)):\n",
    "            W_all_.append(net.mp_layers[i].W.detach().cpu().numpy())\n",
    "        W_ = W_all_[0]\n",
    "        \n",
    "    else:\n",
    "        ind = len(marker_lst)-1\n",
    "        # ind = 0\n",
    "        network_at_percent = (marker_lst[ind]+1)/train_params['n_datasets']*100\n",
    "        print(f\"Using network at {network_at_percent}%\")\n",
    "        net_out = netout_lst[ind]\n",
    "        db = db_lst[ind]\n",
    "        W_output = Woutput_lst[ind]\n",
    "        if net_params[\"net_type\"] == \"dmpn\":\n",
    "            W_ = Wall_lst[ind][0]\n",
    "    \n",
    "    if net_params['loss_type'] in ('MSE',):\n",
    "        fig, axs = plt.subplots(5, 1, figsize=(4, 5*2))\n",
    "        figin, axsin = plt.subplots(5, 1, figsize=(4, 5*2))\n",
    "    \n",
    "        if test_output_np.shape[-1] == 1:\n",
    "            for batch_idx, ax in enumerate(axs):\n",
    "                ax.plot(net_out[batch_idx, :, 0], color=c_vals[batch_idx])\n",
    "                ax.plot(test_output_np[batch_idx, :, 0], color=c_vals_l[batch_idx])\n",
    "    \n",
    "        else:\n",
    "            for batch_idx, ax in enumerate(axs):\n",
    "                task_label = test_input_np[batch_idx, 0, 6-shift_index:]\n",
    "                # task_label_index = np.where(task_label == 1)[0][0] \n",
    "                task_label_index = np.where(np.isclose(task_label, 1, atol=0.1))[0][0]\n",
    "                for out_idx in range(test_output_np.shape[-1]):\n",
    "                    axs[batch_idx].plot(net_out[batch_idx, :, out_idx], color=c_vals[out_idx], label=out_idx)\n",
    "                    axs[batch_idx].plot(test_output_np[batch_idx, :, out_idx], color=c_vals_l[out_idx], linewidth=5, alpha=0.5)\n",
    "                axs[batch_idx].legend()\n",
    "                axs[batch_idx].set_ylim([-2, 2])\n",
    "    \n",
    "                input_batch = test_input[batch_idx,:,:].cpu().numpy()\n",
    "                for inp_idx in range(input_batch.shape[-1]):\n",
    "                    axsin[batch_idx].plot(input_batch[:,inp_idx], color=c_vals[inp_idx], label=inp_idx)\n",
    "                axsin[batch_idx].legend()\n",
    "                axsin[batch_idx].set_ylim([-2, 2])\n",
    "    \n",
    "        fig.suptitle(f\"Validation Set Output Comparison using Network at Percentage {network_at_percent}%\")\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f\"./hardtask/lowD_{hyp_dict['ruleset']}_mpndepth_{mpn_depth}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "    \n",
    "        figin.suptitle(f\"Validation Set Output Comparison using Network at Percentage {network_at_percent}%\")\n",
    "        figin.tight_layout()\n",
    "        figin.savefig(f\"./hardtask/lowD_{hyp_dict['ruleset']}_input_mpndepth_{mpn_depth}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ef97e4-f4b0-426e-9145-2a699c7fe38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(db.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2668c5dd-1143-44ab-ad42-7574b8087e31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
