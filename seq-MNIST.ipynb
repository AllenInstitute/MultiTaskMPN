{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5c433c7-d919-4398-892e-a442859b8c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import gc \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import nonlinear_dagmpn as mpn \n",
    "\n",
    "class SequentialMNIST(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, root, train=True, download=True, normalize=True):\n",
    "        tfms = [transforms.ToTensor()]\n",
    "        self.ds = datasets.MNIST(root=root, train=train, download=download, transform=transforms.Compose(tfms))\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.ds[idx]             \n",
    "        x = x.view(-1)                  \n",
    "        if self.normalize:\n",
    "            x = (x - 0.1307) / 0.3081\n",
    "        x_seq = x.unsqueeze(-1)        \n",
    "        return x_seq, y\n",
    "\n",
    "\n",
    "def collate_seq(batch):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    xs, ys = zip(*batch)\n",
    "    x = torch.stack(xs, dim=0)\n",
    "    y = torch.tensor(ys, dtype=torch.long)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4c10c2f-3859-4a6b-8830-7699095d2e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cuda_tensor_shapes(limit=None, sort_by_numel=True, include_nonleaf=True):\n",
    "    \"\"\"\n",
    "    Prints shapes (and a bit more) for all live torch tensors on CUDA.\n",
    "\n",
    "    Notes:\n",
    "    - This lists tensors that are still referenced by Python (reachable by GC).\n",
    "    - It may include duplicates (views). We de-duplicate by storage data_ptr.\n",
    "    \"\"\"\n",
    "    cuda_tensors = []\n",
    "    seen = set()\n",
    "\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj):\n",
    "                t = obj\n",
    "            elif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\n",
    "                # Parameters and some wrappers\n",
    "                t = obj.data\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if t.is_cuda:\n",
    "                # de-dup by underlying storage pointer (works for views)\n",
    "                try:\n",
    "                    key = (t.untyped_storage().data_ptr(), t.storage_offset(), tuple(t.size()), str(t.dtype))\n",
    "                except Exception:\n",
    "                    key = (t.data_ptr(), tuple(t.size()), str(t.dtype))\n",
    "\n",
    "                if key in seen:\n",
    "                    continue\n",
    "                seen.add(key)\n",
    "\n",
    "                if (not include_nonleaf) and (t.grad_fn is not None):\n",
    "                    continue\n",
    "\n",
    "                cuda_tensors.append(t)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if sort_by_numel:\n",
    "        cuda_tensors.sort(key=lambda x: x.numel(), reverse=True)\n",
    "\n",
    "    if limit is not None:\n",
    "        cuda_tensors = cuda_tensors[:limit]\n",
    "\n",
    "    total_bytes = 0\n",
    "    for i, t in enumerate(cuda_tensors, 1):\n",
    "        nbytes = t.numel() * t.element_size()\n",
    "        total_bytes += nbytes\n",
    "        print(\n",
    "            f\"[{i:04d}] shape={tuple(t.shape)} dtype={t.dtype} \"\n",
    "            f\"device={t.device} requires_grad={t.requires_grad} \"\n",
    "            f\"bytes={nbytes/1024**2:.2f}MB\"\n",
    "        )\n",
    "\n",
    "    print(f\"\\nCount: {len(cuda_tensors)} tensors\")\n",
    "    print(f\"Estimated total (sum of listed tensor sizes): {total_bytes/1024**2:.2f}MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61bc8db8-733c-4fc1-9dcd-07dc3bd8f920",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(net, loader, device, chunk_size=64):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y in loader:\n",
    "        x = x.to(device, non_blocking=True)  # (B,T,D)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        # Convert to (T,B,D)\n",
    "        x_TBD = x.transpose(0, 1).contiguous()\n",
    "\n",
    "        logits_last = net.forward_sequence_checkpointed(\n",
    "            x_TBD,\n",
    "            chunk_size=chunk_size,\n",
    "            Ms0=None,\n",
    "            run_mode=\"minimal\",\n",
    "        )\n",
    "\n",
    "        pred = logits_last.argmax(dim=-1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.numel()\n",
    "\n",
    "    return correct / max(total, 1)\n",
    "\n",
    "def count_parameter(net):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    trainable = [(n, p) for n, p in net.named_parameters() if p.requires_grad]\n",
    "    \n",
    "    # Print a readable summary\n",
    "    total = 0\n",
    "    for n, p in trainable:\n",
    "        num = p.numel()\n",
    "        total += num\n",
    "        print(f\"{n:50s}  shape={tuple(p.shape)}  numel={num}\")\n",
    "    \n",
    "    print(f\"\\nTotal trainable parameters: {total}\")\n",
    "\n",
    "# Example usage:\n",
    "# print_cuda_params(net)\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def train_sequential_mnist(\n",
    "    device=\"cuda\",\n",
    "    data_root=\"./data\",\n",
    "    hidden_dim=256,\n",
    "    batch_size=64,\n",
    "    lr=1.0,\n",
    "    eta=1e-3,\n",
    "    epochs=5,\n",
    "    chunk_size=28,\n",
    "    use_amp=False,\n",
    "):\n",
    "    device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"device: {device}\")\n",
    "\n",
    "    # Data\n",
    "    train_ds = SequentialMNIST(root=data_root, train=True, download=True, normalize=True)\n",
    "    test_ds  = SequentialMNIST(root=data_root, train=False, download=True, normalize=True)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2,\n",
    "                              pin_memory=True, collate_fn=collate_seq)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=batch_size*10, shuffle=False, num_workers=2,\n",
    "                              pin_memory=True, collate_fn=collate_seq)\n",
    "\n",
    "    net_params = {\n",
    "        \"n_neurons\": [1] + [hidden_dim] + [10],\n",
    "        \"linear_embed\": 100,\n",
    "        \"dt\": 1.0,\n",
    "        \"activation\": \"tanh\",\n",
    "        \"output_bias\": True,\n",
    "        \"W_output_init\": \"xavier\",\n",
    "        \"input_layer_add\": True,\n",
    "        'input_layer_add_trainable': True,\n",
    "        'input_layer_bias': False,\n",
    "        \"output_matrix\": \"\",\n",
    "\n",
    "        'ml_params': {\n",
    "            'bias': True,\n",
    "            'mp_type': 'mult',\n",
    "            'm_update_type': 'hebb_assoc',\n",
    "            'eta_type': 'scalar',\n",
    "            'eta_train': False,\n",
    "            'lam_type': 'scalar',\n",
    "            'm_time_scale': 1000,\n",
    "            'lam_train': False,\n",
    "            'W_freeze': False,\n",
    "        },\n",
    "    }\n",
    "    net_params.update({\n",
    "        \"dag_W_gain\": 0.9,\n",
    "        \"dag_W_diag_decay\": 0.01,\n",
    "        \"dag_row_l1_target\": 0,\n",
    "    })\n",
    "    net_params[\"ml_params\"].update({\n",
    "        \"m_time_scale\": 1000,  # can override / ensure present\n",
    "        \"eta\": eta,\n",
    "        # optionally: \"eta\": 1e-3,  # if you want to hard-set eta instead of eta0\n",
    "    })\n",
    "    \n",
    "    net = mpn.DeepMultiPlasticNet(net_params, verbose=True, forzihan=False).to(device)\n",
    "\n",
    "    count_parameter(net)\n",
    "\n",
    "    opt = torch.optim.Adam([p for p in net.parameters() if p.requires_grad], lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        opt, mode=\"max\", factor=0.5, patience=2, threshold=1e-3, min_lr=1e-6, verbose=True\n",
    "    )\n",
    "\n",
    "    scaler = GradScaler(enabled=(use_amp and device.type == \"cuda\"))\n",
    "\n",
    "    stats = {\"test_acc\": []}\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        net.train()\n",
    "        t0 = time.time()\n",
    "        running_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device, non_blocking=True)   # (B,T,1)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "\n",
    "            # Convert to (T,B,D) for forward_sequence_checkpointed\n",
    "            x_TBD = x.transpose(0, 1).contiguous()\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Forward with time checkpointing\n",
    "            with autocast(enabled=(use_amp and device.type == \"cuda\")):#autocast(device_type=\"cuda\", enabled=(use_amp and device.type == \"cuda\")):\n",
    "                logits_last = net.forward_sequence_checkpointed(\n",
    "                    x_TBD,\n",
    "                    chunk_size=chunk_size,\n",
    "                    Ms0=None,\n",
    "                    run_mode=\"minimal\",\n",
    "                )\n",
    "                loss = criterion(logits_last, y)\n",
    "\n",
    "            # Backward\n",
    "            if scaler.is_enabled():\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                print(\"grad W_out:\", None if net.W_out.grad is None else net.W_out.grad.norm().item())\n",
    "                print(\"grad W:\",     None if net.W.grad is None else net.W.grad.norm().item())\n",
    "                print(\"grad W_in:\", None if net.W_in.grad is None else net.W_in.grad.norm().item())\n",
    "                opt.step()\n",
    "\n",
    "            net.param_clamp()\n",
    "\n",
    "            # Logging: \n",
    "            if (n_batches % 2) == 0:\n",
    "                with torch.no_grad():\n",
    "                    W0 = net.mp_layers[0].W.detach()# add .detach()? \n",
    "                    W_abs_mean = W0.abs().mean().item()\n",
    "\n",
    "                print(f\"[ep {ep} | batch {n_batches}] loss={loss.item():.4f} \"\n",
    "                      f\"|M| |W| mean={W_abs_mean:.3e}\")\n",
    "                torch.cuda.synchronize()\n",
    "                alloc = torch.cuda.memory_allocated() / 1024**3\n",
    "                reserv = torch.cuda.memory_reserved() / 1024**3\n",
    "                print(f\"ep {ep} batch {n_batches} | loss={loss.item():.4f} | alloc={alloc:.2f}GB reserv={reserv:.2f}GB\", flush=True)\n",
    "\n",
    "            running_loss += float(loss.item())\n",
    "            n_batches += 1\n",
    "            del logits_last, loss\n",
    "\n",
    "        train_loss = running_loss / max(n_batches, 1)\n",
    "        test_acc = evaluate(net, test_loader, device=device, chunk_size=max(chunk_size, 64))\n",
    "        scheduler.step(test_acc)\n",
    "\n",
    "        dt = time.time() - t0\n",
    "        current_lr = opt.param_groups[0][\"lr\"]\n",
    "        print(f\"Epoch {ep:02d}/{epochs} | lr={current_lr:.2e} | loss={train_loss:.4f} | test_acc={test_acc*100:.2f}% | dt={dt:.1f}s\")\n",
    "\n",
    "        stats[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    return net, net_params, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6f26a72-27ca-4130-bc5b-83c675cc2350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "[Nonlinear DAG-MPN per-sample M] d_in=1, d_h=300, d_out=10, lam=0.999000, eta=0.9, h_mode=blockwise, h_block=64\n",
      "W_in                                                shape=(300, 1)  numel=300\n",
      "W                                                   shape=(300, 300)  numel=90000\n",
      "W_out                                               shape=(10, 300)  numel=3000\n",
      "b_out                                               shape=(10,)  numel=10\n",
      "\n",
      "Total trainable parameters: 93310\n",
      "grad W_out: 0.030221162363886833\n",
      "grad W: 0.03092757798731327\n",
      "grad W_in: 0.03363049775362015\n",
      "[ep 1 | batch 0] loss=2.3043 |M| |W| mean=1.358e-02\n",
      "ep 1 batch 0 | loss=2.3043 | alloc=0.01GB reserv=12.23GB\n",
      "grad W_out: 0.014759446494281292\n",
      "grad W: 0.014993693679571152\n",
      "grad W_in: 0.016082575544714928\n",
      "grad W_out: 0.031168051064014435\n",
      "grad W: 0.03255341202020645\n",
      "grad W_in: 0.03271140530705452\n",
      "[ep 1 | batch 2] loss=2.3057 |M| |W| mean=1.362e-02\n",
      "ep 1 batch 2 | loss=2.3057 | alloc=0.01GB reserv=12.24GB\n",
      "grad W_out: 0.023596804589033127\n",
      "grad W: 0.021525228396058083\n",
      "grad W_in: 0.024067705497145653\n",
      "grad W_out: 0.032291632145643234\n",
      "grad W: 0.03160766512155533\n",
      "grad W_in: 0.03294284641742706\n",
      "[ep 1 | batch 4] loss=2.3022 |M| |W| mean=1.366e-02\n",
      "ep 1 batch 4 | loss=2.3022 | alloc=0.01GB reserv=12.24GB\n",
      "grad W_out: 0.015835439786314964\n",
      "grad W: 0.020377252250909805\n",
      "grad W_in: 0.018576545640826225\n",
      "grad W_out: 0.0362839438021183\n",
      "grad W: 0.037617795169353485\n",
      "grad W_in: 0.038991160690784454\n",
      "[ep 1 | batch 6] loss=2.3032 |M| |W| mean=1.369e-02\n",
      "ep 1 batch 6 | loss=2.3032 | alloc=0.01GB reserv=12.24GB\n",
      "grad W_out: 0.027539679780602455\n",
      "grad W: 0.026821911334991455\n",
      "grad W_in: 0.027585573494434357\n",
      "grad W_out: 0.027325481176376343\n",
      "grad W: 0.02805524878203869\n",
      "grad W_in: 0.028005296364426613\n",
      "[ep 1 | batch 8] loss=2.3007 |M| |W| mean=1.371e-02\n",
      "ep 1 batch 8 | loss=2.3007 | alloc=0.01GB reserv=12.24GB\n",
      "grad W_out: 0.025950176641345024\n",
      "grad W: 0.03731680288910866\n",
      "grad W_in: 0.032599564641714096\n",
      "grad W_out: 0.025028502568602562\n",
      "grad W: 0.0320395790040493\n",
      "grad W_in: 0.025510795414447784\n",
      "[ep 1 | batch 10] loss=2.2975 |M| |W| mean=1.375e-02\n",
      "ep 1 batch 10 | loss=2.2975 | alloc=0.01GB reserv=12.24GB\n",
      "grad W_out: 0.030627507716417313\n",
      "grad W: 0.03203041851520538\n",
      "grad W_in: 0.028987562283873558\n",
      "grad W_out: 0.035684846341609955\n",
      "grad W: 0.045079611241817474\n",
      "grad W_in: 0.03912828490138054\n",
      "[ep 1 | batch 12] loss=2.2957 |M| |W| mean=1.380e-02\n",
      "ep 1 batch 12 | loss=2.2957 | alloc=0.01GB reserv=12.24GB\n",
      "grad W_out: 0.034108277410268784\n",
      "grad W: 0.0514279343187809\n",
      "grad W_in: 0.03947247564792633\n",
      "grad W_out: 0.05188750848174095\n",
      "grad W: 0.07634332776069641\n",
      "grad W_in: 0.05959668755531311\n",
      "[ep 1 | batch 14] loss=2.2868 |M| |W| mean=1.386e-02\n",
      "ep 1 batch 14 | loss=2.2868 | alloc=0.02GB reserv=12.24GB\n",
      "grad W_out: 0.04775860905647278\n",
      "grad W: 0.07659698277711868\n",
      "grad W_in: 0.0595519095659256\n",
      "grad W_out: 0.05388353392481804\n",
      "grad W: 0.07518960535526276\n",
      "grad W_in: 0.05424315109848976\n",
      "[ep 1 | batch 16] loss=2.2825 |M| |W| mean=1.394e-02\n",
      "ep 1 batch 16 | loss=2.2825 | alloc=0.02GB reserv=12.25GB\n",
      "grad W_out: 0.06668910384178162\n",
      "grad W: 0.07595506310462952\n",
      "grad W_in: 0.05503110587596893\n",
      "grad W_out: 0.09276769310235977\n",
      "grad W: 0.10073591023683548\n",
      "grad W_in: 0.061735138297080994\n",
      "[ep 1 | batch 18] loss=2.2781 |M| |W| mean=1.402e-02\n",
      "ep 1 batch 18 | loss=2.2781 | alloc=0.02GB reserv=12.25GB\n",
      "grad W_out: 0.05486152321100235\n",
      "grad W: 0.06728284806013107\n",
      "grad W_in: 0.04492926597595215\n",
      "grad W_out: 0.0939822718501091\n",
      "grad W: 0.09959755837917328\n",
      "grad W_in: 0.0645938366651535\n",
      "[ep 1 | batch 20] loss=2.2661 |M| |W| mean=1.411e-02\n",
      "ep 1 batch 20 | loss=2.2661 | alloc=0.02GB reserv=12.25GB\n",
      "grad W_out: 0.10322916507720947\n",
      "grad W: 0.18629390001296997\n",
      "grad W_in: 0.11070196330547333\n",
      "grad W_out: 0.08398964256048203\n",
      "grad W: 0.16127729415893555\n",
      "grad W_in: 0.09390804171562195\n",
      "[ep 1 | batch 22] loss=2.2561 |M| |W| mean=1.418e-02\n",
      "ep 1 batch 22 | loss=2.2561 | alloc=0.02GB reserv=12.25GB\n",
      "grad W_out: 0.07757160812616348\n",
      "grad W: 0.15171977877616882\n",
      "grad W_in: 0.07945264130830765\n",
      "grad W_out: 0.11635833978652954\n",
      "grad W: 0.815001368522644\n",
      "grad W_in: 0.46632441878318787\n",
      "[ep 1 | batch 24] loss=2.2622 |M| |W| mean=1.426e-02\n",
      "ep 1 batch 24 | loss=2.2622 | alloc=0.02GB reserv=12.25GB\n",
      "grad W_out: 0.17108875513076782\n",
      "grad W: 0.3925155699253082\n",
      "grad W_in: 0.2243170142173767\n",
      "grad W_out: 0.2239932119846344\n",
      "grad W: 0.35947373509407043\n",
      "grad W_in: 0.19447508454322815\n",
      "[ep 1 | batch 26] loss=2.2619 |M| |W| mean=1.434e-02\n",
      "ep 1 batch 26 | loss=2.2619 | alloc=0.02GB reserv=12.25GB\n",
      "grad W_out: 0.1873313933610916\n",
      "grad W: 0.9627253413200378\n",
      "grad W_in: 0.5295307040214539\n",
      "grad W_out: 0.11064144968986511\n",
      "grad W: 0.2557596266269684\n",
      "grad W_in: 0.1318884789943695\n",
      "[ep 1 | batch 28] loss=2.2397 |M| |W| mean=1.443e-02\n",
      "ep 1 batch 28 | loss=2.2397 | alloc=0.03GB reserv=12.26GB\n",
      "grad W_out: 0.17643427848815918\n",
      "grad W: 0.325756311416626\n",
      "grad W_in: 0.17627881467342377\n",
      "grad W_out: 0.18011601269245148\n",
      "grad W: 0.4608844518661499\n",
      "grad W_in: 0.2661318778991699\n",
      "[ep 1 | batch 30] loss=2.2473 |M| |W| mean=1.452e-02\n",
      "ep 1 batch 30 | loss=2.2473 | alloc=0.03GB reserv=12.26GB\n",
      "grad W_out: 0.14944453537464142\n",
      "grad W: 0.18916848301887512\n",
      "grad W_in: 0.08987227082252502\n",
      "grad W_out: 0.1271745264530182\n",
      "grad W: 0.21025848388671875\n",
      "grad W_in: 0.11226844042539597\n",
      "[ep 1 | batch 32] loss=2.2215 |M| |W| mean=1.461e-02\n",
      "ep 1 batch 32 | loss=2.2215 | alloc=0.03GB reserv=12.26GB\n",
      "grad W_out: 0.2092294991016388\n",
      "grad W: 0.5660927891731262\n",
      "grad W_in: 0.30986204743385315\n",
      "grad W_out: 0.18085986375808716\n",
      "grad W: 3.2375218868255615\n",
      "grad W_in: 1.7265822887420654\n",
      "[ep 1 | batch 34] loss=2.2156 |M| |W| mean=1.471e-02\n",
      "ep 1 batch 34 | loss=2.2156 | alloc=0.03GB reserv=12.26GB\n",
      "grad W_out: 0.20419692993164062\n",
      "grad W: 0.35127103328704834\n",
      "grad W_in: 0.22074759006500244\n",
      "grad W_out: 0.17897504568099976\n",
      "grad W: 0.19844047725200653\n",
      "grad W_in: 0.09541147947311401\n",
      "[ep 1 | batch 36] loss=2.2174 |M| |W| mean=1.481e-02\n",
      "ep 1 batch 36 | loss=2.2174 | alloc=0.03GB reserv=12.26GB\n",
      "grad W_out: 0.15766985714435577\n",
      "grad W: 0.2450631558895111\n",
      "grad W_in: 0.15761244297027588\n",
      "grad W_out: 0.2279590219259262\n",
      "grad W: 0.4485316574573517\n",
      "grad W_in: 0.33763331174850464\n",
      "[ep 1 | batch 38] loss=2.2295 |M| |W| mean=1.491e-02\n",
      "ep 1 batch 38 | loss=2.2295 | alloc=0.03GB reserv=12.26GB\n",
      "grad W_out: 0.23268672823905945\n",
      "grad W: 4.099789142608643\n",
      "grad W_in: 2.048321485519409\n",
      "grad W_out: 0.1359367072582245\n",
      "grad W: 0.3606049716472626\n",
      "grad W_in: 0.26756221055984497\n",
      "[ep 1 | batch 40] loss=2.1625 |M| |W| mean=1.500e-02\n",
      "ep 1 batch 40 | loss=2.1625 | alloc=0.03GB reserv=12.26GB\n",
      "grad W_out: 0.11786936223506927\n",
      "grad W: 0.19339674711227417\n",
      "grad W_in: 0.11658033728599548\n",
      "grad W_out: 0.13455775380134583\n",
      "grad W: 0.8579182624816895\n",
      "grad W_in: 0.43427905440330505\n",
      "[ep 1 | batch 42] loss=2.1753 |M| |W| mean=1.510e-02\n",
      "ep 1 batch 42 | loss=2.1753 | alloc=0.04GB reserv=12.27GB\n",
      "grad W_out: 0.1387472301721573\n",
      "grad W: 0.26580196619033813\n",
      "grad W_in: 0.13682293891906738\n",
      "grad W_out: 0.15612930059432983\n",
      "grad W: 0.2217509001493454\n",
      "grad W_in: 0.11024750024080276\n",
      "[ep 1 | batch 44] loss=2.1832 |M| |W| mean=1.520e-02\n",
      "ep 1 batch 44 | loss=2.1832 | alloc=0.04GB reserv=12.27GB\n",
      "grad W_out: 0.16966752707958221\n",
      "grad W: 0.31732258200645447\n",
      "grad W_in: 0.15775525569915771\n",
      "grad W_out: 0.1380123496055603\n",
      "grad W: 0.2724531888961792\n",
      "grad W_in: 0.19826079905033112\n",
      "[ep 1 | batch 46] loss=2.1668 |M| |W| mean=1.531e-02\n",
      "ep 1 batch 46 | loss=2.1668 | alloc=0.04GB reserv=12.27GB\n",
      "grad W_out: 0.15686678886413574\n",
      "grad W: 0.2819477617740631\n",
      "grad W_in: 0.22994449734687805\n",
      "grad W_out: 0.13059130311012268\n",
      "grad W: 0.2218111902475357\n",
      "grad W_in: 0.12893390655517578\n",
      "[ep 1 | batch 48] loss=2.1418 |M| |W| mean=1.542e-02\n",
      "ep 1 batch 48 | loss=2.1418 | alloc=0.04GB reserv=12.27GB\n",
      "grad W_out: 0.13214533030986786\n",
      "grad W: 0.3047628700733185\n",
      "grad W_in: 0.12570910155773163\n",
      "grad W_out: 0.17908234894275665\n",
      "grad W: 0.3143971264362335\n",
      "grad W_in: 0.21799920499324799\n",
      "[ep 1 | batch 50] loss=2.1337 |M| |W| mean=1.553e-02\n",
      "ep 1 batch 50 | loss=2.1337 | alloc=0.04GB reserv=12.27GB\n",
      "grad W_out: 0.22830282151699066\n",
      "grad W: 1.044570803642273\n",
      "grad W_in: 0.6343989372253418\n",
      "grad W_out: 0.23612383008003235\n",
      "grad W: 0.45151248574256897\n",
      "grad W_in: 0.3391806483268738\n",
      "[ep 1 | batch 52] loss=2.1508 |M| |W| mean=1.563e-02\n",
      "ep 1 batch 52 | loss=2.1508 | alloc=0.04GB reserv=12.27GB\n",
      "grad W_out: 0.22156895697116852\n",
      "grad W: 0.6224475502967834\n",
      "grad W_in: 0.5040563941001892\n",
      "grad W_out: 0.17921054363250732\n",
      "grad W: 0.3556059002876282\n",
      "grad W_in: 0.3009797930717468\n",
      "[ep 1 | batch 54] loss=2.1211 |M| |W| mean=1.572e-02\n",
      "ep 1 batch 54 | loss=2.1211 | alloc=0.04GB reserv=12.28GB\n",
      "grad W_out: 0.1780194789171219\n",
      "grad W: 0.2215477079153061\n",
      "grad W_in: 0.08309140056371689\n",
      "grad W_out: 0.18386536836624146\n",
      "grad W: 0.5925478339195251\n",
      "grad W_in: 0.4498446583747864\n",
      "[ep 1 | batch 56] loss=2.1117 |M| |W| mean=1.581e-02\n",
      "ep 1 batch 56 | loss=2.1117 | alloc=0.05GB reserv=12.28GB\n",
      "grad W_out: 0.27497217059135437\n",
      "grad W: 0.728488028049469\n",
      "grad W_in: 0.40791240334510803\n",
      "grad W_out: 0.22864922881126404\n",
      "grad W: 0.292345255613327\n",
      "grad W_in: 0.1770826280117035\n",
      "[ep 1 | batch 58] loss=2.1567 |M| |W| mean=1.589e-02\n",
      "ep 1 batch 58 | loss=2.1567 | alloc=0.05GB reserv=12.28GB\n",
      "grad W_out: 0.2208986133337021\n",
      "grad W: 0.34199103713035583\n",
      "grad W_in: 0.26363837718963623\n",
      "grad W_out: 0.2977709174156189\n",
      "grad W: 0.5413870215415955\n",
      "grad W_in: 0.37810924649238586\n",
      "[ep 1 | batch 60] loss=2.1107 |M| |W| mean=1.595e-02\n",
      "ep 1 batch 60 | loss=2.1107 | alloc=0.05GB reserv=12.28GB\n",
      "grad W_out: 0.16446402668952942\n",
      "grad W: 0.32888278365135193\n",
      "grad W_in: 0.21039526164531708\n",
      "grad W_out: 0.22829777002334595\n",
      "grad W: 0.5753107666969299\n",
      "grad W_in: 0.44247448444366455\n",
      "[ep 1 | batch 62] loss=2.1315 |M| |W| mean=1.601e-02\n",
      "ep 1 batch 62 | loss=2.1315 | alloc=0.05GB reserv=12.28GB\n",
      "grad W_out: 0.23263689875602722\n",
      "grad W: 0.3045301139354706\n",
      "grad W_in: 0.18793028593063354\n",
      "grad W_out: 0.20251300930976868\n",
      "grad W: 0.29495155811309814\n",
      "grad W_in: 0.10294336080551147\n",
      "[ep 1 | batch 64] loss=2.1202 |M| |W| mean=1.606e-02\n",
      "ep 1 batch 64 | loss=2.1202 | alloc=0.05GB reserv=12.29GB\n",
      "grad W_out: 0.25677022337913513\n",
      "grad W: 0.26960688829421997\n",
      "grad W_in: 0.10407713800668716\n",
      "grad W_out: 0.1725640892982483\n",
      "grad W: 0.3724544048309326\n",
      "grad W_in: 0.144243985414505\n",
      "[ep 1 | batch 66] loss=2.1233 |M| |W| mean=1.610e-02\n",
      "ep 1 batch 66 | loss=2.1233 | alloc=0.05GB reserv=12.29GB\n",
      "grad W_out: 0.31504911184310913\n",
      "grad W: 0.9614591002464294\n",
      "grad W_in: 0.5550304055213928\n",
      "grad W_out: 0.20283879339694977\n",
      "grad W: 0.7146206498146057\n",
      "grad W_in: 0.6152222156524658\n",
      "[ep 1 | batch 68] loss=2.1028 |M| |W| mean=1.613e-02\n",
      "ep 1 batch 68 | loss=2.1028 | alloc=0.05GB reserv=12.29GB\n",
      "grad W_out: 0.2594980001449585\n",
      "grad W: 0.5873476266860962\n",
      "grad W_in: 0.4385811984539032\n",
      "grad W_out: 0.17434370517730713\n",
      "grad W: 0.22446070611476898\n",
      "grad W_in: 0.11786779016256332\n",
      "[ep 1 | batch 70] loss=2.0901 |M| |W| mean=1.615e-02\n",
      "ep 1 batch 70 | loss=2.0901 | alloc=0.06GB reserv=12.29GB\n",
      "grad W_out: 0.2523224949836731\n",
      "grad W: 1.1999447345733643\n",
      "grad W_in: 1.051314115524292\n",
      "grad W_out: 0.17953555285930634\n",
      "grad W: 0.5623655319213867\n",
      "grad W_in: 0.45426619052886963\n",
      "[ep 1 | batch 72] loss=2.0851 |M| |W| mean=1.617e-02\n",
      "ep 1 batch 72 | loss=2.0851 | alloc=0.06GB reserv=12.29GB\n",
      "grad W_out: 0.18067556619644165\n",
      "grad W: 0.5464186072349548\n",
      "grad W_in: 0.46027231216430664\n",
      "grad W_out: 0.14174672961235046\n",
      "grad W: 0.4431234896183014\n",
      "grad W_in: 0.3084357976913452\n",
      "[ep 1 | batch 74] loss=2.1204 |M| |W| mean=1.620e-02\n",
      "ep 1 batch 74 | loss=2.1204 | alloc=0.06GB reserv=12.29GB\n",
      "grad W_out: 0.23136329650878906\n",
      "grad W: 0.6992678046226501\n",
      "grad W_in: 0.5905571579933167\n",
      "grad W_out: 0.17519055306911469\n",
      "grad W: 0.5380969643592834\n",
      "grad W_in: 0.4855525493621826\n",
      "[ep 1 | batch 76] loss=2.0949 |M| |W| mean=1.623e-02\n",
      "ep 1 batch 76 | loss=2.0949 | alloc=0.06GB reserv=12.30GB\n",
      "grad W_out: 0.16031233966350555\n",
      "grad W: 0.45891597867012024\n",
      "grad W_in: 0.37577131390571594\n",
      "grad W_out: 0.34273552894592285\n",
      "grad W: 1.3292369842529297\n",
      "grad W_in: 1.0865347385406494\n",
      "[ep 1 | batch 78] loss=2.1035 |M| |W| mean=1.626e-02\n",
      "ep 1 batch 78 | loss=2.1035 | alloc=0.06GB reserv=12.30GB\n",
      "grad W_out: 0.26775306463241577\n",
      "grad W: 1.2362254858016968\n",
      "grad W_in: 1.1490869522094727\n",
      "grad W_out: 0.17423439025878906\n",
      "grad W: 0.25910326838493347\n",
      "grad W_in: 0.06871014833450317\n",
      "[ep 1 | batch 80] loss=2.0711 |M| |W| mean=1.629e-02\n",
      "ep 1 batch 80 | loss=2.0711 | alloc=0.06GB reserv=12.30GB\n",
      "grad W_out: 0.22455650568008423\n",
      "grad W: 0.764214277267456\n",
      "grad W_in: 0.6864487528800964\n",
      "grad W_out: 0.2383919358253479\n",
      "grad W: 0.7103784084320068\n",
      "grad W_in: 0.5431616902351379\n",
      "[ep 1 | batch 82] loss=2.1022 |M| |W| mean=1.631e-02\n",
      "ep 1 batch 82 | loss=2.1022 | alloc=0.06GB reserv=12.30GB\n",
      "grad W_out: 0.20099154114723206\n",
      "grad W: 0.6126521229743958\n",
      "grad W_in: 0.4591696560382843\n",
      "grad W_out: 0.17661577463150024\n",
      "grad W: 0.4373226463794708\n",
      "grad W_in: 0.3152790367603302\n",
      "[ep 1 | batch 84] loss=2.0991 |M| |W| mean=1.634e-02\n",
      "ep 1 batch 84 | loss=2.0991 | alloc=0.07GB reserv=12.30GB\n",
      "grad W_out: 0.16255760192871094\n",
      "grad W: 0.6097788214683533\n",
      "grad W_in: 0.5875725746154785\n",
      "grad W_out: 0.3006931245326996\n",
      "grad W: 0.7396270632743835\n",
      "grad W_in: 0.7061102390289307\n",
      "[ep 1 | batch 86] loss=2.0773 |M| |W| mean=1.636e-02\n",
      "ep 1 batch 86 | loss=2.0773 | alloc=0.07GB reserv=12.31GB\n",
      "grad W_out: 0.22811183333396912\n",
      "grad W: 0.3728182017803192\n",
      "grad W_in: 0.11546658724546432\n",
      "grad W_out: 0.17331747710704803\n",
      "grad W: 0.2913024425506592\n",
      "grad W_in: 0.10088026523590088\n",
      "[ep 1 | batch 88] loss=2.0676 |M| |W| mean=1.639e-02\n",
      "ep 1 batch 88 | loss=2.0676 | alloc=0.07GB reserv=12.31GB\n",
      "grad W_out: 0.1874758005142212\n",
      "grad W: 0.47153812646865845\n",
      "grad W_in: 0.291001558303833\n",
      "grad W_out: 0.17076556384563446\n",
      "grad W: 0.5477991104125977\n",
      "grad W_in: 0.43790924549102783\n",
      "[ep 1 | batch 90] loss=2.0893 |M| |W| mean=1.642e-02\n",
      "ep 1 batch 90 | loss=2.0893 | alloc=0.07GB reserv=12.31GB\n",
      "grad W_out: 0.20122361183166504\n",
      "grad W: 0.37553560733795166\n",
      "grad W_in: 0.2740592062473297\n",
      "grad W_out: 0.2429414838552475\n",
      "grad W: 0.3366270661354065\n",
      "grad W_in: 0.14033058285713196\n",
      "[ep 1 | batch 92] loss=2.0517 |M| |W| mean=1.647e-02\n",
      "ep 1 batch 92 | loss=2.0517 | alloc=0.07GB reserv=12.31GB\n",
      "grad W_out: 0.24562759697437286\n",
      "grad W: 0.5630788207054138\n",
      "grad W_in: 0.40259039402008057\n",
      "Epoch 01/20 | lr=1.00e-03 | loss=2.1822 | test_acc=23.25% | dt=1175.6s\n",
      "grad W_out: 0.2609575688838959\n",
      "grad W: 0.6679909825325012\n",
      "grad W_in: 0.5948703289031982\n",
      "[ep 2 | batch 0] loss=2.0671 |M| |W| mean=1.651e-02\n",
      "ep 2 batch 0 | loss=2.0671 | alloc=0.07GB reserv=26.05GB\n",
      "grad W_out: 0.18537914752960205\n",
      "grad W: 0.42492836713790894\n",
      "grad W_in: 0.3203543722629547\n",
      "grad W_out: 0.21382123231887817\n",
      "grad W: 0.6552867889404297\n",
      "grad W_in: 0.47319167852401733\n",
      "[ep 2 | batch 2] loss=2.0699 |M| |W| mean=1.655e-02\n",
      "ep 2 batch 2 | loss=2.0699 | alloc=0.07GB reserv=26.05GB\n",
      "grad W_out: 0.20579493045806885\n",
      "grad W: 0.34806495904922485\n",
      "grad W_in: 0.1867407113313675\n",
      "grad W_out: 0.3081952929496765\n",
      "grad W: 0.527843713760376\n",
      "grad W_in: 0.17026984691619873\n",
      "[ep 2 | batch 4] loss=2.0750 |M| |W| mean=1.660e-02\n",
      "ep 2 batch 4 | loss=2.0750 | alloc=0.08GB reserv=26.05GB\n",
      "grad W_out: 0.23731939494609833\n",
      "grad W: 0.45001620054244995\n",
      "grad W_in: 0.41060397028923035\n",
      "grad W_out: 0.19183030724525452\n",
      "grad W: 0.3932577073574066\n",
      "grad W_in: 0.13112379610538483\n",
      "[ep 2 | batch 6] loss=2.0459 |M| |W| mean=1.665e-02\n",
      "ep 2 batch 6 | loss=2.0459 | alloc=0.08GB reserv=26.05GB\n",
      "grad W_out: 0.21036474406719208\n",
      "grad W: 0.5146017670631409\n",
      "grad W_in: 0.41686737537384033\n",
      "grad W_out: 0.2080090194940567\n",
      "grad W: 0.4088743329048157\n",
      "grad W_in: 0.162753626704216\n",
      "[ep 2 | batch 8] loss=2.0913 |M| |W| mean=1.670e-02\n",
      "ep 2 batch 8 | loss=2.0913 | alloc=0.08GB reserv=26.06GB\n",
      "grad W_out: 0.22847133874893188\n",
      "grad W: 0.786783754825592\n",
      "grad W_in: 0.46778038144111633\n",
      "grad W_out: 0.32188349962234497\n",
      "grad W: 0.7520191669464111\n",
      "grad W_in: 0.26252901554107666\n",
      "[ep 2 | batch 10] loss=2.0566 |M| |W| mean=1.675e-02\n",
      "ep 2 batch 10 | loss=2.0566 | alloc=0.08GB reserv=26.06GB\n",
      "grad W_out: 0.2553424835205078\n",
      "grad W: 1.0267667770385742\n",
      "grad W_in: 0.4466407001018524\n",
      "grad W_out: 0.28978651762008667\n",
      "grad W: 0.723103940486908\n",
      "grad W_in: 0.2591887414455414\n",
      "[ep 2 | batch 12] loss=2.0140 |M| |W| mean=1.680e-02\n",
      "ep 2 batch 12 | loss=2.0140 | alloc=0.08GB reserv=26.06GB\n",
      "grad W_out: 0.18115943670272827\n",
      "grad W: 0.38356077671051025\n",
      "grad W_in: 0.10694043338298798\n",
      "grad W_out: 0.2176797240972519\n",
      "grad W: 0.5406793355941772\n",
      "grad W_in: 0.1367984116077423\n",
      "[ep 2 | batch 14] loss=2.0229 |M| |W| mean=1.686e-02\n",
      "ep 2 batch 14 | loss=2.0229 | alloc=0.08GB reserv=26.06GB\n",
      "grad W_out: 0.32861393690109253\n",
      "grad W: 0.722739577293396\n",
      "grad W_in: 0.3792724013328552\n",
      "grad W_out: 0.24939821660518646\n",
      "grad W: 0.507603645324707\n",
      "grad W_in: 0.345723032951355\n",
      "[ep 2 | batch 16] loss=2.0552 |M| |W| mean=1.692e-02\n",
      "ep 2 batch 16 | loss=2.0552 | alloc=0.08GB reserv=26.06GB\n",
      "grad W_out: 0.23810075223445892\n",
      "grad W: 0.7393609881401062\n",
      "grad W_in: 0.6023687720298767\n",
      "grad W_out: 0.20322105288505554\n",
      "grad W: 0.8619077801704407\n",
      "grad W_in: 0.41648638248443604\n",
      "[ep 2 | batch 18] loss=2.0067 |M| |W| mean=1.698e-02\n",
      "ep 2 batch 18 | loss=2.0067 | alloc=0.09GB reserv=26.07GB\n",
      "grad W_out: 0.23235350847244263\n",
      "grad W: 0.9183162450790405\n",
      "grad W_in: 0.4520970582962036\n",
      "grad W_out: 0.25606802105903625\n",
      "grad W: 0.8445672392845154\n",
      "grad W_in: 0.47303470969200134\n",
      "[ep 2 | batch 20] loss=1.9805 |M| |W| mean=1.704e-02\n",
      "ep 2 batch 20 | loss=1.9805 | alloc=0.09GB reserv=26.07GB\n",
      "grad W_out: 0.3215564787387848\n",
      "grad W: 0.6313616037368774\n",
      "grad W_in: 0.22068455815315247\n",
      "grad W_out: 0.25925377011299133\n",
      "grad W: 0.9338381886482239\n",
      "grad W_in: 0.8464017510414124\n",
      "[ep 2 | batch 22] loss=1.9792 |M| |W| mean=1.711e-02\n",
      "ep 2 batch 22 | loss=1.9792 | alloc=0.09GB reserv=26.07GB\n",
      "grad W_out: 0.28412508964538574\n",
      "grad W: 1.2443859577178955\n",
      "grad W_in: 1.1125904321670532\n",
      "grad W_out: 0.31682369112968445\n",
      "grad W: 1.0394679307937622\n",
      "grad W_in: 0.7637823224067688\n",
      "[ep 2 | batch 24] loss=2.0068 |M| |W| mean=1.718e-02\n",
      "ep 2 batch 24 | loss=2.0068 | alloc=0.09GB reserv=26.07GB\n",
      "grad W_out: 0.31312036514282227\n",
      "grad W: 1.4760327339172363\n",
      "grad W_in: 1.221738576889038\n",
      "grad W_out: 0.2504027783870697\n",
      "grad W: 0.4515025019645691\n",
      "grad W_in: 0.15200789272785187\n",
      "[ep 2 | batch 26] loss=1.9794 |M| |W| mean=1.724e-02\n",
      "ep 2 batch 26 | loss=1.9794 | alloc=0.09GB reserv=26.07GB\n",
      "grad W_out: 0.3003127872943878\n",
      "grad W: 1.201926350593567\n",
      "grad W_in: 0.6050885915756226\n",
      "grad W_out: 0.3075995445251465\n",
      "grad W: 1.1418790817260742\n",
      "grad W_in: 0.890948474407196\n",
      "[ep 2 | batch 28] loss=1.9378 |M| |W| mean=1.731e-02\n",
      "ep 2 batch 28 | loss=1.9378 | alloc=0.09GB reserv=26.08GB\n",
      "grad W_out: 0.23096314072608948\n",
      "grad W: 0.7755782604217529\n",
      "grad W_in: 0.664202094078064\n",
      "grad W_out: 0.3295043110847473\n",
      "grad W: 1.3533791303634644\n",
      "grad W_in: 0.47191092371940613\n",
      "[ep 2 | batch 30] loss=1.9276 |M| |W| mean=1.738e-02\n",
      "ep 2 batch 30 | loss=1.9276 | alloc=0.09GB reserv=26.08GB\n",
      "grad W_out: 0.2969232201576233\n",
      "grad W: 1.4324718713760376\n",
      "grad W_in: 0.3069732189178467\n",
      "grad W_out: 0.2706674039363861\n",
      "grad W: 1.1062978506088257\n",
      "grad W_in: 0.3255886435508728\n",
      "[ep 2 | batch 32] loss=1.9450 |M| |W| mean=1.745e-02\n",
      "ep 2 batch 32 | loss=1.9450 | alloc=0.10GB reserv=26.08GB\n",
      "grad W_out: 0.4013289213180542\n",
      "grad W: 2.412440299987793\n",
      "grad W_in: 0.9360134601593018\n",
      "grad W_out: 0.2994697391986847\n",
      "grad W: 7.992766857147217\n",
      "grad W_in: 2.3909754753112793\n",
      "[ep 2 | batch 34] loss=1.9342 |M| |W| mean=1.750e-02\n",
      "ep 2 batch 34 | loss=1.9342 | alloc=0.10GB reserv=26.08GB\n",
      "grad W_out: 0.2725933790206909\n",
      "grad W: 1.108906626701355\n",
      "grad W_in: 0.2695901095867157\n",
      "grad W_out: 0.3438069522380829\n",
      "grad W: 2.3389899730682373\n",
      "grad W_in: 0.49244749546051025\n",
      "[ep 2 | batch 36] loss=1.9084 |M| |W| mean=1.756e-02\n",
      "ep 2 batch 36 | loss=1.9084 | alloc=0.10GB reserv=26.08GB\n",
      "grad W_out: 0.304666668176651\n",
      "grad W: 1.3602735996246338\n",
      "grad W_in: 0.7025905251502991\n",
      "grad W_out: 0.37878257036209106\n",
      "grad W: 4.243854522705078\n",
      "grad W_in: 1.645065426826477\n",
      "[ep 2 | batch 38] loss=1.9044 |M| |W| mean=1.761e-02\n",
      "ep 2 batch 38 | loss=1.9044 | alloc=0.10GB reserv=26.09GB\n",
      "grad W_out: 0.23933742940425873\n",
      "grad W: 1.4776527881622314\n",
      "grad W_in: 0.5162960886955261\n",
      "grad W_out: 0.29879477620124817\n",
      "grad W: 0.925998866558075\n",
      "grad W_in: 0.659980058670044\n",
      "[ep 2 | batch 40] loss=1.9067 |M| |W| mean=1.766e-02\n",
      "ep 2 batch 40 | loss=1.9067 | alloc=0.10GB reserv=26.09GB\n",
      "grad W_out: 0.3372921943664551\n",
      "grad W: 1.93398118019104\n",
      "grad W_in: 0.5806703567504883\n",
      "grad W_out: 0.28870391845703125\n",
      "grad W: 0.8998820781707764\n",
      "grad W_in: 0.3692696690559387\n",
      "[ep 2 | batch 42] loss=1.8524 |M| |W| mean=1.772e-02\n",
      "ep 2 batch 42 | loss=1.8524 | alloc=0.10GB reserv=26.09GB\n",
      "grad W_out: 0.2634851038455963\n",
      "grad W: 1.2983124256134033\n",
      "grad W_in: 0.36241233348846436\n",
      "grad W_out: 0.3519473075866699\n",
      "grad W: 2.376281261444092\n",
      "grad W_in: 0.7249248623847961\n",
      "[ep 2 | batch 44] loss=1.9131 |M| |W| mean=1.777e-02\n",
      "ep 2 batch 44 | loss=1.9131 | alloc=0.10GB reserv=26.09GB\n",
      "grad W_out: 0.2824258804321289\n",
      "grad W: 1.8068006038665771\n",
      "grad W_in: 0.7552593350410461\n",
      "grad W_out: 0.269382506608963\n",
      "grad W: 2.708392858505249\n",
      "grad W_in: 1.0340275764465332\n",
      "[ep 2 | batch 46] loss=1.8395 |M| |W| mean=1.782e-02\n",
      "ep 2 batch 46 | loss=1.8395 | alloc=0.11GB reserv=26.09GB\n",
      "grad W_out: 0.24720236659049988\n",
      "grad W: 1.319584608078003\n",
      "grad W_in: 0.697077214717865\n",
      "grad W_out: 0.4188769459724426\n",
      "grad W: 5.184531211853027\n",
      "grad W_in: 1.5576974153518677\n",
      "[ep 2 | batch 48] loss=1.9160 |M| |W| mean=1.786e-02\n",
      "ep 2 batch 48 | loss=1.9160 | alloc=0.11GB reserv=26.10GB\n",
      "grad W_out: 0.3104112148284912\n",
      "grad W: 1.1590843200683594\n",
      "grad W_in: 0.2797548770904541\n",
      "grad W_out: 0.37734055519104004\n",
      "grad W: 3.1124181747436523\n",
      "grad W_in: 2.2613656520843506\n",
      "[ep 2 | batch 50] loss=1.8883 |M| |W| mean=1.790e-02\n",
      "ep 2 batch 50 | loss=1.8883 | alloc=0.11GB reserv=26.10GB\n",
      "grad W_out: 0.4274124503135681\n",
      "grad W: 1.7645443677902222\n",
      "grad W_in: 0.36823880672454834\n",
      "grad W_out: 0.34653568267822266\n",
      "grad W: 1.2244396209716797\n",
      "grad W_in: 0.7749218940734863\n",
      "[ep 2 | batch 52] loss=1.7937 |M| |W| mean=1.794e-02\n",
      "ep 2 batch 52 | loss=1.7937 | alloc=0.11GB reserv=26.10GB\n",
      "grad W_out: 0.6367172598838806\n",
      "grad W: 3.533090353012085\n",
      "grad W_in: 2.3440463542938232\n",
      "grad W_out: 0.38869133591651917\n",
      "grad W: 42.336368560791016\n",
      "grad W_in: 7.211554527282715\n",
      "[ep 2 | batch 54] loss=1.7927 |M| |W| mean=1.797e-02\n",
      "ep 2 batch 54 | loss=1.7927 | alloc=0.11GB reserv=26.10GB\n",
      "grad W_out: 0.5222653746604919\n",
      "grad W: 1.4844048023223877\n",
      "grad W_in: 0.7993950843811035\n",
      "grad W_out: 0.4254245460033417\n",
      "grad W: 1.6066564321517944\n",
      "grad W_in: 1.2895336151123047\n",
      "[ep 2 | batch 56] loss=1.8591 |M| |W| mean=1.803e-02\n",
      "ep 2 batch 56 | loss=1.8591 | alloc=0.11GB reserv=26.10GB\n",
      "grad W_out: 0.376473605632782\n",
      "grad W: 1.7444151639938354\n",
      "grad W_in: 0.5092405080795288\n",
      "grad W_out: 0.33420485258102417\n",
      "grad W: 2.3709874153137207\n",
      "grad W_in: 1.2433698177337646\n",
      "[ep 2 | batch 58] loss=1.7926 |M| |W| mean=1.809e-02\n",
      "ep 2 batch 58 | loss=1.7926 | alloc=0.11GB reserv=26.11GB\n",
      "grad W_out: 0.2774689793586731\n",
      "grad W: 1.6472973823547363\n",
      "grad W_in: 0.8218510150909424\n",
      "grad W_out: 0.47472232580184937\n",
      "grad W: 1.5685137510299683\n",
      "grad W_in: 0.3676092326641083\n",
      "[ep 2 | batch 60] loss=1.7875 |M| |W| mean=1.815e-02\n",
      "ep 2 batch 60 | loss=1.7875 | alloc=0.12GB reserv=26.11GB\n",
      "grad W_out: 0.4475252032279968\n",
      "grad W: 2.4232330322265625\n",
      "grad W_in: 0.9170135259628296\n",
      "grad W_out: 0.32147717475891113\n",
      "grad W: 0.9180555939674377\n",
      "grad W_in: 0.4006786644458771\n",
      "[ep 2 | batch 62] loss=1.7778 |M| |W| mean=1.822e-02\n",
      "ep 2 batch 62 | loss=1.7778 | alloc=0.12GB reserv=26.11GB\n",
      "grad W_out: 0.3962565064430237\n",
      "grad W: 3.622443437576294\n",
      "grad W_in: 0.9501996636390686\n",
      "grad W_out: 0.40221524238586426\n",
      "grad W: 4.419750213623047\n",
      "grad W_in: 1.3016177415847778\n",
      "[ep 2 | batch 64] loss=1.8278 |M| |W| mean=1.828e-02\n",
      "ep 2 batch 64 | loss=1.8278 | alloc=0.12GB reserv=26.11GB\n",
      "grad W_out: 0.46073976159095764\n",
      "grad W: 5.46329402923584\n",
      "grad W_in: 2.0071465969085693\n",
      "grad W_out: 0.4672269821166992\n",
      "grad W: 1.1118214130401611\n",
      "grad W_in: 0.742290735244751\n",
      "[ep 2 | batch 66] loss=1.7342 |M| |W| mean=1.833e-02\n",
      "ep 2 batch 66 | loss=1.7342 | alloc=0.12GB reserv=26.11GB\n",
      "grad W_out: 0.34420838952064514\n",
      "grad W: 2.3072030544281006\n",
      "grad W_in: 1.4560707807540894\n",
      "grad W_out: 0.36408913135528564\n",
      "grad W: 1.234026551246643\n",
      "grad W_in: 0.7479060292243958\n",
      "[ep 2 | batch 68] loss=1.7864 |M| |W| mean=1.838e-02\n",
      "ep 2 batch 68 | loss=1.7864 | alloc=0.12GB reserv=26.12GB\n",
      "grad W_out: 0.5062699317932129\n",
      "grad W: 1.227127194404602\n",
      "grad W_in: 0.2978496551513672\n",
      "grad W_out: 0.4252412021160126\n",
      "grad W: 1.1839622259140015\n",
      "grad W_in: 0.5732773542404175\n",
      "[ep 2 | batch 70] loss=1.7791 |M| |W| mean=1.842e-02\n",
      "ep 2 batch 70 | loss=1.7791 | alloc=0.12GB reserv=26.12GB\n",
      "grad W_out: 0.6387308835983276\n",
      "grad W: 9.638909339904785\n",
      "grad W_in: 2.272891044616699\n",
      "grad W_out: 0.2939029037952423\n",
      "grad W: 3.810307264328003\n",
      "grad W_in: 1.0701053142547607\n",
      "[ep 2 | batch 72] loss=1.7975 |M| |W| mean=1.845e-02\n",
      "ep 2 batch 72 | loss=1.7975 | alloc=0.12GB reserv=26.12GB\n",
      "grad W_out: 0.585684597492218\n",
      "grad W: 4.8274970054626465\n",
      "grad W_in: 1.7477315664291382\n",
      "grad W_out: 0.44617897272109985\n",
      "grad W: 3.9970309734344482\n",
      "grad W_in: 1.5967618227005005\n",
      "[ep 2 | batch 74] loss=1.8775 |M| |W| mean=1.847e-02\n",
      "ep 2 batch 74 | loss=1.8775 | alloc=0.13GB reserv=26.12GB\n",
      "grad W_out: 0.41153836250305176\n",
      "grad W: 2.963369369506836\n",
      "grad W_in: 1.9149830341339111\n",
      "grad W_out: 0.48001405596733093\n",
      "grad W: 2.5097386837005615\n",
      "grad W_in: 1.5526835918426514\n",
      "[ep 2 | batch 76] loss=1.8278 |M| |W| mean=1.850e-02\n",
      "ep 2 batch 76 | loss=1.8278 | alloc=0.13GB reserv=26.12GB\n",
      "grad W_out: 0.3545267879962921\n",
      "grad W: 2.57202410697937\n",
      "grad W_in: 0.6302849054336548\n",
      "grad W_out: 0.4118207097053528\n",
      "grad W: 2.3351283073425293\n",
      "grad W_in: 2.0258164405822754\n",
      "[ep 2 | batch 78] loss=1.8187 |M| |W| mean=1.854e-02\n",
      "ep 2 batch 78 | loss=1.8187 | alloc=0.13GB reserv=26.12GB\n",
      "grad W_out: 0.5492379069328308\n",
      "grad W: 4.159158229827881\n",
      "grad W_in: 3.163203477859497\n",
      "grad W_out: 0.35714617371559143\n",
      "grad W: 2.022397994995117\n",
      "grad W_in: 1.066474437713623\n",
      "[ep 2 | batch 80] loss=1.8076 |M| |W| mean=1.855e-02\n",
      "ep 2 batch 80 | loss=1.8076 | alloc=0.13GB reserv=26.13GB\n",
      "grad W_out: 0.5414252281188965\n",
      "grad W: 2.4989564418792725\n",
      "grad W_in: 0.8111728429794312\n",
      "grad W_out: 0.3809446096420288\n",
      "grad W: 1.350928783416748\n",
      "grad W_in: 0.9033395648002625\n",
      "[ep 2 | batch 82] loss=1.8028 |M| |W| mean=1.858e-02\n",
      "ep 2 batch 82 | loss=1.8028 | alloc=0.13GB reserv=26.13GB\n",
      "grad W_out: 0.46666237711906433\n",
      "grad W: 1.970473051071167\n",
      "grad W_in: 1.2931331396102905\n",
      "grad W_out: 0.38229647278785706\n",
      "grad W: 2.0422089099884033\n",
      "grad W_in: 1.508806586265564\n",
      "[ep 2 | batch 84] loss=1.8644 |M| |W| mean=1.863e-02\n",
      "ep 2 batch 84 | loss=1.8644 | alloc=0.13GB reserv=26.13GB\n",
      "grad W_out: 0.3745979964733124\n",
      "grad W: 2.216625213623047\n",
      "grad W_in: 1.1124573945999146\n",
      "grad W_out: 0.34121784567832947\n",
      "grad W: 2.0162293910980225\n",
      "grad W_in: 0.31191593408584595\n",
      "[ep 2 | batch 86] loss=1.8154 |M| |W| mean=1.866e-02\n",
      "ep 2 batch 86 | loss=1.8154 | alloc=0.13GB reserv=26.13GB\n",
      "grad W_out: 0.2613944411277771\n",
      "grad W: 1.369482159614563\n",
      "grad W_in: 0.27021467685699463\n",
      "grad W_out: 0.4604264795780182\n",
      "grad W: 1.9778106212615967\n",
      "grad W_in: 1.313094139099121\n",
      "[ep 2 | batch 88] loss=1.8122 |M| |W| mean=1.869e-02\n",
      "ep 2 batch 88 | loss=1.8122 | alloc=0.14GB reserv=26.13GB\n",
      "grad W_out: 0.4976629912853241\n",
      "grad W: 2.2547826766967773\n",
      "grad W_in: 2.092546224594116\n",
      "grad W_out: 0.4107770323753357\n",
      "grad W: 1.22310471534729\n",
      "grad W_in: 0.6826204657554626\n",
      "[ep 2 | batch 90] loss=1.8256 |M| |W| mean=1.870e-02\n",
      "ep 2 batch 90 | loss=1.8256 | alloc=0.14GB reserv=26.14GB\n",
      "grad W_out: 0.4404672682285309\n",
      "grad W: 1.5998951196670532\n",
      "grad W_in: 0.34190353751182556\n",
      "grad W_out: 0.4454355835914612\n",
      "grad W: 2.2761738300323486\n",
      "grad W_in: 1.115949034690857\n",
      "[ep 2 | batch 92] loss=1.7545 |M| |W| mean=1.871e-02\n",
      "ep 2 batch 92 | loss=1.7545 | alloc=0.14GB reserv=26.14GB\n",
      "grad W_out: 0.376039981842041\n",
      "grad W: 1.2586760520935059\n",
      "grad W_in: 0.5747726559638977\n",
      "Epoch 02/20 | lr=1.00e-03 | loss=1.9006 | test_acc=39.90% | dt=1175.1s\n",
      "grad W_out: 0.4265536069869995\n",
      "grad W: 1.9983547925949097\n",
      "grad W_in: 1.1574897766113281\n",
      "[ep 3 | batch 0] loss=1.7872 |M| |W| mean=1.874e-02\n",
      "ep 3 batch 0 | loss=1.7872 | alloc=0.14GB reserv=26.14GB\n",
      "grad W_out: 0.3245161175727844\n",
      "grad W: 1.3605599403381348\n",
      "grad W_in: 0.26423606276512146\n",
      "grad W_out: 0.2839990556240082\n",
      "grad W: 1.2227951288223267\n",
      "grad W_in: 0.3712559938430786\n",
      "[ep 3 | batch 2] loss=1.7522 |M| |W| mean=1.878e-02\n",
      "ep 3 batch 2 | loss=1.7522 | alloc=0.14GB reserv=26.14GB\n",
      "grad W_out: 0.40710917115211487\n",
      "grad W: 2.1528372764587402\n",
      "grad W_in: 0.8034892082214355\n",
      "grad W_out: 0.43398818373680115\n",
      "grad W: 2.3368313312530518\n",
      "grad W_in: 1.221732258796692\n",
      "[ep 3 | batch 4] loss=1.7825 |M| |W| mean=1.880e-02\n",
      "ep 3 batch 4 | loss=1.7825 | alloc=0.14GB reserv=26.14GB\n",
      "grad W_out: 0.4062172770500183\n",
      "grad W: 2.277653217315674\n",
      "grad W_in: 1.1101372241973877\n",
      "grad W_out: 0.34573984146118164\n",
      "grad W: 0.5774976015090942\n",
      "grad W_in: 0.1666233092546463\n",
      "[ep 3 | batch 6] loss=1.7423 |M| |W| mean=1.881e-02\n",
      "ep 3 batch 6 | loss=1.7423 | alloc=0.14GB reserv=26.15GB\n",
      "grad W_out: 0.3721030056476593\n",
      "grad W: 1.9875848293304443\n",
      "grad W_in: 0.9561367034912109\n",
      "grad W_out: 0.4370860457420349\n",
      "grad W: 2.0960843563079834\n",
      "grad W_in: 1.6183664798736572\n",
      "[ep 3 | batch 8] loss=1.7481 |M| |W| mean=1.883e-02\n",
      "ep 3 batch 8 | loss=1.7481 | alloc=0.15GB reserv=26.15GB\n",
      "grad W_out: 0.2706280052661896\n",
      "grad W: 1.0738108158111572\n",
      "grad W_in: 0.6028072834014893\n",
      "grad W_out: 0.2481199949979782\n",
      "grad W: 3.637667179107666\n",
      "grad W_in: 0.9708030819892883\n",
      "[ep 3 | batch 10] loss=1.7543 |M| |W| mean=1.886e-02\n",
      "ep 3 batch 10 | loss=1.7543 | alloc=0.15GB reserv=26.15GB\n",
      "grad W_out: 0.288003534078598\n",
      "grad W: 2.2451171875\n",
      "grad W_in: 0.46983179450035095\n",
      "grad W_out: 0.36973029375076294\n",
      "grad W: 5.851346015930176\n",
      "grad W_in: 1.7179946899414062\n",
      "[ep 3 | batch 12] loss=1.7907 |M| |W| mean=1.888e-02\n",
      "ep 3 batch 12 | loss=1.7907 | alloc=0.15GB reserv=26.15GB\n",
      "grad W_out: 0.43753165006637573\n",
      "grad W: 3.746030569076538\n",
      "grad W_in: 0.7685427665710449\n",
      "grad W_out: 0.4111470878124237\n",
      "grad W: 3.6054954528808594\n",
      "grad W_in: 0.9234248399734497\n",
      "[ep 3 | batch 14] loss=1.7213 |M| |W| mean=1.890e-02\n",
      "ep 3 batch 14 | loss=1.7213 | alloc=0.15GB reserv=26.15GB\n",
      "grad W_out: 0.4403212368488312\n",
      "grad W: 1.6969176530838013\n",
      "grad W_in: 1.0246005058288574\n",
      "grad W_out: 0.2778749167919159\n",
      "grad W: 4.31874942779541\n",
      "grad W_in: 0.7770794630050659\n",
      "[ep 3 | batch 16] loss=1.7090 |M| |W| mean=1.892e-02\n",
      "ep 3 batch 16 | loss=1.7090 | alloc=0.15GB reserv=26.16GB\n",
      "grad W_out: 0.3936544954776764\n",
      "grad W: 2.8070497512817383\n",
      "grad W_in: 1.4483027458190918\n",
      "grad W_out: 0.26635628938674927\n",
      "grad W: 1.4301223754882812\n",
      "grad W_in: 0.7242332100868225\n",
      "[ep 3 | batch 18] loss=1.7015 |M| |W| mean=1.894e-02\n",
      "ep 3 batch 18 | loss=1.7015 | alloc=0.15GB reserv=26.16GB\n",
      "grad W_out: 0.4017462432384491\n",
      "grad W: 1.7041480541229248\n",
      "grad W_in: 1.1856963634490967\n",
      "grad W_out: 0.44763970375061035\n",
      "grad W: 1.3221914768218994\n",
      "grad W_in: 0.4762272238731384\n",
      "[ep 3 | batch 20] loss=1.7127 |M| |W| mean=1.896e-02\n",
      "ep 3 batch 20 | loss=1.7127 | alloc=0.15GB reserv=26.16GB\n",
      "grad W_out: 0.35381051898002625\n",
      "grad W: 1.1396846771240234\n",
      "grad W_in: 0.6382424831390381\n",
      "grad W_out: 0.3056145906448364\n",
      "grad W: 2.540102481842041\n",
      "grad W_in: 0.4816988706588745\n",
      "[ep 3 | batch 22] loss=1.7011 |M| |W| mean=1.898e-02\n",
      "ep 3 batch 22 | loss=1.7011 | alloc=0.16GB reserv=26.16GB\n",
      "grad W_out: 0.4026729464530945\n",
      "grad W: 1.776484489440918\n",
      "grad W_in: 0.3266952931880951\n",
      "grad W_out: 0.33075231313705444\n",
      "grad W: 1.643288016319275\n",
      "grad W_in: 1.0489957332611084\n",
      "[ep 3 | batch 24] loss=1.6661 |M| |W| mean=1.900e-02\n",
      "ep 3 batch 24 | loss=1.6661 | alloc=0.16GB reserv=26.16GB\n",
      "grad W_out: 0.374588280916214\n",
      "grad W: 2.031381845474243\n",
      "grad W_in: 0.3271355926990509\n",
      "grad W_out: 0.42815694212913513\n",
      "grad W: 1.237480878829956\n",
      "grad W_in: 0.349906325340271\n",
      "[ep 3 | batch 26] loss=1.7155 |M| |W| mean=1.902e-02\n",
      "ep 3 batch 26 | loss=1.7155 | alloc=0.16GB reserv=26.17GB\n",
      "grad W_out: 0.31645679473876953\n",
      "grad W: 1.0794953107833862\n",
      "grad W_in: 0.22841523587703705\n",
      "grad W_out: 0.3069845139980316\n",
      "grad W: 2.891723394393921\n",
      "grad W_in: 1.0613285303115845\n",
      "[ep 3 | batch 28] loss=1.7017 |M| |W| mean=1.903e-02\n",
      "ep 3 batch 28 | loss=1.7017 | alloc=0.16GB reserv=26.17GB\n",
      "grad W_out: 0.351657897233963\n",
      "grad W: 2.618117570877075\n",
      "grad W_in: 1.0122389793395996\n",
      "grad W_out: 0.3432576358318329\n",
      "grad W: 2.510751485824585\n",
      "grad W_in: 0.8118230700492859\n",
      "[ep 3 | batch 30] loss=1.6791 |M| |W| mean=1.904e-02\n",
      "ep 3 batch 30 | loss=1.6791 | alloc=0.16GB reserv=26.17GB\n",
      "grad W_out: 0.5277696847915649\n",
      "grad W: 4.312810897827148\n",
      "grad W_in: 1.7667136192321777\n",
      "grad W_out: 0.2664356529712677\n",
      "grad W: 1.6173498630523682\n",
      "grad W_in: 0.33299869298934937\n",
      "[ep 3 | batch 32] loss=1.6874 |M| |W| mean=1.906e-02\n",
      "ep 3 batch 32 | loss=1.6874 | alloc=0.16GB reserv=26.17GB\n",
      "grad W_out: 0.3133596181869507\n",
      "grad W: 2.330775022506714\n",
      "grad W_in: 0.7126010060310364\n",
      "grad W_out: 0.3646191358566284\n",
      "grad W: 2.5466535091400146\n",
      "grad W_in: 1.0040754079818726\n",
      "[ep 3 | batch 34] loss=1.6655 |M| |W| mean=1.909e-02\n",
      "ep 3 batch 34 | loss=1.6655 | alloc=0.16GB reserv=26.17GB\n",
      "grad W_out: 0.26220330595970154\n",
      "grad W: 1.8157304525375366\n",
      "grad W_in: 0.6896292567253113\n",
      "grad W_out: 0.38765934109687805\n",
      "grad W: 2.6048097610473633\n",
      "grad W_in: 0.6912039518356323\n",
      "[ep 3 | batch 36] loss=1.6819 |M| |W| mean=1.910e-02\n",
      "ep 3 batch 36 | loss=1.6819 | alloc=0.17GB reserv=26.18GB\n",
      "grad W_out: 0.280041366815567\n",
      "grad W: 2.0865495204925537\n",
      "grad W_in: 0.22740085422992706\n",
      "grad W_out: 0.40654638409614563\n",
      "grad W: 1.894521951675415\n",
      "grad W_in: 1.1637574434280396\n",
      "[ep 3 | batch 38] loss=1.6646 |M| |W| mean=1.911e-02\n",
      "ep 3 batch 38 | loss=1.6646 | alloc=0.17GB reserv=26.18GB\n",
      "grad W_out: 0.28655970096588135\n",
      "grad W: 5.317519187927246\n",
      "grad W_in: 1.3485695123672485\n",
      "grad W_out: 0.30754774808883667\n",
      "grad W: 1.2896356582641602\n",
      "grad W_in: 0.6807178258895874\n",
      "[ep 3 | batch 40] loss=1.6429 |M| |W| mean=1.912e-02\n",
      "ep 3 batch 40 | loss=1.6429 | alloc=0.17GB reserv=26.18GB\n",
      "grad W_out: 0.29474449157714844\n",
      "grad W: 2.377372980117798\n",
      "grad W_in: 0.920264720916748\n",
      "grad W_out: 0.3983074128627777\n",
      "grad W: 7.202343940734863\n",
      "grad W_in: 2.6650290489196777\n",
      "[ep 3 | batch 42] loss=1.7039 |M| |W| mean=1.914e-02\n",
      "ep 3 batch 42 | loss=1.7039 | alloc=0.17GB reserv=26.18GB\n",
      "grad W_out: 0.26513636112213135\n",
      "grad W: 5.05305814743042\n",
      "grad W_in: 1.4200941324234009\n",
      "grad W_out: 0.4346405267715454\n",
      "grad W: 7.683534622192383\n",
      "grad W_in: 1.5301127433776855\n",
      "[ep 3 | batch 44] loss=1.6645 |M| |W| mean=1.916e-02\n",
      "ep 3 batch 44 | loss=1.6645 | alloc=0.17GB reserv=26.18GB\n",
      "grad W_out: 0.2916812598705292\n",
      "grad W: 2.9434475898742676\n",
      "grad W_in: 1.2023868560791016\n",
      "grad W_out: 0.4692995250225067\n",
      "grad W: 5.781301021575928\n",
      "grad W_in: 1.7085235118865967\n",
      "[ep 3 | batch 46] loss=1.7303 |M| |W| mean=1.917e-02\n",
      "ep 3 batch 46 | loss=1.7303 | alloc=0.17GB reserv=26.19GB\n",
      "grad W_out: 0.32937297224998474\n",
      "grad W: 3.893571615219116\n",
      "grad W_in: 1.3208427429199219\n",
      "grad W_out: 0.49228301644325256\n",
      "grad W: 3.6162590980529785\n",
      "grad W_in: 0.9823868274688721\n",
      "[ep 3 | batch 48] loss=1.6449 |M| |W| mean=1.917e-02\n",
      "ep 3 batch 48 | loss=1.6449 | alloc=0.17GB reserv=26.19GB\n",
      "grad W_out: 0.6063123941421509\n",
      "grad W: 3.583996057510376\n",
      "grad W_in: 1.0453096628189087\n",
      "grad W_out: 0.36777645349502563\n",
      "grad W: 3.5856332778930664\n",
      "grad W_in: 1.2147787809371948\n",
      "[ep 3 | batch 50] loss=1.6300 |M| |W| mean=1.919e-02\n",
      "ep 3 batch 50 | loss=1.6300 | alloc=0.18GB reserv=26.19GB\n",
      "grad W_out: 0.38084471225738525\n",
      "grad W: 2.4812490940093994\n",
      "grad W_in: 0.6771048903465271\n",
      "grad W_out: 0.44304966926574707\n",
      "grad W: 4.581548690795898\n",
      "grad W_in: 0.9792031645774841\n",
      "[ep 3 | batch 52] loss=1.6946 |M| |W| mean=1.922e-02\n",
      "ep 3 batch 52 | loss=1.6946 | alloc=0.18GB reserv=26.19GB\n",
      "grad W_out: 0.42387980222702026\n",
      "grad W: 5.887036323547363\n",
      "grad W_in: 0.6979010105133057\n",
      "grad W_out: 0.5115346312522888\n",
      "grad W: 3.2504799365997314\n",
      "grad W_in: 1.1322153806686401\n",
      "[ep 3 | batch 54] loss=1.7154 |M| |W| mean=1.924e-02\n",
      "ep 3 batch 54 | loss=1.7154 | alloc=0.18GB reserv=26.19GB\n",
      "grad W_out: 0.5566146969795227\n",
      "grad W: 6.071030616760254\n",
      "grad W_in: 1.434977412223816\n",
      "grad W_out: 0.34594908356666565\n",
      "grad W: 3.490494966506958\n",
      "grad W_in: 0.6787914633750916\n",
      "[ep 3 | batch 56] loss=1.6124 |M| |W| mean=1.924e-02\n",
      "ep 3 batch 56 | loss=1.6124 | alloc=0.18GB reserv=26.20GB\n",
      "grad W_out: 0.34869584441185\n",
      "grad W: 1.6637510061264038\n",
      "grad W_in: 0.6209227442741394\n",
      "grad W_out: 0.4949951767921448\n",
      "grad W: 3.492086172103882\n",
      "grad W_in: 0.9361569285392761\n",
      "[ep 3 | batch 58] loss=1.6193 |M| |W| mean=1.925e-02\n",
      "ep 3 batch 58 | loss=1.6193 | alloc=0.18GB reserv=26.20GB\n",
      "grad W_out: 0.47628137469291687\n",
      "grad W: 3.6705970764160156\n",
      "grad W_in: 1.4364145994186401\n",
      "grad W_out: 0.4736039340496063\n",
      "grad W: 2.1090590953826904\n",
      "grad W_in: 0.5794203281402588\n",
      "[ep 3 | batch 60] loss=1.6085 |M| |W| mean=1.927e-02\n",
      "ep 3 batch 60 | loss=1.6085 | alloc=0.18GB reserv=26.20GB\n",
      "grad W_out: 0.3974343538284302\n",
      "grad W: 1.195141315460205\n",
      "grad W_in: 0.3275018632411957\n",
      "grad W_out: 0.4048490524291992\n",
      "grad W: 4.4772186279296875\n",
      "grad W_in: 2.1019046306610107\n",
      "[ep 3 | batch 62] loss=1.6164 |M| |W| mean=1.930e-02\n",
      "ep 3 batch 62 | loss=1.6164 | alloc=0.18GB reserv=26.20GB\n",
      "grad W_out: 0.3337571918964386\n",
      "grad W: 1.780801773071289\n",
      "grad W_in: 0.7035833597183228\n",
      "grad W_out: 0.3402571678161621\n",
      "grad W: 2.44433331489563\n",
      "grad W_in: 1.1552996635437012\n",
      "[ep 3 | batch 64] loss=1.5854 |M| |W| mean=1.931e-02\n",
      "ep 3 batch 64 | loss=1.5854 | alloc=0.19GB reserv=26.20GB\n",
      "grad W_out: 0.33076512813568115\n",
      "grad W: 1.2022498846054077\n",
      "grad W_in: 0.8500782251358032\n",
      "grad W_out: 0.3194415867328644\n",
      "grad W: 1.5912803411483765\n",
      "grad W_in: 0.5420145392417908\n",
      "[ep 3 | batch 66] loss=1.6592 |M| |W| mean=1.933e-02\n",
      "ep 3 batch 66 | loss=1.6592 | alloc=0.19GB reserv=26.21GB\n",
      "grad W_out: 0.3287907838821411\n",
      "grad W: 4.0673675537109375\n",
      "grad W_in: 1.9543237686157227\n",
      "grad W_out: 0.2407483160495758\n",
      "grad W: 1.32496178150177\n",
      "grad W_in: 0.25160127878189087\n",
      "[ep 3 | batch 68] loss=1.6249 |M| |W| mean=1.936e-02\n",
      "ep 3 batch 68 | loss=1.6249 | alloc=0.19GB reserv=26.21GB\n",
      "grad W_out: 0.2631171643733978\n",
      "grad W: 2.586075782775879\n",
      "grad W_in: 0.582168459892273\n",
      "grad W_out: 0.3578147292137146\n",
      "grad W: 5.36704158782959\n",
      "grad W_in: 2.535742998123169\n",
      "[ep 3 | batch 70] loss=1.5804 |M| |W| mean=1.939e-02\n",
      "ep 3 batch 70 | loss=1.5804 | alloc=0.19GB reserv=26.21GB\n",
      "grad W_out: 0.44986146688461304\n",
      "grad W: 2.899486541748047\n",
      "grad W_in: 2.5387179851531982\n",
      "grad W_out: 0.3139914870262146\n",
      "grad W: 6.279878616333008\n",
      "grad W_in: 1.4289932250976562\n",
      "[ep 3 | batch 72] loss=1.5993 |M| |W| mean=1.940e-02\n",
      "ep 3 batch 72 | loss=1.5993 | alloc=0.19GB reserv=26.21GB\n",
      "grad W_out: 0.4855453372001648\n",
      "grad W: 4.7892303466796875\n",
      "grad W_in: 2.20475697517395\n",
      "grad W_out: 0.3010346591472626\n",
      "grad W: 2.3952102661132812\n",
      "grad W_in: 1.463870644569397\n",
      "[ep 3 | batch 74] loss=1.5799 |M| |W| mean=1.940e-02\n",
      "ep 3 batch 74 | loss=1.5799 | alloc=0.19GB reserv=26.21GB\n",
      "grad W_out: 0.3083697259426117\n",
      "grad W: 1.9859496355056763\n",
      "grad W_in: 0.7673421502113342\n",
      "grad W_out: 0.4486690163612366\n",
      "grad W: 3.8127925395965576\n",
      "grad W_in: 1.4840091466903687\n",
      "[ep 3 | batch 76] loss=1.5837 |M| |W| mean=1.942e-02\n",
      "ep 3 batch 76 | loss=1.5837 | alloc=0.19GB reserv=26.21GB\n",
      "grad W_out: 0.4353925883769989\n",
      "grad W: 5.791507244110107\n",
      "grad W_in: 2.577974796295166\n",
      "grad W_out: 0.25867369771003723\n",
      "grad W: 1.493635654449463\n",
      "grad W_in: 0.38507208228111267\n",
      "[ep 3 | batch 78] loss=1.6103 |M| |W| mean=1.944e-02\n",
      "ep 3 batch 78 | loss=1.6103 | alloc=0.20GB reserv=26.22GB\n",
      "grad W_out: 0.28879421949386597\n",
      "grad W: 2.625763177871704\n",
      "grad W_in: 2.716248035430908\n",
      "grad W_out: 0.4269393980503082\n",
      "grad W: 3.2167344093322754\n",
      "grad W_in: 1.0525115728378296\n",
      "[ep 3 | batch 80] loss=1.6007 |M| |W| mean=1.946e-02\n",
      "ep 3 batch 80 | loss=1.6007 | alloc=0.20GB reserv=26.22GB\n",
      "grad W_out: 0.5241848230361938\n",
      "grad W: 5.109673023223877\n",
      "grad W_in: 0.9765864610671997\n",
      "grad W_out: 0.4779975712299347\n",
      "grad W: 5.867115497589111\n",
      "grad W_in: 1.5016134977340698\n",
      "[ep 3 | batch 82] loss=1.5320 |M| |W| mean=1.948e-02\n",
      "ep 3 batch 82 | loss=1.5320 | alloc=0.20GB reserv=26.22GB\n",
      "grad W_out: 0.2298823744058609\n",
      "grad W: 3.249835729598999\n",
      "grad W_in: 1.3122252225875854\n",
      "grad W_out: 0.23228313028812408\n",
      "grad W: 2.5676302909851074\n",
      "grad W_in: 2.0529987812042236\n",
      "[ep 3 | batch 84] loss=1.5686 |M| |W| mean=1.948e-02\n",
      "ep 3 batch 84 | loss=1.5686 | alloc=0.20GB reserv=26.22GB\n",
      "grad W_out: 0.410224050283432\n",
      "grad W: 4.091475009918213\n",
      "grad W_in: 0.9595533013343811\n",
      "grad W_out: 0.35770922899246216\n",
      "grad W: 2.5163826942443848\n",
      "grad W_in: 0.8284225463867188\n",
      "[ep 3 | batch 86] loss=1.6063 |M| |W| mean=1.949e-02\n",
      "ep 3 batch 86 | loss=1.6063 | alloc=0.20GB reserv=26.22GB\n",
      "grad W_out: 0.3009321391582489\n",
      "grad W: 2.788024425506592\n",
      "grad W_in: 0.5829668641090393\n",
      "grad W_out: 0.3842758238315582\n",
      "grad W: 3.9679503440856934\n",
      "grad W_in: 0.874928891658783\n",
      "[ep 3 | batch 88] loss=1.5221 |M| |W| mean=1.952e-02\n",
      "ep 3 batch 88 | loss=1.5221 | alloc=0.20GB reserv=26.23GB\n",
      "grad W_out: 0.30148589611053467\n",
      "grad W: 3.152580738067627\n",
      "grad W_in: 0.9071342945098877\n",
      "grad W_out: 0.35739049315452576\n",
      "grad W: 4.646735668182373\n",
      "grad W_in: 0.9104099273681641\n",
      "[ep 3 | batch 90] loss=1.5269 |M| |W| mean=1.954e-02\n",
      "ep 3 batch 90 | loss=1.5269 | alloc=0.20GB reserv=26.23GB\n",
      "grad W_out: 0.37656065821647644\n",
      "grad W: 2.3595569133758545\n",
      "grad W_in: 1.0337703227996826\n",
      "grad W_out: 0.28293171525001526\n",
      "grad W: 2.4330859184265137\n",
      "grad W_in: 1.1527036428451538\n",
      "[ep 3 | batch 92] loss=1.5128 |M| |W| mean=1.956e-02\n",
      "ep 3 batch 92 | loss=1.5128 | alloc=0.21GB reserv=26.23GB\n",
      "grad W_out: 0.28443998098373413\n",
      "grad W: 4.683201313018799\n",
      "grad W_in: 1.2107793092727661\n",
      "Epoch 03/20 | lr=1.00e-03 | loss=1.6538 | test_acc=52.09% | dt=1175.3s\n",
      "grad W_out: 0.3477766215801239\n",
      "grad W: 5.231977462768555\n",
      "grad W_in: 0.9528282880783081\n",
      "[ep 4 | batch 0] loss=1.4224 |M| |W| mean=1.957e-02\n",
      "ep 4 batch 0 | loss=1.4224 | alloc=0.21GB reserv=26.23GB\n",
      "grad W_out: 0.42813482880592346\n",
      "grad W: 5.414836883544922\n",
      "grad W_in: 1.4348844289779663\n",
      "grad W_out: 0.2963002920150757\n",
      "grad W: 4.5095038414001465\n",
      "grad W_in: 1.9818129539489746\n",
      "[ep 4 | batch 2] loss=1.4398 |M| |W| mean=1.958e-02\n",
      "ep 4 batch 2 | loss=1.4398 | alloc=0.21GB reserv=26.23GB\n",
      "grad W_out: 0.3238126337528229\n",
      "grad W: 2.170600652694702\n",
      "grad W_in: 1.1009639501571655\n",
      "grad W_out: 0.3189360797405243\n",
      "grad W: 2.4785866737365723\n",
      "grad W_in: 1.4162240028381348\n",
      "[ep 4 | batch 4] loss=1.4587 |M| |W| mean=1.960e-02\n",
      "ep 4 batch 4 | loss=1.4587 | alloc=0.21GB reserv=26.24GB\n",
      "grad W_out: 0.39637458324432373\n",
      "grad W: 3.6258785724639893\n",
      "grad W_in: 1.594833493232727\n",
      "grad W_out: 0.33006465435028076\n",
      "grad W: 2.084343194961548\n",
      "grad W_in: 0.7678690552711487\n",
      "[ep 4 | batch 6] loss=1.4491 |M| |W| mean=1.962e-02\n",
      "ep 4 batch 6 | loss=1.4491 | alloc=0.21GB reserv=26.24GB\n",
      "grad W_out: 0.44211113452911377\n",
      "grad W: 5.817159175872803\n",
      "grad W_in: 2.4808218479156494\n",
      "grad W_out: 0.38487330079078674\n",
      "grad W: 3.1711933612823486\n",
      "grad W_in: 1.8311898708343506\n",
      "[ep 4 | batch 8] loss=1.5021 |M| |W| mean=1.964e-02\n",
      "ep 4 batch 8 | loss=1.5021 | alloc=0.21GB reserv=26.24GB\n",
      "grad W_out: 0.28755801916122437\n",
      "grad W: 2.45345401763916\n",
      "grad W_in: 1.210028886795044\n",
      "grad W_out: 0.40916314721107483\n",
      "grad W: 3.957101821899414\n",
      "grad W_in: 3.019345760345459\n",
      "[ep 4 | batch 10] loss=1.5116 |M| |W| mean=1.965e-02\n",
      "ep 4 batch 10 | loss=1.5116 | alloc=0.21GB reserv=26.24GB\n",
      "grad W_out: 0.29450684785842896\n",
      "grad W: 1.9160382747650146\n",
      "grad W_in: 0.9384365081787109\n",
      "grad W_out: 0.4000118672847748\n",
      "grad W: 3.171721935272217\n",
      "grad W_in: 2.819810628890991\n",
      "[ep 4 | batch 12] loss=1.5150 |M| |W| mean=1.967e-02\n",
      "ep 4 batch 12 | loss=1.5150 | alloc=0.22GB reserv=26.24GB\n",
      "grad W_out: 0.4286426603794098\n",
      "grad W: 5.016091346740723\n",
      "grad W_in: 1.7220412492752075\n",
      "grad W_out: 0.26305562257766724\n",
      "grad W: 1.3542157411575317\n",
      "grad W_in: 0.9032432436943054\n",
      "[ep 4 | batch 14] loss=1.4369 |M| |W| mean=1.969e-02\n",
      "ep 4 batch 14 | loss=1.4369 | alloc=0.22GB reserv=26.25GB\n",
      "grad W_out: 0.46467533707618713\n",
      "grad W: 4.894962310791016\n",
      "grad W_in: 1.5827747583389282\n",
      "grad W_out: 0.46090269088745117\n",
      "grad W: 5.445616245269775\n",
      "grad W_in: 1.1071722507476807\n",
      "[ep 4 | batch 16] loss=1.4734 |M| |W| mean=1.970e-02\n",
      "ep 4 batch 16 | loss=1.4734 | alloc=0.22GB reserv=26.25GB\n",
      "grad W_out: 0.23640985786914825\n",
      "grad W: 1.9322900772094727\n",
      "grad W_in: 0.9020083546638489\n",
      "grad W_out: 0.4121975898742676\n",
      "grad W: 4.866261959075928\n",
      "grad W_in: 1.2453784942626953\n",
      "[ep 4 | batch 18] loss=1.4864 |M| |W| mean=1.971e-02\n",
      "ep 4 batch 18 | loss=1.4864 | alloc=0.22GB reserv=26.25GB\n",
      "grad W_out: 0.3696100115776062\n",
      "grad W: 5.553536891937256\n",
      "grad W_in: 1.370523452758789\n",
      "grad W_out: 0.33612480759620667\n",
      "grad W: 1.7959765195846558\n",
      "grad W_in: 0.8359822630882263\n",
      "[ep 4 | batch 20] loss=1.4138 |M| |W| mean=1.972e-02\n",
      "ep 4 batch 20 | loss=1.4138 | alloc=0.22GB reserv=26.25GB\n",
      "grad W_out: 0.3672228455543518\n",
      "grad W: 3.3219494819641113\n",
      "grad W_in: 0.8348110318183899\n",
      "grad W_out: 0.3848676085472107\n",
      "grad W: 6.805578708648682\n",
      "grad W_in: 1.9909597635269165\n",
      "[ep 4 | batch 22] loss=1.4826 |M| |W| mean=1.974e-02\n",
      "ep 4 batch 22 | loss=1.4826 | alloc=0.22GB reserv=26.25GB\n",
      "grad W_out: 0.48680034279823303\n",
      "grad W: 4.2954206466674805\n",
      "grad W_in: 2.2611217498779297\n",
      "grad W_out: 0.4261017143726349\n",
      "grad W: 7.173050880432129\n",
      "grad W_in: 1.7501442432403564\n",
      "[ep 4 | batch 24] loss=1.4645 |M| |W| mean=1.975e-02\n",
      "ep 4 batch 24 | loss=1.4645 | alloc=0.22GB reserv=26.26GB\n",
      "grad W_out: 0.3313799500465393\n",
      "grad W: 3.869751214981079\n",
      "grad W_in: 0.6552443504333496\n",
      "grad W_out: 0.34940946102142334\n",
      "grad W: 4.884083271026611\n",
      "grad W_in: 4.464658260345459\n",
      "[ep 4 | batch 26] loss=1.4255 |M| |W| mean=1.977e-02\n",
      "ep 4 batch 26 | loss=1.4255 | alloc=0.23GB reserv=26.26GB\n",
      "grad W_out: 0.2695889472961426\n",
      "grad W: 1.6268842220306396\n",
      "grad W_in: 0.8621889352798462\n",
      "grad W_out: 0.35982978343963623\n",
      "grad W: 3.95977520942688\n",
      "grad W_in: 2.8027336597442627\n",
      "[ep 4 | batch 28] loss=1.3565 |M| |W| mean=1.979e-02\n",
      "ep 4 batch 28 | loss=1.3565 | alloc=0.23GB reserv=26.26GB\n",
      "grad W_out: 0.4443387985229492\n",
      "grad W: 5.670415878295898\n",
      "grad W_in: 2.0617825984954834\n",
      "grad W_out: 0.31443285942077637\n",
      "grad W: 4.317348957061768\n",
      "grad W_in: 2.5494329929351807\n",
      "[ep 4 | batch 30] loss=1.3803 |M| |W| mean=1.980e-02\n",
      "ep 4 batch 30 | loss=1.3803 | alloc=0.23GB reserv=26.26GB\n",
      "grad W_out: 0.40179404616355896\n",
      "grad W: 4.14415168762207\n",
      "grad W_in: 1.129974126815796\n",
      "grad W_out: 0.30359259247779846\n",
      "grad W: 2.111332416534424\n",
      "grad W_in: 0.47622692584991455\n",
      "[ep 4 | batch 32] loss=1.3759 |M| |W| mean=1.982e-02\n",
      "ep 4 batch 32 | loss=1.3759 | alloc=0.23GB reserv=26.26GB\n",
      "grad W_out: 0.28656381368637085\n",
      "grad W: 4.074221611022949\n",
      "grad W_in: 2.4167327880859375\n",
      "grad W_out: 0.3355196416378021\n",
      "grad W: 2.5139429569244385\n",
      "grad W_in: 0.38888585567474365\n",
      "[ep 4 | batch 34] loss=1.4250 |M| |W| mean=1.984e-02\n",
      "ep 4 batch 34 | loss=1.4250 | alloc=0.23GB reserv=26.27GB\n",
      "grad W_out: 0.4069041609764099\n",
      "grad W: 3.79491925239563\n",
      "grad W_in: 2.752572774887085\n",
      "grad W_out: 0.3354719877243042\n",
      "grad W: 4.652428150177002\n",
      "grad W_in: 0.9619894027709961\n",
      "[ep 4 | batch 36] loss=1.4394 |M| |W| mean=1.986e-02\n",
      "ep 4 batch 36 | loss=1.4394 | alloc=0.23GB reserv=26.27GB\n",
      "grad W_out: 0.2615940570831299\n",
      "grad W: 3.361952781677246\n",
      "grad W_in: 1.3811936378479004\n",
      "grad W_out: 0.2551133632659912\n",
      "grad W: 4.886057376861572\n",
      "grad W_in: 1.0882502794265747\n",
      "[ep 4 | batch 38] loss=1.3537 |M| |W| mean=1.987e-02\n",
      "ep 4 batch 38 | loss=1.3537 | alloc=0.23GB reserv=26.27GB\n",
      "grad W_out: 0.3573512136936188\n",
      "grad W: 13.293402671813965\n",
      "grad W_in: 6.072534561157227\n",
      "grad W_out: 0.9665452837944031\n",
      "grad W: 11.89348316192627\n",
      "grad W_in: 9.215487480163574\n",
      "[ep 4 | batch 40] loss=1.6128 |M| |W| mean=1.989e-02\n",
      "ep 4 batch 40 | loss=1.6128 | alloc=0.24GB reserv=26.27GB\n",
      "grad W_out: 0.7898367643356323\n",
      "grad W: 8.462850570678711\n",
      "grad W_in: 2.270643949508667\n",
      "grad W_out: 0.7229031920433044\n",
      "grad W: 13.975308418273926\n",
      "grad W_in: 5.176577091217041\n",
      "[ep 4 | batch 42] loss=1.5202 |M| |W| mean=1.990e-02\n",
      "ep 4 batch 42 | loss=1.5202 | alloc=0.24GB reserv=26.27GB\n",
      "grad W_out: 0.7386773824691772\n",
      "grad W: 5.413718223571777\n",
      "grad W_in: 3.3167564868927\n",
      "grad W_out: 0.6047362089157104\n",
      "grad W: 7.198981761932373\n",
      "grad W_in: 4.511582374572754\n",
      "[ep 4 | batch 44] loss=1.4359 |M| |W| mean=1.993e-02\n",
      "ep 4 batch 44 | loss=1.4359 | alloc=0.24GB reserv=26.28GB\n",
      "grad W_out: 0.7402466535568237\n",
      "grad W: 4.744039535522461\n",
      "grad W_in: 3.4351558685302734\n",
      "grad W_out: 0.6839209794998169\n",
      "grad W: 6.378284454345703\n",
      "grad W_in: 5.510377883911133\n",
      "[ep 4 | batch 46] loss=1.5505 |M| |W| mean=1.994e-02\n",
      "ep 4 batch 46 | loss=1.5505 | alloc=0.24GB reserv=26.28GB\n",
      "grad W_out: 0.546089768409729\n",
      "grad W: 2.482445478439331\n",
      "grad W_in: 2.7701809406280518\n",
      "grad W_out: 0.5026437640190125\n",
      "grad W: 3.8510515689849854\n",
      "grad W_in: 0.7216554284095764\n",
      "[ep 4 | batch 48] loss=1.5308 |M| |W| mean=1.994e-02\n",
      "ep 4 batch 48 | loss=1.5308 | alloc=0.24GB reserv=26.28GB\n",
      "grad W_out: 0.5629563927650452\n",
      "grad W: 4.375163555145264\n",
      "grad W_in: 1.6326563358306885\n",
      "grad W_out: 0.4303678274154663\n",
      "grad W: 4.271986961364746\n",
      "grad W_in: 1.8847103118896484\n",
      "[ep 4 | batch 50] loss=1.4741 |M| |W| mean=1.997e-02\n",
      "ep 4 batch 50 | loss=1.4741 | alloc=0.24GB reserv=26.28GB\n",
      "grad W_out: 0.38560518622398376\n",
      "grad W: 4.51317024230957\n",
      "grad W_in: 1.4732303619384766\n",
      "grad W_out: 0.5365810990333557\n",
      "grad W: 5.0147294998168945\n",
      "grad W_in: 1.288037657737732\n",
      "[ep 4 | batch 52] loss=1.4761 |M| |W| mean=1.999e-02\n",
      "ep 4 batch 52 | loss=1.4761 | alloc=0.24GB reserv=26.28GB\n",
      "grad W_out: 0.6519754528999329\n",
      "grad W: 3.883039712905884\n",
      "grad W_in: 2.1453988552093506\n",
      "grad W_out: 0.41523048281669617\n",
      "grad W: 2.7580981254577637\n",
      "grad W_in: 0.8583233952522278\n",
      "[ep 4 | batch 54] loss=1.4884 |M| |W| mean=2.000e-02\n",
      "ep 4 batch 54 | loss=1.4884 | alloc=0.25GB reserv=26.29GB\n",
      "grad W_out: 0.40891045331954956\n",
      "grad W: 3.190098524093628\n",
      "grad W_in: 1.1192160844802856\n",
      "grad W_out: 0.555223286151886\n",
      "grad W: 4.232626438140869\n",
      "grad W_in: 2.0855259895324707\n",
      "[ep 4 | batch 56] loss=1.4930 |M| |W| mean=2.002e-02\n",
      "ep 4 batch 56 | loss=1.4930 | alloc=0.25GB reserv=26.29GB\n",
      "grad W_out: 0.44998493790626526\n",
      "grad W: 2.671189308166504\n",
      "grad W_in: 1.5992074012756348\n",
      "grad W_out: 0.4612002968788147\n",
      "grad W: 2.8464503288269043\n",
      "grad W_in: 1.1708574295043945\n",
      "[ep 4 | batch 58] loss=1.4532 |M| |W| mean=2.004e-02\n",
      "ep 4 batch 58 | loss=1.4532 | alloc=0.25GB reserv=26.29GB\n",
      "grad W_out: 0.2993104159832001\n",
      "grad W: 2.394263505935669\n",
      "grad W_in: 1.218591332435608\n",
      "grad W_out: 0.5078214406967163\n",
      "grad W: 3.942201852798462\n",
      "grad W_in: 0.811103105545044\n",
      "[ep 4 | batch 60] loss=1.4690 |M| |W| mean=2.006e-02\n",
      "ep 4 batch 60 | loss=1.4690 | alloc=0.25GB reserv=26.29GB\n",
      "grad W_out: 0.5135858058929443\n",
      "grad W: 3.777451992034912\n",
      "grad W_in: 0.6880349516868591\n",
      "grad W_out: 0.32241562008857727\n",
      "grad W: 2.228257656097412\n",
      "grad W_in: 1.1357587575912476\n",
      "[ep 4 | batch 62] loss=1.4117 |M| |W| mean=2.007e-02\n",
      "ep 4 batch 62 | loss=1.4117 | alloc=0.25GB reserv=26.29GB\n",
      "grad W_out: 0.4376819133758545\n",
      "grad W: 3.6812734603881836\n",
      "grad W_in: 1.2431529760360718\n",
      "grad W_out: 0.37533408403396606\n",
      "grad W: 3.4087204933166504\n",
      "grad W_in: 0.7650296092033386\n",
      "[ep 4 | batch 64] loss=1.3957 |M| |W| mean=2.009e-02\n",
      "ep 4 batch 64 | loss=1.3957 | alloc=0.25GB reserv=26.29GB\n",
      "grad W_out: 0.5005882382392883\n",
      "grad W: 3.1462132930755615\n",
      "grad W_in: 0.6258497834205627\n",
      "grad W_out: 0.3874163329601288\n",
      "grad W: 4.016982078552246\n",
      "grad W_in: 1.3241722583770752\n",
      "[ep 4 | batch 66] loss=1.4226 |M| |W| mean=2.011e-02\n",
      "ep 4 batch 66 | loss=1.4226 | alloc=0.25GB reserv=26.30GB\n",
      "grad W_out: 0.4262964725494385\n",
      "grad W: 3.8644845485687256\n",
      "grad W_in: 1.3203622102737427\n",
      "grad W_out: 0.46839064359664917\n",
      "grad W: 4.688704967498779\n",
      "grad W_in: 0.8298697471618652\n",
      "[ep 4 | batch 68] loss=1.4625 |M| |W| mean=2.013e-02\n",
      "ep 4 batch 68 | loss=1.4625 | alloc=0.26GB reserv=26.30GB\n",
      "grad W_out: 0.521052360534668\n",
      "grad W: 2.0192129611968994\n",
      "grad W_in: 0.37222787737846375\n",
      "grad W_out: 0.4931493401527405\n",
      "grad W: 4.257978916168213\n",
      "grad W_in: 1.3475662469863892\n",
      "[ep 4 | batch 70] loss=1.3688 |M| |W| mean=2.014e-02\n",
      "ep 4 batch 70 | loss=1.3688 | alloc=0.26GB reserv=26.30GB\n",
      "grad W_out: 0.4158118963241577\n",
      "grad W: 4.086062908172607\n",
      "grad W_in: 0.7662088871002197\n",
      "grad W_out: 0.9440239071846008\n",
      "grad W: 7.765587329864502\n",
      "grad W_in: 1.10704505443573\n",
      "[ep 4 | batch 72] loss=1.4830 |M| |W| mean=2.016e-02\n",
      "ep 4 batch 72 | loss=1.4830 | alloc=0.26GB reserv=26.30GB\n",
      "grad W_out: 0.565025269985199\n",
      "grad W: 1.9776442050933838\n",
      "grad W_in: 0.7499809265136719\n",
      "grad W_out: 0.35754647850990295\n",
      "grad W: 2.8366827964782715\n",
      "grad W_in: 1.4598217010498047\n",
      "[ep 4 | batch 74] loss=1.4296 |M| |W| mean=2.017e-02\n",
      "ep 4 batch 74 | loss=1.4296 | alloc=0.26GB reserv=26.30GB\n",
      "grad W_out: 0.42874380946159363\n",
      "grad W: 4.198542594909668\n",
      "grad W_in: 0.9158813953399658\n",
      "grad W_out: 0.4826469421386719\n",
      "grad W: 3.155271291732788\n",
      "grad W_in: 1.1843957901000977\n",
      "[ep 4 | batch 76] loss=1.4877 |M| |W| mean=2.019e-02\n",
      "ep 4 batch 76 | loss=1.4877 | alloc=0.26GB reserv=26.31GB\n",
      "grad W_out: 0.3969261050224304\n",
      "grad W: 3.444658041000366\n",
      "grad W_in: 0.591221034526825\n",
      "grad W_out: 0.26231661438941956\n",
      "grad W: 3.4822471141815186\n",
      "grad W_in: 1.1454533338546753\n",
      "[ep 4 | batch 78] loss=1.4182 |M| |W| mean=2.021e-02\n",
      "ep 4 batch 78 | loss=1.4182 | alloc=0.26GB reserv=26.31GB\n",
      "grad W_out: 0.42622798681259155\n",
      "grad W: 2.3865437507629395\n",
      "grad W_in: 0.4246951937675476\n",
      "grad W_out: 0.419660359621048\n",
      "grad W: 2.1487176418304443\n",
      "grad W_in: 0.26755204796791077\n",
      "[ep 4 | batch 80] loss=1.4228 |M| |W| mean=2.023e-02\n",
      "ep 4 batch 80 | loss=1.4228 | alloc=0.26GB reserv=26.31GB\n",
      "grad W_out: 0.568411648273468\n",
      "grad W: 3.318193197250366\n",
      "grad W_in: 0.5912912487983704\n",
      "grad W_out: 0.5617533326148987\n",
      "grad W: 4.344428539276123\n",
      "grad W_in: 3.5597503185272217\n",
      "[ep 4 | batch 82] loss=1.3531 |M| |W| mean=2.025e-02\n",
      "ep 4 batch 82 | loss=1.3531 | alloc=0.27GB reserv=26.31GB\n",
      "grad W_out: 0.4218580424785614\n",
      "grad W: 2.7946207523345947\n",
      "grad W_in: 1.804560661315918\n",
      "grad W_out: 0.47704336047172546\n",
      "grad W: 4.330167770385742\n",
      "grad W_in: 3.312617540359497\n",
      "[ep 4 | batch 84] loss=1.3201 |M| |W| mean=2.026e-02\n",
      "ep 4 batch 84 | loss=1.3201 | alloc=0.27GB reserv=26.31GB\n",
      "grad W_out: 0.37582266330718994\n",
      "grad W: 3.5236411094665527\n",
      "grad W_in: 2.063014268875122\n",
      "grad W_out: 0.2749561369419098\n",
      "grad W: 1.8601349592208862\n",
      "grad W_in: 2.0845553874969482\n",
      "[ep 4 | batch 86] loss=1.3433 |M| |W| mean=2.027e-02\n",
      "ep 4 batch 86 | loss=1.3433 | alloc=0.27GB reserv=26.32GB\n",
      "grad W_out: 0.5729615092277527\n",
      "grad W: 7.980696678161621\n",
      "grad W_in: 4.30346155166626\n",
      "grad W_out: 0.5786566138267517\n",
      "grad W: 4.640967845916748\n",
      "grad W_in: 4.31662654876709\n",
      "[ep 4 | batch 88] loss=1.4447 |M| |W| mean=2.028e-02\n",
      "ep 4 batch 88 | loss=1.4447 | alloc=0.27GB reserv=26.32GB\n",
      "grad W_out: 0.6329383254051208\n",
      "grad W: 5.331275939941406\n",
      "grad W_in: 5.071568965911865\n",
      "grad W_out: 0.2694597542285919\n",
      "grad W: 2.5960872173309326\n",
      "grad W_in: 1.848503828048706\n",
      "[ep 4 | batch 90] loss=1.3765 |M| |W| mean=2.030e-02\n",
      "ep 4 batch 90 | loss=1.3765 | alloc=0.27GB reserv=26.32GB\n",
      "grad W_out: 0.5694469809532166\n",
      "grad W: 23.328964233398438\n",
      "grad W_in: 5.065794467926025\n",
      "grad W_out: 0.4508579969406128\n",
      "grad W: 3.1357040405273438\n",
      "grad W_in: 2.959437847137451\n",
      "[ep 4 | batch 92] loss=1.3601 |M| |W| mean=2.031e-02\n",
      "ep 4 batch 92 | loss=1.3601 | alloc=0.27GB reserv=26.32GB\n",
      "grad W_out: 0.6190013289451599\n",
      "grad W: 2.99090576171875\n",
      "grad W_in: 1.5463696718215942\n",
      "Epoch 04/20 | lr=1.00e-03 | loss=1.4434 | test_acc=52.77% | dt=1175.6s\n",
      "grad W_out: 0.6208117604255676\n",
      "grad W: 4.338308811187744\n",
      "grad W_in: 1.7559298276901245\n",
      "[ep 5 | batch 0] loss=1.4285 |M| |W| mean=2.033e-02\n",
      "ep 5 batch 0 | loss=1.4285 | alloc=0.27GB reserv=26.32GB\n",
      "grad W_out: 0.6056695580482483\n",
      "grad W: 2.364410877227783\n",
      "grad W_in: 0.5709142684936523\n",
      "grad W_out: 0.452759325504303\n",
      "grad W: 3.538745641708374\n",
      "grad W_in: 2.152454137802124\n",
      "[ep 5 | batch 2] loss=1.3828 |M| |W| mean=2.035e-02\n",
      "ep 5 batch 2 | loss=1.3828 | alloc=0.28GB reserv=26.33GB\n",
      "grad W_out: 0.3722805976867676\n",
      "grad W: 3.069227695465088\n",
      "grad W_in: 1.3861942291259766\n",
      "grad W_out: 0.26250335574150085\n",
      "grad W: 6.736090183258057\n",
      "grad W_in: 4.680903911590576\n",
      "[ep 5 | batch 4] loss=1.3352 |M| |W| mean=2.037e-02\n",
      "ep 5 batch 4 | loss=1.3352 | alloc=0.28GB reserv=26.33GB\n",
      "grad W_out: 0.39666205644607544\n",
      "grad W: 2.618741512298584\n",
      "grad W_in: 1.6481667757034302\n",
      "grad W_out: 0.6500639319419861\n",
      "grad W: 3.450977087020874\n",
      "grad W_in: 3.1309938430786133\n",
      "[ep 5 | batch 6] loss=1.3991 |M| |W| mean=2.039e-02\n",
      "ep 5 batch 6 | loss=1.3991 | alloc=0.28GB reserv=26.33GB\n",
      "grad W_out: 0.7271479368209839\n",
      "grad W: 3.263484477996826\n",
      "grad W_in: 3.032052516937256\n",
      "grad W_out: 0.4440602958202362\n",
      "grad W: 3.8158066272735596\n",
      "grad W_in: 1.142685890197754\n",
      "[ep 5 | batch 8] loss=1.4100 |M| |W| mean=2.042e-02\n",
      "ep 5 batch 8 | loss=1.4100 | alloc=0.28GB reserv=26.33GB\n",
      "grad W_out: 0.5445461273193359\n",
      "grad W: 4.304086208343506\n",
      "grad W_in: 2.595796585083008\n",
      "grad W_out: 0.5134419798851013\n",
      "grad W: 3.9553887844085693\n",
      "grad W_in: 1.3393689393997192\n",
      "[ep 5 | batch 10] loss=1.3735 |M| |W| mean=2.043e-02\n",
      "ep 5 batch 10 | loss=1.3735 | alloc=0.28GB reserv=26.33GB\n",
      "grad W_out: 0.4889882504940033\n",
      "grad W: 2.6055824756622314\n",
      "grad W_in: 1.8482413291931152\n",
      "grad W_out: 0.3878951072692871\n",
      "grad W: 2.2233927249908447\n",
      "grad W_in: 0.4680054485797882\n",
      "[ep 5 | batch 12] loss=1.3506 |M| |W| mean=2.044e-02\n",
      "ep 5 batch 12 | loss=1.3506 | alloc=0.28GB reserv=26.34GB\n",
      "grad W_out: 0.39611780643463135\n",
      "grad W: 2.80920672416687\n",
      "grad W_in: 1.4499589204788208\n",
      "grad W_out: 0.34253981709480286\n",
      "grad W: 3.9100193977355957\n",
      "grad W_in: 2.5445291996002197\n",
      "[ep 5 | batch 14] loss=1.3423 |M| |W| mean=2.046e-02\n",
      "ep 5 batch 14 | loss=1.3423 | alloc=0.28GB reserv=26.34GB\n",
      "grad W_out: 0.5234901309013367\n",
      "grad W: 6.600765228271484\n",
      "grad W_in: 3.7161571979522705\n",
      "grad W_out: 0.35379528999328613\n",
      "grad W: 2.427057981491089\n",
      "grad W_in: 1.1014297008514404\n",
      "[ep 5 | batch 16] loss=1.3277 |M| |W| mean=2.046e-02\n",
      "ep 5 batch 16 | loss=1.3277 | alloc=0.29GB reserv=26.34GB\n",
      "grad W_out: 0.47816362977027893\n",
      "grad W: 5.134721755981445\n",
      "grad W_in: 3.23922061920166\n",
      "grad W_out: 0.5370038151741028\n",
      "grad W: 6.461252689361572\n",
      "grad W_in: 3.987551212310791\n",
      "[ep 5 | batch 18] loss=1.3441 |M| |W| mean=2.048e-02\n",
      "ep 5 batch 18 | loss=1.3441 | alloc=0.29GB reserv=26.34GB\n",
      "grad W_out: 0.3687400817871094\n",
      "grad W: 3.4692556858062744\n",
      "grad W_in: 1.0000697374343872\n",
      "grad W_out: 0.34930771589279175\n",
      "grad W: 3.944284439086914\n",
      "grad W_in: 1.0480090379714966\n",
      "[ep 5 | batch 20] loss=1.2947 |M| |W| mean=2.049e-02\n",
      "ep 5 batch 20 | loss=1.2947 | alloc=0.29GB reserv=26.34GB\n",
      "grad W_out: 0.3473275899887085\n",
      "grad W: 3.6046974658966064\n",
      "grad W_in: 3.083977460861206\n",
      "grad W_out: 0.4256775975227356\n",
      "grad W: 3.4941658973693848\n",
      "grad W_in: 1.5296868085861206\n",
      "[ep 5 | batch 22] loss=1.3169 |M| |W| mean=2.050e-02\n",
      "ep 5 batch 22 | loss=1.3169 | alloc=0.29GB reserv=26.35GB\n",
      "grad W_out: 0.4596085548400879\n",
      "grad W: 3.6232457160949707\n",
      "grad W_in: 1.3365776538848877\n",
      "grad W_out: 0.4385297894477844\n",
      "grad W: 2.846590042114258\n",
      "grad W_in: 0.8383354544639587\n",
      "[ep 5 | batch 24] loss=1.2816 |M| |W| mean=2.051e-02\n",
      "ep 5 batch 24 | loss=1.2816 | alloc=0.29GB reserv=26.35GB\n",
      "grad W_out: 0.37482187151908875\n",
      "grad W: 2.5137743949890137\n",
      "grad W_in: 1.272000789642334\n",
      "grad W_out: 0.42540523409843445\n",
      "grad W: 3.2297751903533936\n",
      "grad W_in: 0.6469045281410217\n",
      "[ep 5 | batch 26] loss=1.2714 |M| |W| mean=2.052e-02\n",
      "ep 5 batch 26 | loss=1.2714 | alloc=0.29GB reserv=26.35GB\n",
      "grad W_out: 0.4282090663909912\n",
      "grad W: 3.6956634521484375\n",
      "grad W_in: 2.4570071697235107\n",
      "grad W_out: 0.2847156822681427\n",
      "grad W: 2.2892611026763916\n",
      "grad W_in: 0.8440853357315063\n",
      "[ep 5 | batch 28] loss=1.2734 |M| |W| mean=2.054e-02\n",
      "ep 5 batch 28 | loss=1.2734 | alloc=0.29GB reserv=26.35GB\n",
      "grad W_out: 0.5038327574729919\n",
      "grad W: 5.511646270751953\n",
      "grad W_in: 3.9829442501068115\n",
      "grad W_out: 0.38047194480895996\n",
      "grad W: 3.840271472930908\n",
      "grad W_in: 1.2467255592346191\n",
      "[ep 5 | batch 30] loss=1.2788 |M| |W| mean=2.055e-02\n",
      "ep 5 batch 30 | loss=1.2788 | alloc=0.30GB reserv=26.35GB\n",
      "grad W_out: 0.46425655484199524\n",
      "grad W: 7.381564140319824\n",
      "grad W_in: 2.6636335849761963\n",
      "grad W_out: 0.39297929406166077\n",
      "grad W: 2.709237813949585\n",
      "grad W_in: 0.9044934511184692\n",
      "[ep 5 | batch 32] loss=1.2881 |M| |W| mean=2.056e-02\n",
      "ep 5 batch 32 | loss=1.2881 | alloc=0.30GB reserv=26.36GB\n",
      "grad W_out: 0.3309404253959656\n",
      "grad W: 4.011776447296143\n",
      "grad W_in: 1.4596240520477295\n",
      "grad W_out: 0.4242977201938629\n",
      "grad W: 5.2938337326049805\n",
      "grad W_in: 1.9375663995742798\n",
      "[ep 5 | batch 34] loss=1.2847 |M| |W| mean=2.056e-02\n",
      "ep 5 batch 34 | loss=1.2847 | alloc=0.30GB reserv=26.36GB\n",
      "grad W_out: 0.3401365280151367\n",
      "grad W: 4.726190567016602\n",
      "grad W_in: 1.657428503036499\n",
      "grad W_out: 0.4577878415584564\n",
      "grad W: 5.215586185455322\n",
      "grad W_in: 2.589961290359497\n",
      "[ep 5 | batch 36] loss=1.3548 |M| |W| mean=2.057e-02\n",
      "ep 5 batch 36 | loss=1.3548 | alloc=0.30GB reserv=26.36GB\n",
      "grad W_out: 0.4156496524810791\n",
      "grad W: 3.0583412647247314\n",
      "grad W_in: 1.3093467950820923\n",
      "grad W_out: 0.4966151714324951\n",
      "grad W: 5.111539840698242\n",
      "grad W_in: 3.5956969261169434\n",
      "[ep 5 | batch 38] loss=1.3653 |M| |W| mean=2.059e-02\n",
      "ep 5 batch 38 | loss=1.3653 | alloc=0.30GB reserv=26.36GB\n",
      "grad W_out: 0.33654677867889404\n",
      "grad W: 3.853567600250244\n",
      "grad W_in: 0.95665442943573\n",
      "grad W_out: 0.3238559067249298\n",
      "grad W: 3.099527597427368\n",
      "grad W_in: 1.3611853122711182\n",
      "[ep 5 | batch 40] loss=1.2684 |M| |W| mean=2.061e-02\n",
      "ep 5 batch 40 | loss=1.2684 | alloc=0.30GB reserv=26.36GB\n",
      "grad W_out: 0.4298035800457001\n",
      "grad W: 5.011122226715088\n",
      "grad W_in: 2.3774664402008057\n",
      "grad W_out: 0.4291670024394989\n",
      "grad W: 2.7187716960906982\n",
      "grad W_in: 0.6390378475189209\n",
      "[ep 5 | batch 42] loss=1.2700 |M| |W| mean=2.062e-02\n",
      "ep 5 batch 42 | loss=1.2700 | alloc=0.30GB reserv=26.37GB\n",
      "grad W_out: 0.40600281953811646\n",
      "grad W: 2.9523892402648926\n",
      "grad W_in: 1.4999476671218872\n",
      "grad W_out: 0.398065447807312\n",
      "grad W: 4.303142070770264\n",
      "grad W_in: 2.8271095752716064\n",
      "[ep 5 | batch 44] loss=1.2283 |M| |W| mean=2.064e-02\n",
      "ep 5 batch 44 | loss=1.2283 | alloc=0.31GB reserv=26.37GB\n",
      "grad W_out: 0.3566301763057709\n",
      "grad W: 3.022312641143799\n",
      "grad W_in: 0.45473307371139526\n",
      "grad W_out: 0.4144226312637329\n",
      "grad W: 4.688394546508789\n",
      "grad W_in: 3.749898672103882\n",
      "[ep 5 | batch 46] loss=1.3298 |M| |W| mean=2.065e-02\n",
      "ep 5 batch 46 | loss=1.3298 | alloc=0.31GB reserv=26.37GB\n",
      "grad W_out: 0.3155278265476227\n",
      "grad W: 3.706176519393921\n",
      "grad W_in: 2.7850921154022217\n",
      "grad W_out: 0.26246321201324463\n",
      "grad W: 5.754086494445801\n",
      "grad W_in: 3.1652135848999023\n",
      "[ep 5 | batch 48] loss=1.2485 |M| |W| mean=2.066e-02\n",
      "ep 5 batch 48 | loss=1.2485 | alloc=0.31GB reserv=26.37GB\n",
      "grad W_out: 0.4336555004119873\n",
      "grad W: 6.085398197174072\n",
      "grad W_in: 1.8949531316757202\n",
      "grad W_out: 0.3698294162750244\n",
      "grad W: 4.120869159698486\n",
      "grad W_in: 1.5700602531433105\n",
      "[ep 5 | batch 50] loss=1.2129 |M| |W| mean=2.067e-02\n",
      "ep 5 batch 50 | loss=1.2129 | alloc=0.31GB reserv=26.37GB\n",
      "grad W_out: 0.34623685479164124\n",
      "grad W: 5.166022777557373\n",
      "grad W_in: 2.815091848373413\n",
      "grad W_out: 0.2965700626373291\n",
      "grad W: 4.334026336669922\n",
      "grad W_in: 1.836039662361145\n",
      "[ep 5 | batch 52] loss=1.1434 |M| |W| mean=2.067e-02\n",
      "ep 5 batch 52 | loss=1.1434 | alloc=0.31GB reserv=26.38GB\n",
      "grad W_out: 0.3789556324481964\n",
      "grad W: 5.175139427185059\n",
      "grad W_in: 1.792318344116211\n",
      "grad W_out: 0.6658247709274292\n",
      "grad W: 7.126323223114014\n",
      "grad W_in: 3.6345362663269043\n",
      "[ep 5 | batch 54] loss=1.2832 |M| |W| mean=2.068e-02\n",
      "ep 5 batch 54 | loss=1.2832 | alloc=0.31GB reserv=26.38GB\n",
      "grad W_out: 0.5319984555244446\n",
      "grad W: 7.185009002685547\n",
      "grad W_in: 0.9815235733985901\n",
      "grad W_out: 0.5342347025871277\n",
      "grad W: 5.427413463592529\n",
      "grad W_in: 3.3823492527008057\n",
      "[ep 5 | batch 56] loss=1.3072 |M| |W| mean=2.068e-02\n",
      "ep 5 batch 56 | loss=1.3072 | alloc=0.31GB reserv=26.38GB\n",
      "grad W_out: 0.2701239287853241\n",
      "grad W: 2.88859486579895\n",
      "grad W_in: 0.5402247309684753\n",
      "grad W_out: 0.35164791345596313\n",
      "grad W: 5.346474647521973\n",
      "grad W_in: 1.3325597047805786\n",
      "[ep 5 | batch 58] loss=1.3142 |M| |W| mean=2.068e-02\n",
      "ep 5 batch 58 | loss=1.3142 | alloc=0.32GB reserv=26.38GB\n",
      "grad W_out: 0.37041011452674866\n",
      "grad W: 3.536144971847534\n",
      "grad W_in: 1.2093161344528198\n",
      "grad W_out: 0.47179722785949707\n",
      "grad W: 4.439996242523193\n",
      "grad W_in: 2.353224277496338\n",
      "[ep 5 | batch 60] loss=1.3090 |M| |W| mean=2.068e-02\n",
      "ep 5 batch 60 | loss=1.3090 | alloc=0.32GB reserv=26.38GB\n",
      "grad W_out: 0.3804275691509247\n",
      "grad W: 4.417418003082275\n",
      "grad W_in: 2.179013729095459\n",
      "grad W_out: 0.38584786653518677\n",
      "grad W: 6.299255847930908\n",
      "grad W_in: 1.0741560459136963\n",
      "[ep 5 | batch 62] loss=1.3487 |M| |W| mean=2.069e-02\n",
      "ep 5 batch 62 | loss=1.3487 | alloc=0.32GB reserv=26.38GB\n",
      "grad W_out: 0.37074726819992065\n",
      "grad W: 75.01448059082031\n",
      "grad W_in: 27.179941177368164\n",
      "grad W_out: 1.0709823369979858\n",
      "grad W: 9.97946834564209\n",
      "grad W_in: 7.893798351287842\n",
      "[ep 5 | batch 64] loss=1.5434 |M| |W| mean=2.068e-02\n",
      "ep 5 batch 64 | loss=1.5434 | alloc=0.32GB reserv=26.39GB\n",
      "grad W_out: 1.3496116399765015\n",
      "grad W: 12.048457145690918\n",
      "grad W_in: 9.377823829650879\n",
      "grad W_out: 1.1430555582046509\n",
      "grad W: 9.758222579956055\n",
      "grad W_in: 3.566969871520996\n",
      "[ep 5 | batch 66] loss=1.5421 |M| |W| mean=2.067e-02\n",
      "ep 5 batch 66 | loss=1.5421 | alloc=0.32GB reserv=26.39GB\n",
      "grad W_out: 0.669238269329071\n",
      "grad W: 89.32142639160156\n",
      "grad W_in: 15.180788040161133\n",
      "grad W_out: 0.9034565687179565\n",
      "grad W: 11.589897155761719\n",
      "grad W_in: 4.117538928985596\n",
      "[ep 5 | batch 68] loss=1.6154 |M| |W| mean=2.069e-02\n",
      "ep 5 batch 68 | loss=1.6154 | alloc=0.32GB reserv=26.39GB\n",
      "grad W_out: 0.8146130442619324\n",
      "grad W: 9.753425598144531\n",
      "grad W_in: 2.7421605587005615\n",
      "grad W_out: 0.6522335410118103\n",
      "grad W: 4.881369113922119\n",
      "grad W_in: 2.001101493835449\n",
      "[ep 5 | batch 70] loss=1.5715 |M| |W| mean=2.074e-02\n",
      "ep 5 batch 70 | loss=1.5715 | alloc=0.32GB reserv=26.39GB\n",
      "grad W_out: 0.7406044602394104\n",
      "grad W: 9.458642959594727\n",
      "grad W_in: 3.180370807647705\n",
      "grad W_out: 0.7597730755805969\n",
      "grad W: 6.977550506591797\n",
      "grad W_in: 2.24469256401062\n",
      "[ep 5 | batch 72] loss=1.5566 |M| |W| mean=2.079e-02\n",
      "ep 5 batch 72 | loss=1.5566 | alloc=0.33GB reserv=26.39GB\n",
      "grad W_out: 0.6286599636077881\n",
      "grad W: 3.6529741287231445\n",
      "grad W_in: 0.9533108472824097\n",
      "grad W_out: 0.7908644080162048\n",
      "grad W: 9.765867233276367\n",
      "grad W_in: 2.7551262378692627\n",
      "[ep 5 | batch 74] loss=1.5407 |M| |W| mean=2.085e-02\n",
      "ep 5 batch 74 | loss=1.5407 | alloc=0.33GB reserv=26.40GB\n",
      "grad W_out: 0.49641868472099304\n",
      "grad W: 5.612043380737305\n",
      "grad W_in: 1.1839016675949097\n",
      "grad W_out: 0.41027793288230896\n",
      "grad W: 3.245755195617676\n",
      "grad W_in: 0.787206768989563\n",
      "[ep 5 | batch 76] loss=1.4696 |M| |W| mean=2.091e-02\n",
      "ep 5 batch 76 | loss=1.4696 | alloc=0.33GB reserv=26.40GB\n",
      "grad W_out: 0.43025439977645874\n",
      "grad W: 3.794469118118286\n",
      "grad W_in: 1.399965524673462\n",
      "grad W_out: 0.5932431817054749\n",
      "grad W: 6.186361312866211\n",
      "grad W_in: 2.4213130474090576\n",
      "[ep 5 | batch 78] loss=1.5336 |M| |W| mean=2.096e-02\n",
      "ep 5 batch 78 | loss=1.5336 | alloc=0.33GB reserv=26.40GB\n",
      "grad W_out: 0.5299269556999207\n",
      "grad W: 3.930665969848633\n",
      "grad W_in: 1.5788185596466064\n",
      "grad W_out: 0.4632774591445923\n",
      "grad W: 3.552560567855835\n",
      "grad W_in: 1.2537636756896973\n",
      "[ep 5 | batch 80] loss=1.4365 |M| |W| mean=2.101e-02\n",
      "ep 5 batch 80 | loss=1.4365 | alloc=0.33GB reserv=26.40GB\n",
      "grad W_out: 0.5350065231323242\n",
      "grad W: 5.00614595413208\n",
      "grad W_in: 2.042912483215332\n",
      "grad W_out: 0.38205820322036743\n",
      "grad W: 3.617918014526367\n",
      "grad W_in: 1.4338785409927368\n",
      "[ep 5 | batch 82] loss=1.3821 |M| |W| mean=2.106e-02\n",
      "ep 5 batch 82 | loss=1.3821 | alloc=0.33GB reserv=26.40GB\n",
      "grad W_out: 0.4174080193042755\n",
      "grad W: 3.8365278244018555\n",
      "grad W_in: 1.1393448114395142\n",
      "grad W_out: 0.4007236063480377\n",
      "grad W: 3.6633048057556152\n",
      "grad W_in: 1.1064149141311646\n",
      "[ep 5 | batch 84] loss=1.4146 |M| |W| mean=2.110e-02\n",
      "ep 5 batch 84 | loss=1.4146 | alloc=0.33GB reserv=26.41GB\n",
      "grad W_out: 0.3895023763179779\n",
      "grad W: 2.177769899368286\n",
      "grad W_in: 0.3634617030620575\n",
      "grad W_out: 0.4091350734233856\n",
      "grad W: 2.5710489749908447\n",
      "grad W_in: 1.4544098377227783\n",
      "[ep 5 | batch 86] loss=1.3935 |M| |W| mean=2.113e-02\n",
      "ep 5 batch 86 | loss=1.3935 | alloc=0.34GB reserv=26.41GB\n",
      "grad W_out: 0.4043395221233368\n",
      "grad W: 4.287245273590088\n",
      "grad W_in: 1.4889390468597412\n",
      "grad W_out: 0.5516608357429504\n",
      "grad W: 3.492539882659912\n",
      "grad W_in: 0.8775848746299744\n",
      "[ep 5 | batch 88] loss=1.3712 |M| |W| mean=2.116e-02\n",
      "ep 5 batch 88 | loss=1.3712 | alloc=0.34GB reserv=26.41GB\n",
      "grad W_out: 0.499482661485672\n",
      "grad W: 3.683689832687378\n",
      "grad W_in: 0.7387404441833496\n",
      "grad W_out: 0.5999438762664795\n",
      "grad W: 5.562612056732178\n",
      "grad W_in: 1.8535069227218628\n",
      "[ep 5 | batch 90] loss=1.4517 |M| |W| mean=2.119e-02\n",
      "ep 5 batch 90 | loss=1.4517 | alloc=0.34GB reserv=26.41GB\n",
      "grad W_out: 0.5305022597312927\n",
      "grad W: 3.7873923778533936\n",
      "grad W_in: 0.9958877563476562\n",
      "grad W_out: 0.2597074806690216\n",
      "grad W: 2.662792444229126\n",
      "grad W_in: 0.6990616917610168\n",
      "[ep 5 | batch 92] loss=1.3475 |M| |W| mean=2.122e-02\n",
      "ep 5 batch 92 | loss=1.3475 | alloc=0.34GB reserv=26.41GB\n",
      "grad W_out: 0.34415245056152344\n",
      "grad W: 3.132819175720215\n",
      "grad W_in: 0.9582193493843079\n",
      "Epoch 05/20 | lr=1.00e-03 | loss=1.3658 | test_acc=55.98% | dt=1175.8s\n",
      "grad W_out: 0.37250906229019165\n",
      "grad W: 3.492269992828369\n",
      "grad W_in: 1.1658787727355957\n",
      "[ep 6 | batch 0] loss=1.4141 |M| |W| mean=2.125e-02\n",
      "ep 6 batch 0 | loss=1.4141 | alloc=0.34GB reserv=26.42GB\n",
      "grad W_out: 0.306302547454834\n",
      "grad W: 2.2860522270202637\n",
      "grad W_in: 0.8153921961784363\n",
      "grad W_out: 0.2787344455718994\n",
      "grad W: 2.233731746673584\n",
      "grad W_in: 1.004927158355713\n",
      "[ep 6 | batch 2] loss=1.3136 |M| |W| mean=2.127e-02\n",
      "ep 6 batch 2 | loss=1.3136 | alloc=0.34GB reserv=26.42GB\n",
      "grad W_out: 0.39501670002937317\n",
      "grad W: 5.941659927368164\n",
      "grad W_in: 2.1100845336914062\n",
      "grad W_out: 0.45948535203933716\n",
      "grad W: 4.402219295501709\n",
      "grad W_in: 1.2023760080337524\n",
      "[ep 6 | batch 4] loss=1.3299 |M| |W| mean=2.129e-02\n",
      "ep 6 batch 4 | loss=1.3299 | alloc=0.34GB reserv=26.42GB\n",
      "grad W_out: 0.3398616313934326\n",
      "grad W: 3.2881217002868652\n",
      "grad W_in: 0.6673958897590637\n",
      "grad W_out: 0.4235116243362427\n",
      "grad W: 4.670229434967041\n",
      "grad W_in: 2.0130224227905273\n",
      "[ep 6 | batch 6] loss=1.3637 |M| |W| mean=2.131e-02\n",
      "ep 6 batch 6 | loss=1.3637 | alloc=0.35GB reserv=26.42GB\n",
      "grad W_out: 0.49065515398979187\n",
      "grad W: 4.772747993469238\n",
      "grad W_in: 1.8086556196212769\n",
      "grad W_out: 0.36376360058784485\n",
      "grad W: 2.363938808441162\n",
      "grad W_in: 0.4645402729511261\n",
      "[ep 6 | batch 8] loss=1.3324 |M| |W| mean=2.134e-02\n",
      "ep 6 batch 8 | loss=1.3324 | alloc=0.35GB reserv=26.42GB\n",
      "grad W_out: 0.4398201107978821\n",
      "grad W: 5.808403491973877\n",
      "grad W_in: 2.7964911460876465\n",
      "grad W_out: 0.3976770341396332\n",
      "grad W: 5.015119552612305\n",
      "grad W_in: 2.374758720397949\n",
      "[ep 6 | batch 10] loss=1.2476 |M| |W| mean=2.137e-02\n",
      "ep 6 batch 10 | loss=1.2476 | alloc=0.35GB reserv=26.43GB\n",
      "grad W_out: 0.297381728887558\n",
      "grad W: 2.5313875675201416\n",
      "grad W_in: 0.7860807776451111\n",
      "grad W_out: 0.30570921301841736\n",
      "grad W: 2.7950124740600586\n",
      "grad W_in: 0.7596150636672974\n",
      "[ep 6 | batch 12] loss=1.2361 |M| |W| mean=2.139e-02\n",
      "ep 6 batch 12 | loss=1.2361 | alloc=0.35GB reserv=26.43GB\n",
      "grad W_out: 0.45297613739967346\n",
      "grad W: 5.0181121826171875\n",
      "grad W_in: 1.4805008172988892\n",
      "grad W_out: 0.2769627571105957\n",
      "grad W: 1.7473247051239014\n",
      "grad W_in: 0.3482266664505005\n",
      "[ep 6 | batch 14] loss=1.2509 |M| |W| mean=2.141e-02\n",
      "ep 6 batch 14 | loss=1.2509 | alloc=0.35GB reserv=26.43GB\n",
      "grad W_out: 0.4168642461299896\n",
      "grad W: 4.405424118041992\n",
      "grad W_in: 1.7063825130462646\n",
      "grad W_out: 0.38757240772247314\n",
      "grad W: 2.873636245727539\n",
      "grad W_in: 1.5972347259521484\n",
      "[ep 6 | batch 16] loss=1.2378 |M| |W| mean=2.143e-02\n",
      "ep 6 batch 16 | loss=1.2378 | alloc=0.35GB reserv=26.43GB\n",
      "grad W_out: 0.31712770462036133\n",
      "grad W: 2.586225986480713\n",
      "grad W_in: 0.5530956983566284\n",
      "grad W_out: 0.2548160254955292\n",
      "grad W: 2.2890264987945557\n",
      "grad W_in: 0.8639394044876099\n",
      "[ep 6 | batch 18] loss=1.2338 |M| |W| mean=2.145e-02\n",
      "ep 6 batch 18 | loss=1.2338 | alloc=0.35GB reserv=26.43GB\n",
      "grad W_out: 0.27178090810775757\n",
      "grad W: 3.9622771739959717\n",
      "grad W_in: 1.6739486455917358\n",
      "grad W_out: 0.3111851215362549\n",
      "grad W: 3.0618245601654053\n",
      "grad W_in: 1.107321858406067\n",
      "[ep 6 | batch 20] loss=1.2306 |M| |W| mean=2.146e-02\n",
      "ep 6 batch 20 | loss=1.2306 | alloc=0.36GB reserv=26.44GB\n",
      "grad W_out: 0.27609631419181824\n",
      "grad W: 1.4823365211486816\n",
      "grad W_in: 0.4189913868904114\n",
      "grad W_out: 0.31774526834487915\n",
      "grad W: 4.638589859008789\n",
      "grad W_in: 2.3017642498016357\n",
      "[ep 6 | batch 22] loss=1.2689 |M| |W| mean=2.148e-02\n",
      "ep 6 batch 22 | loss=1.2689 | alloc=0.36GB reserv=26.44GB\n",
      "grad W_out: 0.2600734829902649\n",
      "grad W: 2.2680697441101074\n",
      "grad W_in: 0.9205420017242432\n",
      "grad W_out: 0.29888051748275757\n",
      "grad W: 2.252532482147217\n",
      "grad W_in: 0.9983065724372864\n",
      "[ep 6 | batch 24] loss=1.1891 |M| |W| mean=2.150e-02\n",
      "ep 6 batch 24 | loss=1.1891 | alloc=0.36GB reserv=26.44GB\n",
      "grad W_out: 0.2657598555088043\n",
      "grad W: 2.1432132720947266\n",
      "grad W_in: 1.3863199949264526\n",
      "grad W_out: 0.39201852679252625\n",
      "grad W: 3.8788347244262695\n",
      "grad W_in: 1.3830268383026123\n",
      "[ep 6 | batch 26] loss=1.2546 |M| |W| mean=2.152e-02\n",
      "ep 6 batch 26 | loss=1.2546 | alloc=0.36GB reserv=26.44GB\n",
      "grad W_out: 0.28397122025489807\n",
      "grad W: 3.593147039413452\n",
      "grad W_in: 1.2342560291290283\n",
      "grad W_out: 0.18236538767814636\n",
      "grad W: 1.828282356262207\n",
      "grad W_in: 0.7654231190681458\n",
      "[ep 6 | batch 28] loss=1.1511 |M| |W| mean=2.153e-02\n",
      "ep 6 batch 28 | loss=1.1511 | alloc=0.36GB reserv=26.44GB\n",
      "grad W_out: 0.4398100972175598\n",
      "grad W: 4.49476957321167\n",
      "grad W_in: 2.9612503051757812\n",
      "grad W_out: 0.2466733306646347\n",
      "grad W: 2.7855401039123535\n",
      "grad W_in: 0.6305975914001465\n",
      "[ep 6 | batch 30] loss=1.2600 |M| |W| mean=2.155e-02\n",
      "ep 6 batch 30 | loss=1.2600 | alloc=0.36GB reserv=26.45GB\n",
      "grad W_out: 0.2572365999221802\n",
      "grad W: 4.271696090698242\n",
      "grad W_in: 2.5936391353607178\n",
      "grad W_out: 0.4023085832595825\n",
      "grad W: 4.935271263122559\n",
      "grad W_in: 2.963038682937622\n",
      "[ep 6 | batch 32] loss=1.2000 |M| |W| mean=2.155e-02\n",
      "ep 6 batch 32 | loss=1.2000 | alloc=0.36GB reserv=26.45GB\n",
      "grad W_out: 0.487210750579834\n",
      "grad W: 7.659090518951416\n",
      "grad W_in: 4.0924882888793945\n",
      "grad W_out: 0.32797306776046753\n",
      "grad W: 2.9162628650665283\n",
      "grad W_in: 0.7900829911231995\n",
      "[ep 6 | batch 34] loss=1.1667 |M| |W| mean=2.156e-02\n",
      "ep 6 batch 34 | loss=1.1667 | alloc=0.37GB reserv=26.45GB\n",
      "grad W_out: 0.35364800691604614\n",
      "grad W: 4.257972717285156\n",
      "grad W_in: 1.530016541481018\n",
      "grad W_out: 0.34242311120033264\n",
      "grad W: 3.7777862548828125\n",
      "grad W_in: 1.2464543581008911\n",
      "[ep 6 | batch 36] loss=1.2237 |M| |W| mean=2.157e-02\n",
      "ep 6 batch 36 | loss=1.2237 | alloc=0.37GB reserv=26.45GB\n",
      "grad W_out: 0.3023480176925659\n",
      "grad W: 4.9124627113342285\n",
      "grad W_in: 2.0013062953948975\n",
      "grad W_out: 0.37179452180862427\n",
      "grad W: 5.165679931640625\n",
      "grad W_in: 1.688462257385254\n",
      "[ep 6 | batch 38] loss=1.1394 |M| |W| mean=2.158e-02\n",
      "ep 6 batch 38 | loss=1.1394 | alloc=0.37GB reserv=26.45GB\n",
      "grad W_out: 0.2245965152978897\n",
      "grad W: 2.7045466899871826\n",
      "grad W_in: 0.6711964011192322\n",
      "grad W_out: 0.25133568048477173\n",
      "grad W: 2.868234634399414\n",
      "grad W_in: 1.1978576183319092\n",
      "[ep 6 | batch 40] loss=1.2466 |M| |W| mean=2.160e-02\n",
      "ep 6 batch 40 | loss=1.2466 | alloc=0.37GB reserv=26.46GB\n",
      "grad W_out: 0.32811829447746277\n",
      "grad W: 2.449653387069702\n",
      "grad W_in: 1.0727872848510742\n",
      "grad W_out: 0.49715742468833923\n",
      "grad W: 2.872037887573242\n",
      "grad W_in: 1.1409083604812622\n",
      "[ep 6 | batch 42] loss=1.1854 |M| |W| mean=2.161e-02\n",
      "ep 6 batch 42 | loss=1.1854 | alloc=0.37GB reserv=26.46GB\n",
      "grad W_out: 0.473100483417511\n",
      "grad W: 5.338670253753662\n",
      "grad W_in: 3.5774903297424316\n",
      "grad W_out: 0.35863450169563293\n",
      "grad W: 6.063948631286621\n",
      "grad W_in: 2.6849732398986816\n",
      "[ep 6 | batch 44] loss=1.2562 |M| |W| mean=2.162e-02\n",
      "ep 6 batch 44 | loss=1.2562 | alloc=0.37GB reserv=26.46GB\n",
      "grad W_out: 0.32678356766700745\n",
      "grad W: 6.660426139831543\n",
      "grad W_in: 2.125967264175415\n",
      "grad W_out: 0.4289182424545288\n",
      "grad W: 6.0731282234191895\n",
      "grad W_in: 2.4807448387145996\n",
      "[ep 6 | batch 46] loss=1.2595 |M| |W| mean=2.164e-02\n",
      "ep 6 batch 46 | loss=1.2595 | alloc=0.37GB reserv=26.46GB\n",
      "grad W_out: 0.4174436032772064\n",
      "grad W: 5.439499378204346\n",
      "grad W_in: 2.7953381538391113\n",
      "grad W_out: 0.4614712595939636\n",
      "grad W: 4.000899791717529\n",
      "grad W_in: 1.5113445520401\n",
      "[ep 6 | batch 48] loss=1.2334 |M| |W| mean=2.164e-02\n",
      "ep 6 batch 48 | loss=1.2334 | alloc=0.38GB reserv=26.46GB\n",
      "grad W_out: 0.5192233920097351\n",
      "grad W: 5.2291669845581055\n",
      "grad W_in: 2.5529165267944336\n",
      "grad W_out: 0.3361624479293823\n",
      "grad W: 2.909163236618042\n",
      "grad W_in: 0.6723059415817261\n",
      "[ep 6 | batch 50] loss=1.1817 |M| |W| mean=2.166e-02\n",
      "ep 6 batch 50 | loss=1.1817 | alloc=0.38GB reserv=26.46GB\n",
      "grad W_out: 0.4864484369754791\n",
      "grad W: 4.726894378662109\n",
      "grad W_in: 1.9266124963760376\n",
      "grad W_out: 0.2681720554828644\n",
      "grad W: 2.665796995162964\n",
      "grad W_in: 1.241265058517456\n",
      "[ep 6 | batch 52] loss=1.1452 |M| |W| mean=2.167e-02\n",
      "ep 6 batch 52 | loss=1.1452 | alloc=0.38GB reserv=26.47GB\n",
      "grad W_out: 0.35791197419166565\n",
      "grad W: 4.130721569061279\n",
      "grad W_in: 1.6788852214813232\n",
      "grad W_out: 0.2384125292301178\n",
      "grad W: 22.020322799682617\n",
      "grad W_in: 4.120450496673584\n",
      "[ep 6 | batch 54] loss=1.1424 |M| |W| mean=2.167e-02\n",
      "ep 6 batch 54 | loss=1.1424 | alloc=0.38GB reserv=26.47GB\n",
      "grad W_out: 0.40779832005500793\n",
      "grad W: 4.870433330535889\n",
      "grad W_in: 1.8895381689071655\n",
      "grad W_out: 0.37804511189460754\n",
      "grad W: 3.488668918609619\n",
      "grad W_in: 1.9240680932998657\n",
      "[ep 6 | batch 56] loss=1.1596 |M| |W| mean=2.168e-02\n",
      "ep 6 batch 56 | loss=1.1596 | alloc=0.38GB reserv=26.47GB\n",
      "grad W_out: 0.315862238407135\n",
      "grad W: 3.1390318870544434\n",
      "grad W_in: 1.6401594877243042\n",
      "grad W_out: 0.3972286581993103\n",
      "grad W: 3.7364487648010254\n",
      "grad W_in: 1.6911894083023071\n",
      "[ep 6 | batch 58] loss=1.1587 |M| |W| mean=2.169e-02\n",
      "ep 6 batch 58 | loss=1.1587 | alloc=0.38GB reserv=26.47GB\n",
      "grad W_out: 0.276796817779541\n",
      "grad W: 1.9454395771026611\n",
      "grad W_in: 0.35338306427001953\n",
      "grad W_out: 0.28552865982055664\n",
      "grad W: 3.0540058612823486\n",
      "grad W_in: 1.220907211303711\n",
      "[ep 6 | batch 60] loss=1.1295 |M| |W| mean=2.170e-02\n",
      "ep 6 batch 60 | loss=1.1295 | alloc=0.38GB reserv=26.47GB\n",
      "grad W_out: 0.37490737438201904\n",
      "grad W: 5.265859127044678\n",
      "grad W_in: 3.326608419418335\n",
      "grad W_out: 0.3303122818470001\n",
      "grad W: 2.2923645973205566\n",
      "grad W_in: 0.429131418466568\n",
      "[ep 6 | batch 62] loss=1.1966 |M| |W| mean=2.173e-02\n",
      "ep 6 batch 62 | loss=1.1966 | alloc=0.39GB reserv=26.48GB\n",
      "grad W_out: 0.2692159116268158\n",
      "grad W: 4.51982307434082\n",
      "grad W_in: 2.788710355758667\n",
      "grad W_out: 0.23596753180027008\n",
      "grad W: 4.032167911529541\n",
      "grad W_in: 1.8796076774597168\n",
      "[ep 6 | batch 64] loss=1.1562 |M| |W| mean=2.174e-02\n",
      "ep 6 batch 64 | loss=1.1562 | alloc=0.39GB reserv=26.48GB\n",
      "grad W_out: 0.2449663281440735\n",
      "grad W: 7.262781620025635\n",
      "grad W_in: 2.007596731185913\n",
      "grad W_out: 0.2892204523086548\n",
      "grad W: 4.443627834320068\n",
      "grad W_in: 1.1576241254806519\n",
      "[ep 6 | batch 66] loss=1.1451 |M| |W| mean=2.175e-02\n",
      "ep 6 batch 66 | loss=1.1451 | alloc=0.39GB reserv=26.48GB\n",
      "grad W_out: 0.3236585259437561\n",
      "grad W: 3.1384716033935547\n",
      "grad W_in: 1.2456351518630981\n",
      "grad W_out: 0.2992974519729614\n",
      "grad W: 2.2748546600341797\n",
      "grad W_in: 0.3895900249481201\n",
      "[ep 6 | batch 68] loss=1.0938 |M| |W| mean=2.176e-02\n",
      "ep 6 batch 68 | loss=1.0938 | alloc=0.39GB reserv=26.48GB\n",
      "grad W_out: 0.4083899259567261\n",
      "grad W: 3.61820387840271\n",
      "grad W_in: 1.6965924501419067\n",
      "grad W_out: 0.38312050700187683\n",
      "grad W: 4.462901592254639\n",
      "grad W_in: 1.7512667179107666\n",
      "[ep 6 | batch 70] loss=1.1877 |M| |W| mean=2.177e-02\n",
      "ep 6 batch 70 | loss=1.1877 | alloc=0.39GB reserv=26.48GB\n",
      "grad W_out: 0.34750160574913025\n",
      "grad W: 5.82069206237793\n",
      "grad W_in: 4.297878265380859\n",
      "grad W_out: 0.39738214015960693\n",
      "grad W: 4.180070400238037\n",
      "grad W_in: 2.7173912525177\n",
      "[ep 6 | batch 72] loss=1.1974 |M| |W| mean=2.179e-02\n",
      "ep 6 batch 72 | loss=1.1974 | alloc=0.39GB reserv=26.49GB\n",
      "grad W_out: 0.31024032831192017\n",
      "grad W: 4.538944244384766\n",
      "grad W_in: 1.572707176208496\n",
      "grad W_out: 0.3879026174545288\n",
      "grad W: 3.5904178619384766\n",
      "grad W_in: 1.0095305442810059\n",
      "[ep 6 | batch 74] loss=1.0555 |M| |W| mean=2.180e-02\n",
      "ep 6 batch 74 | loss=1.0555 | alloc=0.39GB reserv=26.49GB\n",
      "grad W_out: 0.3735221028327942\n",
      "grad W: 4.707982063293457\n",
      "grad W_in: 1.5845431089401245\n",
      "grad W_out: 0.2994536757469177\n",
      "grad W: 5.796055793762207\n",
      "grad W_in: 1.9690961837768555\n",
      "[ep 6 | batch 76] loss=1.1552 |M| |W| mean=2.181e-02\n",
      "ep 6 batch 76 | loss=1.1552 | alloc=0.40GB reserv=26.49GB\n",
      "grad W_out: 0.3323726952075958\n",
      "grad W: 7.896707534790039\n",
      "grad W_in: 3.8522207736968994\n",
      "grad W_out: 0.32655301690101624\n",
      "grad W: 4.2566237449646\n",
      "grad W_in: 2.260545492172241\n",
      "[ep 6 | batch 78] loss=1.0993 |M| |W| mean=2.181e-02\n",
      "ep 6 batch 78 | loss=1.0993 | alloc=0.40GB reserv=26.49GB\n",
      "grad W_out: 0.6390276551246643\n",
      "grad W: 6.400945663452148\n",
      "grad W_in: 1.7555742263793945\n",
      "grad W_out: 0.4340582489967346\n",
      "grad W: 4.550132751464844\n",
      "grad W_in: 1.376293659210205\n",
      "[ep 6 | batch 80] loss=1.1403 |M| |W| mean=2.182e-02\n",
      "ep 6 batch 80 | loss=1.1403 | alloc=0.40GB reserv=26.49GB\n",
      "grad W_out: 0.4327068626880646\n",
      "grad W: 5.181281089782715\n",
      "grad W_in: 2.40177321434021\n",
      "grad W_out: 0.3110584616661072\n",
      "grad W: 3.929856061935425\n",
      "grad W_in: 2.1161136627197266\n",
      "[ep 6 | batch 82] loss=1.1503 |M| |W| mean=2.183e-02\n",
      "ep 6 batch 82 | loss=1.1503 | alloc=0.40GB reserv=26.50GB\n",
      "grad W_out: 0.30713754892349243\n",
      "grad W: 4.7373881340026855\n",
      "grad W_in: 2.9538450241088867\n",
      "grad W_out: 0.4274071753025055\n",
      "grad W: 6.076399803161621\n",
      "grad W_in: 3.0305347442626953\n",
      "[ep 6 | batch 84] loss=1.1709 |M| |W| mean=2.184e-02\n",
      "ep 6 batch 84 | loss=1.1709 | alloc=0.40GB reserv=26.50GB\n",
      "grad W_out: 0.26157599687576294\n",
      "grad W: 3.7177319526672363\n",
      "grad W_in: 2.105015516281128\n",
      "grad W_out: 0.3630462884902954\n",
      "grad W: 5.809258460998535\n",
      "grad W_in: 3.398829460144043\n",
      "[ep 6 | batch 86] loss=1.1163 |M| |W| mean=2.185e-02\n",
      "ep 6 batch 86 | loss=1.1163 | alloc=0.40GB reserv=26.50GB\n",
      "grad W_out: 0.27636873722076416\n",
      "grad W: 5.863280773162842\n",
      "grad W_in: 2.6049487590789795\n",
      "grad W_out: 0.30941832065582275\n",
      "grad W: 4.078986167907715\n",
      "grad W_in: 1.9504895210266113\n",
      "[ep 6 | batch 88] loss=1.0929 |M| |W| mean=2.186e-02\n",
      "ep 6 batch 88 | loss=1.0929 | alloc=0.40GB reserv=26.50GB\n",
      "grad W_out: 0.43103310465812683\n",
      "grad W: 7.705977916717529\n",
      "grad W_in: 4.707923412322998\n",
      "grad W_out: 0.3949239253997803\n",
      "grad W: 4.188363075256348\n",
      "grad W_in: 1.2919286489486694\n",
      "[ep 6 | batch 90] loss=1.1434 |M| |W| mean=2.186e-02\n",
      "ep 6 batch 90 | loss=1.1434 | alloc=0.41GB reserv=26.50GB\n",
      "grad W_out: 0.43271762132644653\n",
      "grad W: 5.270760536193848\n",
      "grad W_in: 2.7772464752197266\n",
      "grad W_out: 0.4760892987251282\n",
      "grad W: 4.96169376373291\n",
      "grad W_in: 1.606339931488037\n",
      "[ep 6 | batch 92] loss=1.1294 |M| |W| mean=2.187e-02\n",
      "ep 6 batch 92 | loss=1.1294 | alloc=0.41GB reserv=26.51GB\n",
      "grad W_out: 0.2943422496318817\n",
      "grad W: 4.925600051879883\n",
      "grad W_in: 2.9451470375061035\n",
      "Epoch 06/20 | lr=1.00e-03 | loss=1.2088 | test_acc=62.97% | dt=1176.1s\n",
      "grad W_out: 0.3475295603275299\n",
      "grad W: 3.104255437850952\n",
      "grad W_in: 1.800403356552124\n",
      "[ep 7 | batch 0] loss=1.1292 |M| |W| mean=2.187e-02\n",
      "ep 7 batch 0 | loss=1.1292 | alloc=0.41GB reserv=26.51GB\n",
      "grad W_out: 0.35909849405288696\n",
      "grad W: 3.606067419052124\n",
      "grad W_in: 1.143262267112732\n",
      "grad W_out: 0.414823055267334\n",
      "grad W: 5.236830711364746\n",
      "grad W_in: 1.9860104322433472\n",
      "[ep 7 | batch 2] loss=1.1846 |M| |W| mean=2.188e-02\n",
      "ep 7 batch 2 | loss=1.1846 | alloc=0.41GB reserv=26.51GB\n",
      "grad W_out: 0.24902372062206268\n",
      "grad W: 3.2228050231933594\n",
      "grad W_in: 0.7011198997497559\n",
      "grad W_out: 0.26689791679382324\n",
      "grad W: 4.61076021194458\n",
      "grad W_in: 1.8955962657928467\n",
      "[ep 7 | batch 4] loss=1.0585 |M| |W| mean=2.189e-02\n",
      "ep 7 batch 4 | loss=1.0585 | alloc=0.41GB reserv=26.51GB\n",
      "grad W_out: 0.29510968923568726\n",
      "grad W: 3.508861541748047\n",
      "grad W_in: 0.9301064610481262\n",
      "grad W_out: 0.22368088364601135\n",
      "grad W: 2.877220869064331\n",
      "grad W_in: 0.6211602091789246\n",
      "[ep 7 | batch 6] loss=1.0309 |M| |W| mean=2.190e-02\n",
      "ep 7 batch 6 | loss=1.0309 | alloc=0.41GB reserv=26.51GB\n",
      "grad W_out: 0.31722989678382874\n",
      "grad W: 3.4283461570739746\n",
      "grad W_in: 1.7318612337112427\n",
      "grad W_out: 0.5053236484527588\n",
      "grad W: 5.197765827178955\n",
      "grad W_in: 1.255089282989502\n",
      "[ep 7 | batch 8] loss=1.0909 |M| |W| mean=2.191e-02\n",
      "ep 7 batch 8 | loss=1.0909 | alloc=0.41GB reserv=26.52GB\n",
      "grad W_out: 0.236074760556221\n",
      "grad W: 3.1343564987182617\n",
      "grad W_in: 0.7226201891899109\n",
      "grad W_out: 0.33729875087738037\n",
      "grad W: 3.5817906856536865\n",
      "grad W_in: 1.483713150024414\n",
      "[ep 7 | batch 10] loss=1.1108 |M| |W| mean=2.191e-02\n",
      "ep 7 batch 10 | loss=1.1108 | alloc=0.42GB reserv=26.52GB\n",
      "grad W_out: 0.2951778471469879\n",
      "grad W: 3.6046276092529297\n",
      "grad W_in: 1.4423651695251465\n",
      "grad W_out: 0.30032405257225037\n",
      "grad W: 5.191230773925781\n",
      "grad W_in: 3.2872073650360107\n",
      "[ep 7 | batch 12] loss=1.1089 |M| |W| mean=2.192e-02\n",
      "ep 7 batch 12 | loss=1.1089 | alloc=0.42GB reserv=26.52GB\n",
      "grad W_out: 0.3507247865200043\n",
      "grad W: 2.8955485820770264\n",
      "grad W_in: 1.5720927715301514\n",
      "grad W_out: 0.37939900159835815\n",
      "grad W: 6.4131598472595215\n",
      "grad W_in: 4.003223896026611\n",
      "[ep 7 | batch 14] loss=1.0439 |M| |W| mean=2.193e-02\n",
      "ep 7 batch 14 | loss=1.0439 | alloc=0.42GB reserv=26.52GB\n",
      "grad W_out: 0.4632105827331543\n",
      "grad W: 3.643624782562256\n",
      "grad W_in: 1.1851211786270142\n",
      "grad W_out: 0.3857343792915344\n",
      "grad W: 9.210482597351074\n",
      "grad W_in: 5.97862434387207\n",
      "[ep 7 | batch 16] loss=1.0674 |M| |W| mean=2.194e-02\n",
      "ep 7 batch 16 | loss=1.0674 | alloc=0.42GB reserv=26.52GB\n",
      "grad W_out: 0.5821099281311035\n",
      "grad W: 6.461434841156006\n",
      "grad W_in: 4.527973175048828\n",
      "grad W_out: 0.4372923970222473\n",
      "grad W: 5.370779037475586\n",
      "grad W_in: 3.5032265186309814\n",
      "[ep 7 | batch 18] loss=1.0783 |M| |W| mean=2.194e-02\n",
      "ep 7 batch 18 | loss=1.0783 | alloc=0.42GB reserv=26.53GB\n",
      "grad W_out: 0.2572444677352905\n",
      "grad W: 3.0929155349731445\n",
      "grad W_in: 0.8861449956893921\n",
      "grad W_out: 0.3319520056247711\n",
      "grad W: 6.771631240844727\n",
      "grad W_in: 5.65566349029541\n",
      "[ep 7 | batch 20] loss=1.1056 |M| |W| mean=2.195e-02\n",
      "ep 7 batch 20 | loss=1.1056 | alloc=0.42GB reserv=26.53GB\n",
      "grad W_out: 0.21491247415542603\n",
      "grad W: 8.283245086669922\n",
      "grad W_in: 4.848151683807373\n",
      "grad W_out: 0.3734380006790161\n",
      "grad W: 6.943807601928711\n",
      "grad W_in: 4.75740909576416\n",
      "[ep 7 | batch 22] loss=1.1164 |M| |W| mean=2.196e-02\n",
      "ep 7 batch 22 | loss=1.1164 | alloc=0.42GB reserv=26.53GB\n",
      "grad W_out: 0.583332359790802\n",
      "grad W: 6.541357517242432\n",
      "grad W_in: 3.0331525802612305\n",
      "grad W_out: 0.4679897427558899\n",
      "grad W: 5.187074184417725\n",
      "grad W_in: 2.6202516555786133\n",
      "[ep 7 | batch 24] loss=1.0803 |M| |W| mean=2.197e-02\n",
      "ep 7 batch 24 | loss=1.0803 | alloc=0.43GB reserv=26.53GB\n",
      "grad W_out: 0.3617604672908783\n",
      "grad W: 4.207876205444336\n",
      "grad W_in: 1.599389672279358\n",
      "grad W_out: 0.433437317609787\n",
      "grad W: 6.374722480773926\n",
      "grad W_in: 1.943345069885254\n",
      "[ep 7 | batch 26] loss=1.0990 |M| |W| mean=2.198e-02\n",
      "ep 7 batch 26 | loss=1.0990 | alloc=0.43GB reserv=26.53GB\n",
      "grad W_out: 0.3508530855178833\n",
      "grad W: 5.550053119659424\n",
      "grad W_in: 2.0621707439422607\n",
      "grad W_out: 0.33434411883354187\n",
      "grad W: 6.636234283447266\n",
      "grad W_in: 5.09381628036499\n",
      "[ep 7 | batch 28] loss=1.1438 |M| |W| mean=2.198e-02\n",
      "ep 7 batch 28 | loss=1.1438 | alloc=0.43GB reserv=26.54GB\n",
      "grad W_out: 0.3073712885379791\n",
      "grad W: 5.151261806488037\n",
      "grad W_in: 1.195948600769043\n",
      "grad W_out: 0.48487916588783264\n",
      "grad W: 6.3896803855896\n",
      "grad W_in: 3.3232693672180176\n",
      "[ep 7 | batch 30] loss=1.1298 |M| |W| mean=2.199e-02\n",
      "ep 7 batch 30 | loss=1.1298 | alloc=0.43GB reserv=26.54GB\n",
      "grad W_out: 0.49472394585609436\n",
      "grad W: 4.786694049835205\n",
      "grad W_in: 1.2740589380264282\n",
      "grad W_out: 0.37757933139801025\n",
      "grad W: 5.685729503631592\n",
      "grad W_in: 4.118647575378418\n",
      "[ep 7 | batch 32] loss=1.0499 |M| |W| mean=2.200e-02\n",
      "ep 7 batch 32 | loss=1.0499 | alloc=0.43GB reserv=26.54GB\n",
      "grad W_out: 0.42441847920417786\n",
      "grad W: 7.1080498695373535\n",
      "grad W_in: 6.388240814208984\n",
      "grad W_out: 0.3391876518726349\n",
      "grad W: 4.934380054473877\n",
      "grad W_in: 3.67417049407959\n",
      "[ep 7 | batch 34] loss=1.0421 |M| |W| mean=2.200e-02\n",
      "ep 7 batch 34 | loss=1.0421 | alloc=0.43GB reserv=26.54GB\n",
      "grad W_out: 0.29115360975265503\n",
      "grad W: 4.639838695526123\n",
      "grad W_in: 4.1391754150390625\n",
      "grad W_out: 0.37283235788345337\n",
      "grad W: 4.087396621704102\n",
      "grad W_in: 3.036128044128418\n",
      "[ep 7 | batch 36] loss=1.0631 |M| |W| mean=2.202e-02\n",
      "ep 7 batch 36 | loss=1.0631 | alloc=0.43GB reserv=26.54GB\n",
      "grad W_out: 0.4290809631347656\n",
      "grad W: 5.13278865814209\n",
      "grad W_in: 2.816309690475464\n",
      "grad W_out: 0.34461602568626404\n",
      "grad W: 5.475183486938477\n",
      "grad W_in: 2.8731536865234375\n",
      "[ep 7 | batch 38] loss=1.0689 |M| |W| mean=2.202e-02\n",
      "ep 7 batch 38 | loss=1.0689 | alloc=0.44GB reserv=26.54GB\n",
      "grad W_out: 0.39932700991630554\n",
      "grad W: 3.2192580699920654\n",
      "grad W_in: 2.2781925201416016\n",
      "grad W_out: 0.3870899975299835\n",
      "grad W: 5.053044319152832\n",
      "grad W_in: 2.84904408454895\n",
      "[ep 7 | batch 40] loss=1.0398 |M| |W| mean=2.202e-02\n",
      "ep 7 batch 40 | loss=1.0398 | alloc=0.44GB reserv=26.55GB\n",
      "grad W_out: 0.4291122853755951\n",
      "grad W: 4.309124946594238\n",
      "grad W_in: 3.4363489151000977\n",
      "grad W_out: 0.40529587864875793\n",
      "grad W: 4.0237040519714355\n",
      "grad W_in: 1.8569215536117554\n",
      "[ep 7 | batch 42] loss=1.1063 |M| |W| mean=2.203e-02\n",
      "ep 7 batch 42 | loss=1.1063 | alloc=0.44GB reserv=26.55GB\n",
      "grad W_out: 0.29360929131507874\n",
      "grad W: 2.9953622817993164\n",
      "grad W_in: 0.8506606817245483\n",
      "grad W_out: 0.4143419563770294\n",
      "grad W: 3.409752130508423\n",
      "grad W_in: 1.2712697982788086\n",
      "[ep 7 | batch 44] loss=0.9909 |M| |W| mean=2.204e-02\n",
      "ep 7 batch 44 | loss=0.9909 | alloc=0.44GB reserv=26.55GB\n",
      "grad W_out: 0.4223272502422333\n",
      "grad W: 3.7646007537841797\n",
      "grad W_in: 1.4365861415863037\n",
      "grad W_out: 0.40292805433273315\n",
      "grad W: 3.191596031188965\n",
      "grad W_in: 0.5948659181594849\n",
      "[ep 7 | batch 46] loss=1.0523 |M| |W| mean=2.206e-02\n",
      "ep 7 batch 46 | loss=1.0523 | alloc=0.44GB reserv=26.55GB\n",
      "grad W_out: 0.4576144516468048\n",
      "grad W: 4.955713272094727\n",
      "grad W_in: 0.8993085622787476\n",
      "grad W_out: 0.41100308299064636\n",
      "grad W: 4.400761127471924\n",
      "grad W_in: 2.7176918983459473\n",
      "[ep 7 | batch 48] loss=1.0386 |M| |W| mean=2.207e-02\n",
      "ep 7 batch 48 | loss=1.0386 | alloc=0.44GB reserv=26.55GB\n",
      "grad W_out: 0.3592471778392792\n",
      "grad W: 3.6435086727142334\n",
      "grad W_in: 2.101637601852417\n",
      "grad W_out: 0.4009559154510498\n",
      "grad W: 6.252340793609619\n",
      "grad W_in: 4.156290531158447\n",
      "[ep 7 | batch 50] loss=0.9988 |M| |W| mean=2.208e-02\n",
      "ep 7 batch 50 | loss=0.9988 | alloc=0.44GB reserv=26.56GB\n",
      "grad W_out: 0.24888093769550323\n",
      "grad W: 5.040188789367676\n",
      "grad W_in: 2.1781415939331055\n",
      "grad W_out: 0.35583627223968506\n",
      "grad W: 4.718453884124756\n",
      "grad W_in: 3.5108582973480225\n",
      "[ep 7 | batch 52] loss=1.0904 |M| |W| mean=2.208e-02\n",
      "ep 7 batch 52 | loss=1.0904 | alloc=0.45GB reserv=26.56GB\n",
      "grad W_out: 0.3539341986179352\n",
      "grad W: 4.253172874450684\n",
      "grad W_in: 1.423784613609314\n",
      "grad W_out: 0.3598780632019043\n",
      "grad W: 4.273119926452637\n",
      "grad W_in: 1.5895013809204102\n",
      "[ep 7 | batch 54] loss=1.0163 |M| |W| mean=2.209e-02\n",
      "ep 7 batch 54 | loss=1.0163 | alloc=0.45GB reserv=26.56GB\n",
      "grad W_out: 0.20417307317256927\n",
      "grad W: 4.972393989562988\n",
      "grad W_in: 3.655460834503174\n",
      "grad W_out: 0.40297308564186096\n",
      "grad W: 6.03214693069458\n",
      "grad W_in: 4.405479907989502\n",
      "[ep 7 | batch 56] loss=1.1063 |M| |W| mean=2.210e-02\n",
      "ep 7 batch 56 | loss=1.1063 | alloc=0.45GB reserv=26.56GB\n",
      "grad W_out: 0.32370686531066895\n",
      "grad W: 3.9436564445495605\n",
      "grad W_in: 1.3513959646224976\n",
      "grad W_out: 0.34403783082962036\n",
      "grad W: 2.195960521697998\n",
      "grad W_in: 0.7301214933395386\n",
      "[ep 7 | batch 58] loss=1.0800 |M| |W| mean=2.210e-02\n",
      "ep 7 batch 58 | loss=1.0800 | alloc=0.45GB reserv=26.56GB\n",
      "grad W_out: 0.31347572803497314\n",
      "grad W: 4.679154872894287\n",
      "grad W_in: 1.5242855548858643\n",
      "grad W_out: 0.43024325370788574\n",
      "grad W: 4.479923248291016\n",
      "grad W_in: 0.9607883095741272\n",
      "[ep 7 | batch 60] loss=1.0711 |M| |W| mean=2.212e-02\n",
      "ep 7 batch 60 | loss=1.0711 | alloc=0.45GB reserv=26.57GB\n",
      "grad W_out: 0.2460857331752777\n",
      "grad W: 4.619994163513184\n",
      "grad W_in: 0.6905022859573364\n",
      "grad W_out: 0.27364635467529297\n",
      "grad W: 2.378303050994873\n",
      "grad W_in: 0.8256271481513977\n",
      "[ep 7 | batch 62] loss=1.0192 |M| |W| mean=2.213e-02\n",
      "ep 7 batch 62 | loss=1.0192 | alloc=0.45GB reserv=26.57GB\n",
      "grad W_out: 0.30591824650764465\n",
      "grad W: 2.8221182823181152\n",
      "grad W_in: 0.8566705584526062\n",
      "grad W_out: 0.2986868619918823\n",
      "grad W: 7.696213245391846\n",
      "grad W_in: 5.26891565322876\n",
      "[ep 7 | batch 64] loss=1.0035 |M| |W| mean=2.213e-02\n",
      "ep 7 batch 64 | loss=1.0035 | alloc=0.45GB reserv=26.57GB\n",
      "grad W_out: 0.32209885120391846\n",
      "grad W: 4.975491046905518\n",
      "grad W_in: 3.0889995098114014\n",
      "grad W_out: 0.33875295519828796\n",
      "grad W: 5.84002161026001\n",
      "grad W_in: 4.693516254425049\n",
      "[ep 7 | batch 66] loss=1.0802 |M| |W| mean=2.214e-02\n",
      "ep 7 batch 66 | loss=1.0802 | alloc=0.46GB reserv=26.57GB\n",
      "grad W_out: 0.46429046988487244\n",
      "grad W: 3.6666834354400635\n",
      "grad W_in: 1.7703015804290771\n",
      "grad W_out: 0.6920467019081116\n",
      "grad W: 6.456798076629639\n",
      "grad W_in: 4.542648792266846\n",
      "[ep 7 | batch 68] loss=1.0685 |M| |W| mean=2.215e-02\n",
      "ep 7 batch 68 | loss=1.0685 | alloc=0.46GB reserv=26.57GB\n",
      "grad W_out: 0.47873157262802124\n",
      "grad W: 8.15578842163086\n",
      "grad W_in: 4.426506042480469\n",
      "grad W_out: 0.3938705325126648\n",
      "grad W: 10.390326499938965\n",
      "grad W_in: 6.6232805252075195\n",
      "[ep 7 | batch 70] loss=1.0875 |M| |W| mean=2.215e-02\n",
      "ep 7 batch 70 | loss=1.0875 | alloc=0.46GB reserv=26.58GB\n",
      "grad W_out: 0.34760069847106934\n",
      "grad W: 7.299166202545166\n",
      "grad W_in: 6.206221580505371\n",
      "grad W_out: 0.6495160460472107\n",
      "grad W: 8.387822151184082\n",
      "grad W_in: 3.5878407955169678\n",
      "[ep 7 | batch 72] loss=1.0484 |M| |W| mean=2.217e-02\n",
      "ep 7 batch 72 | loss=1.0484 | alloc=0.46GB reserv=26.58GB\n",
      "grad W_out: 0.6418622136116028\n",
      "grad W: 9.206521987915039\n",
      "grad W_in: 7.127684116363525\n",
      "grad W_out: 0.3587541878223419\n",
      "grad W: 2.5692179203033447\n",
      "grad W_in: 1.211793065071106\n",
      "[ep 7 | batch 74] loss=0.9971 |M| |W| mean=2.217e-02\n",
      "ep 7 batch 74 | loss=0.9971 | alloc=0.46GB reserv=26.58GB\n",
      "grad W_out: 0.5537988543510437\n",
      "grad W: 9.424895286560059\n",
      "grad W_in: 6.474563121795654\n",
      "grad W_out: 0.325153112411499\n",
      "grad W: 6.984126091003418\n",
      "grad W_in: 4.677883148193359\n",
      "[ep 7 | batch 76] loss=1.0656 |M| |W| mean=2.219e-02\n",
      "ep 7 batch 76 | loss=1.0656 | alloc=0.46GB reserv=26.58GB\n",
      "grad W_out: 0.39921310544013977\n",
      "grad W: 5.012988090515137\n",
      "grad W_in: 3.5618486404418945\n",
      "grad W_out: 0.6119105219841003\n",
      "grad W: 10.396987915039062\n",
      "grad W_in: 8.461213111877441\n",
      "[ep 7 | batch 78] loss=1.0936 |M| |W| mean=2.219e-02\n",
      "ep 7 batch 78 | loss=1.0936 | alloc=0.46GB reserv=26.58GB\n",
      "grad W_out: 0.3655124008655548\n",
      "grad W: 4.556519508361816\n",
      "grad W_in: 1.7245962619781494\n",
      "grad W_out: 0.4263001084327698\n",
      "grad W: 7.393125534057617\n",
      "grad W_in: 3.0417346954345703\n",
      "[ep 7 | batch 80] loss=1.1205 |M| |W| mean=2.219e-02\n",
      "ep 7 batch 80 | loss=1.1205 | alloc=0.47GB reserv=26.59GB\n",
      "grad W_out: 0.5710459351539612\n",
      "grad W: 9.181669235229492\n",
      "grad W_in: 8.218867301940918\n",
      "grad W_out: 0.4849611222743988\n",
      "grad W: 3.474226713180542\n",
      "grad W_in: 0.9090370535850525\n",
      "[ep 7 | batch 82] loss=1.0282 |M| |W| mean=2.220e-02\n",
      "ep 7 batch 82 | loss=1.0282 | alloc=0.47GB reserv=26.59GB\n",
      "grad W_out: 0.4011101722717285\n",
      "grad W: 5.222049713134766\n",
      "grad W_in: 3.810955286026001\n",
      "grad W_out: 0.5095909237861633\n",
      "grad W: 6.791783809661865\n",
      "grad W_in: 4.148800373077393\n",
      "[ep 7 | batch 84] loss=1.0680 |M| |W| mean=2.221e-02\n",
      "ep 7 batch 84 | loss=1.0680 | alloc=0.47GB reserv=26.59GB\n",
      "grad W_out: 0.34729644656181335\n",
      "grad W: 5.439539909362793\n",
      "grad W_in: 1.7624727487564087\n",
      "grad W_out: 0.30596351623535156\n",
      "grad W: 2.8990676403045654\n",
      "grad W_in: 0.6133644580841064\n",
      "[ep 7 | batch 86] loss=0.9809 |M| |W| mean=2.221e-02\n",
      "ep 7 batch 86 | loss=0.9809 | alloc=0.47GB reserv=26.59GB\n",
      "grad W_out: 0.4300176501274109\n",
      "grad W: 5.863741397857666\n",
      "grad W_in: 4.071649074554443\n",
      "grad W_out: 0.4534914195537567\n",
      "grad W: 6.239973545074463\n",
      "grad W_in: 4.640860080718994\n",
      "[ep 7 | batch 88] loss=1.0942 |M| |W| mean=2.222e-02\n",
      "ep 7 batch 88 | loss=1.0942 | alloc=0.47GB reserv=26.59GB\n",
      "grad W_out: 0.3622497320175171\n",
      "grad W: 2.8356285095214844\n",
      "grad W_in: 0.5246413350105286\n",
      "grad W_out: 0.2434907853603363\n",
      "grad W: 5.4965901374816895\n",
      "grad W_in: 3.0312178134918213\n",
      "[ep 7 | batch 90] loss=1.0467 |M| |W| mean=2.223e-02\n",
      "ep 7 batch 90 | loss=1.0467 | alloc=0.47GB reserv=26.60GB\n",
      "grad W_out: 0.38050028681755066\n",
      "grad W: 6.987098217010498\n",
      "grad W_in: 3.045628070831299\n",
      "grad W_out: 0.21363411843776703\n",
      "grad W: 3.3629953861236572\n",
      "grad W_in: 1.7090582847595215\n",
      "[ep 7 | batch 92] loss=1.0216 |M| |W| mean=2.223e-02\n",
      "ep 7 batch 92 | loss=1.0216 | alloc=0.47GB reserv=26.60GB\n",
      "grad W_out: 0.3570578098297119\n",
      "grad W: 4.589012145996094\n",
      "grad W_in: 1.379282832145691\n",
      "Epoch 07/20 | lr=1.00e-03 | loss=1.0704 | test_acc=65.29% | dt=1176.4s\n",
      "grad W_out: 0.34372785687446594\n",
      "grad W: 13.880339622497559\n",
      "grad W_in: 10.430706977844238\n",
      "[ep 8 | batch 0] loss=0.9803 |M| |W| mean=2.223e-02\n",
      "ep 8 batch 0 | loss=0.9803 | alloc=0.48GB reserv=26.60GB\n",
      "grad W_out: 0.7076919078826904\n",
      "grad W: 10.4596529006958\n",
      "grad W_in: 8.613125801086426\n",
      "grad W_out: 0.3880728781223297\n",
      "grad W: 5.928329944610596\n",
      "grad W_in: 5.0075907707214355\n",
      "[ep 8 | batch 2] loss=1.0064 |M| |W| mean=2.224e-02\n",
      "ep 8 batch 2 | loss=1.0064 | alloc=0.48GB reserv=26.60GB\n",
      "grad W_out: 0.41969266533851624\n",
      "grad W: 7.640266418457031\n",
      "grad W_in: 3.8843812942504883\n",
      "grad W_out: 0.5046374201774597\n",
      "grad W: 8.479129791259766\n",
      "grad W_in: 2.18717360496521\n",
      "[ep 8 | batch 4] loss=1.0942 |M| |W| mean=2.225e-02\n",
      "ep 8 batch 4 | loss=1.0942 | alloc=0.48GB reserv=26.60GB\n",
      "grad W_out: 0.24831001460552216\n",
      "grad W: 6.598203659057617\n",
      "grad W_in: 2.3379404544830322\n",
      "grad W_out: 0.40882909297943115\n",
      "grad W: 6.271807670593262\n",
      "grad W_in: 3.979996681213379\n",
      "[ep 8 | batch 6] loss=0.9748 |M| |W| mean=2.224e-02\n",
      "ep 8 batch 6 | loss=0.9748 | alloc=0.48GB reserv=26.61GB\n",
      "grad W_out: 0.41432294249534607\n",
      "grad W: 18.54031753540039\n",
      "grad W_in: 7.550239086151123\n",
      "grad W_out: 0.26238468289375305\n",
      "grad W: 4.243597984313965\n",
      "grad W_in: 1.2227531671524048\n",
      "[ep 8 | batch 8] loss=1.0447 |M| |W| mean=2.225e-02\n",
      "ep 8 batch 8 | loss=1.0447 | alloc=0.48GB reserv=26.61GB\n",
      "grad W_out: 0.4263896644115448\n",
      "grad W: 5.651676177978516\n",
      "grad W_in: 2.9240331649780273\n",
      "grad W_out: 0.42090651392936707\n",
      "grad W: 6.518936634063721\n",
      "grad W_in: 3.098287343978882\n",
      "[ep 8 | batch 10] loss=1.0634 |M| |W| mean=2.226e-02\n",
      "ep 8 batch 10 | loss=1.0634 | alloc=0.48GB reserv=26.61GB\n",
      "grad W_out: 0.45076411962509155\n",
      "grad W: 6.2740631103515625\n",
      "grad W_in: 2.819844961166382\n",
      "grad W_out: 0.3427390456199646\n",
      "grad W: 5.9498982429504395\n",
      "grad W_in: 3.9496190547943115\n",
      "[ep 8 | batch 12] loss=1.0691 |M| |W| mean=2.227e-02\n",
      "ep 8 batch 12 | loss=1.0691 | alloc=0.48GB reserv=26.61GB\n",
      "grad W_out: 0.490864634513855\n",
      "grad W: 8.175540924072266\n",
      "grad W_in: 2.652900218963623\n",
      "grad W_out: 0.46913883090019226\n",
      "grad W: 5.763319969177246\n",
      "grad W_in: 3.0197339057922363\n",
      "[ep 8 | batch 14] loss=1.1083 |M| |W| mean=2.229e-02\n",
      "ep 8 batch 14 | loss=1.1083 | alloc=0.49GB reserv=26.61GB\n",
      "grad W_out: 0.3984116017818451\n",
      "grad W: 3.5988399982452393\n",
      "grad W_in: 3.206364631652832\n",
      "grad W_out: 0.32837581634521484\n",
      "grad W: 4.249324321746826\n",
      "grad W_in: 1.745911717414856\n",
      "[ep 8 | batch 16] loss=1.0200 |M| |W| mean=2.229e-02\n",
      "ep 8 batch 16 | loss=1.0200 | alloc=0.49GB reserv=26.62GB\n",
      "grad W_out: 0.5084883570671082\n",
      "grad W: 4.869630336761475\n",
      "grad W_in: 2.8165504932403564\n",
      "grad W_out: 0.3500863015651703\n",
      "grad W: 5.218675136566162\n",
      "grad W_in: 2.5257325172424316\n",
      "[ep 8 | batch 18] loss=1.0229 |M| |W| mean=2.230e-02\n",
      "ep 8 batch 18 | loss=1.0229 | alloc=0.49GB reserv=26.62GB\n",
      "grad W_out: 0.4327053129673004\n",
      "grad W: 6.845150947570801\n",
      "grad W_in: 2.469172716140747\n",
      "grad W_out: 0.311740517616272\n",
      "grad W: 5.038780212402344\n",
      "grad W_in: 2.1470565795898438\n",
      "[ep 8 | batch 20] loss=1.0115 |M| |W| mean=2.230e-02\n",
      "ep 8 batch 20 | loss=1.0115 | alloc=0.49GB reserv=26.62GB\n",
      "grad W_out: 0.4773713946342468\n",
      "grad W: 4.913689613342285\n",
      "grad W_in: 2.8553178310394287\n",
      "grad W_out: 0.3839663565158844\n",
      "grad W: 5.187981128692627\n",
      "grad W_in: 3.6053013801574707\n",
      "[ep 8 | batch 22] loss=1.0290 |M| |W| mean=2.232e-02\n",
      "ep 8 batch 22 | loss=1.0290 | alloc=0.49GB reserv=26.62GB\n",
      "grad W_out: 0.43795114755630493\n",
      "grad W: 4.2051215171813965\n",
      "grad W_in: 1.8027299642562866\n",
      "grad W_out: 0.4706633985042572\n",
      "grad W: 5.915184020996094\n",
      "grad W_in: 4.097969055175781\n",
      "[ep 8 | batch 24] loss=1.0567 |M| |W| mean=2.234e-02\n",
      "ep 8 batch 24 | loss=1.0567 | alloc=0.49GB reserv=26.62GB\n",
      "grad W_out: 0.30412328243255615\n",
      "grad W: 2.935889482498169\n",
      "grad W_in: 0.9911621809005737\n",
      "grad W_out: 0.2117426097393036\n",
      "grad W: 2.254598379135132\n",
      "grad W_in: 0.40870970487594604\n",
      "[ep 8 | batch 26] loss=1.0364 |M| |W| mean=2.235e-02\n",
      "ep 8 batch 26 | loss=1.0364 | alloc=0.49GB reserv=26.62GB\n",
      "grad W_out: 0.2341519445180893\n",
      "grad W: 4.958032608032227\n",
      "grad W_in: 3.1552202701568604\n",
      "grad W_out: 0.3866938352584839\n",
      "grad W: 5.633420467376709\n",
      "grad W_in: 3.774362564086914\n",
      "[ep 8 | batch 28] loss=0.9925 |M| |W| mean=2.236e-02\n",
      "ep 8 batch 28 | loss=0.9925 | alloc=0.50GB reserv=26.63GB\n",
      "grad W_out: 0.17213985323905945\n",
      "grad W: 5.390939235687256\n",
      "grad W_in: 1.964914083480835\n",
      "grad W_out: 0.4655124843120575\n",
      "grad W: 5.73698616027832\n",
      "grad W_in: 2.9380056858062744\n",
      "[ep 8 | batch 30] loss=1.0424 |M| |W| mean=2.237e-02\n",
      "ep 8 batch 30 | loss=1.0424 | alloc=0.50GB reserv=26.63GB\n",
      "grad W_out: 0.3264996111392975\n",
      "grad W: 5.4715142250061035\n",
      "grad W_in: 1.424070954322815\n",
      "grad W_out: 0.2574312686920166\n",
      "grad W: 3.663958787918091\n",
      "grad W_in: 3.1511483192443848\n",
      "[ep 8 | batch 32] loss=0.9915 |M| |W| mean=2.238e-02\n",
      "ep 8 batch 32 | loss=0.9915 | alloc=0.50GB reserv=26.63GB\n",
      "grad W_out: 0.33688199520111084\n",
      "grad W: 5.43325662612915\n",
      "grad W_in: 2.5620522499084473\n",
      "grad W_out: 0.3444201350212097\n",
      "grad W: 4.770291805267334\n",
      "grad W_in: 1.8052688837051392\n",
      "[ep 8 | batch 34] loss=1.0285 |M| |W| mean=2.239e-02\n",
      "ep 8 batch 34 | loss=1.0285 | alloc=0.50GB reserv=26.63GB\n",
      "grad W_out: 0.23584318161010742\n",
      "grad W: 5.803624629974365\n",
      "grad W_in: 3.6933701038360596\n",
      "grad W_out: 0.4731528162956238\n",
      "grad W: 13.464493751525879\n",
      "grad W_in: 13.829985618591309\n",
      "[ep 8 | batch 36] loss=1.0625 |M| |W| mean=2.239e-02\n",
      "ep 8 batch 36 | loss=1.0625 | alloc=0.50GB reserv=26.63GB\n",
      "grad W_out: 0.3723931610584259\n",
      "grad W: 4.518900394439697\n",
      "grad W_in: 3.0052542686462402\n",
      "grad W_out: 0.5801752209663391\n",
      "grad W: 11.998438835144043\n",
      "grad W_in: 11.41794490814209\n",
      "[ep 8 | batch 38] loss=0.9965 |M| |W| mean=2.239e-02\n",
      "ep 8 batch 38 | loss=0.9965 | alloc=0.50GB reserv=26.64GB\n",
      "grad W_out: 0.20953567326068878\n",
      "grad W: 3.0210349559783936\n",
      "grad W_in: 1.947861671447754\n",
      "grad W_out: 0.5449793338775635\n",
      "grad W: 11.58673095703125\n",
      "grad W_in: 8.584633827209473\n",
      "[ep 8 | batch 40] loss=1.0733 |M| |W| mean=2.239e-02\n",
      "ep 8 batch 40 | loss=1.0733 | alloc=0.50GB reserv=26.64GB\n",
      "grad W_out: 0.3823206424713135\n",
      "grad W: 9.364884376525879\n",
      "grad W_in: 2.220789909362793\n",
      "grad W_out: 0.35698580741882324\n",
      "grad W: 4.819273471832275\n",
      "grad W_in: 1.9375451803207397\n",
      "[ep 8 | batch 42] loss=1.0028 |M| |W| mean=2.240e-02\n",
      "ep 8 batch 42 | loss=1.0028 | alloc=0.51GB reserv=26.64GB\n",
      "grad W_out: 0.3575485646724701\n",
      "grad W: 6.876387119293213\n",
      "grad W_in: 6.071502685546875\n",
      "grad W_out: 0.40199074149131775\n",
      "grad W: 5.829234600067139\n",
      "grad W_in: 1.4185295104980469\n",
      "[ep 8 | batch 44] loss=1.0151 |M| |W| mean=2.241e-02\n",
      "ep 8 batch 44 | loss=1.0151 | alloc=0.51GB reserv=26.64GB\n",
      "grad W_out: 0.3870493769645691\n",
      "grad W: 3.2112302780151367\n",
      "grad W_in: 0.6735155582427979\n",
      "grad W_out: 0.4021684527397156\n",
      "grad W: 6.047270774841309\n",
      "grad W_in: 2.7150235176086426\n",
      "[ep 8 | batch 46] loss=1.0298 |M| |W| mean=2.242e-02\n",
      "ep 8 batch 46 | loss=1.0298 | alloc=0.51GB reserv=26.64GB\n",
      "grad W_out: 0.35023581981658936\n",
      "grad W: 8.131052017211914\n",
      "grad W_in: 5.87092924118042\n",
      "grad W_out: 0.30284127593040466\n",
      "grad W: 2.6800947189331055\n",
      "grad W_in: 1.6682913303375244\n",
      "[ep 8 | batch 48] loss=0.9454 |M| |W| mean=2.243e-02\n",
      "ep 8 batch 48 | loss=0.9454 | alloc=0.51GB reserv=26.65GB\n",
      "grad W_out: 0.43643462657928467\n",
      "grad W: 4.800065517425537\n",
      "grad W_in: 2.9707934856414795\n",
      "grad W_out: 0.5240638256072998\n",
      "grad W: 11.17688274383545\n",
      "grad W_in: 5.078879356384277\n",
      "[ep 8 | batch 50] loss=1.0220 |M| |W| mean=2.244e-02\n",
      "ep 8 batch 50 | loss=1.0220 | alloc=0.51GB reserv=26.65GB\n",
      "grad W_out: 0.35557860136032104\n",
      "grad W: 3.9847025871276855\n",
      "grad W_in: 1.720726728439331\n",
      "grad W_out: 0.5354330539703369\n",
      "grad W: 7.169767379760742\n",
      "grad W_in: 5.655113697052002\n",
      "[ep 8 | batch 52] loss=0.9510 |M| |W| mean=2.245e-02\n",
      "ep 8 batch 52 | loss=0.9510 | alloc=0.51GB reserv=26.65GB\n",
      "grad W_out: 0.40502992272377014\n",
      "grad W: 6.603172302246094\n",
      "grad W_in: 3.395399570465088\n",
      "grad W_out: 0.3760416805744171\n",
      "grad W: 5.126928806304932\n",
      "grad W_in: 1.7833493947982788\n",
      "[ep 8 | batch 54] loss=0.9886 |M| |W| mean=2.246e-02\n",
      "ep 8 batch 54 | loss=0.9886 | alloc=0.51GB reserv=26.65GB\n",
      "grad W_out: 0.29029953479766846\n",
      "grad W: 7.828397750854492\n",
      "grad W_in: 4.17683219909668\n",
      "grad W_out: 0.31537461280822754\n",
      "grad W: 12.303387641906738\n",
      "grad W_in: 10.941473007202148\n",
      "[ep 8 | batch 56] loss=1.0150 |M| |W| mean=2.245e-02\n",
      "ep 8 batch 56 | loss=1.0150 | alloc=0.52GB reserv=26.65GB\n",
      "grad W_out: 0.30028265714645386\n",
      "grad W: 5.929032325744629\n",
      "grad W_in: 2.456860065460205\n",
      "grad W_out: 0.530950129032135\n",
      "grad W: 5.608551502227783\n",
      "grad W_in: 5.925070285797119\n",
      "[ep 8 | batch 58] loss=0.9990 |M| |W| mean=2.245e-02\n",
      "ep 8 batch 58 | loss=0.9990 | alloc=0.52GB reserv=26.66GB\n",
      "grad W_out: 0.4400585889816284\n",
      "grad W: 4.407311916351318\n",
      "grad W_in: 4.954499244689941\n",
      "grad W_out: 0.40751367807388306\n",
      "grad W: 5.085409641265869\n",
      "grad W_in: 1.6256476640701294\n",
      "[ep 8 | batch 60] loss=1.0052 |M| |W| mean=2.246e-02\n",
      "ep 8 batch 60 | loss=1.0052 | alloc=0.52GB reserv=26.66GB\n",
      "grad W_out: 0.4859864115715027\n",
      "grad W: 5.424045085906982\n",
      "grad W_in: 3.0375235080718994\n",
      "grad W_out: 0.3365834355354309\n",
      "grad W: 4.368467330932617\n",
      "grad W_in: 1.6752309799194336\n",
      "[ep 8 | batch 62] loss=1.0176 |M| |W| mean=2.246e-02\n",
      "ep 8 batch 62 | loss=1.0176 | alloc=0.52GB reserv=26.66GB\n",
      "grad W_out: 0.2599331736564636\n",
      "grad W: 4.877689361572266\n",
      "grad W_in: 1.9456076622009277\n",
      "grad W_out: 0.2713066041469574\n",
      "grad W: 4.551445960998535\n",
      "grad W_in: 1.4425255060195923\n",
      "[ep 8 | batch 64] loss=0.9137 |M| |W| mean=2.246e-02\n",
      "ep 8 batch 64 | loss=0.9137 | alloc=0.52GB reserv=26.66GB\n",
      "grad W_out: 0.3161832094192505\n",
      "grad W: 4.001369476318359\n",
      "grad W_in: 3.70525860786438\n",
      "grad W_out: 0.44635945558547974\n",
      "grad W: 4.1189446449279785\n",
      "grad W_in: 2.312847852706909\n",
      "[ep 8 | batch 66] loss=0.9608 |M| |W| mean=2.247e-02\n",
      "ep 8 batch 66 | loss=0.9608 | alloc=0.52GB reserv=26.66GB\n",
      "grad W_out: 0.2273474782705307\n",
      "grad W: 2.9587156772613525\n",
      "grad W_in: 1.0471972227096558\n",
      "grad W_out: 0.39368003606796265\n",
      "grad W: 4.651974678039551\n",
      "grad W_in: 2.2481892108917236\n",
      "[ep 8 | batch 68] loss=0.9857 |M| |W| mean=2.248e-02\n",
      "ep 8 batch 68 | loss=0.9857 | alloc=0.52GB reserv=26.67GB\n",
      "grad W_out: 0.3633766770362854\n",
      "grad W: 3.758713722229004\n",
      "grad W_in: 1.842384696006775\n",
      "grad W_out: 0.19173651933670044\n",
      "grad W: 2.327366828918457\n",
      "grad W_in: 1.29997718334198\n",
      "[ep 8 | batch 70] loss=0.9315 |M| |W| mean=2.249e-02\n",
      "ep 8 batch 70 | loss=0.9315 | alloc=0.53GB reserv=26.67GB\n",
      "grad W_out: 0.40130817890167236\n",
      "grad W: 4.526149272918701\n",
      "grad W_in: 2.280360460281372\n",
      "grad W_out: 0.502953052520752\n",
      "grad W: 8.983689308166504\n",
      "grad W_in: 5.56682825088501\n",
      "[ep 8 | batch 72] loss=1.0222 |M| |W| mean=2.251e-02\n",
      "ep 8 batch 72 | loss=1.0222 | alloc=0.53GB reserv=26.67GB\n",
      "grad W_out: 0.3749696910381317\n",
      "grad W: 4.879054546356201\n",
      "grad W_in: 5.429851055145264\n",
      "grad W_out: 0.43736565113067627\n",
      "grad W: 6.182231903076172\n",
      "grad W_in: 4.518718242645264\n",
      "[ep 8 | batch 74] loss=1.0391 |M| |W| mean=2.252e-02\n",
      "ep 8 batch 74 | loss=1.0391 | alloc=0.53GB reserv=26.67GB\n",
      "grad W_out: 0.35184526443481445\n",
      "grad W: 5.8758864402771\n",
      "grad W_in: 4.186628818511963\n",
      "grad W_out: 0.24205385148525238\n",
      "grad W: 2.4742391109466553\n",
      "grad W_in: 1.1095649003982544\n",
      "[ep 8 | batch 76] loss=0.9674 |M| |W| mean=2.252e-02\n",
      "ep 8 batch 76 | loss=0.9674 | alloc=0.53GB reserv=26.67GB\n",
      "grad W_out: 0.524937093257904\n",
      "grad W: 9.487067222595215\n",
      "grad W_in: 7.539106845855713\n",
      "grad W_out: 0.34892377257347107\n",
      "grad W: 8.534425735473633\n",
      "grad W_in: 3.025399684906006\n",
      "[ep 8 | batch 78] loss=0.9444 |M| |W| mean=2.254e-02\n",
      "ep 8 batch 78 | loss=0.9444 | alloc=0.53GB reserv=26.68GB\n",
      "grad W_out: 0.3227163255214691\n",
      "grad W: 5.808729648590088\n",
      "grad W_in: 2.2185423374176025\n",
      "grad W_out: 0.47230446338653564\n",
      "grad W: 6.736532688140869\n",
      "grad W_in: 4.299586296081543\n",
      "[ep 8 | batch 80] loss=0.9908 |M| |W| mean=2.255e-02\n",
      "ep 8 batch 80 | loss=0.9908 | alloc=0.53GB reserv=26.68GB\n",
      "grad W_out: 0.47199514508247375\n",
      "grad W: 5.645608901977539\n",
      "grad W_in: 2.280383586883545\n",
      "grad W_out: 0.2651103734970093\n",
      "grad W: 3.7952961921691895\n",
      "grad W_in: 1.655442237854004\n",
      "[ep 8 | batch 82] loss=0.9974 |M| |W| mean=2.254e-02\n",
      "ep 8 batch 82 | loss=0.9974 | alloc=0.53GB reserv=26.68GB\n",
      "grad W_out: 0.32329365611076355\n",
      "grad W: 7.2696428298950195\n",
      "grad W_in: 6.762876987457275\n",
      "grad W_out: 0.487151175737381\n",
      "grad W: 4.75186824798584\n",
      "grad W_in: 3.022939443588257\n",
      "[ep 8 | batch 84] loss=0.9639 |M| |W| mean=2.255e-02\n",
      "ep 8 batch 84 | loss=0.9639 | alloc=0.54GB reserv=26.68GB\n",
      "grad W_out: 0.5120409727096558\n",
      "grad W: 5.956830024719238\n",
      "grad W_in: 2.144533157348633\n",
      "grad W_out: 0.5157588720321655\n",
      "grad W: 8.551992416381836\n",
      "grad W_in: 6.580148220062256\n",
      "[ep 8 | batch 86] loss=1.0036 |M| |W| mean=2.256e-02\n",
      "ep 8 batch 86 | loss=1.0036 | alloc=0.54GB reserv=26.68GB\n",
      "grad W_out: 0.2677881717681885\n",
      "grad W: 3.7358880043029785\n",
      "grad W_in: 1.0366593599319458\n",
      "grad W_out: 0.39754658937454224\n",
      "grad W: 13.554732322692871\n",
      "grad W_in: 9.520955085754395\n",
      "[ep 8 | batch 88] loss=0.9934 |M| |W| mean=2.257e-02\n",
      "ep 8 batch 88 | loss=0.9934 | alloc=0.54GB reserv=26.69GB\n",
      "grad W_out: 0.4363195598125458\n",
      "grad W: 6.402584075927734\n",
      "grad W_in: 2.0661516189575195\n",
      "grad W_out: 0.3093436360359192\n",
      "grad W: 4.30855655670166\n",
      "grad W_in: 2.698824405670166\n",
      "[ep 8 | batch 90] loss=0.9379 |M| |W| mean=2.258e-02\n",
      "ep 8 batch 90 | loss=0.9379 | alloc=0.54GB reserv=26.69GB\n",
      "grad W_out: 0.42880624532699585\n",
      "grad W: 4.483433246612549\n",
      "grad W_in: 2.5310516357421875\n",
      "grad W_out: 0.5503988265991211\n",
      "grad W: 7.824650287628174\n",
      "grad W_in: 3.0649702548980713\n",
      "[ep 8 | batch 92] loss=0.9756 |M| |W| mean=2.258e-02\n",
      "ep 8 batch 92 | loss=0.9756 | alloc=0.54GB reserv=26.69GB\n",
      "grad W_out: 0.3572034239768982\n",
      "grad W: 4.1054301261901855\n",
      "grad W_in: 1.057829737663269\n",
      "Epoch 08/20 | lr=1.00e-03 | loss=1.0064 | test_acc=67.49% | dt=1176.6s\n",
      "grad W_out: 0.3851100206375122\n",
      "grad W: 6.209870338439941\n",
      "grad W_in: 4.207982540130615\n",
      "[ep 9 | batch 0] loss=1.0237 |M| |W| mean=2.260e-02\n",
      "ep 9 batch 0 | loss=1.0237 | alloc=0.54GB reserv=26.69GB\n",
      "grad W_out: 0.35167548060417175\n",
      "grad W: 6.110356330871582\n",
      "grad W_in: 2.9501850605010986\n",
      "grad W_out: 0.46022823452949524\n",
      "grad W: 4.655770301818848\n",
      "grad W_in: 1.5744606256484985\n",
      "[ep 9 | batch 2] loss=1.0515 |M| |W| mean=2.261e-02\n",
      "ep 9 batch 2 | loss=1.0515 | alloc=0.54GB reserv=26.69GB\n",
      "grad W_out: 0.31245875358581543\n",
      "grad W: 4.510287284851074\n",
      "grad W_in: 1.0517736673355103\n",
      "grad W_out: 0.34825873374938965\n",
      "grad W: 3.3014442920684814\n",
      "grad W_in: 1.200195550918579\n",
      "[ep 9 | batch 4] loss=0.9107 |M| |W| mean=2.262e-02\n",
      "ep 9 batch 4 | loss=0.9107 | alloc=0.55GB reserv=26.70GB\n",
      "grad W_out: 0.2910284399986267\n",
      "grad W: 5.292033672332764\n",
      "grad W_in: 2.6416358947753906\n",
      "grad W_out: 0.48634132742881775\n",
      "grad W: 6.172809600830078\n",
      "grad W_in: 1.7875231504440308\n",
      "[ep 9 | batch 6] loss=0.9182 |M| |W| mean=2.264e-02\n",
      "ep 9 batch 6 | loss=0.9182 | alloc=0.55GB reserv=26.70GB\n",
      "grad W_out: 0.33077916502952576\n",
      "grad W: 5.702497482299805\n",
      "grad W_in: 4.23309326171875\n",
      "grad W_out: 0.36150291562080383\n",
      "grad W: 9.083686828613281\n",
      "grad W_in: 3.4249324798583984\n",
      "[ep 9 | batch 8] loss=0.9758 |M| |W| mean=2.265e-02\n",
      "ep 9 batch 8 | loss=0.9758 | alloc=0.55GB reserv=26.70GB\n",
      "grad W_out: 0.575614869594574\n",
      "grad W: 7.7689313888549805\n",
      "grad W_in: 3.4350805282592773\n",
      "grad W_out: 0.5054113268852234\n",
      "grad W: 9.039915084838867\n",
      "grad W_in: 8.487258911132812\n",
      "[ep 9 | batch 10] loss=0.9618 |M| |W| mean=2.266e-02\n",
      "ep 9 batch 10 | loss=0.9618 | alloc=0.55GB reserv=26.70GB\n",
      "grad W_out: 0.3472531735897064\n",
      "grad W: 2.9777185916900635\n",
      "grad W_in: 0.6297178268432617\n",
      "grad W_out: 0.41694140434265137\n",
      "grad W: 8.504220962524414\n",
      "grad W_in: 6.603416442871094\n",
      "[ep 9 | batch 12] loss=0.9646 |M| |W| mean=2.266e-02\n",
      "ep 9 batch 12 | loss=0.9646 | alloc=0.55GB reserv=26.70GB\n",
      "grad W_out: 0.3203812539577484\n",
      "grad W: 6.653505325317383\n",
      "grad W_in: 5.408363342285156\n",
      "grad W_out: 0.28093111515045166\n",
      "grad W: 4.072883605957031\n",
      "grad W_in: 1.71322762966156\n",
      "[ep 9 | batch 14] loss=0.9400 |M| |W| mean=2.266e-02\n",
      "ep 9 batch 14 | loss=0.9400 | alloc=0.55GB reserv=26.71GB\n",
      "grad W_out: 0.4323197305202484\n",
      "grad W: 8.611917495727539\n",
      "grad W_in: 7.004875659942627\n",
      "grad W_out: 0.47438573837280273\n",
      "grad W: 6.74717903137207\n",
      "grad W_in: 3.4540812969207764\n",
      "[ep 9 | batch 16] loss=0.9405 |M| |W| mean=2.268e-02\n",
      "ep 9 batch 16 | loss=0.9405 | alloc=0.55GB reserv=26.71GB\n",
      "grad W_out: 0.5218933820724487\n",
      "grad W: 7.691306114196777\n",
      "grad W_in: 6.774003028869629\n",
      "grad W_out: 0.3070235252380371\n",
      "grad W: 4.688380241394043\n",
      "grad W_in: 3.5236592292785645\n",
      "[ep 9 | batch 18] loss=0.9618 |M| |W| mean=2.268e-02\n",
      "ep 9 batch 18 | loss=0.9618 | alloc=0.56GB reserv=26.71GB\n",
      "grad W_out: 0.40844249725341797\n",
      "grad W: 4.507927894592285\n",
      "grad W_in: 0.7366383075714111\n",
      "grad W_out: 0.5708666443824768\n",
      "grad W: 5.065515995025635\n",
      "grad W_in: 2.5718629360198975\n",
      "[ep 9 | batch 20] loss=0.9627 |M| |W| mean=2.268e-02\n",
      "ep 9 batch 20 | loss=0.9627 | alloc=0.56GB reserv=26.71GB\n",
      "grad W_out: 0.33716535568237305\n",
      "grad W: 3.9122767448425293\n",
      "grad W_in: 2.664968967437744\n",
      "grad W_out: 0.2609058916568756\n",
      "grad W: 5.5342817306518555\n",
      "grad W_in: 4.204355239868164\n",
      "[ep 9 | batch 22] loss=0.9432 |M| |W| mean=2.269e-02\n",
      "ep 9 batch 22 | loss=0.9432 | alloc=0.56GB reserv=26.71GB\n",
      "grad W_out: 0.25352418422698975\n",
      "grad W: 4.741552829742432\n",
      "grad W_in: 1.7687008380889893\n",
      "grad W_out: 0.43076419830322266\n",
      "grad W: 7.008918762207031\n",
      "grad W_in: 4.9783148765563965\n",
      "[ep 9 | batch 24] loss=1.0026 |M| |W| mean=2.270e-02\n",
      "ep 9 batch 24 | loss=1.0026 | alloc=0.56GB reserv=26.71GB\n",
      "grad W_out: 0.27212148904800415\n",
      "grad W: 4.850408554077148\n",
      "grad W_in: 2.281078815460205\n",
      "grad W_out: 0.299718976020813\n",
      "grad W: 5.11638069152832\n",
      "grad W_in: 3.5836548805236816\n",
      "[ep 9 | batch 26] loss=0.9314 |M| |W| mean=2.270e-02\n",
      "ep 9 batch 26 | loss=0.9314 | alloc=0.56GB reserv=26.72GB\n",
      "grad W_out: 0.40062811970710754\n",
      "grad W: 6.256110668182373\n",
      "grad W_in: 3.263004779815674\n",
      "grad W_out: 0.3226020634174347\n",
      "grad W: 4.778886795043945\n",
      "grad W_in: 3.2168641090393066\n",
      "[ep 9 | batch 28] loss=0.9370 |M| |W| mean=2.272e-02\n",
      "ep 9 batch 28 | loss=0.9370 | alloc=0.56GB reserv=26.72GB\n",
      "grad W_out: 0.49624985456466675\n",
      "grad W: 5.899900913238525\n",
      "grad W_in: 2.4095332622528076\n",
      "grad W_out: 0.3332873582839966\n",
      "grad W: 4.977281093597412\n",
      "grad W_in: 4.5382537841796875\n",
      "[ep 9 | batch 30] loss=0.8925 |M| |W| mean=2.273e-02\n",
      "ep 9 batch 30 | loss=0.8925 | alloc=0.56GB reserv=26.72GB\n",
      "grad W_out: 0.2987164556980133\n",
      "grad W: 4.757296085357666\n",
      "grad W_in: 3.3978796005249023\n",
      "grad W_out: 0.2688887119293213\n",
      "grad W: 4.485908508300781\n",
      "grad W_in: 2.432251214981079\n",
      "[ep 9 | batch 32] loss=0.9214 |M| |W| mean=2.273e-02\n",
      "ep 9 batch 32 | loss=0.9214 | alloc=0.57GB reserv=26.72GB\n",
      "grad W_out: 0.35735148191452026\n",
      "grad W: 4.942535877227783\n",
      "grad W_in: 3.9843811988830566\n",
      "grad W_out: 0.23175668716430664\n",
      "grad W: 5.420691967010498\n",
      "grad W_in: 1.9009275436401367\n",
      "[ep 9 | batch 34] loss=0.8775 |M| |W| mean=2.274e-02\n",
      "ep 9 batch 34 | loss=0.8775 | alloc=0.57GB reserv=26.72GB\n",
      "grad W_out: 0.2600313723087311\n",
      "grad W: 4.359601974487305\n",
      "grad W_in: 1.907589077949524\n",
      "grad W_out: 0.3067077100276947\n",
      "grad W: 5.158614635467529\n",
      "grad W_in: 2.7211685180664062\n",
      "[ep 9 | batch 36] loss=0.9095 |M| |W| mean=2.274e-02\n",
      "ep 9 batch 36 | loss=0.9095 | alloc=0.57GB reserv=26.73GB\n",
      "grad W_out: 0.30580830574035645\n",
      "grad W: 3.3592846393585205\n",
      "grad W_in: 2.5818111896514893\n",
      "grad W_out: 0.21490491926670074\n",
      "grad W: 5.454151630401611\n",
      "grad W_in: 3.285659074783325\n",
      "[ep 9 | batch 38] loss=0.8787 |M| |W| mean=2.275e-02\n",
      "ep 9 batch 38 | loss=0.8787 | alloc=0.57GB reserv=26.73GB\n",
      "grad W_out: 0.19064606726169586\n",
      "grad W: 2.5159778594970703\n",
      "grad W_in: 1.7543175220489502\n",
      "grad W_out: 0.25652533769607544\n",
      "grad W: 2.7199959754943848\n",
      "grad W_in: 2.247791290283203\n",
      "[ep 9 | batch 40] loss=0.8150 |M| |W| mean=2.276e-02\n",
      "ep 9 batch 40 | loss=0.8150 | alloc=0.57GB reserv=26.73GB\n",
      "grad W_out: 0.2246832698583603\n",
      "grad W: 5.012503147125244\n",
      "grad W_in: 4.189037799835205\n",
      "grad W_out: 0.27753588557243347\n",
      "grad W: 4.018922805786133\n",
      "grad W_in: 1.06324303150177\n",
      "[ep 9 | batch 42] loss=0.9097 |M| |W| mean=2.276e-02\n",
      "ep 9 batch 42 | loss=0.9097 | alloc=0.57GB reserv=26.73GB\n",
      "grad W_out: 0.2727895677089691\n",
      "grad W: 2.82521390914917\n",
      "grad W_in: 1.8706015348434448\n",
      "grad W_out: 0.2484196573495865\n",
      "grad W: 5.026660919189453\n",
      "grad W_in: 2.803126573562622\n",
      "[ep 9 | batch 44] loss=0.9191 |M| |W| mean=2.277e-02\n",
      "ep 9 batch 44 | loss=0.9191 | alloc=0.57GB reserv=26.73GB\n",
      "grad W_out: 0.27737316489219666\n",
      "grad W: 4.189892768859863\n",
      "grad W_in: 1.3203701972961426\n",
      "grad W_out: 0.377907931804657\n",
      "grad W: 7.3880438804626465\n",
      "grad W_in: 3.9789140224456787\n",
      "[ep 9 | batch 46] loss=0.8900 |M| |W| mean=2.278e-02\n",
      "ep 9 batch 46 | loss=0.8900 | alloc=0.58GB reserv=26.74GB\n",
      "grad W_out: 0.303426593542099\n",
      "grad W: 4.887868881225586\n",
      "grad W_in: 4.172585964202881\n",
      "grad W_out: 0.26419368386268616\n",
      "grad W: 12.530391693115234\n",
      "grad W_in: 5.454733371734619\n",
      "[ep 9 | batch 48] loss=0.9198 |M| |W| mean=2.279e-02\n",
      "ep 9 batch 48 | loss=0.9198 | alloc=0.58GB reserv=26.74GB\n",
      "grad W_out: 0.3520186245441437\n",
      "grad W: 6.580469608306885\n",
      "grad W_in: 3.790766716003418\n",
      "grad W_out: 0.42379894852638245\n",
      "grad W: 8.980576515197754\n",
      "grad W_in: 6.717215061187744\n",
      "[ep 9 | batch 50] loss=1.0052 |M| |W| mean=2.279e-02\n",
      "ep 9 batch 50 | loss=1.0052 | alloc=0.58GB reserv=26.74GB\n",
      "grad W_out: 0.30436012148857117\n",
      "grad W: 3.7594943046569824\n",
      "grad W_in: 2.11789608001709\n",
      "grad W_out: 0.36535176634788513\n",
      "grad W: 8.638318061828613\n",
      "grad W_in: 8.809341430664062\n",
      "[ep 9 | batch 52] loss=0.9241 |M| |W| mean=2.279e-02\n",
      "ep 9 batch 52 | loss=0.9241 | alloc=0.58GB reserv=26.74GB\n",
      "grad W_out: 0.3224698305130005\n",
      "grad W: 6.187435150146484\n",
      "grad W_in: 2.716904401779175\n",
      "grad W_out: 0.3820141553878784\n",
      "grad W: 5.0065016746521\n",
      "grad W_in: 2.7261195182800293\n",
      "[ep 9 | batch 54] loss=0.9397 |M| |W| mean=2.280e-02\n",
      "ep 9 batch 54 | loss=0.9397 | alloc=0.58GB reserv=26.74GB\n",
      "grad W_out: 0.5223495364189148\n",
      "grad W: 24.97877311706543\n",
      "grad W_in: 5.314157962799072\n",
      "grad W_out: 0.3070242702960968\n",
      "grad W: 6.730926990509033\n",
      "grad W_in: 1.7518681287765503\n",
      "[ep 9 | batch 56] loss=0.8734 |M| |W| mean=2.282e-02\n",
      "ep 9 batch 56 | loss=0.8734 | alloc=0.58GB reserv=26.75GB\n",
      "grad W_out: 0.6175632476806641\n",
      "grad W: 10.87600326538086\n",
      "grad W_in: 7.660590648651123\n",
      "grad W_out: 0.4081408679485321\n",
      "grad W: 5.926211833953857\n",
      "grad W_in: 6.184081554412842\n",
      "[ep 9 | batch 58] loss=0.9029 |M| |W| mean=2.283e-02\n",
      "ep 9 batch 58 | loss=0.9029 | alloc=0.58GB reserv=26.75GB\n",
      "grad W_out: 0.3886905312538147\n",
      "grad W: 7.14608097076416\n",
      "grad W_in: 4.783783912658691\n",
      "grad W_out: 0.5122929215431213\n",
      "grad W: 8.930597305297852\n",
      "grad W_in: 6.6158342361450195\n",
      "[ep 9 | batch 60] loss=0.9556 |M| |W| mean=2.283e-02\n",
      "ep 9 batch 60 | loss=0.9556 | alloc=0.59GB reserv=26.75GB\n",
      "grad W_out: 0.40764492750167847\n",
      "grad W: 5.127493381500244\n",
      "grad W_in: 3.6649413108825684\n",
      "grad W_out: 0.37427419424057007\n",
      "grad W: 5.926740646362305\n",
      "grad W_in: 1.9155120849609375\n",
      "[ep 9 | batch 62] loss=0.9451 |M| |W| mean=2.284e-02\n",
      "ep 9 batch 62 | loss=0.9451 | alloc=0.59GB reserv=26.75GB\n",
      "grad W_out: 0.3739529550075531\n",
      "grad W: 6.102798938751221\n",
      "grad W_in: 3.056699752807617\n",
      "grad W_out: 0.3892251253128052\n",
      "grad W: 6.1586456298828125\n",
      "grad W_in: 1.8592170476913452\n",
      "[ep 9 | batch 64] loss=0.9233 |M| |W| mean=2.285e-02\n",
      "ep 9 batch 64 | loss=0.9233 | alloc=0.59GB reserv=26.75GB\n",
      "grad W_out: 0.3682232201099396\n",
      "grad W: 5.699604511260986\n",
      "grad W_in: 1.1638245582580566\n",
      "grad W_out: 0.3153192102909088\n",
      "grad W: 6.0630717277526855\n",
      "grad W_in: 2.8957128524780273\n",
      "[ep 9 | batch 66] loss=0.8812 |M| |W| mean=2.286e-02\n",
      "ep 9 batch 66 | loss=0.8812 | alloc=0.59GB reserv=26.76GB\n",
      "grad W_out: 0.3062439262866974\n",
      "grad W: 4.350405693054199\n",
      "grad W_in: 1.362464189529419\n",
      "grad W_out: 0.48628944158554077\n",
      "grad W: 7.825454235076904\n",
      "grad W_in: 6.273757457733154\n",
      "[ep 9 | batch 68] loss=0.9399 |M| |W| mean=2.287e-02\n",
      "ep 9 batch 68 | loss=0.9399 | alloc=0.59GB reserv=26.76GB\n",
      "grad W_out: 0.44503262639045715\n",
      "grad W: 5.813626289367676\n",
      "grad W_in: 1.5261577367782593\n",
      "grad W_out: 0.46049007773399353\n",
      "grad W: 7.541934490203857\n",
      "grad W_in: 7.543442726135254\n",
      "[ep 9 | batch 70] loss=0.9375 |M| |W| mean=2.287e-02\n",
      "ep 9 batch 70 | loss=0.9375 | alloc=0.59GB reserv=26.76GB\n",
      "grad W_out: 0.29210054874420166\n",
      "grad W: 6.63676643371582\n",
      "grad W_in: 4.345373630523682\n",
      "grad W_out: 0.3465680181980133\n",
      "grad W: 5.476202964782715\n",
      "grad W_in: 3.9838201999664307\n",
      "[ep 9 | batch 72] loss=0.8878 |M| |W| mean=2.287e-02\n",
      "ep 9 batch 72 | loss=0.8878 | alloc=0.59GB reserv=26.76GB\n",
      "grad W_out: 0.5560838580131531\n",
      "grad W: 10.00181770324707\n",
      "grad W_in: 6.053645133972168\n",
      "grad W_out: 0.43262091279029846\n",
      "grad W: 6.5125532150268555\n",
      "grad W_in: 1.6701078414916992\n",
      "[ep 9 | batch 74] loss=0.9025 |M| |W| mean=2.289e-02\n",
      "ep 9 batch 74 | loss=0.9025 | alloc=0.60GB reserv=26.76GB\n",
      "grad W_out: 0.44124075770378113\n",
      "grad W: 9.032196044921875\n",
      "grad W_in: 5.918308734893799\n",
      "grad W_out: 0.35297635197639465\n",
      "grad W: 4.709427356719971\n",
      "grad W_in: 4.25745153427124\n",
      "[ep 9 | batch 76] loss=0.8594 |M| |W| mean=2.289e-02\n",
      "ep 9 batch 76 | loss=0.8594 | alloc=0.60GB reserv=26.77GB\n",
      "grad W_out: 0.3200976550579071\n",
      "grad W: 6.855072021484375\n",
      "grad W_in: 1.9251216650009155\n",
      "grad W_out: 0.3717931807041168\n",
      "grad W: 6.748734951019287\n",
      "grad W_in: 2.5544204711914062\n",
      "[ep 9 | batch 78] loss=0.9151 |M| |W| mean=2.290e-02\n",
      "ep 9 batch 78 | loss=0.9151 | alloc=0.60GB reserv=26.77GB\n",
      "grad W_out: 0.33536142110824585\n",
      "grad W: 5.025709629058838\n",
      "grad W_in: 3.822972536087036\n",
      "grad W_out: 0.2615082263946533\n",
      "grad W: 4.101675987243652\n",
      "grad W_in: 1.4823360443115234\n",
      "[ep 9 | batch 80] loss=0.9099 |M| |W| mean=2.291e-02\n",
      "ep 9 batch 80 | loss=0.9099 | alloc=0.60GB reserv=26.77GB\n",
      "grad W_out: 0.22714468836784363\n",
      "grad W: 5.867306709289551\n",
      "grad W_in: 3.9469101428985596\n",
      "grad W_out: 0.27918770909309387\n",
      "grad W: 5.459717273712158\n",
      "grad W_in: 1.3302158117294312\n",
      "[ep 9 | batch 82] loss=0.9051 |M| |W| mean=2.291e-02\n",
      "ep 9 batch 82 | loss=0.9051 | alloc=0.60GB reserv=26.77GB\n",
      "grad W_out: 0.33060377836227417\n",
      "grad W: 6.067924976348877\n",
      "grad W_in: 2.7652807235717773\n",
      "grad W_out: 0.2736383378505707\n",
      "grad W: 5.554422855377197\n",
      "grad W_in: 3.1192073822021484\n",
      "[ep 9 | batch 84] loss=0.9310 |M| |W| mean=2.291e-02\n",
      "ep 9 batch 84 | loss=0.9310 | alloc=0.60GB reserv=26.77GB\n",
      "grad W_out: 0.3036399483680725\n",
      "grad W: 5.969311714172363\n",
      "grad W_in: 1.6928622722625732\n",
      "grad W_out: 0.38455575704574585\n",
      "grad W: 5.540627479553223\n",
      "grad W_in: 3.9975392818450928\n",
      "[ep 9 | batch 86] loss=0.8697 |M| |W| mean=2.292e-02\n",
      "ep 9 batch 86 | loss=0.8697 | alloc=0.60GB reserv=26.78GB\n",
      "grad W_out: 0.3299282491207123\n",
      "grad W: 5.924708843231201\n",
      "grad W_in: 1.9036318063735962\n",
      "grad W_out: 0.31455379724502563\n",
      "grad W: 6.214205265045166\n",
      "grad W_in: 4.828514575958252\n",
      "[ep 9 | batch 88] loss=0.8732 |M| |W| mean=2.293e-02\n",
      "ep 9 batch 88 | loss=0.8732 | alloc=0.61GB reserv=26.78GB\n",
      "grad W_out: 0.27310308814048767\n",
      "grad W: 2.913909912109375\n",
      "grad W_in: 1.1432254314422607\n",
      "grad W_out: 0.3742320239543915\n",
      "grad W: 6.146341800689697\n",
      "grad W_in: 3.2397046089172363\n",
      "[ep 9 | batch 90] loss=0.8899 |M| |W| mean=2.293e-02\n",
      "ep 9 batch 90 | loss=0.8899 | alloc=0.61GB reserv=26.78GB\n",
      "grad W_out: 0.41218605637550354\n",
      "grad W: 4.645203590393066\n",
      "grad W_in: 0.5844120979309082\n",
      "grad W_out: 0.4338172674179077\n",
      "grad W: 7.316224098205566\n",
      "grad W_in: 3.480872392654419\n",
      "[ep 9 | batch 92] loss=0.8519 |M| |W| mean=2.294e-02\n",
      "ep 9 batch 92 | loss=0.8519 | alloc=0.61GB reserv=26.78GB\n",
      "grad W_out: 0.38746708631515503\n",
      "grad W: 3.8514657020568848\n",
      "grad W_in: 2.2002251148223877\n",
      "Epoch 09/20 | lr=1.00e-03 | loss=0.9214 | test_acc=70.98% | dt=1176.9s\n",
      "grad W_out: 0.362330824136734\n",
      "grad W: 6.82894229888916\n",
      "grad W_in: 4.846632957458496\n",
      "[ep 10 | batch 0] loss=0.9083 |M| |W| mean=2.294e-02\n",
      "ep 10 batch 0 | loss=0.9083 | alloc=0.61GB reserv=26.78GB\n",
      "grad W_out: 0.46045300364494324\n",
      "grad W: 5.915275573730469\n",
      "grad W_in: 2.5466256141662598\n",
      "grad W_out: 0.3015700876712799\n",
      "grad W: 4.919917583465576\n",
      "grad W_in: 2.9531962871551514\n",
      "[ep 10 | batch 2] loss=0.8334 |M| |W| mean=2.295e-02\n",
      "ep 10 batch 2 | loss=0.8334 | alloc=0.61GB reserv=26.79GB\n",
      "grad W_out: 0.3148624897003174\n",
      "grad W: 3.609142780303955\n",
      "grad W_in: 2.815699577331543\n",
      "grad W_out: 0.24133844673633575\n",
      "grad W: 3.0847537517547607\n",
      "grad W_in: 0.8756563663482666\n",
      "[ep 10 | batch 4] loss=0.8273 |M| |W| mean=2.296e-02\n",
      "ep 10 batch 4 | loss=0.8273 | alloc=0.61GB reserv=26.79GB\n",
      "grad W_out: 0.4347168207168579\n",
      "grad W: 7.909987449645996\n",
      "grad W_in: 4.521938323974609\n",
      "grad W_out: 0.4930883049964905\n",
      "grad W: 7.536480903625488\n",
      "grad W_in: 4.90496301651001\n",
      "[ep 10 | batch 6] loss=0.9444 |M| |W| mean=2.297e-02\n",
      "ep 10 batch 6 | loss=0.9444 | alloc=0.61GB reserv=26.79GB\n",
      "grad W_out: 0.3795095980167389\n",
      "grad W: 4.753475666046143\n",
      "grad W_in: 0.9990475177764893\n",
      "grad W_out: 0.26703494787216187\n",
      "grad W: 4.411334991455078\n",
      "grad W_in: 2.210620403289795\n",
      "[ep 10 | batch 8] loss=0.8370 |M| |W| mean=2.297e-02\n",
      "ep 10 batch 8 | loss=0.8370 | alloc=0.62GB reserv=26.79GB\n",
      "grad W_out: 0.29519277811050415\n",
      "grad W: 4.478392124176025\n",
      "grad W_in: 2.338092565536499\n",
      "grad W_out: 0.32530641555786133\n",
      "grad W: 3.34808349609375\n",
      "grad W_in: 0.9395694732666016\n",
      "[ep 10 | batch 10] loss=0.8207 |M| |W| mean=2.298e-02\n",
      "ep 10 batch 10 | loss=0.8207 | alloc=0.62GB reserv=26.79GB\n",
      "grad W_out: 0.3598003685474396\n",
      "grad W: 4.198870658874512\n",
      "grad W_in: 0.7388524413108826\n",
      "grad W_out: 0.1780449002981186\n",
      "grad W: 2.6606850624084473\n",
      "grad W_in: 0.2566338777542114\n",
      "[ep 10 | batch 12] loss=0.8856 |M| |W| mean=2.298e-02\n",
      "ep 10 batch 12 | loss=0.8856 | alloc=0.62GB reserv=26.79GB\n",
      "grad W_out: 0.29475998878479004\n",
      "grad W: 4.2854743003845215\n",
      "grad W_in: 1.6874281167984009\n",
      "grad W_out: 0.32834070920944214\n",
      "grad W: 4.265851020812988\n",
      "grad W_in: 1.3312079906463623\n",
      "[ep 10 | batch 14] loss=0.8688 |M| |W| mean=2.298e-02\n",
      "ep 10 batch 14 | loss=0.8688 | alloc=0.62GB reserv=26.80GB\n",
      "grad W_out: 0.32891714572906494\n",
      "grad W: 3.3445935249328613\n",
      "grad W_in: 0.6148326992988586\n",
      "grad W_out: 0.3230951726436615\n",
      "grad W: 3.892554521560669\n",
      "grad W_in: 1.7811936140060425\n",
      "[ep 10 | batch 16] loss=0.8250 |M| |W| mean=2.298e-02\n",
      "ep 10 batch 16 | loss=0.8250 | alloc=0.62GB reserv=26.80GB\n",
      "grad W_out: 0.29380786418914795\n",
      "grad W: 6.663208484649658\n",
      "grad W_in: 1.6430511474609375\n",
      "grad W_out: 0.24412399530410767\n",
      "grad W: 3.37196946144104\n",
      "grad W_in: 0.5258773565292358\n",
      "[ep 10 | batch 18] loss=0.8492 |M| |W| mean=2.299e-02\n",
      "ep 10 batch 18 | loss=0.8492 | alloc=0.62GB reserv=26.80GB\n",
      "grad W_out: 0.2830510139465332\n",
      "grad W: 5.015523433685303\n",
      "grad W_in: 3.1942951679229736\n",
      "grad W_out: 0.34262362122535706\n",
      "grad W: 5.415910720825195\n",
      "grad W_in: 4.018460273742676\n",
      "[ep 10 | batch 20] loss=0.8568 |M| |W| mean=2.301e-02\n",
      "ep 10 batch 20 | loss=0.8568 | alloc=0.62GB reserv=26.80GB\n",
      "grad W_out: 0.2757931649684906\n",
      "grad W: 2.903123617172241\n",
      "grad W_in: 1.8405632972717285\n",
      "grad W_out: 0.27792128920555115\n",
      "grad W: 4.1964216232299805\n",
      "grad W_in: 2.8806262016296387\n",
      "[ep 10 | batch 22] loss=0.8295 |M| |W| mean=2.302e-02\n",
      "ep 10 batch 22 | loss=0.8295 | alloc=0.63GB reserv=26.80GB\n",
      "grad W_out: 0.34391194581985474\n",
      "grad W: 7.755492687225342\n",
      "grad W_in: 5.405616760253906\n",
      "grad W_out: 0.26103368401527405\n",
      "grad W: 3.7016165256500244\n",
      "grad W_in: 2.7622578144073486\n",
      "[ep 10 | batch 24] loss=0.8832 |M| |W| mean=2.303e-02\n",
      "ep 10 batch 24 | loss=0.8832 | alloc=0.63GB reserv=26.81GB\n",
      "grad W_out: 0.243626207113266\n",
      "grad W: 5.8316850662231445\n",
      "grad W_in: 4.3011956214904785\n",
      "grad W_out: 0.2506256401538849\n",
      "grad W: 4.694884300231934\n",
      "grad W_in: 1.1861757040023804\n",
      "[ep 10 | batch 26] loss=0.8852 |M| |W| mean=2.303e-02\n",
      "ep 10 batch 26 | loss=0.8852 | alloc=0.63GB reserv=26.81GB\n",
      "grad W_out: 0.17857833206653595\n",
      "grad W: 9.919916152954102\n",
      "grad W_in: 9.709920883178711\n",
      "grad W_out: 0.3526555001735687\n",
      "grad W: 6.0634918212890625\n",
      "grad W_in: 1.8957436084747314\n",
      "[ep 10 | batch 28] loss=0.8518 |M| |W| mean=2.304e-02\n",
      "ep 10 batch 28 | loss=0.8518 | alloc=0.63GB reserv=26.81GB\n",
      "grad W_out: 0.49937036633491516\n",
      "grad W: 8.350765228271484\n",
      "grad W_in: 7.220022201538086\n",
      "grad W_out: 0.3323647975921631\n",
      "grad W: 5.518087863922119\n",
      "grad W_in: 1.1479016542434692\n",
      "[ep 10 | batch 30] loss=0.8714 |M| |W| mean=2.305e-02\n",
      "ep 10 batch 30 | loss=0.8714 | alloc=0.63GB reserv=26.81GB\n",
      "grad W_out: 0.265445739030838\n",
      "grad W: 6.320657730102539\n",
      "grad W_in: 1.7745389938354492\n",
      "grad W_out: 0.4128657877445221\n",
      "grad W: 5.662917613983154\n",
      "grad W_in: 2.5159828662872314\n",
      "[ep 10 | batch 32] loss=0.8957 |M| |W| mean=2.306e-02\n",
      "ep 10 batch 32 | loss=0.8957 | alloc=0.63GB reserv=26.81GB\n",
      "grad W_out: 0.5144035220146179\n",
      "grad W: 9.807709693908691\n",
      "grad W_in: 5.802624225616455\n",
      "grad W_out: 0.32592830061912537\n",
      "grad W: 4.915639877319336\n",
      "grad W_in: 2.2208058834075928\n",
      "[ep 10 | batch 34] loss=0.8723 |M| |W| mean=2.306e-02\n",
      "ep 10 batch 34 | loss=0.8723 | alloc=0.63GB reserv=26.82GB\n",
      "grad W_out: 0.4137546122074127\n",
      "grad W: 9.799479484558105\n",
      "grad W_in: 5.155003547668457\n",
      "grad W_out: 0.28998681902885437\n",
      "grad W: 6.68071985244751\n",
      "grad W_in: 2.517930030822754\n",
      "[ep 10 | batch 36] loss=0.8784 |M| |W| mean=2.307e-02\n",
      "ep 10 batch 36 | loss=0.8784 | alloc=0.64GB reserv=26.82GB\n",
      "grad W_out: 0.24538899958133698\n",
      "grad W: 3.0481064319610596\n",
      "grad W_in: 1.676525354385376\n",
      "grad W_out: 0.29628172516822815\n",
      "grad W: 5.8264946937561035\n",
      "grad W_in: 2.781245231628418\n",
      "[ep 10 | batch 38] loss=0.7849 |M| |W| mean=2.307e-02\n",
      "ep 10 batch 38 | loss=0.7849 | alloc=0.64GB reserv=26.82GB\n",
      "grad W_out: 0.4717552065849304\n",
      "grad W: 6.6686015129089355\n",
      "grad W_in: 2.445708751678467\n",
      "grad W_out: 0.42778804898262024\n",
      "grad W: 5.8513288497924805\n",
      "grad W_in: 4.685940265655518\n",
      "[ep 10 | batch 40] loss=0.8966 |M| |W| mean=2.308e-02\n",
      "ep 10 batch 40 | loss=0.8966 | alloc=0.64GB reserv=26.82GB\n",
      "grad W_out: 0.23032666742801666\n",
      "grad W: 3.277100086212158\n",
      "grad W_in: 2.7412919998168945\n",
      "grad W_out: 0.4216413199901581\n",
      "grad W: 6.471597671508789\n",
      "grad W_in: 4.490166664123535\n",
      "[ep 10 | batch 42] loss=0.8697 |M| |W| mean=2.309e-02\n",
      "ep 10 batch 42 | loss=0.8697 | alloc=0.64GB reserv=26.82GB\n",
      "grad W_out: 0.3802763819694519\n",
      "grad W: 3.251220703125\n",
      "grad W_in: 1.385886549949646\n",
      "grad W_out: 0.3057529330253601\n",
      "grad W: 5.362690448760986\n",
      "grad W_in: 4.247918128967285\n",
      "[ep 10 | batch 44] loss=0.9001 |M| |W| mean=2.309e-02\n",
      "ep 10 batch 44 | loss=0.9001 | alloc=0.64GB reserv=26.83GB\n",
      "grad W_out: 0.2843725085258484\n",
      "grad W: 4.258244514465332\n",
      "grad W_in: 2.5889761447906494\n",
      "grad W_out: 0.18875525891780853\n",
      "grad W: 2.99356746673584\n",
      "grad W_in: 1.3502708673477173\n",
      "[ep 10 | batch 46] loss=0.8410 |M| |W| mean=2.309e-02\n",
      "ep 10 batch 46 | loss=0.8410 | alloc=0.64GB reserv=26.83GB\n",
      "grad W_out: 0.30352407693862915\n",
      "grad W: 7.5866804122924805\n",
      "grad W_in: 6.28236722946167\n",
      "grad W_out: 0.3571707606315613\n",
      "grad W: 2.428386688232422\n",
      "grad W_in: 0.4381387233734131\n",
      "[ep 10 | batch 48] loss=0.7922 |M| |W| mean=2.310e-02\n",
      "ep 10 batch 48 | loss=0.7922 | alloc=0.64GB reserv=26.83GB\n",
      "grad W_out: 0.4023915231227875\n",
      "grad W: 6.969520092010498\n",
      "grad W_in: 6.1592559814453125\n",
      "grad W_out: 0.2997901141643524\n",
      "grad W: 4.622798919677734\n",
      "grad W_in: 0.7995041012763977\n",
      "[ep 10 | batch 50] loss=0.8180 |M| |W| mean=2.311e-02\n",
      "ep 10 batch 50 | loss=0.8180 | alloc=0.65GB reserv=26.83GB\n",
      "grad W_out: 0.3732510805130005\n",
      "grad W: 5.757620811462402\n",
      "grad W_in: 4.916918754577637\n",
      "grad W_out: 0.3639894425868988\n",
      "grad W: 3.5488126277923584\n",
      "grad W_in: 2.1655542850494385\n",
      "[ep 10 | batch 52] loss=0.8247 |M| |W| mean=2.311e-02\n",
      "ep 10 batch 52 | loss=0.8247 | alloc=0.65GB reserv=26.83GB\n",
      "grad W_out: 0.23385663330554962\n",
      "grad W: 3.367584228515625\n",
      "grad W_in: 2.2964401245117188\n",
      "grad W_out: 0.4277758300304413\n",
      "grad W: 6.698676586151123\n",
      "grad W_in: 2.2742838859558105\n",
      "[ep 10 | batch 54] loss=0.9034 |M| |W| mean=2.311e-02\n",
      "ep 10 batch 54 | loss=0.9034 | alloc=0.65GB reserv=26.84GB\n",
      "grad W_out: 0.17540785670280457\n",
      "grad W: 3.1162734031677246\n",
      "grad W_in: 2.8553402423858643\n",
      "grad W_out: 0.292359322309494\n",
      "grad W: 4.894229888916016\n",
      "grad W_in: 3.488292694091797\n",
      "[ep 10 | batch 56] loss=0.8875 |M| |W| mean=2.312e-02\n",
      "ep 10 batch 56 | loss=0.8875 | alloc=0.65GB reserv=26.84GB\n",
      "grad W_out: 0.24881373345851898\n",
      "grad W: 2.971740245819092\n",
      "grad W_in: 2.2620649337768555\n",
      "grad W_out: 0.2230345755815506\n",
      "grad W: 2.2976276874542236\n",
      "grad W_in: 0.9062536358833313\n",
      "[ep 10 | batch 58] loss=0.8161 |M| |W| mean=2.312e-02\n",
      "ep 10 batch 58 | loss=0.8161 | alloc=0.65GB reserv=26.84GB\n",
      "grad W_out: 0.26875677704811096\n",
      "grad W: 4.285991668701172\n",
      "grad W_in: 1.896181583404541\n",
      "grad W_out: 0.31098541617393494\n",
      "grad W: 4.675012111663818\n",
      "grad W_in: 3.3880839347839355\n",
      "[ep 10 | batch 60] loss=0.7893 |M| |W| mean=2.313e-02\n",
      "ep 10 batch 60 | loss=0.7893 | alloc=0.65GB reserv=26.84GB\n",
      "grad W_out: 0.24828459322452545\n",
      "grad W: 4.512214660644531\n",
      "grad W_in: 3.026021718978882\n",
      "grad W_out: 0.27461516857147217\n",
      "grad W: 4.972294330596924\n",
      "grad W_in: 4.476285934448242\n",
      "[ep 10 | batch 62] loss=0.8250 |M| |W| mean=2.314e-02\n",
      "ep 10 batch 62 | loss=0.8250 | alloc=0.65GB reserv=26.84GB\n",
      "grad W_out: 0.296179860830307\n",
      "grad W: 7.984828472137451\n",
      "grad W_in: 5.454193592071533\n",
      "grad W_out: 0.3805815577507019\n",
      "grad W: 5.340978622436523\n",
      "grad W_in: 2.391376256942749\n",
      "[ep 10 | batch 64] loss=0.9105 |M| |W| mean=2.314e-02\n",
      "ep 10 batch 64 | loss=0.9105 | alloc=0.66GB reserv=26.85GB\n",
      "grad W_out: 0.33855700492858887\n",
      "grad W: 5.456855297088623\n",
      "grad W_in: 4.497664451599121\n",
      "grad W_out: 0.296239972114563\n",
      "grad W: 3.767019510269165\n",
      "grad W_in: 1.5005196332931519\n",
      "[ep 10 | batch 66] loss=0.8578 |M| |W| mean=2.314e-02\n",
      "ep 10 batch 66 | loss=0.8578 | alloc=0.66GB reserv=26.85GB\n",
      "grad W_out: 0.34479713439941406\n",
      "grad W: 7.312414169311523\n",
      "grad W_in: 4.205368995666504\n",
      "grad W_out: 0.1366400569677353\n",
      "grad W: 3.505164861679077\n",
      "grad W_in: 3.0207231044769287\n",
      "[ep 10 | batch 68] loss=0.7972 |M| |W| mean=2.315e-02\n",
      "ep 10 batch 68 | loss=0.7972 | alloc=0.66GB reserv=26.85GB\n",
      "grad W_out: 0.2971917986869812\n",
      "grad W: 3.4011709690093994\n",
      "grad W_in: 0.6269762516021729\n",
      "grad W_out: 0.2741464376449585\n",
      "grad W: 4.946772575378418\n",
      "grad W_in: 4.1062703132629395\n",
      "[ep 10 | batch 70] loss=0.7901 |M| |W| mean=2.315e-02\n",
      "ep 10 batch 70 | loss=0.7901 | alloc=0.66GB reserv=26.85GB\n",
      "grad W_out: 0.2072220742702484\n",
      "grad W: 4.29042387008667\n",
      "grad W_in: 2.5277762413024902\n",
      "grad W_out: 0.3743603825569153\n",
      "grad W: 7.49505090713501\n",
      "grad W_in: 6.444052696228027\n",
      "[ep 10 | batch 72] loss=0.7587 |M| |W| mean=2.315e-02\n",
      "ep 10 batch 72 | loss=0.7587 | alloc=0.66GB reserv=26.85GB\n",
      "grad W_out: 0.33804476261138916\n",
      "grad W: 4.397567272186279\n",
      "grad W_in: 1.8064470291137695\n",
      "grad W_out: 0.35102665424346924\n",
      "grad W: 9.307266235351562\n",
      "grad W_in: 6.275975227355957\n",
      "[ep 10 | batch 74] loss=0.8554 |M| |W| mean=2.316e-02\n",
      "ep 10 batch 74 | loss=0.8554 | alloc=0.66GB reserv=26.86GB\n",
      "grad W_out: 0.26562178134918213\n",
      "grad W: 7.498979091644287\n",
      "grad W_in: 1.7356055974960327\n",
      "grad W_out: 0.3429581820964813\n",
      "grad W: 5.621803283691406\n",
      "grad W_in: 4.406790256500244\n",
      "[ep 10 | batch 76] loss=0.8767 |M| |W| mean=2.317e-02\n",
      "ep 10 batch 76 | loss=0.8767 | alloc=0.66GB reserv=26.86GB\n",
      "grad W_out: 0.24593105912208557\n",
      "grad W: 4.166618824005127\n",
      "grad W_in: 1.3171812295913696\n",
      "grad W_out: 0.22563844919204712\n",
      "grad W: 2.360159397125244\n",
      "grad W_in: 1.3338218927383423\n",
      "[ep 10 | batch 78] loss=0.7692 |M| |W| mean=2.317e-02\n",
      "ep 10 batch 78 | loss=0.7692 | alloc=0.67GB reserv=26.86GB\n",
      "grad W_out: 0.20862649381160736\n",
      "grad W: 3.3829658031463623\n",
      "grad W_in: 2.992004156112671\n",
      "grad W_out: 0.219753697514534\n",
      "grad W: 3.731071949005127\n",
      "grad W_in: 3.1913864612579346\n",
      "[ep 10 | batch 80] loss=0.7836 |M| |W| mean=2.318e-02\n",
      "ep 10 batch 80 | loss=0.7836 | alloc=0.67GB reserv=26.86GB\n",
      "grad W_out: 0.2640587389469147\n",
      "grad W: 3.2822353839874268\n",
      "grad W_in: 2.0140016078948975\n",
      "grad W_out: 0.17591680586338043\n",
      "grad W: 4.460228443145752\n",
      "grad W_in: 2.6181929111480713\n",
      "[ep 10 | batch 82] loss=0.8140 |M| |W| mean=2.319e-02\n",
      "ep 10 batch 82 | loss=0.8140 | alloc=0.67GB reserv=26.86GB\n",
      "grad W_out: 0.2479284256696701\n",
      "grad W: 2.343848466873169\n",
      "grad W_in: 0.7297651767730713\n",
      "grad W_out: 0.19395464658737183\n",
      "grad W: 4.861005783081055\n",
      "grad W_in: 2.7885074615478516\n",
      "[ep 10 | batch 84] loss=0.8353 |M| |W| mean=2.320e-02\n",
      "ep 10 batch 84 | loss=0.8353 | alloc=0.67GB reserv=26.87GB\n",
      "grad W_out: 0.2670316994190216\n",
      "grad W: 5.265509605407715\n",
      "grad W_in: 1.6911327838897705\n",
      "grad W_out: 0.28438982367515564\n",
      "grad W: 5.55311918258667\n",
      "grad W_in: 5.201100826263428\n",
      "[ep 10 | batch 86] loss=0.8200 |M| |W| mean=2.321e-02\n",
      "ep 10 batch 86 | loss=0.8200 | alloc=0.67GB reserv=26.87GB\n",
      "grad W_out: 0.3276718854904175\n",
      "grad W: 5.702216148376465\n",
      "grad W_in: 2.0860729217529297\n",
      "grad W_out: 0.29948893189430237\n",
      "grad W: 4.172194957733154\n",
      "grad W_in: 2.483043670654297\n",
      "[ep 10 | batch 88] loss=0.7504 |M| |W| mean=2.321e-02\n",
      "ep 10 batch 88 | loss=0.7504 | alloc=0.67GB reserv=26.87GB\n",
      "grad W_out: 0.26215121150016785\n",
      "grad W: 4.693972587585449\n",
      "grad W_in: 3.2517809867858887\n",
      "grad W_out: 0.2038928121328354\n",
      "grad W: 3.6833159923553467\n",
      "grad W_in: 0.9835695028305054\n",
      "[ep 10 | batch 90] loss=0.8270 |M| |W| mean=2.322e-02\n",
      "ep 10 batch 90 | loss=0.8270 | alloc=0.67GB reserv=26.87GB\n",
      "grad W_out: 0.19584646821022034\n",
      "grad W: 6.6268815994262695\n",
      "grad W_in: 4.458873271942139\n",
      "grad W_out: 0.32044896483421326\n",
      "grad W: 3.931389808654785\n",
      "grad W_in: 3.8153820037841797\n",
      "[ep 10 | batch 92] loss=0.7846 |M| |W| mean=2.322e-02\n",
      "ep 10 batch 92 | loss=0.7846 | alloc=0.68GB reserv=26.87GB\n",
      "grad W_out: 0.26672059297561646\n",
      "grad W: 5.237081050872803\n",
      "grad W_in: 3.964668035507202\n",
      "Epoch 10/20 | lr=1.00e-03 | loss=0.8430 | test_acc=72.77% | dt=1176.9s\n",
      "grad W_out: 0.35348013043403625\n",
      "grad W: 7.981461524963379\n",
      "grad W_in: 5.929897308349609\n",
      "[ep 11 | batch 0] loss=0.8297 |M| |W| mean=2.322e-02\n",
      "ep 11 batch 0 | loss=0.8297 | alloc=0.68GB reserv=26.88GB\n",
      "grad W_out: 0.23809286952018738\n",
      "grad W: 4.433988571166992\n",
      "grad W_in: 2.1102216243743896\n",
      "grad W_out: 0.3189380168914795\n",
      "grad W: 8.942827224731445\n",
      "grad W_in: 8.331199645996094\n",
      "[ep 11 | batch 2] loss=0.8032 |M| |W| mean=2.323e-02\n",
      "ep 11 batch 2 | loss=0.8032 | alloc=0.68GB reserv=26.88GB\n",
      "grad W_out: 0.373742938041687\n",
      "grad W: 5.778466701507568\n",
      "grad W_in: 1.504983901977539\n",
      "grad W_out: 0.29448074102401733\n",
      "grad W: 7.386670112609863\n",
      "grad W_in: 7.481383323669434\n",
      "[ep 11 | batch 4] loss=0.7936 |M| |W| mean=2.323e-02\n",
      "ep 11 batch 4 | loss=0.7936 | alloc=0.68GB reserv=26.88GB\n",
      "grad W_out: 0.2795245349407196\n",
      "grad W: 3.5646820068359375\n",
      "grad W_in: 1.1930537223815918\n",
      "grad W_out: 0.3433825373649597\n",
      "grad W: 8.54250431060791\n",
      "grad W_in: 6.850266933441162\n",
      "[ep 11 | batch 6] loss=0.8509 |M| |W| mean=2.323e-02\n",
      "ep 11 batch 6 | loss=0.8509 | alloc=0.68GB reserv=26.88GB\n",
      "grad W_out: 0.4164698123931885\n",
      "grad W: 4.989972114562988\n",
      "grad W_in: 3.5754878520965576\n",
      "grad W_out: 0.17525501549243927\n",
      "grad W: 1.7553322315216064\n",
      "grad W_in: 0.5042634606361389\n",
      "[ep 11 | batch 8] loss=0.8617 |M| |W| mean=2.323e-02\n",
      "ep 11 batch 8 | loss=0.8617 | alloc=0.68GB reserv=26.88GB\n",
      "grad W_out: 0.2393803745508194\n",
      "grad W: 4.136518478393555\n",
      "grad W_in: 2.1406943798065186\n",
      "grad W_out: 0.24104268848896027\n",
      "grad W: 6.060683727264404\n",
      "grad W_in: 4.4493513107299805\n",
      "[ep 11 | batch 10] loss=0.8156 |M| |W| mean=2.323e-02\n",
      "ep 11 batch 10 | loss=0.8156 | alloc=0.68GB reserv=26.88GB\n",
      "grad W_out: 0.30418506264686584\n",
      "grad W: 5.729059219360352\n",
      "grad W_in: 3.964721441268921\n",
      "grad W_out: 0.3345377743244171\n",
      "grad W: 4.310099124908447\n",
      "grad W_in: 1.3703058958053589\n",
      "[ep 11 | batch 12] loss=0.8662 |M| |W| mean=2.323e-02\n",
      "ep 11 batch 12 | loss=0.8662 | alloc=0.69GB reserv=26.89GB\n",
      "grad W_out: 0.28178778290748596\n",
      "grad W: 4.6062726974487305\n",
      "grad W_in: 3.405057907104492\n",
      "grad W_out: 0.2593170404434204\n",
      "grad W: 3.866316556930542\n",
      "grad W_in: 1.8008350133895874\n",
      "[ep 11 | batch 14] loss=0.7716 |M| |W| mean=2.324e-02\n",
      "ep 11 batch 14 | loss=0.7716 | alloc=0.69GB reserv=26.89GB\n",
      "grad W_out: 0.20386050641536713\n",
      "grad W: 9.401637077331543\n",
      "grad W_in: 1.5054590702056885\n",
      "grad W_out: 0.23825564980506897\n",
      "grad W: 5.250936508178711\n",
      "grad W_in: 4.47710657119751\n",
      "[ep 11 | batch 16] loss=0.8264 |M| |W| mean=2.324e-02\n",
      "ep 11 batch 16 | loss=0.8264 | alloc=0.69GB reserv=26.89GB\n",
      "grad W_out: 0.24392269551753998\n",
      "grad W: 4.64456033706665\n",
      "grad W_in: 4.00601863861084\n",
      "grad W_out: 0.34818655252456665\n",
      "grad W: 5.721701145172119\n",
      "grad W_in: 3.736957311630249\n",
      "[ep 11 | batch 18] loss=0.7527 |M| |W| mean=2.324e-02\n",
      "ep 11 batch 18 | loss=0.7527 | alloc=0.69GB reserv=26.89GB\n",
      "grad W_out: 0.39002326130867004\n",
      "grad W: 7.143301963806152\n",
      "grad W_in: 4.2342376708984375\n",
      "grad W_out: 0.2779209613800049\n",
      "grad W: 5.208118438720703\n",
      "grad W_in: 4.349005222320557\n",
      "[ep 11 | batch 20] loss=0.8140 |M| |W| mean=2.325e-02\n",
      "ep 11 batch 20 | loss=0.8140 | alloc=0.69GB reserv=26.89GB\n",
      "grad W_out: 0.24605385959148407\n",
      "grad W: 7.053807258605957\n",
      "grad W_in: 4.816113471984863\n",
      "grad W_out: 0.27657169103622437\n",
      "grad W: 5.329178333282471\n",
      "grad W_in: 1.4434648752212524\n",
      "[ep 11 | batch 22] loss=0.8625 |M| |W| mean=2.326e-02\n",
      "ep 11 batch 22 | loss=0.8625 | alloc=0.69GB reserv=26.90GB\n",
      "grad W_out: 0.4554692804813385\n",
      "grad W: 7.862575054168701\n",
      "grad W_in: 5.495998382568359\n",
      "grad W_out: 0.3718659281730652\n",
      "grad W: 3.503093957901001\n",
      "grad W_in: 1.1324856281280518\n",
      "[ep 11 | batch 24] loss=0.7766 |M| |W| mean=2.326e-02\n",
      "ep 11 batch 24 | loss=0.7766 | alloc=0.69GB reserv=26.90GB\n",
      "grad W_out: 0.29589998722076416\n",
      "grad W: 6.022188663482666\n",
      "grad W_in: 6.0698089599609375\n",
      "grad W_out: 0.3400026261806488\n",
      "grad W: 6.298699378967285\n",
      "grad W_in: 2.0289316177368164\n",
      "[ep 11 | batch 26] loss=0.8404 |M| |W| mean=2.327e-02\n",
      "ep 11 batch 26 | loss=0.8404 | alloc=0.70GB reserv=26.90GB\n",
      "grad W_out: 0.27721837162971497\n",
      "grad W: 4.281869411468506\n",
      "grad W_in: 2.18098521232605\n",
      "grad W_out: 0.34384050965309143\n",
      "grad W: 16.47041893005371\n",
      "grad W_in: 4.04345178604126\n",
      "[ep 11 | batch 28] loss=0.8194 |M| |W| mean=2.327e-02\n",
      "ep 11 batch 28 | loss=0.8194 | alloc=0.70GB reserv=26.90GB\n",
      "grad W_out: 0.306414932012558\n",
      "grad W: 5.973844528198242\n",
      "grad W_in: 6.118328094482422\n",
      "grad W_out: 0.14886359870433807\n",
      "grad W: 2.5129313468933105\n",
      "grad W_in: 0.4934430718421936\n",
      "[ep 11 | batch 30] loss=0.7604 |M| |W| mean=2.329e-02\n",
      "ep 11 batch 30 | loss=0.7604 | alloc=0.70GB reserv=26.90GB\n",
      "grad W_out: 0.3693360984325409\n",
      "grad W: 3.976837158203125\n",
      "grad W_in: 2.97648286819458\n",
      "grad W_out: 0.2610377371311188\n",
      "grad W: 4.161832332611084\n",
      "grad W_in: 0.6452434659004211\n",
      "[ep 11 | batch 32] loss=0.8223 |M| |W| mean=2.329e-02\n",
      "ep 11 batch 32 | loss=0.8223 | alloc=0.70GB reserv=26.91GB\n",
      "grad W_out: 0.3045361042022705\n",
      "grad W: 4.330057144165039\n",
      "grad W_in: 1.84897780418396\n",
      "grad W_out: 0.23050400614738464\n",
      "grad W: 4.177217960357666\n",
      "grad W_in: 3.0919902324676514\n",
      "[ep 11 | batch 34] loss=0.8567 |M| |W| mean=2.330e-02\n",
      "ep 11 batch 34 | loss=0.8567 | alloc=0.70GB reserv=26.91GB\n",
      "grad W_out: 0.2821899950504303\n",
      "grad W: 4.734768390655518\n",
      "grad W_in: 2.12868332862854\n",
      "grad W_out: 0.30797266960144043\n",
      "grad W: 3.646505832672119\n",
      "grad W_in: 1.814789056777954\n",
      "[ep 11 | batch 36] loss=0.8262 |M| |W| mean=2.331e-02\n",
      "ep 11 batch 36 | loss=0.8262 | alloc=0.70GB reserv=26.91GB\n",
      "grad W_out: 0.28673219680786133\n",
      "grad W: 4.131988048553467\n",
      "grad W_in: 2.0123960971832275\n",
      "grad W_out: 0.21819721162319183\n",
      "grad W: 3.343036651611328\n",
      "grad W_in: 2.2365972995758057\n",
      "[ep 11 | batch 38] loss=0.7923 |M| |W| mean=2.330e-02\n",
      "ep 11 batch 38 | loss=0.7923 | alloc=0.70GB reserv=26.91GB\n",
      "grad W_out: 0.3454201817512512\n",
      "grad W: 6.8782525062561035\n",
      "grad W_in: 7.482985019683838\n",
      "grad W_out: 0.3520638048648834\n",
      "grad W: 4.167064189910889\n",
      "grad W_in: 2.2200400829315186\n",
      "[ep 11 | batch 40] loss=0.8372 |M| |W| mean=2.332e-02\n",
      "ep 11 batch 40 | loss=0.8372 | alloc=0.71GB reserv=26.91GB\n",
      "grad W_out: 0.3813953697681427\n",
      "grad W: 7.7969889640808105\n",
      "grad W_in: 6.062571048736572\n",
      "grad W_out: 0.36022481322288513\n",
      "grad W: 8.147793769836426\n",
      "grad W_in: 1.653938889503479\n",
      "[ep 11 | batch 42] loss=0.7234 |M| |W| mean=2.332e-02\n",
      "ep 11 batch 42 | loss=0.7234 | alloc=0.71GB reserv=26.92GB\n",
      "grad W_out: 0.34702539443969727\n",
      "grad W: 6.176796913146973\n",
      "grad W_in: 5.767062664031982\n",
      "grad W_out: 0.5044437050819397\n",
      "grad W: 8.33502197265625\n",
      "grad W_in: 2.824678421020508\n",
      "[ep 11 | batch 44] loss=0.8775 |M| |W| mean=2.333e-02\n",
      "ep 11 batch 44 | loss=0.8775 | alloc=0.71GB reserv=26.92GB\n",
      "grad W_out: 0.4383997321128845\n",
      "grad W: 6.098567485809326\n",
      "grad W_in: 0.755190908908844\n",
      "grad W_out: 0.40467149019241333\n",
      "grad W: 5.682769298553467\n",
      "grad W_in: 3.1528117656707764\n",
      "[ep 11 | batch 46] loss=0.8967 |M| |W| mean=2.335e-02\n",
      "ep 11 batch 46 | loss=0.8967 | alloc=0.71GB reserv=26.92GB\n",
      "grad W_out: 0.482680082321167\n",
      "grad W: 7.448770523071289\n",
      "grad W_in: 2.094180107116699\n",
      "grad W_out: 0.35255008935928345\n",
      "grad W: 7.644883155822754\n",
      "grad W_in: 6.688896656036377\n",
      "[ep 11 | batch 48] loss=0.8594 |M| |W| mean=2.335e-02\n",
      "ep 11 batch 48 | loss=0.8594 | alloc=0.71GB reserv=26.92GB\n",
      "grad W_out: 0.29406556487083435\n",
      "grad W: 5.844315528869629\n",
      "grad W_in: 1.2991652488708496\n",
      "grad W_out: 0.2872689664363861\n",
      "grad W: 7.855437278747559\n",
      "grad W_in: 6.2975616455078125\n",
      "[ep 11 | batch 50] loss=0.8053 |M| |W| mean=2.335e-02\n",
      "ep 11 batch 50 | loss=0.8053 | alloc=0.71GB reserv=26.92GB\n",
      "grad W_out: 0.2887764871120453\n",
      "grad W: 4.133460998535156\n",
      "grad W_in: 1.2684478759765625\n",
      "grad W_out: 0.41969743371009827\n",
      "grad W: 7.76152229309082\n",
      "grad W_in: 6.414022445678711\n",
      "[ep 11 | batch 52] loss=0.8106 |M| |W| mean=2.336e-02\n",
      "ep 11 batch 52 | loss=0.8106 | alloc=0.71GB reserv=26.93GB\n",
      "grad W_out: 0.22142255306243896\n",
      "grad W: 4.8966383934021\n",
      "grad W_in: 3.541729688644409\n",
      "grad W_out: 0.4193950891494751\n",
      "grad W: 8.172074317932129\n",
      "grad W_in: 8.40687370300293\n",
      "[ep 11 | batch 54] loss=0.8531 |M| |W| mean=2.336e-02\n",
      "ep 11 batch 54 | loss=0.8531 | alloc=0.72GB reserv=26.93GB\n",
      "grad W_out: 0.23781414330005646\n",
      "grad W: 6.565383434295654\n",
      "grad W_in: 6.928538799285889\n",
      "grad W_out: 0.33775123953819275\n",
      "grad W: 7.439212799072266\n",
      "grad W_in: 8.341364860534668\n",
      "[ep 11 | batch 56] loss=0.7955 |M| |W| mean=2.336e-02\n",
      "ep 11 batch 56 | loss=0.7955 | alloc=0.72GB reserv=26.93GB\n",
      "grad W_out: 0.39680778980255127\n",
      "grad W: 7.495969772338867\n",
      "grad W_in: 7.044193267822266\n",
      "grad W_out: 0.4099639356136322\n",
      "grad W: 8.12712287902832\n",
      "grad W_in: 7.0661749839782715\n",
      "[ep 11 | batch 58] loss=0.7882 |M| |W| mean=2.337e-02\n",
      "ep 11 batch 58 | loss=0.7882 | alloc=0.72GB reserv=26.93GB\n",
      "grad W_out: 0.45728984475135803\n",
      "grad W: 6.3482747077941895\n",
      "grad W_in: 5.042713642120361\n",
      "grad W_out: 0.30976560711860657\n",
      "grad W: 5.351601600646973\n",
      "grad W_in: 3.349655866622925\n",
      "[ep 11 | batch 60] loss=0.7487 |M| |W| mean=2.337e-02\n",
      "ep 11 batch 60 | loss=0.7487 | alloc=0.72GB reserv=26.93GB\n",
      "grad W_out: 0.3263164460659027\n",
      "grad W: 7.110357761383057\n",
      "grad W_in: 4.811546325683594\n",
      "grad W_out: 0.3096494674682617\n",
      "grad W: 3.2281672954559326\n",
      "grad W_in: 1.3475950956344604\n",
      "[ep 11 | batch 62] loss=0.7827 |M| |W| mean=2.338e-02\n",
      "ep 11 batch 62 | loss=0.7827 | alloc=0.72GB reserv=26.94GB\n",
      "grad W_out: 0.3053656816482544\n",
      "grad W: 7.284346580505371\n",
      "grad W_in: 6.7425432205200195\n",
      "grad W_out: 0.3157513439655304\n",
      "grad W: 5.761165142059326\n",
      "grad W_in: 1.6677887439727783\n",
      "[ep 11 | batch 64] loss=0.8483 |M| |W| mean=2.338e-02\n",
      "ep 11 batch 64 | loss=0.8483 | alloc=0.72GB reserv=26.94GB\n",
      "grad W_out: 0.2474648654460907\n",
      "grad W: 3.2815473079681396\n",
      "grad W_in: 0.904536783695221\n",
      "grad W_out: 0.2777578830718994\n",
      "grad W: 4.801549911499023\n",
      "grad W_in: 3.7649288177490234\n",
      "[ep 11 | batch 66] loss=0.7858 |M| |W| mean=2.339e-02\n",
      "ep 11 batch 66 | loss=0.7858 | alloc=0.72GB reserv=26.94GB\n",
      "grad W_out: 0.21393336355686188\n",
      "grad W: 3.2804534435272217\n",
      "grad W_in: 3.5480074882507324\n",
      "grad W_out: 0.24648895859718323\n",
      "grad W: 4.461115837097168\n",
      "grad W_in: 2.194132089614868\n",
      "[ep 11 | batch 68] loss=0.8126 |M| |W| mean=2.340e-02\n",
      "ep 11 batch 68 | loss=0.8126 | alloc=0.73GB reserv=26.94GB\n",
      "grad W_out: 0.21704308688640594\n",
      "grad W: 4.951367378234863\n",
      "grad W_in: 2.627849817276001\n",
      "grad W_out: 0.21744105219841003\n",
      "grad W: 5.185832500457764\n",
      "grad W_in: 3.268007755279541\n",
      "[ep 11 | batch 70] loss=0.7770 |M| |W| mean=2.340e-02\n",
      "ep 11 batch 70 | loss=0.7770 | alloc=0.73GB reserv=26.94GB\n",
      "grad W_out: 0.33641305565834045\n",
      "grad W: 6.297901630401611\n",
      "grad W_in: 4.997296333312988\n",
      "grad W_out: 0.235494926571846\n",
      "grad W: 4.133823871612549\n",
      "grad W_in: 3.9687447547912598\n",
      "[ep 11 | batch 72] loss=0.8085 |M| |W| mean=2.341e-02\n",
      "ep 11 batch 72 | loss=0.8085 | alloc=0.73GB reserv=26.95GB\n",
      "grad W_out: 0.19854854047298431\n",
      "grad W: 3.3670284748077393\n",
      "grad W_in: 1.4284899234771729\n",
      "grad W_out: 0.2129206359386444\n",
      "grad W: 4.397672653198242\n",
      "grad W_in: 1.413567304611206\n",
      "[ep 11 | batch 74] loss=0.7756 |M| |W| mean=2.342e-02\n",
      "ep 11 batch 74 | loss=0.7756 | alloc=0.73GB reserv=26.95GB\n",
      "grad W_out: 0.3369455337524414\n",
      "grad W: 5.1667046546936035\n",
      "grad W_in: 2.8854527473449707\n",
      "grad W_out: 0.344937264919281\n",
      "grad W: 6.233668327331543\n",
      "grad W_in: 6.88658332824707\n",
      "[ep 11 | batch 76] loss=0.7755 |M| |W| mean=2.341e-02\n",
      "ep 11 batch 76 | loss=0.7755 | alloc=0.73GB reserv=26.95GB\n",
      "grad W_out: 0.34438103437423706\n",
      "grad W: 11.81572437286377\n",
      "grad W_in: 8.929832458496094\n",
      "grad W_out: 0.35537955164909363\n",
      "grad W: 6.728163242340088\n",
      "grad W_in: 1.9388840198516846\n",
      "[ep 11 | batch 78] loss=0.7644 |M| |W| mean=2.342e-02\n",
      "ep 11 batch 78 | loss=0.7644 | alloc=0.73GB reserv=26.95GB\n",
      "grad W_out: 0.34340140223503113\n",
      "grad W: 8.288546562194824\n",
      "grad W_in: 2.3541924953460693\n",
      "grad W_out: 0.22197119891643524\n",
      "grad W: 4.101182460784912\n",
      "grad W_in: 2.6453874111175537\n",
      "[ep 11 | batch 80] loss=0.7799 |M| |W| mean=2.342e-02\n",
      "ep 11 batch 80 | loss=0.7799 | alloc=0.73GB reserv=26.95GB\n",
      "grad W_out: 0.43642163276672363\n",
      "grad W: 5.109448432922363\n",
      "grad W_in: 2.9744975566864014\n",
      "grad W_out: 0.41972288489341736\n",
      "grad W: 6.573997497558594\n",
      "grad W_in: 5.336321830749512\n",
      "[ep 11 | batch 82] loss=0.8324 |M| |W| mean=2.343e-02\n",
      "ep 11 batch 82 | loss=0.8324 | alloc=0.74GB reserv=26.96GB\n",
      "grad W_out: 0.38814491033554077\n",
      "grad W: 7.366574764251709\n",
      "grad W_in: 6.209918975830078\n",
      "grad W_out: 0.20411650836467743\n",
      "grad W: 3.470561981201172\n",
      "grad W_in: 0.674360454082489\n",
      "[ep 11 | batch 84] loss=0.7983 |M| |W| mean=2.343e-02\n",
      "ep 11 batch 84 | loss=0.7983 | alloc=0.74GB reserv=26.96GB\n",
      "grad W_out: 0.29388123750686646\n",
      "grad W: 5.754239559173584\n",
      "grad W_in: 1.2534019947052002\n",
      "grad W_out: 0.30046355724334717\n",
      "grad W: 5.085973739624023\n",
      "grad W_in: 1.419451117515564\n",
      "[ep 11 | batch 86] loss=0.7458 |M| |W| mean=2.343e-02\n",
      "ep 11 batch 86 | loss=0.7458 | alloc=0.74GB reserv=26.96GB\n",
      "grad W_out: 0.308729887008667\n",
      "grad W: 10.693833351135254\n",
      "grad W_in: 11.659234046936035\n",
      "grad W_out: 0.40625321865081787\n",
      "grad W: 10.942343711853027\n",
      "grad W_in: 11.879483222961426\n",
      "[ep 11 | batch 88] loss=0.7947 |M| |W| mean=2.344e-02\n",
      "ep 11 batch 88 | loss=0.7947 | alloc=0.74GB reserv=26.96GB\n",
      "grad W_out: 0.28827688097953796\n",
      "grad W: 16.380645751953125\n",
      "grad W_in: 16.825637817382812\n",
      "grad W_out: 0.4377281665802002\n",
      "grad W: 16.256492614746094\n",
      "grad W_in: 14.449991226196289\n",
      "[ep 11 | batch 90] loss=0.8536 |M| |W| mean=2.343e-02\n",
      "ep 11 batch 90 | loss=0.8536 | alloc=0.74GB reserv=26.96GB\n",
      "grad W_out: 0.44567862153053284\n",
      "grad W: 8.354806900024414\n",
      "grad W_in: 6.341942310333252\n",
      "grad W_out: 0.46185919642448425\n",
      "grad W: 11.649603843688965\n",
      "grad W_in: 7.522507667541504\n",
      "[ep 11 | batch 92] loss=0.8806 |M| |W| mean=2.344e-02\n",
      "ep 11 batch 92 | loss=0.8806 | alloc=0.74GB reserv=26.96GB\n",
      "grad W_out: 0.5573426485061646\n",
      "grad W: 15.182265281677246\n",
      "grad W_in: 10.706853866577148\n",
      "Epoch 11/20 | lr=1.00e-03 | loss=0.8031 | test_acc=73.01% | dt=1177.1s\n",
      "grad W_out: 0.28914952278137207\n",
      "grad W: 2.7268879413604736\n",
      "grad W_in: 1.000691533088684\n",
      "[ep 12 | batch 0] loss=0.8288 |M| |W| mean=2.345e-02\n",
      "ep 12 batch 0 | loss=0.8288 | alloc=0.74GB reserv=26.97GB\n",
      "grad W_out: 0.7515509724617004\n",
      "grad W: 18.786226272583008\n",
      "grad W_in: 12.914902687072754\n",
      "grad W_out: 0.500319242477417\n",
      "grad W: 9.923473358154297\n",
      "grad W_in: 7.349050521850586\n",
      "[ep 12 | batch 2] loss=0.8695 |M| |W| mean=2.346e-02\n",
      "ep 12 batch 2 | loss=0.8695 | alloc=0.75GB reserv=26.97GB\n",
      "grad W_out: 0.5089724659919739\n",
      "grad W: 6.671392440795898\n",
      "grad W_in: 5.312667369842529\n",
      "grad W_out: 0.562993586063385\n",
      "grad W: 10.715049743652344\n",
      "grad W_in: 7.293931484222412\n",
      "[ep 12 | batch 4] loss=0.8877 |M| |W| mean=2.347e-02\n",
      "ep 12 batch 4 | loss=0.8877 | alloc=0.75GB reserv=26.97GB\n",
      "grad W_out: 0.6748490333557129\n",
      "grad W: 12.080827713012695\n",
      "grad W_in: 6.493522644042969\n",
      "grad W_out: 0.32978254556655884\n",
      "grad W: 5.799455165863037\n",
      "grad W_in: 3.3850200176239014\n",
      "[ep 12 | batch 6] loss=0.8212 |M| |W| mean=2.347e-02\n",
      "ep 12 batch 6 | loss=0.8212 | alloc=0.75GB reserv=26.97GB\n",
      "grad W_out: 0.428861141204834\n",
      "grad W: 8.660508155822754\n",
      "grad W_in: 6.977725505828857\n",
      "grad W_out: 0.44713541865348816\n",
      "grad W: 10.478760719299316\n",
      "grad W_in: 9.24682331085205\n",
      "[ep 12 | batch 8] loss=0.8222 |M| |W| mean=2.349e-02\n",
      "ep 12 batch 8 | loss=0.8222 | alloc=0.75GB reserv=26.97GB\n",
      "grad W_out: 0.3610803782939911\n",
      "grad W: 8.148469924926758\n",
      "grad W_in: 7.8542375564575195\n",
      "grad W_out: 0.2906176447868347\n",
      "grad W: 3.698850154876709\n",
      "grad W_in: 3.4359419345855713\n",
      "[ep 12 | batch 10] loss=0.9104 |M| |W| mean=2.350e-02\n",
      "ep 12 batch 10 | loss=0.9104 | alloc=0.75GB reserv=26.98GB\n",
      "grad W_out: 0.36939939856529236\n",
      "grad W: 8.020561218261719\n",
      "grad W_in: 7.10228967666626\n",
      "grad W_out: 0.48127907514572144\n",
      "grad W: 4.444960594177246\n",
      "grad W_in: 2.083719253540039\n",
      "[ep 12 | batch 12] loss=0.7654 |M| |W| mean=2.350e-02\n",
      "ep 12 batch 12 | loss=0.7654 | alloc=0.75GB reserv=26.98GB\n",
      "grad W_out: 0.5383939146995544\n",
      "grad W: 5.9132561683654785\n",
      "grad W_in: 5.423689842224121\n",
      "grad W_out: 0.47756949067115784\n",
      "grad W: 7.1207451820373535\n",
      "grad W_in: 3.7687149047851562\n",
      "[ep 12 | batch 14] loss=0.8169 |M| |W| mean=2.351e-02\n",
      "ep 12 batch 14 | loss=0.8169 | alloc=0.75GB reserv=26.98GB\n",
      "grad W_out: 0.34688612818717957\n",
      "grad W: 7.428410053253174\n",
      "grad W_in: 1.8488752841949463\n",
      "grad W_out: 0.3567608892917633\n",
      "grad W: 6.53121280670166\n",
      "grad W_in: 3.169733762741089\n",
      "[ep 12 | batch 16] loss=0.7936 |M| |W| mean=2.352e-02\n",
      "ep 12 batch 16 | loss=0.7936 | alloc=0.76GB reserv=26.98GB\n",
      "grad W_out: 0.2948881685733795\n",
      "grad W: 4.106716156005859\n",
      "grad W_in: 1.50608491897583\n",
      "grad W_out: 0.19608816504478455\n",
      "grad W: 3.9721832275390625\n",
      "grad W_in: 0.6493638753890991\n",
      "[ep 12 | batch 18] loss=0.7228 |M| |W| mean=2.353e-02\n",
      "ep 12 batch 18 | loss=0.7228 | alloc=0.76GB reserv=26.98GB\n",
      "grad W_out: 0.5184300541877747\n",
      "grad W: 9.20008659362793\n",
      "grad W_in: 5.232868194580078\n",
      "grad W_out: 0.4471880793571472\n",
      "grad W: 3.6313092708587646\n",
      "grad W_in: 1.3880900144577026\n",
      "[ep 12 | batch 20] loss=0.8280 |M| |W| mean=2.353e-02\n",
      "ep 12 batch 20 | loss=0.8280 | alloc=0.76GB reserv=26.99GB\n",
      "grad W_out: 0.37337779998779297\n",
      "grad W: 4.420472621917725\n",
      "grad W_in: 1.4151651859283447\n",
      "grad W_out: 0.20108799636363983\n",
      "grad W: 2.7447102069854736\n",
      "grad W_in: 0.7295958399772644\n",
      "[ep 12 | batch 22] loss=0.8609 |M| |W| mean=2.354e-02\n",
      "ep 12 batch 22 | loss=0.8609 | alloc=0.76GB reserv=26.99GB\n",
      "grad W_out: 0.23097103834152222\n",
      "grad W: 2.736839771270752\n",
      "grad W_in: 1.6387343406677246\n",
      "grad W_out: 0.46031254529953003\n",
      "grad W: 7.303918838500977\n",
      "grad W_in: 4.14244270324707\n",
      "[ep 12 | batch 24] loss=0.8306 |M| |W| mean=2.355e-02\n",
      "ep 12 batch 24 | loss=0.8306 | alloc=0.76GB reserv=26.99GB\n",
      "grad W_out: 0.2988346517086029\n",
      "grad W: 5.413694858551025\n",
      "grad W_in: 3.7919769287109375\n",
      "grad W_out: 0.3253687024116516\n",
      "grad W: 3.8142871856689453\n",
      "grad W_in: 2.659701108932495\n",
      "[ep 12 | batch 26] loss=0.7473 |M| |W| mean=2.355e-02\n",
      "ep 12 batch 26 | loss=0.7473 | alloc=0.76GB reserv=26.99GB\n",
      "grad W_out: 0.28762614727020264\n",
      "grad W: 6.067314624786377\n",
      "grad W_in: 5.147682189941406\n",
      "grad W_out: 0.2704346776008606\n",
      "grad W: 4.119534015655518\n",
      "grad W_in: 2.2755467891693115\n",
      "[ep 12 | batch 28] loss=0.6939 |M| |W| mean=2.355e-02\n",
      "ep 12 batch 28 | loss=0.6939 | alloc=0.76GB reserv=26.99GB\n",
      "grad W_out: 0.24668936431407928\n",
      "grad W: 5.550881385803223\n",
      "grad W_in: 1.2057567834854126\n",
      "grad W_out: 0.44364234805107117\n",
      "grad W: 8.418896675109863\n",
      "grad W_in: 7.821691036224365\n",
      "[ep 12 | batch 30] loss=0.8699 |M| |W| mean=2.355e-02\n",
      "ep 12 batch 30 | loss=0.8699 | alloc=0.77GB reserv=27.00GB\n",
      "grad W_out: 0.21561503410339355\n",
      "grad W: 4.251573085784912\n",
      "grad W_in: 1.1461509466171265\n",
      "grad W_out: 0.36242276430130005\n",
      "grad W: 6.289735317230225\n",
      "grad W_in: 4.237994194030762\n",
      "[ep 12 | batch 32] loss=0.7693 |M| |W| mean=2.355e-02\n",
      "ep 12 batch 32 | loss=0.7693 | alloc=0.77GB reserv=27.00GB\n",
      "grad W_out: 0.37949255108833313\n",
      "grad W: 6.989980697631836\n",
      "grad W_in: 5.020721912384033\n",
      "grad W_out: 0.24553610384464264\n",
      "grad W: 3.5333237648010254\n",
      "grad W_in: 1.421568751335144\n",
      "[ep 12 | batch 34] loss=0.7684 |M| |W| mean=2.355e-02\n",
      "ep 12 batch 34 | loss=0.7684 | alloc=0.77GB reserv=27.00GB\n",
      "grad W_out: 0.3598334789276123\n",
      "grad W: 5.155313491821289\n",
      "grad W_in: 3.133810043334961\n",
      "grad W_out: 0.304427832365036\n",
      "grad W: 4.759647369384766\n",
      "grad W_in: 2.9740123748779297\n",
      "[ep 12 | batch 36] loss=0.8071 |M| |W| mean=2.356e-02\n",
      "ep 12 batch 36 | loss=0.8071 | alloc=0.77GB reserv=27.00GB\n",
      "grad W_out: 0.4166131317615509\n",
      "grad W: 5.307487964630127\n",
      "grad W_in: 1.5256561040878296\n",
      "grad W_out: 0.35994839668273926\n",
      "grad W: 6.933276176452637\n",
      "grad W_in: 5.496633529663086\n",
      "[ep 12 | batch 38] loss=0.7612 |M| |W| mean=2.356e-02\n",
      "ep 12 batch 38 | loss=0.7612 | alloc=0.77GB reserv=27.00GB\n",
      "grad W_out: 0.1937614381313324\n",
      "grad W: 4.332122325897217\n",
      "grad W_in: 2.105926752090454\n",
      "grad W_out: 0.2792986333370209\n",
      "grad W: 4.3275346755981445\n",
      "grad W_in: 1.1345345973968506\n",
      "[ep 12 | batch 40] loss=0.7375 |M| |W| mean=2.356e-02\n",
      "ep 12 batch 40 | loss=0.7375 | alloc=0.77GB reserv=27.01GB\n",
      "grad W_out: 0.33200591802597046\n",
      "grad W: 5.848411560058594\n",
      "grad W_in: 5.725734710693359\n",
      "grad W_out: 0.22490744292736053\n",
      "grad W: 4.447879791259766\n",
      "grad W_in: 4.225009441375732\n",
      "[ep 12 | batch 42] loss=0.7288 |M| |W| mean=2.357e-02\n",
      "ep 12 batch 42 | loss=0.7288 | alloc=0.77GB reserv=27.01GB\n",
      "grad W_out: 0.2754455804824829\n",
      "grad W: 5.555490970611572\n",
      "grad W_in: 4.734759330749512\n",
      "grad W_out: 0.26758694648742676\n",
      "grad W: 4.251441478729248\n",
      "grad W_in: 3.2896270751953125\n",
      "[ep 12 | batch 44] loss=0.7564 |M| |W| mean=2.357e-02\n",
      "ep 12 batch 44 | loss=0.7564 | alloc=0.78GB reserv=27.01GB\n",
      "grad W_out: 0.25914835929870605\n",
      "grad W: 2.3206660747528076\n",
      "grad W_in: 1.0869140625\n",
      "grad W_out: 0.3965629041194916\n",
      "grad W: 3.9463508129119873\n",
      "grad W_in: 1.2521021366119385\n",
      "[ep 12 | batch 46] loss=0.8524 |M| |W| mean=2.357e-02\n",
      "ep 12 batch 46 | loss=0.8524 | alloc=0.78GB reserv=27.01GB\n",
      "grad W_out: 0.3347698152065277\n",
      "grad W: 7.282585144042969\n",
      "grad W_in: 4.282912254333496\n",
      "grad W_out: 0.33607926964759827\n",
      "grad W: 5.3491926193237305\n",
      "grad W_in: 1.937174677848816\n",
      "[ep 12 | batch 48] loss=0.7413 |M| |W| mean=2.358e-02\n",
      "ep 12 batch 48 | loss=0.7413 | alloc=0.78GB reserv=27.01GB\n",
      "grad W_out: 0.30803874135017395\n",
      "grad W: 6.494103908538818\n",
      "grad W_in: 5.969542026519775\n",
      "grad W_out: 0.2852128744125366\n",
      "grad W: 4.117556095123291\n",
      "grad W_in: 3.650752544403076\n",
      "[ep 12 | batch 50] loss=0.7646 |M| |W| mean=2.358e-02\n",
      "ep 12 batch 50 | loss=0.7646 | alloc=0.78GB reserv=27.02GB\n",
      "grad W_out: 0.24389202892780304\n",
      "grad W: 5.30079984664917\n",
      "grad W_in: 3.5221922397613525\n",
      "grad W_out: 0.31475740671157837\n",
      "grad W: 5.7566118240356445\n",
      "grad W_in: 5.375842094421387\n",
      "[ep 12 | batch 52] loss=0.7178 |M| |W| mean=2.359e-02\n",
      "ep 12 batch 52 | loss=0.7178 | alloc=0.78GB reserv=27.02GB\n",
      "grad W_out: 0.2372024953365326\n",
      "grad W: 3.370460271835327\n",
      "grad W_in: 0.5712658166885376\n",
      "grad W_out: 0.3371455669403076\n",
      "grad W: 8.584800720214844\n",
      "grad W_in: 6.594089984893799\n",
      "[ep 12 | batch 54] loss=0.7631 |M| |W| mean=2.359e-02\n",
      "ep 12 batch 54 | loss=0.7631 | alloc=0.78GB reserv=27.02GB\n",
      "grad W_out: 0.19238169491291046\n",
      "grad W: 2.6455750465393066\n",
      "grad W_in: 0.5081378817558289\n",
      "grad W_out: 0.2607967257499695\n",
      "grad W: 2.6325507164001465\n",
      "grad W_in: 1.8257699012756348\n",
      "[ep 12 | batch 56] loss=0.7678 |M| |W| mean=2.359e-02\n",
      "ep 12 batch 56 | loss=0.7678 | alloc=0.78GB reserv=27.02GB\n",
      "grad W_out: 0.2548312544822693\n",
      "grad W: 6.30339241027832\n",
      "grad W_in: 6.02942419052124\n",
      "grad W_out: 0.2792699933052063\n",
      "grad W: 5.2721333503723145\n",
      "grad W_in: 3.4217631816864014\n",
      "[ep 12 | batch 58] loss=0.7769 |M| |W| mean=2.360e-02\n",
      "ep 12 batch 58 | loss=0.7769 | alloc=0.79GB reserv=27.02GB\n",
      "grad W_out: 0.27518290281295776\n",
      "grad W: 3.2307753562927246\n",
      "grad W_in: 1.350456953048706\n",
      "grad W_out: 0.4052039384841919\n",
      "grad W: 5.729497909545898\n",
      "grad W_in: 4.335212230682373\n",
      "[ep 12 | batch 60] loss=0.7895 |M| |W| mean=2.359e-02\n",
      "ep 12 batch 60 | loss=0.7895 | alloc=0.79GB reserv=27.03GB\n",
      "grad W_out: 0.3329886496067047\n",
      "grad W: 6.456919193267822\n",
      "grad W_in: 3.4499404430389404\n",
      "grad W_out: 0.2867714762687683\n",
      "grad W: 5.368885517120361\n",
      "grad W_in: 3.3264894485473633\n",
      "[ep 12 | batch 62] loss=0.8208 |M| |W| mean=2.359e-02\n",
      "ep 12 batch 62 | loss=0.8208 | alloc=0.79GB reserv=27.03GB\n",
      "grad W_out: 0.2349337935447693\n",
      "grad W: 4.106166839599609\n",
      "grad W_in: 3.1622250080108643\n",
      "grad W_out: 0.37845084071159363\n",
      "grad W: 4.4702982902526855\n",
      "grad W_in: 0.6543699502944946\n",
      "[ep 12 | batch 64] loss=0.7508 |M| |W| mean=2.360e-02\n",
      "ep 12 batch 64 | loss=0.7508 | alloc=0.79GB reserv=27.03GB\n",
      "grad W_out: 0.2711518108844757\n",
      "grad W: 3.551647424697876\n",
      "grad W_in: 0.729946494102478\n",
      "grad W_out: 0.3628219664096832\n",
      "grad W: 4.2671966552734375\n",
      "grad W_in: 1.82759428024292\n",
      "[ep 12 | batch 66] loss=0.7659 |M| |W| mean=2.360e-02\n",
      "ep 12 batch 66 | loss=0.7659 | alloc=0.79GB reserv=27.03GB\n",
      "grad W_out: 0.2652718722820282\n",
      "grad W: 3.5223426818847656\n",
      "grad W_in: 0.8939433693885803\n",
      "grad W_out: 0.35088208317756653\n",
      "grad W: 5.690357685089111\n",
      "grad W_in: 5.149224758148193\n",
      "[ep 12 | batch 68] loss=0.8057 |M| |W| mean=2.361e-02\n",
      "ep 12 batch 68 | loss=0.8057 | alloc=0.79GB reserv=27.03GB\n",
      "grad W_out: 0.1976422369480133\n",
      "grad W: 2.658156156539917\n",
      "grad W_in: 0.5835697650909424\n",
      "grad W_out: 0.20375344157218933\n",
      "grad W: 6.741440773010254\n",
      "grad W_in: 6.1782355308532715\n",
      "[ep 12 | batch 70] loss=0.7424 |M| |W| mean=2.362e-02\n",
      "ep 12 batch 70 | loss=0.7424 | alloc=0.79GB reserv=27.04GB\n",
      "grad W_out: 0.26764798164367676\n",
      "grad W: 4.355380535125732\n",
      "grad W_in: 2.0639147758483887\n",
      "grad W_out: 0.3905857801437378\n",
      "grad W: 10.115477561950684\n",
      "grad W_in: 6.47830057144165\n",
      "[ep 12 | batch 72] loss=0.7783 |M| |W| mean=2.363e-02\n",
      "ep 12 batch 72 | loss=0.7783 | alloc=0.80GB reserv=27.04GB\n",
      "grad W_out: 0.2772577702999115\n",
      "grad W: 8.7803316116333\n",
      "grad W_in: 8.359042167663574\n",
      "grad W_out: 0.253677636384964\n",
      "grad W: 5.995970726013184\n",
      "grad W_in: 2.0891783237457275\n",
      "[ep 12 | batch 74] loss=0.7929 |M| |W| mean=2.363e-02\n",
      "ep 12 batch 74 | loss=0.7929 | alloc=0.80GB reserv=27.04GB\n",
      "grad W_out: 0.48756253719329834\n",
      "grad W: 5.953292369842529\n",
      "grad W_in: 2.5791866779327393\n",
      "grad W_out: 0.38282230496406555\n",
      "grad W: 9.196891784667969\n",
      "grad W_in: 6.920193195343018\n",
      "[ep 12 | batch 76] loss=0.7535 |M| |W| mean=2.363e-02\n",
      "ep 12 batch 76 | loss=0.7535 | alloc=0.80GB reserv=27.04GB\n",
      "grad W_out: 0.44828200340270996\n",
      "grad W: 7.117685317993164\n",
      "grad W_in: 4.416268825531006\n",
      "grad W_out: 0.43476206064224243\n",
      "grad W: 5.151497840881348\n",
      "grad W_in: 0.9338651895523071\n",
      "[ep 12 | batch 78] loss=0.7141 |M| |W| mean=2.363e-02\n",
      "ep 12 batch 78 | loss=0.7141 | alloc=0.80GB reserv=27.04GB\n",
      "grad W_out: 0.23056438565254211\n",
      "grad W: 5.906136989593506\n",
      "grad W_in: 2.084857940673828\n",
      "grad W_out: 0.2933632731437683\n",
      "grad W: 4.769463062286377\n",
      "grad W_in: 1.1578377485275269\n",
      "[ep 12 | batch 80] loss=0.7722 |M| |W| mean=2.363e-02\n",
      "ep 12 batch 80 | loss=0.7722 | alloc=0.80GB reserv=27.04GB\n",
      "grad W_out: 0.25033995509147644\n",
      "grad W: 5.489210605621338\n",
      "grad W_in: 2.0135738849639893\n",
      "grad W_out: 0.3572479784488678\n",
      "grad W: 6.830269813537598\n",
      "grad W_in: 5.417660713195801\n",
      "[ep 12 | batch 82] loss=0.7592 |M| |W| mean=2.364e-02\n",
      "ep 12 batch 82 | loss=0.7592 | alloc=0.80GB reserv=27.05GB\n",
      "grad W_out: 0.26064929366111755\n",
      "grad W: 7.429795265197754\n",
      "grad W_in: 6.368921756744385\n",
      "grad W_out: 0.34696123003959656\n",
      "grad W: 10.722822189331055\n",
      "grad W_in: 10.743182182312012\n",
      "[ep 12 | batch 84] loss=0.7793 |M| |W| mean=2.364e-02\n",
      "ep 12 batch 84 | loss=0.7793 | alloc=0.80GB reserv=27.05GB\n",
      "grad W_out: 0.3821197748184204\n",
      "grad W: 10.200152397155762\n",
      "grad W_in: 7.440738677978516\n",
      "grad W_out: 0.29595962166786194\n",
      "grad W: 3.436379909515381\n",
      "grad W_in: 1.1472102403640747\n",
      "[ep 12 | batch 86] loss=0.8173 |M| |W| mean=2.365e-02\n",
      "ep 12 batch 86 | loss=0.8173 | alloc=0.81GB reserv=27.05GB\n",
      "grad W_out: 0.41627931594848633\n",
      "grad W: 13.228938102722168\n",
      "grad W_in: 13.037826538085938\n",
      "grad W_out: 0.502662181854248\n",
      "grad W: 7.221303462982178\n",
      "grad W_in: 1.8508903980255127\n",
      "[ep 12 | batch 88] loss=0.7372 |M| |W| mean=2.365e-02\n",
      "ep 12 batch 88 | loss=0.7372 | alloc=0.81GB reserv=27.05GB\n",
      "grad W_out: 0.6646898984909058\n",
      "grad W: 10.132079124450684\n",
      "grad W_in: 7.605556964874268\n",
      "grad W_out: 0.40460172295570374\n",
      "grad W: 6.794976234436035\n",
      "grad W_in: 4.584707260131836\n",
      "[ep 12 | batch 90] loss=0.7171 |M| |W| mean=2.366e-02\n",
      "ep 12 batch 90 | loss=0.7171 | alloc=0.81GB reserv=27.05GB\n",
      "grad W_out: 0.2718981206417084\n",
      "grad W: 4.216087341308594\n",
      "grad W_in: 1.9564154148101807\n",
      "grad W_out: 0.47740447521209717\n",
      "grad W: 10.83792781829834\n",
      "grad W_in: 9.335142135620117\n",
      "[ep 12 | batch 92] loss=0.7864 |M| |W| mean=2.367e-02\n",
      "ep 12 batch 92 | loss=0.7864 | alloc=0.81GB reserv=27.06GB\n",
      "grad W_out: 0.3256370425224304\n",
      "grad W: 5.080665111541748\n",
      "grad W_in: 3.4669010639190674\n",
      "Epoch 12/20 | lr=1.00e-03 | loss=0.7831 | test_acc=75.21% | dt=1177.4s\n",
      "grad W_out: 0.27461445331573486\n",
      "grad W: 4.9276323318481445\n",
      "grad W_in: 3.603466510772705\n",
      "[ep 13 | batch 0] loss=0.6779 |M| |W| mean=2.366e-02\n",
      "ep 13 batch 0 | loss=0.6779 | alloc=0.81GB reserv=27.06GB\n",
      "grad W_out: 0.2991327941417694\n",
      "grad W: 7.347798824310303\n",
      "grad W_in: 8.056495666503906\n",
      "grad W_out: 0.31797799468040466\n",
      "grad W: 6.192108154296875\n",
      "grad W_in: 1.9998716115951538\n",
      "[ep 13 | batch 2] loss=0.7877 |M| |W| mean=2.367e-02\n",
      "ep 13 batch 2 | loss=0.7877 | alloc=0.81GB reserv=27.06GB\n",
      "grad W_out: 0.2976653277873993\n",
      "grad W: 4.212334156036377\n",
      "grad W_in: 1.6894198656082153\n",
      "grad W_out: 0.29349514842033386\n",
      "grad W: 4.787280559539795\n",
      "grad W_in: 3.291407823562622\n",
      "[ep 13 | batch 4] loss=0.7357 |M| |W| mean=2.368e-02\n",
      "ep 13 batch 4 | loss=0.7357 | alloc=0.81GB reserv=27.06GB\n",
      "grad W_out: 0.419097900390625\n",
      "grad W: 6.735556125640869\n",
      "grad W_in: 3.7335448265075684\n",
      "grad W_out: 0.42893216013908386\n",
      "grad W: 7.53270149230957\n",
      "grad W_in: 7.3253302574157715\n",
      "[ep 13 | batch 6] loss=0.8007 |M| |W| mean=2.368e-02\n",
      "ep 13 batch 6 | loss=0.8007 | alloc=0.82GB reserv=27.06GB\n",
      "grad W_out: 0.3135805130004883\n",
      "grad W: 6.18488073348999\n",
      "grad W_in: 1.8567618131637573\n",
      "grad W_out: 0.5771945118904114\n",
      "grad W: 8.004910469055176\n",
      "grad W_in: 6.383522987365723\n",
      "[ep 13 | batch 8] loss=0.7469 |M| |W| mean=2.368e-02\n",
      "ep 13 batch 8 | loss=0.7469 | alloc=0.82GB reserv=27.07GB\n",
      "grad W_out: 0.31846433877944946\n",
      "grad W: 7.922920227050781\n",
      "grad W_in: 6.240514278411865\n",
      "grad W_out: 0.3473014831542969\n",
      "grad W: 8.881046295166016\n",
      "grad W_in: 5.701900959014893\n",
      "[ep 13 | batch 10] loss=0.7833 |M| |W| mean=2.369e-02\n",
      "ep 13 batch 10 | loss=0.7833 | alloc=0.82GB reserv=27.07GB\n",
      "grad W_out: 0.41313672065734863\n",
      "grad W: 4.364476680755615\n",
      "grad W_in: 2.227226972579956\n",
      "grad W_out: 0.448163241147995\n",
      "grad W: 5.1764678955078125\n",
      "grad W_in: 4.211934566497803\n",
      "[ep 13 | batch 12] loss=0.7483 |M| |W| mean=2.369e-02\n",
      "ep 13 batch 12 | loss=0.7483 | alloc=0.82GB reserv=27.07GB\n",
      "grad W_out: 0.26319947838783264\n",
      "grad W: 5.939990520477295\n",
      "grad W_in: 1.256922721862793\n",
      "grad W_out: 0.39131274819374084\n",
      "grad W: 7.0414042472839355\n",
      "grad W_in: 4.774313449859619\n",
      "[ep 13 | batch 14] loss=0.7189 |M| |W| mean=2.369e-02\n",
      "ep 13 batch 14 | loss=0.7189 | alloc=0.82GB reserv=27.07GB\n",
      "grad W_out: 0.24724026024341583\n",
      "grad W: 4.167781829833984\n",
      "grad W_in: 2.9869894981384277\n",
      "grad W_out: 0.3615707755088806\n",
      "grad W: 5.759345531463623\n",
      "grad W_in: 1.5195313692092896\n",
      "[ep 13 | batch 16] loss=0.7681 |M| |W| mean=2.370e-02\n",
      "ep 13 batch 16 | loss=0.7681 | alloc=0.82GB reserv=27.07GB\n",
      "grad W_out: 0.3938414454460144\n",
      "grad W: 5.765093803405762\n",
      "grad W_in: 2.038259267807007\n",
      "grad W_out: 0.40191230177879333\n",
      "grad W: 7.086096286773682\n",
      "grad W_in: 4.406910419464111\n",
      "[ep 13 | batch 18] loss=0.7251 |M| |W| mean=2.370e-02\n",
      "ep 13 batch 18 | loss=0.7251 | alloc=0.82GB reserv=27.08GB\n",
      "grad W_out: 0.3004894256591797\n",
      "grad W: 2.891172170639038\n",
      "grad W_in: 0.8976898193359375\n",
      "grad W_out: 0.39716941118240356\n",
      "grad W: 9.37943172454834\n",
      "grad W_in: 4.558437347412109\n",
      "[ep 13 | batch 20] loss=0.8092 |M| |W| mean=2.371e-02\n",
      "ep 13 batch 20 | loss=0.8092 | alloc=0.83GB reserv=27.08GB\n",
      "grad W_out: 0.28025949001312256\n",
      "grad W: 6.964066028594971\n",
      "grad W_in: 4.819356918334961\n",
      "grad W_out: 0.21765074133872986\n",
      "grad W: 3.951580286026001\n",
      "grad W_in: 2.0336737632751465\n",
      "[ep 13 | batch 22] loss=0.7978 |M| |W| mean=2.371e-02\n",
      "ep 13 batch 22 | loss=0.7978 | alloc=0.83GB reserv=27.08GB\n",
      "grad W_out: 0.4446607530117035\n",
      "grad W: 9.052647590637207\n",
      "grad W_in: 6.4456095695495605\n",
      "grad W_out: 0.3227802813053131\n",
      "grad W: 4.900554656982422\n",
      "grad W_in: 0.7550745010375977\n",
      "[ep 13 | batch 24] loss=0.8000 |M| |W| mean=2.371e-02\n",
      "ep 13 batch 24 | loss=0.8000 | alloc=0.83GB reserv=27.08GB\n",
      "grad W_out: 0.2689342498779297\n",
      "grad W: 5.316840648651123\n",
      "grad W_in: 2.534331798553467\n",
      "grad W_out: 0.2389778345823288\n",
      "grad W: 3.921513795852661\n",
      "grad W_in: 1.6911842823028564\n",
      "[ep 13 | batch 26] loss=0.7381 |M| |W| mean=2.372e-02\n",
      "ep 13 batch 26 | loss=0.7381 | alloc=0.83GB reserv=27.08GB\n",
      "grad W_out: 0.3640722334384918\n",
      "grad W: 7.040214538574219\n",
      "grad W_in: 3.7942588329315186\n",
      "grad W_out: 0.32602164149284363\n",
      "grad W: 5.878937244415283\n",
      "grad W_in: 1.2312544584274292\n",
      "[ep 13 | batch 28] loss=0.7295 |M| |W| mean=2.373e-02\n",
      "ep 13 batch 28 | loss=0.7295 | alloc=0.83GB reserv=27.09GB\n",
      "grad W_out: 0.2203574776649475\n",
      "grad W: 4.309238910675049\n",
      "grad W_in: 1.3239092826843262\n",
      "grad W_out: 0.23571982979774475\n",
      "grad W: 7.481989860534668\n",
      "grad W_in: 4.84160852432251\n",
      "[ep 13 | batch 30] loss=0.6969 |M| |W| mean=2.374e-02\n",
      "ep 13 batch 30 | loss=0.6969 | alloc=0.83GB reserv=27.09GB\n",
      "grad W_out: 0.2643192410469055\n",
      "grad W: 5.354424476623535\n",
      "grad W_in: 2.5122387409210205\n",
      "grad W_out: 0.24873265624046326\n",
      "grad W: 6.332388877868652\n",
      "grad W_in: 1.8127590417861938\n",
      "[ep 13 | batch 32] loss=0.7668 |M| |W| mean=2.374e-02\n",
      "ep 13 batch 32 | loss=0.7668 | alloc=0.83GB reserv=27.09GB\n",
      "grad W_out: 0.34280261397361755\n",
      "grad W: 8.665024757385254\n",
      "grad W_in: 6.168066024780273\n",
      "grad W_out: 0.24641846120357513\n",
      "grad W: 4.466530799865723\n",
      "grad W_in: 4.017527103424072\n",
      "[ep 13 | batch 34] loss=0.7040 |M| |W| mean=2.375e-02\n",
      "ep 13 batch 34 | loss=0.7040 | alloc=0.84GB reserv=27.09GB\n",
      "grad W_out: 0.48560231924057007\n",
      "grad W: 10.665066719055176\n",
      "grad W_in: 8.949825286865234\n",
      "grad W_out: 0.39048540592193604\n",
      "grad W: 6.3778605461120605\n",
      "grad W_in: 3.7199530601501465\n",
      "[ep 13 | batch 36] loss=0.7119 |M| |W| mean=2.375e-02\n",
      "ep 13 batch 36 | loss=0.7119 | alloc=0.84GB reserv=27.09GB\n",
      "grad W_out: 0.2537803649902344\n",
      "grad W: 7.059324741363525\n",
      "grad W_in: 5.34370231628418\n",
      "grad W_out: 0.3216244876384735\n",
      "grad W: 5.711568832397461\n",
      "grad W_in: 3.744190216064453\n",
      "[ep 13 | batch 38] loss=0.7105 |M| |W| mean=2.376e-02\n",
      "ep 13 batch 38 | loss=0.7105 | alloc=0.84GB reserv=27.10GB\n",
      "grad W_out: 0.27102306485176086\n",
      "grad W: 4.2338104248046875\n",
      "grad W_in: 2.2028610706329346\n",
      "grad W_out: 0.3251914381980896\n",
      "grad W: 5.501771926879883\n",
      "grad W_in: 1.4227923154830933\n",
      "[ep 13 | batch 40] loss=0.7685 |M| |W| mean=2.376e-02\n",
      "ep 13 batch 40 | loss=0.7685 | alloc=0.84GB reserv=27.10GB\n",
      "grad W_out: 0.328543096780777\n",
      "grad W: 7.24897575378418\n",
      "grad W_in: 5.614850997924805\n",
      "grad W_out: 0.2813367247581482\n",
      "grad W: 3.5490877628326416\n",
      "grad W_in: 0.5788900852203369\n",
      "[ep 13 | batch 42] loss=0.6466 |M| |W| mean=2.376e-02\n",
      "ep 13 batch 42 | loss=0.6466 | alloc=0.84GB reserv=27.10GB\n",
      "grad W_out: 0.24451014399528503\n",
      "grad W: 4.879997730255127\n",
      "grad W_in: 4.682497501373291\n",
      "grad W_out: 0.3682619333267212\n",
      "grad W: 6.06504487991333\n",
      "grad W_in: 5.179419994354248\n",
      "[ep 13 | batch 44] loss=0.7858 |M| |W| mean=2.377e-02\n",
      "ep 13 batch 44 | loss=0.7858 | alloc=0.84GB reserv=27.10GB\n",
      "grad W_out: 0.2808395028114319\n",
      "grad W: 3.8629493713378906\n",
      "grad W_in: 1.7897722721099854\n",
      "grad W_out: 0.2981830835342407\n",
      "grad W: 4.605800628662109\n",
      "grad W_in: 3.5595884323120117\n",
      "[ep 13 | batch 46] loss=0.6665 |M| |W| mean=2.378e-02\n",
      "ep 13 batch 46 | loss=0.6665 | alloc=0.84GB reserv=27.10GB\n",
      "grad W_out: 0.24699938297271729\n",
      "grad W: 4.741885662078857\n",
      "grad W_in: 5.150623321533203\n",
      "grad W_out: 0.3528890609741211\n",
      "grad W: 3.3936080932617188\n",
      "grad W_in: 2.507526397705078\n",
      "[ep 13 | batch 48] loss=0.7405 |M| |W| mean=2.378e-02\n",
      "ep 13 batch 48 | loss=0.7405 | alloc=0.85GB reserv=27.11GB\n",
      "grad W_out: 0.297434002161026\n",
      "grad W: 6.099167346954346\n",
      "grad W_in: 2.5013999938964844\n",
      "grad W_out: 0.3550824522972107\n",
      "grad W: 4.99141788482666\n",
      "grad W_in: 3.8907718658447266\n",
      "[ep 13 | batch 50] loss=0.7061 |M| |W| mean=2.378e-02\n",
      "ep 13 batch 50 | loss=0.7061 | alloc=0.85GB reserv=27.11GB\n",
      "grad W_out: 0.27356377243995667\n",
      "grad W: 4.368648052215576\n",
      "grad W_in: 1.322503924369812\n",
      "grad W_out: 0.32876551151275635\n",
      "grad W: 6.3882365226745605\n",
      "grad W_in: 3.699888229370117\n",
      "[ep 13 | batch 52] loss=0.7650 |M| |W| mean=2.378e-02\n",
      "ep 13 batch 52 | loss=0.7650 | alloc=0.85GB reserv=27.11GB\n",
      "grad W_out: 0.2546848952770233\n",
      "grad W: 4.431929111480713\n",
      "grad W_in: 2.520244598388672\n",
      "grad W_out: 0.3437662124633789\n",
      "grad W: 5.231024265289307\n",
      "grad W_in: 1.6440688371658325\n",
      "[ep 13 | batch 54] loss=0.7734 |M| |W| mean=2.378e-02\n",
      "ep 13 batch 54 | loss=0.7734 | alloc=0.85GB reserv=27.11GB\n",
      "grad W_out: 0.43633055686950684\n",
      "grad W: 8.118974685668945\n",
      "grad W_in: 6.112516403198242\n",
      "grad W_out: 0.2671443819999695\n",
      "grad W: 4.901883602142334\n",
      "grad W_in: 1.6391735076904297\n",
      "[ep 13 | batch 56] loss=0.7090 |M| |W| mean=2.378e-02\n",
      "ep 13 batch 56 | loss=0.7090 | alloc=0.85GB reserv=27.11GB\n",
      "grad W_out: 0.25535836815834045\n",
      "grad W: 5.496939182281494\n",
      "grad W_in: 1.1103034019470215\n",
      "grad W_out: 0.29120945930480957\n",
      "grad W: 8.06045913696289\n",
      "grad W_in: 6.1828203201293945\n",
      "[ep 13 | batch 58] loss=0.7509 |M| |W| mean=2.378e-02\n",
      "ep 13 batch 58 | loss=0.7509 | alloc=0.85GB reserv=27.12GB\n",
      "grad W_out: 0.36730846762657166\n",
      "grad W: 7.8113813400268555\n",
      "grad W_in: 4.919068336486816\n",
      "grad W_out: 0.3000137507915497\n",
      "grad W: 6.397247314453125\n",
      "grad W_in: 5.688024044036865\n",
      "[ep 13 | batch 60] loss=0.7090 |M| |W| mean=2.378e-02\n",
      "ep 13 batch 60 | loss=0.7090 | alloc=0.85GB reserv=27.12GB\n",
      "grad W_out: 0.4243995249271393\n",
      "grad W: 11.39589786529541\n",
      "grad W_in: 7.2119622230529785\n",
      "grad W_out: 0.2264229953289032\n",
      "grad W: 3.4503355026245117\n",
      "grad W_in: 2.828801393508911\n",
      "[ep 13 | batch 62] loss=0.7299 |M| |W| mean=2.379e-02\n",
      "ep 13 batch 62 | loss=0.7299 | alloc=0.86GB reserv=27.12GB\n",
      "grad W_out: 0.35612252354621887\n",
      "grad W: 10.71306037902832\n",
      "grad W_in: 11.964371681213379\n",
      "grad W_out: 0.23274287581443787\n",
      "grad W: 3.6160888671875\n",
      "grad W_in: 1.5770552158355713\n",
      "[ep 13 | batch 64] loss=0.7232 |M| |W| mean=2.378e-02\n",
      "ep 13 batch 64 | loss=0.7232 | alloc=0.86GB reserv=27.12GB\n",
      "grad W_out: 0.3868119418621063\n",
      "grad W: 7.422406196594238\n",
      "grad W_in: 5.004904270172119\n",
      "grad W_out: 0.31340181827545166\n",
      "grad W: 6.561508655548096\n",
      "grad W_in: 5.952383995056152\n",
      "[ep 13 | batch 66] loss=0.7631 |M| |W| mean=2.378e-02\n",
      "ep 13 batch 66 | loss=0.7631 | alloc=0.86GB reserv=27.12GB\n",
      "grad W_out: 0.22179831564426422\n",
      "grad W: 2.273085355758667\n",
      "grad W_in: 0.6910533308982849\n",
      "grad W_out: 0.34223058819770813\n",
      "grad W: 5.530046463012695\n",
      "grad W_in: 1.883545994758606\n",
      "[ep 13 | batch 68] loss=0.6979 |M| |W| mean=2.378e-02\n",
      "ep 13 batch 68 | loss=0.6979 | alloc=0.86GB reserv=27.12GB\n",
      "grad W_out: 0.40264201164245605\n",
      "grad W: 9.607150077819824\n",
      "grad W_in: 9.455409049987793\n",
      "grad W_out: 0.33163881301879883\n",
      "grad W: 5.814377307891846\n",
      "grad W_in: 1.2188477516174316\n",
      "[ep 13 | batch 70] loss=0.7283 |M| |W| mean=2.377e-02\n",
      "ep 13 batch 70 | loss=0.7283 | alloc=0.86GB reserv=27.13GB\n",
      "grad W_out: 0.3439547121524811\n",
      "grad W: 10.065590858459473\n",
      "grad W_in: 8.601993560791016\n",
      "grad W_out: 0.2896299362182617\n",
      "grad W: 3.624058246612549\n",
      "grad W_in: 2.4670915603637695\n",
      "[ep 13 | batch 72] loss=0.7869 |M| |W| mean=2.378e-02\n",
      "ep 13 batch 72 | loss=0.7869 | alloc=0.86GB reserv=27.13GB\n",
      "grad W_out: 0.3496282994747162\n",
      "grad W: 8.435871124267578\n",
      "grad W_in: 6.062819004058838\n",
      "grad W_out: 0.4958169460296631\n",
      "grad W: 11.208090782165527\n",
      "grad W_in: 9.559653282165527\n",
      "[ep 13 | batch 74] loss=0.8180 |M| |W| mean=2.379e-02\n",
      "ep 13 batch 74 | loss=0.8180 | alloc=0.86GB reserv=27.13GB\n",
      "grad W_out: 0.482812762260437\n",
      "grad W: 8.821664810180664\n",
      "grad W_in: 6.917684078216553\n",
      "grad W_out: 0.3473018407821655\n",
      "grad W: 5.205434799194336\n",
      "grad W_in: 2.2019553184509277\n",
      "[ep 13 | batch 76] loss=0.7544 |M| |W| mean=2.379e-02\n",
      "ep 13 batch 76 | loss=0.7544 | alloc=0.87GB reserv=27.13GB\n",
      "grad W_out: 0.6279689073562622\n",
      "grad W: 16.250600814819336\n",
      "grad W_in: 12.560845375061035\n",
      "grad W_out: 0.34713149070739746\n",
      "grad W: 7.822262763977051\n",
      "grad W_in: 2.6513123512268066\n",
      "[ep 13 | batch 78] loss=0.8098 |M| |W| mean=2.380e-02\n",
      "ep 13 batch 78 | loss=0.8098 | alloc=0.87GB reserv=27.13GB\n",
      "grad W_out: 0.2859448790550232\n",
      "grad W: 7.163233280181885\n",
      "grad W_in: 6.191277503967285\n",
      "grad W_out: 0.2941594123840332\n",
      "grad W: 7.531551361083984\n",
      "grad W_in: 2.5979676246643066\n",
      "[ep 13 | batch 80] loss=0.7104 |M| |W| mean=2.380e-02\n",
      "ep 13 batch 80 | loss=0.7104 | alloc=0.87GB reserv=27.14GB\n",
      "grad W_out: 0.36947524547576904\n",
      "grad W: 6.56087064743042\n",
      "grad W_in: 3.367388963699341\n",
      "grad W_out: 0.4414951205253601\n",
      "grad W: 3.886366128921509\n",
      "grad W_in: 2.4575369358062744\n",
      "[ep 13 | batch 82] loss=0.7616 |M| |W| mean=2.381e-02\n",
      "ep 13 batch 82 | loss=0.7616 | alloc=0.87GB reserv=27.14GB\n",
      "grad W_out: 0.49164727330207825\n",
      "grad W: 6.465158939361572\n",
      "grad W_in: 5.206025123596191\n",
      "grad W_out: 0.4517557621002197\n",
      "grad W: 6.106390476226807\n",
      "grad W_in: 1.2733439207077026\n",
      "[ep 13 | batch 84] loss=0.7061 |M| |W| mean=2.382e-02\n",
      "ep 13 batch 84 | loss=0.7061 | alloc=0.87GB reserv=27.14GB\n",
      "grad W_out: 0.19736498594284058\n",
      "grad W: 4.1679301261901855\n",
      "grad W_in: 2.6854448318481445\n",
      "grad W_out: 0.28725701570510864\n",
      "grad W: 4.3975653648376465\n",
      "grad W_in: 0.5730270743370056\n",
      "[ep 13 | batch 86] loss=0.7190 |M| |W| mean=2.382e-02\n",
      "ep 13 batch 86 | loss=0.7190 | alloc=0.87GB reserv=27.14GB\n",
      "grad W_out: 0.30371955037117004\n",
      "grad W: 5.08040189743042\n",
      "grad W_in: 1.5112191438674927\n",
      "grad W_out: 0.3582737445831299\n",
      "grad W: 5.2519941329956055\n",
      "grad W_in: 1.3347342014312744\n",
      "[ep 13 | batch 88] loss=0.7707 |M| |W| mean=2.382e-02\n",
      "ep 13 batch 88 | loss=0.7707 | alloc=0.87GB reserv=27.14GB\n",
      "grad W_out: 0.24973979592323303\n",
      "grad W: 3.7476806640625\n",
      "grad W_in: 3.7321829795837402\n",
      "grad W_out: 0.18883123993873596\n",
      "grad W: 3.841447591781616\n",
      "grad W_in: 0.5232548117637634\n",
      "[ep 13 | batch 90] loss=0.6593 |M| |W| mean=2.383e-02\n",
      "ep 13 batch 90 | loss=0.6593 | alloc=0.88GB reserv=27.15GB\n",
      "grad W_out: 0.3517003655433655\n",
      "grad W: 6.661038398742676\n",
      "grad W_in: 4.033154487609863\n",
      "grad W_out: 0.42817968130111694\n",
      "grad W: 6.421283721923828\n",
      "grad W_in: 1.8668237924575806\n",
      "[ep 13 | batch 92] loss=0.7534 |M| |W| mean=2.383e-02\n",
      "ep 13 batch 92 | loss=0.7534 | alloc=0.88GB reserv=27.15GB\n",
      "grad W_out: 0.2073807716369629\n",
      "grad W: 4.58411979675293\n",
      "grad W_in: 4.280404090881348\n",
      "Epoch 13/20 | lr=1.00e-03 | loss=0.7419 | test_acc=75.11% | dt=1177.5s\n",
      "grad W_out: 0.2687060236930847\n",
      "grad W: 5.885343551635742\n",
      "grad W_in: 1.1582037210464478\n",
      "[ep 14 | batch 0] loss=0.6671 |M| |W| mean=2.384e-02\n",
      "ep 14 batch 0 | loss=0.6671 | alloc=0.88GB reserv=27.15GB\n",
      "grad W_out: 0.26881736516952515\n",
      "grad W: 9.246816635131836\n",
      "grad W_in: 4.9701128005981445\n",
      "grad W_out: 0.2014206349849701\n",
      "grad W: 3.8764514923095703\n",
      "grad W_in: 1.240138292312622\n",
      "[ep 14 | batch 2] loss=0.6697 |M| |W| mean=2.385e-02\n",
      "ep 14 batch 2 | loss=0.6697 | alloc=0.88GB reserv=27.15GB\n",
      "grad W_out: 0.2738422155380249\n",
      "grad W: 4.588683605194092\n",
      "grad W_in: 3.1429295539855957\n",
      "grad W_out: 0.31872859597206116\n",
      "grad W: 6.863844871520996\n",
      "grad W_in: 3.110006809234619\n",
      "[ep 14 | batch 4] loss=0.7130 |M| |W| mean=2.386e-02\n",
      "ep 14 batch 4 | loss=0.7130 | alloc=0.88GB reserv=27.15GB\n",
      "grad W_out: 0.40231889486312866\n",
      "grad W: 8.805109024047852\n",
      "grad W_in: 2.171372175216675\n",
      "grad W_out: 0.3512095808982849\n",
      "grad W: 4.769611835479736\n",
      "grad W_in: 2.7597222328186035\n",
      "[ep 14 | batch 6] loss=0.7396 |M| |W| mean=2.386e-02\n",
      "ep 14 batch 6 | loss=0.7396 | alloc=0.88GB reserv=27.16GB\n",
      "grad W_out: 0.2407827377319336\n",
      "grad W: 3.1423709392547607\n",
      "grad W_in: 1.7847375869750977\n",
      "grad W_out: 0.30431339144706726\n",
      "grad W: 8.00325870513916\n",
      "grad W_in: 1.7370432615280151\n",
      "[ep 14 | batch 8] loss=0.6715 |M| |W| mean=2.386e-02\n",
      "ep 14 batch 8 | loss=0.6715 | alloc=0.88GB reserv=27.16GB\n",
      "grad W_out: 0.37576472759246826\n",
      "grad W: 6.848927974700928\n",
      "grad W_in: 2.2465598583221436\n",
      "grad W_out: 0.2793601155281067\n",
      "grad W: 5.20662784576416\n",
      "grad W_in: 3.8380441665649414\n",
      "[ep 14 | batch 10] loss=0.7167 |M| |W| mean=2.386e-02\n",
      "ep 14 batch 10 | loss=0.7167 | alloc=0.89GB reserv=27.16GB\n",
      "grad W_out: 0.3451220989227295\n",
      "grad W: 5.803466796875\n",
      "grad W_in: 3.682436227798462\n",
      "grad W_out: 0.41455283761024475\n",
      "grad W: 6.902495861053467\n",
      "grad W_in: 1.388585090637207\n",
      "[ep 14 | batch 12] loss=0.6947 |M| |W| mean=2.386e-02\n",
      "ep 14 batch 12 | loss=0.6947 | alloc=0.89GB reserv=27.16GB\n",
      "grad W_out: 0.29081040620803833\n",
      "grad W: 5.612151145935059\n",
      "grad W_in: 3.133362054824829\n",
      "grad W_out: 0.3559301197528839\n",
      "grad W: 6.096992015838623\n",
      "grad W_in: 1.950995683670044\n",
      "[ep 14 | batch 14] loss=0.7022 |M| |W| mean=2.386e-02\n",
      "ep 14 batch 14 | loss=0.7022 | alloc=0.89GB reserv=27.16GB\n",
      "grad W_out: 0.16973575949668884\n",
      "grad W: 4.232792377471924\n",
      "grad W_in: 2.041384696960449\n",
      "grad W_out: 0.2754744291305542\n",
      "grad W: 4.9555792808532715\n",
      "grad W_in: 3.1500213146209717\n",
      "[ep 14 | batch 16] loss=0.6707 |M| |W| mean=2.387e-02\n",
      "ep 14 batch 16 | loss=0.6707 | alloc=0.89GB reserv=27.17GB\n",
      "grad W_out: 0.16641643643379211\n",
      "grad W: 3.2028605937957764\n",
      "grad W_in: 0.9697487354278564\n",
      "grad W_out: 0.2885657250881195\n",
      "grad W: 3.6613035202026367\n",
      "grad W_in: 0.7225819826126099\n",
      "[ep 14 | batch 18] loss=0.7639 |M| |W| mean=2.387e-02\n",
      "ep 14 batch 18 | loss=0.7639 | alloc=0.89GB reserv=27.17GB\n",
      "grad W_out: 0.2511030435562134\n",
      "grad W: 5.658684253692627\n",
      "grad W_in: 1.8454053401947021\n",
      "grad W_out: 0.25410547852516174\n",
      "grad W: 4.253021717071533\n",
      "grad W_in: 1.5563993453979492\n",
      "[ep 14 | batch 20] loss=0.6941 |M| |W| mean=2.388e-02\n",
      "ep 14 batch 20 | loss=0.6941 | alloc=0.89GB reserv=27.17GB\n",
      "grad W_out: 0.3465794324874878\n",
      "grad W: 4.6689677238464355\n",
      "grad W_in: 1.9085203409194946\n",
      "grad W_out: 0.3479248583316803\n",
      "grad W: 5.69884729385376\n",
      "grad W_in: 0.7939456701278687\n",
      "[ep 14 | batch 22] loss=0.7301 |M| |W| mean=2.389e-02\n",
      "ep 14 batch 22 | loss=0.7301 | alloc=0.89GB reserv=27.17GB\n",
      "grad W_out: 0.17922569811344147\n",
      "grad W: 4.29092264175415\n",
      "grad W_in: 4.437662124633789\n",
      "grad W_out: 0.19447365403175354\n",
      "grad W: 3.0217204093933105\n",
      "grad W_in: 2.764171600341797\n",
      "[ep 14 | batch 24] loss=0.6768 |M| |W| mean=2.390e-02\n",
      "ep 14 batch 24 | loss=0.6768 | alloc=0.90GB reserv=27.17GB\n",
      "grad W_out: 0.2690144181251526\n",
      "grad W: 4.281572341918945\n",
      "grad W_in: 2.867116928100586\n",
      "grad W_out: 0.29875728487968445\n",
      "grad W: 4.817417144775391\n",
      "grad W_in: 2.1895956993103027\n",
      "[ep 14 | batch 26] loss=0.7234 |M| |W| mean=2.391e-02\n",
      "ep 14 batch 26 | loss=0.7234 | alloc=0.90GB reserv=27.18GB\n",
      "grad W_out: 0.27499836683273315\n",
      "grad W: 3.736558437347412\n",
      "grad W_in: 1.792248249053955\n",
      "grad W_out: 0.21090158820152283\n",
      "grad W: 3.6350550651550293\n",
      "grad W_in: 0.9694270491600037\n",
      "[ep 14 | batch 28] loss=0.6360 |M| |W| mean=2.391e-02\n",
      "ep 14 batch 28 | loss=0.6360 | alloc=0.90GB reserv=27.18GB\n",
      "grad W_out: 0.19181878864765167\n",
      "grad W: 5.563469886779785\n",
      "grad W_in: 6.446378707885742\n",
      "grad W_out: 0.21735738217830658\n",
      "grad W: 4.940649509429932\n",
      "grad W_in: 2.017974615097046\n",
      "[ep 14 | batch 30] loss=0.7018 |M| |W| mean=2.391e-02\n",
      "ep 14 batch 30 | loss=0.7018 | alloc=0.90GB reserv=27.18GB\n",
      "grad W_out: 0.2722536325454712\n",
      "grad W: 5.757749557495117\n",
      "grad W_in: 2.9906530380249023\n",
      "grad W_out: 0.18863266706466675\n",
      "grad W: 4.841367244720459\n",
      "grad W_in: 4.016556262969971\n",
      "[ep 14 | batch 32] loss=0.7335 |M| |W| mean=2.392e-02\n",
      "ep 14 batch 32 | loss=0.7335 | alloc=0.90GB reserv=27.18GB\n",
      "grad W_out: 0.28102049231529236\n",
      "grad W: 5.555707931518555\n",
      "grad W_in: 3.3296213150024414\n",
      "grad W_out: 0.23904773592948914\n",
      "grad W: 5.501364231109619\n",
      "grad W_in: 1.7226214408874512\n",
      "[ep 14 | batch 34] loss=0.6884 |M| |W| mean=2.392e-02\n",
      "ep 14 batch 34 | loss=0.6884 | alloc=0.90GB reserv=27.18GB\n",
      "grad W_out: 0.248045414686203\n",
      "grad W: 3.6235110759735107\n",
      "grad W_in: 1.145149827003479\n",
      "grad W_out: 0.31375622749328613\n",
      "grad W: 4.501340866088867\n",
      "grad W_in: 3.57853364944458\n",
      "[ep 14 | batch 36] loss=0.7909 |M| |W| mean=2.392e-02\n",
      "ep 14 batch 36 | loss=0.7909 | alloc=0.90GB reserv=27.19GB\n",
      "grad W_out: 0.34472212195396423\n",
      "grad W: 6.5117316246032715\n",
      "grad W_in: 1.1404134035110474\n",
      "grad W_out: 0.34103330969810486\n",
      "grad W: 6.320435523986816\n",
      "grad W_in: 2.401862859725952\n",
      "[ep 14 | batch 38] loss=0.6628 |M| |W| mean=2.392e-02\n",
      "ep 14 batch 38 | loss=0.6628 | alloc=0.91GB reserv=27.19GB\n",
      "grad W_out: 0.21860209107398987\n",
      "grad W: 4.1725263595581055\n",
      "grad W_in: 3.01774525642395\n",
      "grad W_out: 0.3095792829990387\n",
      "grad W: 5.728246688842773\n",
      "grad W_in: 1.9371651411056519\n",
      "[ep 14 | batch 40] loss=0.7069 |M| |W| mean=2.393e-02\n",
      "ep 14 batch 40 | loss=0.7069 | alloc=0.91GB reserv=27.19GB\n",
      "grad W_out: 0.32360023260116577\n",
      "grad W: 9.13394546508789\n",
      "grad W_in: 5.049030780792236\n",
      "grad W_out: 0.3448069393634796\n",
      "grad W: 5.731328010559082\n",
      "grad W_in: 2.2343862056732178\n",
      "[ep 14 | batch 42] loss=0.6302 |M| |W| mean=2.393e-02\n",
      "ep 14 batch 42 | loss=0.6302 | alloc=0.91GB reserv=27.19GB\n",
      "grad W_out: 0.26338860392570496\n",
      "grad W: 5.502446174621582\n",
      "grad W_in: 3.7589359283447266\n",
      "grad W_out: 0.20736750960350037\n",
      "grad W: 5.982615947723389\n",
      "grad W_in: 1.3850399255752563\n",
      "[ep 14 | batch 44] loss=0.6631 |M| |W| mean=2.393e-02\n",
      "ep 14 batch 44 | loss=0.6631 | alloc=0.91GB reserv=27.19GB\n",
      "grad W_out: 0.19943846762180328\n",
      "grad W: 5.775843620300293\n",
      "grad W_in: 2.8306357860565186\n",
      "grad W_out: 0.24528661370277405\n",
      "grad W: 4.340603828430176\n",
      "grad W_in: 0.7916269302368164\n",
      "[ep 14 | batch 46] loss=0.7295 |M| |W| mean=2.393e-02\n",
      "ep 14 batch 46 | loss=0.7295 | alloc=0.91GB reserv=27.20GB\n",
      "grad W_out: 0.3085666298866272\n",
      "grad W: 5.8463826179504395\n",
      "grad W_in: 0.9982234239578247\n",
      "grad W_out: 0.3186624050140381\n",
      "grad W: 8.180243492126465\n",
      "grad W_in: 2.1633195877075195\n",
      "[ep 14 | batch 48] loss=0.7589 |M| |W| mean=2.394e-02\n",
      "ep 14 batch 48 | loss=0.7589 | alloc=0.91GB reserv=27.20GB\n",
      "grad W_out: 0.23464657366275787\n",
      "grad W: 6.0657806396484375\n",
      "grad W_in: 4.040243148803711\n",
      "grad W_out: 0.22164742648601532\n",
      "grad W: 4.988034248352051\n",
      "grad W_in: 2.0652151107788086\n",
      "[ep 14 | batch 50] loss=0.6172 |M| |W| mean=2.395e-02\n",
      "ep 14 batch 50 | loss=0.6172 | alloc=0.91GB reserv=27.20GB\n",
      "grad W_out: 0.32559844851493835\n",
      "grad W: 7.702756404876709\n",
      "grad W_in: 5.450370788574219\n",
      "grad W_out: 0.3278098702430725\n",
      "grad W: 6.530339241027832\n",
      "grad W_in: 2.2935566902160645\n",
      "[ep 14 | batch 52] loss=0.6989 |M| |W| mean=2.395e-02\n",
      "ep 14 batch 52 | loss=0.6989 | alloc=0.92GB reserv=27.20GB\n",
      "grad W_out: 0.221090629696846\n",
      "grad W: 3.878844976425171\n",
      "grad W_in: 1.2667771577835083\n",
      "grad W_out: 0.3260764181613922\n",
      "grad W: 3.930701732635498\n",
      "grad W_in: 0.8544359803199768\n",
      "[ep 14 | batch 54] loss=0.6656 |M| |W| mean=2.396e-02\n",
      "ep 14 batch 54 | loss=0.6656 | alloc=0.92GB reserv=27.20GB\n",
      "grad W_out: 0.273904949426651\n",
      "grad W: 5.813589572906494\n",
      "grad W_in: 1.3349658250808716\n",
      "grad W_out: 0.18865084648132324\n",
      "grad W: 3.232537269592285\n",
      "grad W_in: 1.3984746932983398\n",
      "[ep 14 | batch 56] loss=0.6687 |M| |W| mean=2.396e-02\n",
      "ep 14 batch 56 | loss=0.6687 | alloc=0.92GB reserv=27.21GB\n",
      "grad W_out: 0.2844628393650055\n",
      "grad W: 6.457539081573486\n",
      "grad W_in: 1.952467441558838\n",
      "grad W_out: 0.2411244958639145\n",
      "grad W: 2.9433581829071045\n",
      "grad W_in: 2.3152413368225098\n",
      "[ep 14 | batch 58] loss=0.6861 |M| |W| mean=2.396e-02\n",
      "ep 14 batch 58 | loss=0.6861 | alloc=0.92GB reserv=27.21GB\n",
      "grad W_out: 0.3362220823764801\n",
      "grad W: 4.23522424697876\n",
      "grad W_in: 3.428332567214966\n",
      "grad W_out: 0.3344050347805023\n",
      "grad W: 5.290539264678955\n",
      "grad W_in: 2.6401331424713135\n",
      "[ep 14 | batch 60] loss=0.7061 |M| |W| mean=2.396e-02\n",
      "ep 14 batch 60 | loss=0.7061 | alloc=0.92GB reserv=27.21GB\n",
      "grad W_out: 0.26893308758735657\n",
      "grad W: 4.558103561401367\n",
      "grad W_in: 0.8398303389549255\n",
      "grad W_out: 0.3487229645252228\n",
      "grad W: 4.221565246582031\n",
      "grad W_in: 1.1531084775924683\n",
      "[ep 14 | batch 62] loss=0.6817 |M| |W| mean=2.396e-02\n",
      "ep 14 batch 62 | loss=0.6817 | alloc=0.92GB reserv=27.21GB\n",
      "grad W_out: 0.35500362515449524\n",
      "grad W: 4.35795259475708\n",
      "grad W_in: 1.6377352476119995\n",
      "grad W_out: 0.2959059476852417\n",
      "grad W: 6.921570301055908\n",
      "grad W_in: 5.648314476013184\n",
      "[ep 14 | batch 64] loss=0.7670 |M| |W| mean=2.397e-02\n",
      "ep 14 batch 64 | loss=0.7670 | alloc=0.92GB reserv=27.21GB\n",
      "grad W_out: 0.34795767068862915\n",
      "grad W: 8.38254451751709\n",
      "grad W_in: 6.94482946395874\n",
      "grad W_out: 0.3733753561973572\n",
      "grad W: 4.578485488891602\n",
      "grad W_in: 1.3417619466781616\n",
      "[ep 14 | batch 66] loss=0.7167 |M| |W| mean=2.398e-02\n",
      "ep 14 batch 66 | loss=0.7167 | alloc=0.93GB reserv=27.21GB\n",
      "grad W_out: 0.3985239565372467\n",
      "grad W: 4.220424652099609\n",
      "grad W_in: 1.463645100593567\n",
      "grad W_out: 0.1804261952638626\n",
      "grad W: 3.388444662094116\n",
      "grad W_in: 2.0348360538482666\n",
      "[ep 14 | batch 68] loss=0.7101 |M| |W| mean=2.399e-02\n",
      "ep 14 batch 68 | loss=0.7101 | alloc=0.93GB reserv=27.22GB\n",
      "grad W_out: 0.16977930068969727\n",
      "grad W: 2.6326544284820557\n",
      "grad W_in: 0.9989981055259705\n",
      "grad W_out: 0.21410347521305084\n",
      "grad W: 4.628537178039551\n",
      "grad W_in: 2.1727890968322754\n",
      "[ep 14 | batch 70] loss=0.6979 |M| |W| mean=2.399e-02\n",
      "ep 14 batch 70 | loss=0.6979 | alloc=0.93GB reserv=27.22GB\n",
      "grad W_out: 0.2828867435455322\n",
      "grad W: 5.758996963500977\n",
      "grad W_in: 4.67448616027832\n",
      "grad W_out: 0.20467860996723175\n",
      "grad W: 4.594937324523926\n",
      "grad W_in: 1.7978302240371704\n",
      "[ep 14 | batch 72] loss=0.6024 |M| |W| mean=2.399e-02\n",
      "ep 14 batch 72 | loss=0.6024 | alloc=0.93GB reserv=27.22GB\n",
      "grad W_out: 0.3144432306289673\n",
      "grad W: 7.301098823547363\n",
      "grad W_in: 4.192876815795898\n",
      "grad W_out: 0.21656647324562073\n",
      "grad W: 5.216272354125977\n",
      "grad W_in: 1.38787841796875\n",
      "[ep 14 | batch 74] loss=0.7064 |M| |W| mean=2.399e-02\n",
      "ep 14 batch 74 | loss=0.7064 | alloc=0.93GB reserv=27.22GB\n",
      "grad W_out: 0.26081085205078125\n",
      "grad W: 7.782729148864746\n",
      "grad W_in: 6.816732883453369\n",
      "grad W_out: 0.40624091029167175\n",
      "grad W: 6.88486385345459\n",
      "grad W_in: 5.914106369018555\n",
      "[ep 14 | batch 76] loss=0.6886 |M| |W| mean=2.400e-02\n",
      "ep 14 batch 76 | loss=0.6886 | alloc=0.93GB reserv=27.22GB\n",
      "grad W_out: 0.3493872582912445\n",
      "grad W: 5.860729217529297\n",
      "grad W_in: 3.2515876293182373\n",
      "grad W_out: 0.3163691461086273\n",
      "grad W: 7.532265663146973\n",
      "grad W_in: 5.930819988250732\n",
      "[ep 14 | batch 78] loss=0.7084 |M| |W| mean=2.400e-02\n",
      "ep 14 batch 78 | loss=0.7084 | alloc=0.93GB reserv=27.23GB\n",
      "grad W_out: 0.22530978918075562\n",
      "grad W: 3.7994444370269775\n",
      "grad W_in: 1.0330668687820435\n",
      "grad W_out: 0.31215423345565796\n",
      "grad W: 5.75355339050293\n",
      "grad W_in: 1.927063226699829\n",
      "[ep 14 | batch 80] loss=0.7726 |M| |W| mean=2.400e-02\n",
      "ep 14 batch 80 | loss=0.7726 | alloc=0.94GB reserv=27.23GB\n",
      "grad W_out: 0.2657513916492462\n",
      "grad W: 7.763514518737793\n",
      "grad W_in: 5.808488845825195\n",
      "grad W_out: 0.33469918370246887\n",
      "grad W: 5.890689849853516\n",
      "grad W_in: 2.206291437149048\n",
      "[ep 14 | batch 82] loss=0.6587 |M| |W| mean=2.401e-02\n",
      "ep 14 batch 82 | loss=0.6587 | alloc=0.94GB reserv=27.23GB\n",
      "grad W_out: 0.2643370032310486\n",
      "grad W: 7.137728691101074\n",
      "grad W_in: 5.531493663787842\n",
      "grad W_out: 0.4266636073589325\n",
      "grad W: 10.400985717773438\n",
      "grad W_in: 7.814583778381348\n",
      "[ep 14 | batch 84] loss=0.7769 |M| |W| mean=2.401e-02\n",
      "ep 14 batch 84 | loss=0.7769 | alloc=0.94GB reserv=27.23GB\n",
      "grad W_out: 0.24729299545288086\n",
      "grad W: 4.9284210205078125\n",
      "grad W_in: 1.0591928958892822\n",
      "grad W_out: 0.25148117542266846\n",
      "grad W: 4.310128211975098\n",
      "grad W_in: 2.3759899139404297\n",
      "[ep 14 | batch 86] loss=0.6657 |M| |W| mean=2.401e-02\n",
      "ep 14 batch 86 | loss=0.6657 | alloc=0.94GB reserv=27.23GB\n",
      "grad W_out: 0.3139422535896301\n",
      "grad W: 4.091672897338867\n",
      "grad W_in: 3.727785110473633\n",
      "grad W_out: 0.34307268261909485\n",
      "grad W: 6.050469398498535\n",
      "grad W_in: 1.2367249727249146\n",
      "[ep 14 | batch 88] loss=0.7188 |M| |W| mean=2.401e-02\n",
      "ep 14 batch 88 | loss=0.7188 | alloc=0.94GB reserv=27.24GB\n",
      "grad W_out: 0.2624674439430237\n",
      "grad W: 3.9768178462982178\n",
      "grad W_in: 2.5830376148223877\n",
      "grad W_out: 0.2826777398586273\n",
      "grad W: 5.152585506439209\n",
      "grad W_in: 3.6945858001708984\n",
      "[ep 14 | batch 90] loss=0.7156 |M| |W| mean=2.401e-02\n",
      "ep 14 batch 90 | loss=0.7156 | alloc=0.94GB reserv=27.24GB\n",
      "grad W_out: 0.21554726362228394\n",
      "grad W: 5.176513671875\n",
      "grad W_in: 2.820425033569336\n",
      "grad W_out: 0.2518852949142456\n",
      "grad W: 5.882665157318115\n",
      "grad W_in: 1.6655526161193848\n",
      "[ep 14 | batch 92] loss=0.6310 |M| |W| mean=2.401e-02\n",
      "ep 14 batch 92 | loss=0.6310 | alloc=0.94GB reserv=27.24GB\n",
      "grad W_out: 0.2468128651380539\n",
      "grad W: 4.639098167419434\n",
      "grad W_in: 3.4323904514312744\n",
      "Epoch 14/20 | lr=1.00e-03 | loss=0.6936 | test_acc=77.71% | dt=1177.7s\n",
      "grad W_out: 0.304165244102478\n",
      "grad W: 4.435557842254639\n",
      "grad W_in: 1.4196746349334717\n",
      "[ep 15 | batch 0] loss=0.7219 |M| |W| mean=2.401e-02\n",
      "ep 15 batch 0 | loss=0.7219 | alloc=0.94GB reserv=27.24GB\n",
      "grad W_out: 0.31695255637168884\n",
      "grad W: 5.986943244934082\n",
      "grad W_in: 2.624255657196045\n",
      "grad W_out: 0.3342072367668152\n",
      "grad W: 4.447917461395264\n",
      "grad W_in: 1.5251916646957397\n",
      "[ep 15 | batch 2] loss=0.6919 |M| |W| mean=2.401e-02\n",
      "ep 15 batch 2 | loss=0.6919 | alloc=0.95GB reserv=27.24GB\n",
      "grad W_out: 0.2755048871040344\n",
      "grad W: 7.871204376220703\n",
      "grad W_in: 6.628724575042725\n",
      "grad W_out: 0.20105549693107605\n",
      "grad W: 3.8456151485443115\n",
      "grad W_in: 1.3643171787261963\n",
      "[ep 15 | batch 4] loss=0.6785 |M| |W| mean=2.402e-02\n",
      "ep 15 batch 4 | loss=0.6785 | alloc=0.95GB reserv=27.25GB\n",
      "grad W_out: 0.26116397976875305\n",
      "grad W: 6.265015125274658\n",
      "grad W_in: 4.8312835693359375\n",
      "grad W_out: 0.29468217492103577\n",
      "grad W: 4.733377456665039\n",
      "grad W_in: 2.336339235305786\n",
      "[ep 15 | batch 6] loss=0.6930 |M| |W| mean=2.402e-02\n",
      "ep 15 batch 6 | loss=0.6930 | alloc=0.95GB reserv=27.25GB\n",
      "grad W_out: 0.24444015324115753\n",
      "grad W: 5.197711944580078\n",
      "grad W_in: 1.8874778747558594\n",
      "grad W_out: 0.4484676122665405\n",
      "grad W: 5.845247745513916\n",
      "grad W_in: 3.159320116043091\n",
      "[ep 15 | batch 8] loss=0.6728 |M| |W| mean=2.402e-02\n",
      "ep 15 batch 8 | loss=0.6728 | alloc=0.95GB reserv=27.25GB\n",
      "grad W_out: 0.18359529972076416\n",
      "grad W: 5.915156841278076\n",
      "grad W_in: 3.6909115314483643\n",
      "grad W_out: 0.3753684461116791\n",
      "grad W: 7.264855861663818\n",
      "grad W_in: 5.587806224822998\n",
      "[ep 15 | batch 10] loss=0.7126 |M| |W| mean=2.403e-02\n",
      "ep 15 batch 10 | loss=0.7126 | alloc=0.95GB reserv=27.25GB\n",
      "grad W_out: 0.46927234530448914\n",
      "grad W: 6.849222183227539\n",
      "grad W_in: 2.2061169147491455\n",
      "grad W_out: 0.26598674058914185\n",
      "grad W: 6.236893177032471\n",
      "grad W_in: 2.5990970134735107\n",
      "[ep 15 | batch 12] loss=0.6603 |M| |W| mean=2.402e-02\n",
      "ep 15 batch 12 | loss=0.6603 | alloc=0.95GB reserv=27.25GB\n",
      "grad W_out: 0.26048538088798523\n",
      "grad W: 9.059556007385254\n",
      "grad W_in: 8.288764953613281\n",
      "grad W_out: 0.35591650009155273\n",
      "grad W: 7.07695198059082\n",
      "grad W_in: 6.753047466278076\n",
      "[ep 15 | batch 14] loss=0.7375 |M| |W| mean=2.403e-02\n",
      "ep 15 batch 14 | loss=0.7375 | alloc=0.96GB reserv=27.26GB\n",
      "grad W_out: 0.33381912112236023\n",
      "grad W: 8.003604888916016\n",
      "grad W_in: 8.456687927246094\n",
      "grad W_out: 0.2785647511482239\n",
      "grad W: 8.898151397705078\n",
      "grad W_in: 9.496095657348633\n",
      "[ep 15 | batch 16] loss=0.6396 |M| |W| mean=2.403e-02\n",
      "ep 15 batch 16 | loss=0.6396 | alloc=0.96GB reserv=27.26GB\n",
      "grad W_out: 0.3797548711299896\n",
      "grad W: 5.0303850173950195\n",
      "grad W_in: 3.101318597793579\n",
      "grad W_out: 0.30607157945632935\n",
      "grad W: 7.04108190536499\n",
      "grad W_in: 6.854614734649658\n",
      "[ep 15 | batch 18] loss=0.7183 |M| |W| mean=2.403e-02\n",
      "ep 15 batch 18 | loss=0.7183 | alloc=0.96GB reserv=27.26GB\n",
      "grad W_out: 0.3367784917354584\n",
      "grad W: 8.488330841064453\n",
      "grad W_in: 9.818033218383789\n",
      "grad W_out: 0.27643418312072754\n",
      "grad W: 9.713902473449707\n",
      "grad W_in: 9.462925910949707\n",
      "[ep 15 | batch 20] loss=0.7236 |M| |W| mean=2.404e-02\n",
      "ep 15 batch 20 | loss=0.7236 | alloc=0.96GB reserv=27.26GB\n",
      "grad W_out: 0.34815502166748047\n",
      "grad W: 6.548177719116211\n",
      "grad W_in: 3.8388149738311768\n",
      "grad W_out: 0.3447834849357605\n",
      "grad W: 3.9831840991973877\n",
      "grad W_in: 2.0912024974823\n",
      "[ep 15 | batch 22] loss=0.6919 |M| |W| mean=2.404e-02\n",
      "ep 15 batch 22 | loss=0.6919 | alloc=0.96GB reserv=27.26GB\n",
      "grad W_out: 0.30186712741851807\n",
      "grad W: 5.821902751922607\n",
      "grad W_in: 5.167258262634277\n",
      "grad W_out: 0.39365750551223755\n",
      "grad W: 8.081238746643066\n",
      "grad W_in: 5.4892144203186035\n",
      "[ep 15 | batch 24] loss=0.6545 |M| |W| mean=2.405e-02\n",
      "ep 15 batch 24 | loss=0.6545 | alloc=0.96GB reserv=27.27GB\n",
      "grad W_out: 0.23661652207374573\n",
      "grad W: 5.5550007820129395\n",
      "grad W_in: 2.7002623081207275\n",
      "grad W_out: 0.33655133843421936\n",
      "grad W: 5.9863457679748535\n",
      "grad W_in: 4.899349212646484\n",
      "[ep 15 | batch 26] loss=0.7429 |M| |W| mean=2.406e-02\n",
      "ep 15 batch 26 | loss=0.7429 | alloc=0.96GB reserv=27.27GB\n",
      "grad W_out: 0.3449574112892151\n",
      "grad W: 9.554742813110352\n",
      "grad W_in: 9.861328125\n",
      "grad W_out: 0.25723665952682495\n",
      "grad W: 5.410411357879639\n",
      "grad W_in: 4.3508758544921875\n",
      "[ep 15 | batch 28] loss=0.6346 |M| |W| mean=2.405e-02\n",
      "ep 15 batch 28 | loss=0.6346 | alloc=0.97GB reserv=27.27GB\n",
      "grad W_out: 0.405085951089859\n",
      "grad W: 11.292243957519531\n",
      "grad W_in: 10.595541000366211\n",
      "grad W_out: 0.36560577154159546\n",
      "grad W: 3.426241159439087\n",
      "grad W_in: 1.3482729196548462\n",
      "[ep 15 | batch 30] loss=0.7134 |M| |W| mean=2.407e-02\n",
      "ep 15 batch 30 | loss=0.7134 | alloc=0.97GB reserv=27.27GB\n",
      "grad W_out: 0.34775054454803467\n",
      "grad W: 5.351243495941162\n",
      "grad W_in: 5.9133076667785645\n",
      "grad W_out: 0.49302029609680176\n",
      "grad W: 8.333184242248535\n",
      "grad W_in: 7.343842506408691\n",
      "[ep 15 | batch 32] loss=0.7622 |M| |W| mean=2.407e-02\n",
      "ep 15 batch 32 | loss=0.7622 | alloc=0.97GB reserv=27.27GB\n",
      "grad W_out: 0.29023277759552\n",
      "grad W: 4.295132160186768\n",
      "grad W_in: 1.40615713596344\n",
      "grad W_out: 0.2553907036781311\n",
      "grad W: 8.006772994995117\n",
      "grad W_in: 7.004242897033691\n",
      "[ep 15 | batch 34] loss=0.7151 |M| |W| mean=2.407e-02\n",
      "ep 15 batch 34 | loss=0.7151 | alloc=0.97GB reserv=27.28GB\n",
      "grad W_out: 0.2874128818511963\n",
      "grad W: 11.809089660644531\n",
      "grad W_in: 10.752275466918945\n",
      "grad W_out: 0.2496473491191864\n",
      "grad W: 4.118740081787109\n",
      "grad W_in: 3.613341808319092\n",
      "[ep 15 | batch 36] loss=0.6884 |M| |W| mean=2.407e-02\n",
      "ep 15 batch 36 | loss=0.6884 | alloc=0.97GB reserv=27.28GB\n",
      "grad W_out: 0.3275597095489502\n",
      "grad W: 10.057024002075195\n",
      "grad W_in: 8.440445899963379\n",
      "grad W_out: 0.38879090547561646\n",
      "grad W: 6.157973766326904\n",
      "grad W_in: 4.850809574127197\n",
      "[ep 15 | batch 38] loss=0.7021 |M| |W| mean=2.407e-02\n",
      "ep 15 batch 38 | loss=0.7021 | alloc=0.97GB reserv=27.28GB\n",
      "grad W_out: 0.2827586531639099\n",
      "grad W: 7.946498394012451\n",
      "grad W_in: 5.7241597175598145\n",
      "grad W_out: 0.33798784017562866\n",
      "grad W: 5.693004608154297\n",
      "grad W_in: 1.7579833269119263\n",
      "[ep 15 | batch 40] loss=0.7198 |M| |W| mean=2.407e-02\n",
      "ep 15 batch 40 | loss=0.7198 | alloc=0.97GB reserv=27.28GB\n",
      "grad W_out: 0.2556389570236206\n",
      "grad W: 5.546350479125977\n",
      "grad W_in: 4.7007951736450195\n",
      "grad W_out: 0.31501227617263794\n",
      "grad W: 4.190073490142822\n",
      "grad W_in: 1.7605884075164795\n",
      "[ep 15 | batch 42] loss=0.6359 |M| |W| mean=2.407e-02\n",
      "ep 15 batch 42 | loss=0.6359 | alloc=0.98GB reserv=27.28GB\n",
      "grad W_out: 0.3035541772842407\n",
      "grad W: 8.849713325500488\n",
      "grad W_in: 6.56382417678833\n",
      "grad W_out: 0.4028608500957489\n",
      "grad W: 6.320057392120361\n",
      "grad W_in: 5.45170783996582\n",
      "[ep 15 | batch 44] loss=0.6804 |M| |W| mean=2.407e-02\n",
      "ep 15 batch 44 | loss=0.6804 | alloc=0.98GB reserv=27.29GB\n",
      "grad W_out: 0.26587679982185364\n",
      "grad W: 7.158199787139893\n",
      "grad W_in: 4.302537441253662\n",
      "grad W_out: 0.27414894104003906\n",
      "grad W: 5.336953163146973\n",
      "grad W_in: 4.203049659729004\n",
      "[ep 15 | batch 46] loss=0.7158 |M| |W| mean=2.408e-02\n",
      "ep 15 batch 46 | loss=0.7158 | alloc=0.98GB reserv=27.29GB\n",
      "grad W_out: 0.3605663478374481\n",
      "grad W: 7.893089771270752\n",
      "grad W_in: 5.258388996124268\n",
      "grad W_out: 0.28090211749076843\n",
      "grad W: 7.926437854766846\n",
      "grad W_in: 8.479351043701172\n",
      "[ep 15 | batch 48] loss=0.6087 |M| |W| mean=2.408e-02\n",
      "ep 15 batch 48 | loss=0.6087 | alloc=0.98GB reserv=27.29GB\n",
      "grad W_out: 0.2913699746131897\n",
      "grad W: 6.641420364379883\n",
      "grad W_in: 3.919693946838379\n",
      "grad W_out: 0.17266815900802612\n",
      "grad W: 5.660311222076416\n",
      "grad W_in: 2.886324644088745\n",
      "[ep 15 | batch 50] loss=0.5788 |M| |W| mean=2.409e-02\n",
      "ep 15 batch 50 | loss=0.5788 | alloc=0.98GB reserv=27.29GB\n",
      "grad W_out: 0.4040355384349823\n",
      "grad W: 5.041802406311035\n",
      "grad W_in: 2.8170948028564453\n",
      "grad W_out: 0.24355360865592957\n",
      "grad W: 5.515316486358643\n",
      "grad W_in: 2.000148296356201\n",
      "[ep 15 | batch 52] loss=0.6169 |M| |W| mean=2.409e-02\n",
      "ep 15 batch 52 | loss=0.6169 | alloc=0.98GB reserv=27.29GB\n",
      "grad W_out: 0.37619173526763916\n",
      "grad W: 9.178563117980957\n",
      "grad W_in: 1.8875856399536133\n",
      "grad W_out: 0.23852182924747467\n",
      "grad W: 5.420686721801758\n",
      "grad W_in: 2.8058645725250244\n",
      "[ep 15 | batch 54] loss=0.6751 |M| |W| mean=2.410e-02\n",
      "ep 15 batch 54 | loss=0.6751 | alloc=0.98GB reserv=27.29GB\n",
      "grad W_out: 0.45302319526672363\n",
      "grad W: 6.120342254638672\n",
      "grad W_in: 2.6928012371063232\n",
      "grad W_out: 0.43443453311920166\n",
      "grad W: 5.085196495056152\n",
      "grad W_in: 1.4445959329605103\n",
      "[ep 15 | batch 56] loss=0.6748 |M| |W| mean=2.410e-02\n",
      "ep 15 batch 56 | loss=0.6748 | alloc=0.99GB reserv=27.30GB\n",
      "grad W_out: 0.46395495533943176\n",
      "grad W: 7.699034214019775\n",
      "grad W_in: 4.353977203369141\n",
      "grad W_out: 0.4334564208984375\n",
      "grad W: 7.437047481536865\n",
      "grad W_in: 3.789524793624878\n",
      "[ep 15 | batch 58] loss=0.7078 |M| |W| mean=2.411e-02\n",
      "ep 15 batch 58 | loss=0.7078 | alloc=0.99GB reserv=27.30GB\n",
      "grad W_out: 0.32117515802383423\n",
      "grad W: 3.990030288696289\n",
      "grad W_in: 2.0945870876312256\n",
      "grad W_out: 0.3356490433216095\n",
      "grad W: 4.284907817840576\n",
      "grad W_in: 3.631593942642212\n",
      "[ep 15 | batch 60] loss=0.6881 |M| |W| mean=2.411e-02\n",
      "ep 15 batch 60 | loss=0.6881 | alloc=0.99GB reserv=27.30GB\n",
      "grad W_out: 0.47285372018814087\n",
      "grad W: 13.425869941711426\n",
      "grad W_in: 2.4759814739227295\n",
      "grad W_out: 0.26884642243385315\n",
      "grad W: 7.01314115524292\n",
      "grad W_in: 1.7508050203323364\n",
      "[ep 15 | batch 62] loss=0.6046 |M| |W| mean=2.411e-02\n",
      "ep 15 batch 62 | loss=0.6046 | alloc=0.99GB reserv=27.30GB\n",
      "grad W_out: 0.4674307107925415\n",
      "grad W: 6.716978549957275\n",
      "grad W_in: 3.3057861328125\n",
      "grad W_out: 0.5140535831451416\n",
      "grad W: 8.5258150100708\n",
      "grad W_in: 4.965484142303467\n",
      "[ep 15 | batch 64] loss=0.7558 |M| |W| mean=2.411e-02\n",
      "ep 15 batch 64 | loss=0.7558 | alloc=0.99GB reserv=27.30GB\n",
      "grad W_out: 0.3077402114868164\n",
      "grad W: 6.129916191101074\n",
      "grad W_in: 4.372439384460449\n",
      "grad W_out: 0.3046070337295532\n",
      "grad W: 10.158726692199707\n",
      "grad W_in: 7.565008640289307\n",
      "[ep 15 | batch 66] loss=0.6976 |M| |W| mean=2.412e-02\n",
      "ep 15 batch 66 | loss=0.6976 | alloc=0.99GB reserv=27.31GB\n",
      "grad W_out: 0.299740195274353\n",
      "grad W: 5.154105186462402\n",
      "grad W_in: 5.038723945617676\n",
      "grad W_out: 0.3953757882118225\n",
      "grad W: 8.587523460388184\n",
      "grad W_in: 5.867904186248779\n",
      "[ep 15 | batch 68] loss=0.7260 |M| |W| mean=2.413e-02\n",
      "ep 15 batch 68 | loss=0.7260 | alloc=0.99GB reserv=27.31GB\n",
      "grad W_out: 0.3152467608451843\n",
      "grad W: 8.307628631591797\n",
      "grad W_in: 4.641425132751465\n",
      "grad W_out: 0.40025267004966736\n",
      "grad W: 4.460602760314941\n",
      "grad W_in: 0.49664661288261414\n",
      "[ep 15 | batch 70] loss=0.6128 |M| |W| mean=2.413e-02\n",
      "ep 15 batch 70 | loss=0.6128 | alloc=1.00GB reserv=27.31GB\n",
      "grad W_out: 0.3337053954601288\n",
      "grad W: 5.2104597091674805\n",
      "grad W_in: 1.1546608209609985\n",
      "grad W_out: 0.3591364920139313\n",
      "grad W: 6.85857629776001\n",
      "grad W_in: 5.433668613433838\n",
      "[ep 15 | batch 72] loss=0.7243 |M| |W| mean=2.414e-02\n",
      "ep 15 batch 72 | loss=0.7243 | alloc=1.00GB reserv=27.31GB\n",
      "grad W_out: 0.4714639484882355\n",
      "grad W: 7.91391134262085\n",
      "grad W_in: 1.121752142906189\n",
      "grad W_out: 0.2819872200489044\n",
      "grad W: 8.01403522491455\n",
      "grad W_in: 5.746114253997803\n",
      "[ep 15 | batch 74] loss=0.6806 |M| |W| mean=2.415e-02\n",
      "ep 15 batch 74 | loss=0.6806 | alloc=1.00GB reserv=27.31GB\n",
      "grad W_out: 0.14315927028656006\n",
      "grad W: 3.072587251663208\n",
      "grad W_in: 1.8568699359893799\n",
      "grad W_out: 0.24365463852882385\n",
      "grad W: 5.853841304779053\n",
      "grad W_in: 0.8592410087585449\n",
      "[ep 15 | batch 76] loss=0.7216 |M| |W| mean=2.415e-02\n",
      "ep 15 batch 76 | loss=0.7216 | alloc=1.00GB reserv=27.32GB\n",
      "grad W_out: 0.23487482964992523\n",
      "grad W: 5.019024848937988\n",
      "grad W_in: 1.0518975257873535\n",
      "grad W_out: 0.18073086440563202\n",
      "grad W: 4.087652206420898\n",
      "grad W_in: 2.1724982261657715\n",
      "[ep 15 | batch 78] loss=0.6108 |M| |W| mean=2.416e-02\n",
      "ep 15 batch 78 | loss=0.6108 | alloc=1.00GB reserv=27.32GB\n",
      "grad W_out: 0.3595288097858429\n",
      "grad W: 5.078803062438965\n",
      "grad W_in: 1.2715541124343872\n",
      "grad W_out: 0.3690757751464844\n",
      "grad W: 4.5745673179626465\n",
      "grad W_in: 1.3964124917984009\n",
      "[ep 15 | batch 80] loss=0.6672 |M| |W| mean=2.417e-02\n",
      "ep 15 batch 80 | loss=0.6672 | alloc=1.00GB reserv=27.32GB\n",
      "grad W_out: 0.2764280140399933\n",
      "grad W: 2.9656708240509033\n",
      "grad W_in: 2.1607742309570312\n",
      "grad W_out: 0.23858614265918732\n",
      "grad W: 4.934027671813965\n",
      "grad W_in: 1.3467823266983032\n",
      "[ep 15 | batch 82] loss=0.7015 |M| |W| mean=2.417e-02\n",
      "ep 15 batch 82 | loss=0.7015 | alloc=1.00GB reserv=27.32GB\n",
      "grad W_out: 0.2001391053199768\n",
      "grad W: 4.197417259216309\n",
      "grad W_in: 0.965431272983551\n",
      "grad W_out: 0.20668861269950867\n",
      "grad W: 4.172183990478516\n",
      "grad W_in: 1.6705535650253296\n",
      "[ep 15 | batch 84] loss=0.6503 |M| |W| mean=2.418e-02\n",
      "ep 15 batch 84 | loss=0.6503 | alloc=1.01GB reserv=27.32GB\n",
      "grad W_out: 0.3028132915496826\n",
      "grad W: 3.466536283493042\n",
      "grad W_in: 1.1745328903198242\n",
      "grad W_out: 0.28956130146980286\n",
      "grad W: 4.923797130584717\n",
      "grad W_in: 3.650134801864624\n",
      "[ep 15 | batch 86] loss=0.6249 |M| |W| mean=2.418e-02\n",
      "ep 15 batch 86 | loss=0.6249 | alloc=1.01GB reserv=27.33GB\n",
      "grad W_out: 0.3080469071865082\n",
      "grad W: 6.015857696533203\n",
      "grad W_in: 2.379040479660034\n",
      "grad W_out: 0.19282300770282745\n",
      "grad W: 2.5933213233947754\n",
      "grad W_in: 1.4122154712677002\n",
      "[ep 15 | batch 88] loss=0.5919 |M| |W| mean=2.418e-02\n",
      "ep 15 batch 88 | loss=0.5919 | alloc=1.01GB reserv=27.33GB\n",
      "grad W_out: 0.20743300020694733\n",
      "grad W: 5.379077911376953\n",
      "grad W_in: 4.932051181793213\n",
      "grad W_out: 0.36299195885658264\n",
      "grad W: 10.153162002563477\n",
      "grad W_in: 9.523102760314941\n",
      "[ep 15 | batch 90] loss=0.6366 |M| |W| mean=2.418e-02\n",
      "ep 15 batch 90 | loss=0.6366 | alloc=1.01GB reserv=27.33GB\n",
      "grad W_out: 0.18140219151973724\n",
      "grad W: 4.6577653884887695\n",
      "grad W_in: 2.709420919418335\n",
      "grad W_out: 0.5864388346672058\n",
      "grad W: 14.142109870910645\n",
      "grad W_in: 13.841919898986816\n",
      "[ep 15 | batch 92] loss=0.7503 |M| |W| mean=2.419e-02\n",
      "ep 15 batch 92 | loss=0.7503 | alloc=1.01GB reserv=27.33GB\n",
      "grad W_out: 0.2797246277332306\n",
      "grad W: 6.0677666664123535\n",
      "grad W_in: 3.6757144927978516\n",
      "Epoch 15/20 | lr=1.00e-03 | loss=0.6803 | test_acc=76.69% | dt=1177.9s\n",
      "grad W_out: 0.3749745190143585\n",
      "grad W: 10.010432243347168\n",
      "grad W_in: 7.577760219573975\n",
      "[ep 16 | batch 0] loss=0.7008 |M| |W| mean=2.420e-02\n",
      "ep 16 batch 0 | loss=0.7008 | alloc=1.01GB reserv=27.33GB\n",
      "grad W_out: 0.34245970845222473\n",
      "grad W: 8.781214714050293\n",
      "grad W_in: 8.158302307128906\n",
      "grad W_out: 0.29316022992134094\n",
      "grad W: 5.567920684814453\n",
      "grad W_in: 1.4297786951065063\n",
      "[ep 16 | batch 2] loss=0.6775 |M| |W| mean=2.419e-02\n",
      "ep 16 batch 2 | loss=0.6775 | alloc=1.01GB reserv=27.34GB\n",
      "grad W_out: 0.3939879536628723\n",
      "grad W: 6.581503391265869\n",
      "grad W_in: 3.6588962078094482\n",
      "grad W_out: 0.252786785364151\n",
      "grad W: 9.237817764282227\n",
      "grad W_in: 9.662353515625\n",
      "[ep 16 | batch 4] loss=0.6638 |M| |W| mean=2.420e-02\n",
      "ep 16 batch 4 | loss=0.6638 | alloc=1.01GB reserv=27.34GB\n",
      "grad W_out: 0.27450811862945557\n",
      "grad W: 8.226608276367188\n",
      "grad W_in: 6.573332786560059\n",
      "grad W_out: 0.2902418375015259\n",
      "grad W: 7.155880451202393\n",
      "grad W_in: 7.2795186042785645\n",
      "[ep 16 | batch 6] loss=0.6777 |M| |W| mean=2.420e-02\n",
      "ep 16 batch 6 | loss=0.6777 | alloc=1.02GB reserv=27.34GB\n",
      "grad W_out: 0.3735184967517853\n",
      "grad W: 6.8794636726379395\n",
      "grad W_in: 4.5602803230285645\n",
      "grad W_out: 0.2813064754009247\n",
      "grad W: 6.66918420791626\n",
      "grad W_in: 7.20920467376709\n",
      "[ep 16 | batch 8] loss=0.6817 |M| |W| mean=2.419e-02\n",
      "ep 16 batch 8 | loss=0.6817 | alloc=1.02GB reserv=27.34GB\n",
      "grad W_out: 0.3761792778968811\n",
      "grad W: 6.1315460205078125\n",
      "grad W_in: 4.32614803314209\n",
      "grad W_out: 0.2893265187740326\n",
      "grad W: 6.012900352478027\n",
      "grad W_in: 4.203960418701172\n",
      "[ep 16 | batch 10] loss=0.6810 |M| |W| mean=2.420e-02\n",
      "ep 16 batch 10 | loss=0.6810 | alloc=1.02GB reserv=27.34GB\n",
      "grad W_out: 0.29811161756515503\n",
      "grad W: 8.33998966217041\n",
      "grad W_in: 6.062856197357178\n",
      "grad W_out: 0.2611953914165497\n",
      "grad W: 4.36077880859375\n",
      "grad W_in: 3.0505454540252686\n",
      "[ep 16 | batch 12] loss=0.6081 |M| |W| mean=2.420e-02\n",
      "ep 16 batch 12 | loss=0.6081 | alloc=1.02GB reserv=27.35GB\n",
      "grad W_out: 0.3841959834098816\n",
      "grad W: 8.876047134399414\n",
      "grad W_in: 4.686992645263672\n",
      "grad W_out: 0.30716460943222046\n",
      "grad W: 4.473194599151611\n",
      "grad W_in: 2.1870510578155518\n",
      "[ep 16 | batch 14] loss=0.7168 |M| |W| mean=2.420e-02\n",
      "ep 16 batch 14 | loss=0.7168 | alloc=1.02GB reserv=27.35GB\n",
      "grad W_out: 0.2795321047306061\n",
      "grad W: 7.646986484527588\n",
      "grad W_in: 7.059266090393066\n",
      "grad W_out: 0.39392611384391785\n",
      "grad W: 6.434559345245361\n",
      "grad W_in: 4.035461902618408\n",
      "[ep 16 | batch 16] loss=0.6144 |M| |W| mean=2.421e-02\n",
      "ep 16 batch 16 | loss=0.6144 | alloc=1.02GB reserv=27.35GB\n",
      "grad W_out: 0.286386102437973\n",
      "grad W: 6.535499572753906\n",
      "grad W_in: 1.790605068206787\n",
      "grad W_out: 0.2874428927898407\n",
      "grad W: 7.869205951690674\n",
      "grad W_in: 7.276350498199463\n",
      "[ep 16 | batch 18] loss=0.6374 |M| |W| mean=2.421e-02\n",
      "ep 16 batch 18 | loss=0.6374 | alloc=1.02GB reserv=27.35GB\n",
      "grad W_out: 0.18268552422523499\n",
      "grad W: 6.723771095275879\n",
      "grad W_in: 3.7614338397979736\n",
      "grad W_out: 0.3703458309173584\n",
      "grad W: 9.638172149658203\n",
      "grad W_in: 7.869078636169434\n",
      "[ep 16 | batch 20] loss=0.7801 |M| |W| mean=2.422e-02\n",
      "ep 16 batch 20 | loss=0.7801 | alloc=1.03GB reserv=27.35GB\n",
      "grad W_out: 0.395993173122406\n",
      "grad W: 8.533854484558105\n",
      "grad W_in: 2.205378770828247\n",
      "grad W_out: 0.2189943641424179\n",
      "grad W: 5.5344109535217285\n",
      "grad W_in: 1.165624976158142\n",
      "[ep 16 | batch 22] loss=0.7039 |M| |W| mean=2.422e-02\n",
      "ep 16 batch 22 | loss=0.7039 | alloc=1.03GB reserv=27.36GB\n",
      "grad W_out: 0.2429368495941162\n",
      "grad W: 5.568018913269043\n",
      "grad W_in: 4.762563228607178\n",
      "grad W_out: 0.38180533051490784\n",
      "grad W: 8.19861888885498\n",
      "grad W_in: 1.2948741912841797\n",
      "[ep 16 | batch 24] loss=0.7050 |M| |W| mean=2.423e-02\n",
      "ep 16 batch 24 | loss=0.7050 | alloc=1.03GB reserv=27.36GB\n",
      "grad W_out: 0.2714517414569855\n",
      "grad W: 7.87344217300415\n",
      "grad W_in: 3.409320592880249\n",
      "grad W_out: 0.34211477637290955\n",
      "grad W: 5.038093566894531\n",
      "grad W_in: 2.6417832374572754\n",
      "[ep 16 | batch 26] loss=0.6560 |M| |W| mean=2.423e-02\n",
      "ep 16 batch 26 | loss=0.6560 | alloc=1.03GB reserv=27.36GB\n",
      "grad W_out: 0.3979833424091339\n",
      "grad W: 7.592158794403076\n",
      "grad W_in: 2.1253747940063477\n",
      "grad W_out: 0.3929162323474884\n",
      "grad W: 8.671238899230957\n",
      "grad W_in: 3.457319974899292\n",
      "[ep 16 | batch 28] loss=0.7473 |M| |W| mean=2.423e-02\n",
      "ep 16 batch 28 | loss=0.7473 | alloc=1.03GB reserv=27.36GB\n",
      "grad W_out: 0.2772318720817566\n",
      "grad W: 5.302059650421143\n",
      "grad W_in: 2.890080690383911\n",
      "grad W_out: 0.2971286475658417\n",
      "grad W: 8.09462833404541\n",
      "grad W_in: 6.191757678985596\n",
      "[ep 16 | batch 30] loss=0.6550 |M| |W| mean=2.424e-02\n",
      "ep 16 batch 30 | loss=0.6550 | alloc=1.03GB reserv=27.36GB\n",
      "grad W_out: 0.2983478903770447\n",
      "grad W: 7.469006061553955\n",
      "grad W_in: 4.124948978424072\n",
      "grad W_out: 0.36074167490005493\n",
      "grad W: 7.166502952575684\n",
      "grad W_in: 3.0635766983032227\n",
      "[ep 16 | batch 32] loss=0.6351 |M| |W| mean=2.424e-02\n",
      "ep 16 batch 32 | loss=0.6351 | alloc=1.03GB reserv=27.37GB\n",
      "grad W_out: 0.19940683245658875\n",
      "grad W: 4.832354545593262\n",
      "grad W_in: 2.297839403152466\n",
      "grad W_out: 0.24178899824619293\n",
      "grad W: 5.200533866882324\n",
      "grad W_in: 4.502040863037109\n",
      "[ep 16 | batch 34] loss=0.6871 |M| |W| mean=2.424e-02\n",
      "ep 16 batch 34 | loss=0.6871 | alloc=1.04GB reserv=27.37GB\n",
      "grad W_out: 0.3126746416091919\n",
      "grad W: 7.824051380157471\n",
      "grad W_in: 6.740736484527588\n",
      "grad W_out: 0.32370105385780334\n",
      "grad W: 8.489718437194824\n",
      "grad W_in: 3.867553472518921\n",
      "[ep 16 | batch 36] loss=0.7254 |M| |W| mean=2.425e-02\n",
      "ep 16 batch 36 | loss=0.7254 | alloc=1.04GB reserv=27.37GB\n",
      "grad W_out: 0.2839983105659485\n",
      "grad W: 5.767800331115723\n",
      "grad W_in: 1.7168854475021362\n",
      "grad W_out: 0.3321824371814728\n",
      "grad W: 8.12571907043457\n",
      "grad W_in: 4.133142471313477\n",
      "[ep 16 | batch 38] loss=0.6760 |M| |W| mean=2.425e-02\n",
      "ep 16 batch 38 | loss=0.6760 | alloc=1.04GB reserv=27.37GB\n",
      "grad W_out: 0.21820111572742462\n",
      "grad W: 5.284970760345459\n",
      "grad W_in: 1.7345412969589233\n",
      "grad W_out: 0.3351950943470001\n",
      "grad W: 6.673411846160889\n",
      "grad W_in: 3.5563559532165527\n",
      "[ep 16 | batch 40] loss=0.6266 |M| |W| mean=2.425e-02\n",
      "ep 16 batch 40 | loss=0.6266 | alloc=1.04GB reserv=27.37GB\n",
      "grad W_out: 0.2856956124305725\n",
      "grad W: 4.8274946212768555\n",
      "grad W_in: 3.5116753578186035\n",
      "grad W_out: 0.35164231061935425\n",
      "grad W: 7.702230453491211\n",
      "grad W_in: 1.9398689270019531\n",
      "[ep 16 | batch 42] loss=0.6547 |M| |W| mean=2.425e-02\n",
      "ep 16 batch 42 | loss=0.6547 | alloc=1.04GB reserv=27.38GB\n",
      "grad W_out: 0.3167828917503357\n",
      "grad W: 6.168889045715332\n",
      "grad W_in: 3.3273539543151855\n",
      "grad W_out: 0.2779427170753479\n",
      "grad W: 7.9792890548706055\n",
      "grad W_in: 6.327314853668213\n",
      "[ep 16 | batch 44] loss=0.6823 |M| |W| mean=2.426e-02\n",
      "ep 16 batch 44 | loss=0.6823 | alloc=1.04GB reserv=27.38GB\n",
      "grad W_out: 0.30601105093955994\n",
      "grad W: 5.612490653991699\n",
      "grad W_in: 5.457831382751465\n",
      "grad W_out: 0.36532238125801086\n",
      "grad W: 4.925536155700684\n",
      "grad W_in: 2.390393018722534\n",
      "[ep 16 | batch 46] loss=0.6485 |M| |W| mean=2.427e-02\n",
      "ep 16 batch 46 | loss=0.6485 | alloc=1.04GB reserv=27.38GB\n",
      "grad W_out: 0.29056769609451294\n",
      "grad W: 4.113489627838135\n",
      "grad W_in: 1.9641103744506836\n",
      "grad W_out: 0.26519614458084106\n",
      "grad W: 5.677888870239258\n",
      "grad W_in: 3.005699634552002\n",
      "[ep 16 | batch 48] loss=0.6536 |M| |W| mean=2.427e-02\n",
      "ep 16 batch 48 | loss=0.6536 | alloc=1.05GB reserv=27.38GB\n",
      "grad W_out: 0.2183547168970108\n",
      "grad W: 4.030182838439941\n",
      "grad W_in: 2.3215713500976562\n",
      "grad W_out: 0.3798416554927826\n",
      "grad W: 10.489521026611328\n",
      "grad W_in: 8.34662914276123\n",
      "[ep 16 | batch 50] loss=0.6345 |M| |W| mean=2.428e-02\n",
      "ep 16 batch 50 | loss=0.6345 | alloc=1.05GB reserv=27.38GB\n",
      "grad W_out: 0.23363780975341797\n",
      "grad W: 2.738600969314575\n",
      "grad W_in: 1.5822347402572632\n",
      "grad W_out: 0.5106926560401917\n",
      "grad W: 8.564387321472168\n",
      "grad W_in: 7.3485798835754395\n",
      "[ep 16 | batch 52] loss=0.6696 |M| |W| mean=2.428e-02\n",
      "ep 16 batch 52 | loss=0.6696 | alloc=1.05GB reserv=27.38GB\n",
      "grad W_out: 0.27108412981033325\n",
      "grad W: 13.495226860046387\n",
      "grad W_in: 11.388737678527832\n",
      "grad W_out: 0.26853036880493164\n",
      "grad W: 10.560933113098145\n",
      "grad W_in: 9.45673656463623\n",
      "[ep 16 | batch 54] loss=0.6539 |M| |W| mean=2.428e-02\n",
      "ep 16 batch 54 | loss=0.6539 | alloc=1.05GB reserv=27.39GB\n",
      "grad W_out: 0.4055061638355255\n",
      "grad W: 9.320518493652344\n",
      "grad W_in: 8.393985748291016\n",
      "grad W_out: 0.21049806475639343\n",
      "grad W: 4.3461594581604\n",
      "grad W_in: 1.1249014139175415\n",
      "[ep 16 | batch 56] loss=0.6496 |M| |W| mean=2.428e-02\n",
      "ep 16 batch 56 | loss=0.6496 | alloc=1.05GB reserv=27.39GB\n",
      "grad W_out: 0.30200058221817017\n",
      "grad W: 8.186686515808105\n",
      "grad W_in: 5.748773574829102\n",
      "grad W_out: 0.3652244806289673\n",
      "grad W: 6.19445276260376\n",
      "grad W_in: 3.849090099334717\n",
      "[ep 16 | batch 58] loss=0.5990 |M| |W| mean=2.428e-02\n",
      "ep 16 batch 58 | loss=0.5990 | alloc=1.05GB reserv=27.39GB\n",
      "grad W_out: 0.1837453693151474\n",
      "grad W: 2.463163137435913\n",
      "grad W_in: 1.441112995147705\n",
      "grad W_out: 0.29046526551246643\n",
      "grad W: 5.951563835144043\n",
      "grad W_in: 4.429817199707031\n",
      "[ep 16 | batch 60] loss=0.6914 |M| |W| mean=2.427e-02\n",
      "ep 16 batch 60 | loss=0.6914 | alloc=1.05GB reserv=27.39GB\n",
      "grad W_out: 0.25384587049484253\n",
      "grad W: 5.2474212646484375\n",
      "grad W_in: 5.470316410064697\n",
      "grad W_out: 0.24127928912639618\n",
      "grad W: 4.557957172393799\n",
      "grad W_in: 1.2448546886444092\n",
      "[ep 16 | batch 62] loss=0.6103 |M| |W| mean=2.428e-02\n",
      "ep 16 batch 62 | loss=0.6103 | alloc=1.06GB reserv=27.39GB\n",
      "grad W_out: 0.3472795784473419\n",
      "grad W: 7.306204319000244\n",
      "grad W_in: 5.2733354568481445\n",
      "grad W_out: 0.3790314793586731\n",
      "grad W: 8.764583587646484\n",
      "grad W_in: 9.57751750946045\n",
      "[ep 16 | batch 64] loss=0.6937 |M| |W| mean=2.429e-02\n",
      "ep 16 batch 64 | loss=0.6937 | alloc=1.06GB reserv=27.40GB\n",
      "grad W_out: 0.2593177556991577\n",
      "grad W: 6.397862911224365\n",
      "grad W_in: 2.8405797481536865\n",
      "grad W_out: 0.32961562275886536\n",
      "grad W: 10.705489158630371\n",
      "grad W_in: 5.210631847381592\n",
      "[ep 16 | batch 66] loss=0.6690 |M| |W| mean=2.429e-02\n",
      "ep 16 batch 66 | loss=0.6690 | alloc=1.06GB reserv=27.40GB\n",
      "grad W_out: 0.2803923487663269\n",
      "grad W: 7.132795810699463\n",
      "grad W_in: 5.5001702308654785\n",
      "grad W_out: 0.39710357785224915\n",
      "grad W: 4.8682942390441895\n",
      "grad W_in: 1.868630290031433\n",
      "[ep 16 | batch 68] loss=0.6559 |M| |W| mean=2.430e-02\n",
      "ep 16 batch 68 | loss=0.6559 | alloc=1.06GB reserv=27.40GB\n",
      "grad W_out: 0.49631279706954956\n",
      "grad W: 10.395879745483398\n",
      "grad W_in: 5.296012878417969\n",
      "grad W_out: 0.446656733751297\n",
      "grad W: 10.111662864685059\n",
      "grad W_in: 3.238767385482788\n",
      "[ep 16 | batch 70] loss=0.7026 |M| |W| mean=2.430e-02\n",
      "ep 16 batch 70 | loss=0.7026 | alloc=1.06GB reserv=27.40GB\n",
      "grad W_out: 0.2783774137496948\n",
      "grad W: 4.451097011566162\n",
      "grad W_in: 3.3718552589416504\n",
      "grad W_out: 0.45388659834861755\n",
      "grad W: 10.387227058410645\n",
      "grad W_in: 7.285891056060791\n",
      "[ep 16 | batch 72] loss=0.7539 |M| |W| mean=2.431e-02\n",
      "ep 16 batch 72 | loss=0.7539 | alloc=1.06GB reserv=27.40GB\n",
      "grad W_out: 0.478352814912796\n",
      "grad W: 12.070610046386719\n",
      "grad W_in: 4.54446268081665\n",
      "grad W_out: 0.4387975037097931\n",
      "grad W: 4.106578826904297\n",
      "grad W_in: 2.234327554702759\n",
      "[ep 16 | batch 74] loss=0.7187 |M| |W| mean=2.432e-02\n",
      "ep 16 batch 74 | loss=0.7187 | alloc=1.06GB reserv=27.41GB\n",
      "grad W_out: 0.4632810056209564\n",
      "grad W: 11.53249740600586\n",
      "grad W_in: 7.992427349090576\n",
      "grad W_out: 0.4233011305332184\n",
      "grad W: 10.706984519958496\n",
      "grad W_in: 7.6628642082214355\n",
      "[ep 16 | batch 76] loss=0.6556 |M| |W| mean=2.431e-02\n",
      "ep 16 batch 76 | loss=0.6556 | alloc=1.07GB reserv=27.41GB\n",
      "grad W_out: 0.2009919285774231\n",
      "grad W: 3.978367805480957\n",
      "grad W_in: 2.5155961513519287\n",
      "grad W_out: 0.31420832872390747\n",
      "grad W: 9.301271438598633\n",
      "grad W_in: 6.0782318115234375\n",
      "[ep 16 | batch 78] loss=0.6450 |M| |W| mean=2.431e-02\n",
      "ep 16 batch 78 | loss=0.6450 | alloc=1.07GB reserv=27.41GB\n",
      "grad W_out: 0.3363731801509857\n",
      "grad W: 5.980946063995361\n",
      "grad W_in: 0.9312466382980347\n",
      "grad W_out: 0.2938515841960907\n",
      "grad W: 5.539287567138672\n",
      "grad W_in: 2.9594554901123047\n",
      "[ep 16 | batch 80] loss=0.7082 |M| |W| mean=2.432e-02\n",
      "ep 16 batch 80 | loss=0.7082 | alloc=1.07GB reserv=27.41GB\n",
      "grad W_out: 0.33303576707839966\n",
      "grad W: 5.536078929901123\n",
      "grad W_in: 3.006840705871582\n",
      "grad W_out: 0.40309378504753113\n",
      "grad W: 9.116682052612305\n",
      "grad W_in: 4.617702484130859\n",
      "[ep 16 | batch 82] loss=0.6860 |M| |W| mean=2.432e-02\n",
      "ep 16 batch 82 | loss=0.6860 | alloc=1.07GB reserv=27.41GB\n",
      "grad W_out: 0.2757735550403595\n",
      "grad W: 6.43823766708374\n",
      "grad W_in: 1.1324317455291748\n",
      "grad W_out: 0.5191888809204102\n",
      "grad W: 7.659740447998047\n",
      "grad W_in: 3.549238681793213\n",
      "[ep 16 | batch 84] loss=0.7981 |M| |W| mean=2.433e-02\n",
      "ep 16 batch 84 | loss=0.7981 | alloc=1.07GB reserv=27.42GB\n",
      "grad W_out: 0.3615049719810486\n",
      "grad W: 8.486879348754883\n",
      "grad W_in: 5.560550689697266\n",
      "grad W_out: 0.3664436936378479\n",
      "grad W: 6.535287380218506\n",
      "grad W_in: 4.784368515014648\n",
      "[ep 16 | batch 86] loss=0.6799 |M| |W| mean=2.433e-02\n",
      "ep 16 batch 86 | loss=0.6799 | alloc=1.07GB reserv=27.42GB\n",
      "grad W_out: 0.3682183623313904\n",
      "grad W: 6.69680643081665\n",
      "grad W_in: 5.099261283874512\n",
      "grad W_out: 0.2647556662559509\n",
      "grad W: 5.587665557861328\n",
      "grad W_in: 3.8153796195983887\n",
      "[ep 16 | batch 88] loss=0.5986 |M| |W| mean=2.434e-02\n",
      "ep 16 batch 88 | loss=0.5986 | alloc=1.07GB reserv=27.42GB\n",
      "grad W_out: 0.21969905495643616\n",
      "grad W: 4.501646518707275\n",
      "grad W_in: 3.02542781829834\n",
      "grad W_out: 0.27524226903915405\n",
      "grad W: 5.943559169769287\n",
      "grad W_in: 5.426587104797363\n",
      "[ep 16 | batch 90] loss=0.6407 |M| |W| mean=2.434e-02\n",
      "ep 16 batch 90 | loss=0.6407 | alloc=1.08GB reserv=27.42GB\n",
      "grad W_out: 0.18440237641334534\n",
      "grad W: 3.4134650230407715\n",
      "grad W_in: 1.8117530345916748\n",
      "grad W_out: 0.23428285121917725\n",
      "grad W: 4.055956840515137\n",
      "grad W_in: 1.8466724157333374\n",
      "[ep 16 | batch 92] loss=0.6338 |M| |W| mean=2.435e-02\n",
      "ep 16 batch 92 | loss=0.6338 | alloc=1.08GB reserv=27.42GB\n",
      "grad W_out: 0.3894011676311493\n",
      "grad W: 6.850466728210449\n",
      "grad W_in: 3.0664470195770264\n",
      "Epoch 16/20 | lr=1.00e-03 | loss=0.6681 | test_acc=78.37% | dt=1178.1s\n",
      "grad W_out: 0.25859296321868896\n",
      "grad W: 6.069488525390625\n",
      "grad W_in: 4.9504780769348145\n",
      "[ep 17 | batch 0] loss=0.6790 |M| |W| mean=2.435e-02\n",
      "ep 17 batch 0 | loss=0.6790 | alloc=1.08GB reserv=27.43GB\n",
      "grad W_out: 0.3054880201816559\n",
      "grad W: 7.135711669921875\n",
      "grad W_in: 1.597644567489624\n",
      "grad W_out: 0.40763959288597107\n",
      "grad W: 7.224465370178223\n",
      "grad W_in: 6.5583696365356445\n",
      "[ep 17 | batch 2] loss=0.6600 |M| |W| mean=2.435e-02\n",
      "ep 17 batch 2 | loss=0.6600 | alloc=1.08GB reserv=27.43GB\n",
      "grad W_out: 0.23254869878292084\n",
      "grad W: 3.623631238937378\n",
      "grad W_in: 0.8819950222969055\n",
      "grad W_out: 0.19218067824840546\n",
      "grad W: 5.106054306030273\n",
      "grad W_in: 4.363803863525391\n",
      "[ep 17 | batch 4] loss=0.6715 |M| |W| mean=2.436e-02\n",
      "ep 17 batch 4 | loss=0.6715 | alloc=1.08GB reserv=27.43GB\n",
      "grad W_out: 0.15340155363082886\n",
      "grad W: 3.226484537124634\n",
      "grad W_in: 1.4015189409255981\n",
      "grad W_out: 0.20662343502044678\n",
      "grad W: 4.742391586303711\n",
      "grad W_in: 2.842999219894409\n",
      "[ep 17 | batch 6] loss=0.6495 |M| |W| mean=2.436e-02\n",
      "ep 17 batch 6 | loss=0.6495 | alloc=1.08GB reserv=27.43GB\n",
      "grad W_out: 0.36307018995285034\n",
      "grad W: 5.404849052429199\n",
      "grad W_in: 2.5005922317504883\n",
      "grad W_out: 0.1442471593618393\n",
      "grad W: 4.064660549163818\n",
      "grad W_in: 4.172390937805176\n",
      "[ep 17 | batch 8] loss=0.6223 |M| |W| mean=2.436e-02\n",
      "ep 17 batch 8 | loss=0.6223 | alloc=1.08GB reserv=27.43GB\n",
      "grad W_out: 0.179524764418602\n",
      "grad W: 4.053589820861816\n",
      "grad W_in: 0.9035359621047974\n",
      "grad W_out: 0.34592205286026\n",
      "grad W: 5.944992542266846\n",
      "grad W_in: 4.645262241363525\n",
      "[ep 17 | batch 10] loss=0.6210 |M| |W| mean=2.437e-02\n",
      "ep 17 batch 10 | loss=0.6210 | alloc=1.09GB reserv=27.44GB\n",
      "grad W_out: 0.2662702798843384\n",
      "grad W: 4.1664204597473145\n",
      "grad W_in: 2.05314040184021\n",
      "grad W_out: 0.2423345297574997\n",
      "grad W: 6.164525985717773\n",
      "grad W_in: 5.447166442871094\n",
      "[ep 17 | batch 12] loss=0.6325 |M| |W| mean=2.437e-02\n",
      "ep 17 batch 12 | loss=0.6325 | alloc=1.09GB reserv=27.44GB\n",
      "grad W_out: 0.17778195440769196\n",
      "grad W: 4.4391655921936035\n",
      "grad W_in: 1.8356680870056152\n",
      "grad W_out: 0.38664552569389343\n",
      "grad W: 3.63444185256958\n",
      "grad W_in: 1.6160833835601807\n",
      "[ep 17 | batch 14] loss=0.6685 |M| |W| mean=2.438e-02\n",
      "ep 17 batch 14 | loss=0.6685 | alloc=1.09GB reserv=27.44GB\n",
      "grad W_out: 0.24595040082931519\n",
      "grad W: 6.082025527954102\n",
      "grad W_in: 4.478169918060303\n",
      "grad W_out: 0.17313461005687714\n",
      "grad W: 6.579267501831055\n",
      "grad W_in: 5.8049187660217285\n",
      "[ep 17 | batch 16] loss=0.6338 |M| |W| mean=2.438e-02\n",
      "ep 17 batch 16 | loss=0.6338 | alloc=1.09GB reserv=27.44GB\n",
      "grad W_out: 0.13881798088550568\n",
      "grad W: 3.7027149200439453\n",
      "grad W_in: 2.6850345134735107\n",
      "grad W_out: 0.31896960735321045\n",
      "grad W: 5.606499671936035\n",
      "grad W_in: 1.221610426902771\n",
      "[ep 17 | batch 18] loss=0.6095 |M| |W| mean=2.439e-02\n",
      "ep 17 batch 18 | loss=0.6095 | alloc=1.09GB reserv=27.44GB\n",
      "grad W_out: 0.4179933965206146\n",
      "grad W: 4.576510906219482\n",
      "grad W_in: 1.9700521230697632\n",
      "grad W_out: 0.3836993873119354\n",
      "grad W: 4.864243030548096\n",
      "grad W_in: 4.566798686981201\n",
      "[ep 17 | batch 20] loss=0.6010 |M| |W| mean=2.440e-02\n",
      "ep 17 batch 20 | loss=0.6010 | alloc=1.09GB reserv=27.45GB\n",
      "grad W_out: 0.2970449924468994\n",
      "grad W: 5.790986061096191\n",
      "grad W_in: 2.6715970039367676\n",
      "grad W_out: 0.22488504648208618\n",
      "grad W: 4.981564998626709\n",
      "grad W_in: 3.7221333980560303\n",
      "[ep 17 | batch 22] loss=0.6359 |M| |W| mean=2.440e-02\n",
      "ep 17 batch 22 | loss=0.6359 | alloc=1.09GB reserv=27.45GB\n",
      "grad W_out: 0.16023176908493042\n",
      "grad W: 3.0985403060913086\n",
      "grad W_in: 0.8813483715057373\n",
      "grad W_out: 0.23698478937149048\n",
      "grad W: 5.095407009124756\n",
      "grad W_in: 1.4774935245513916\n",
      "[ep 17 | batch 24] loss=0.6418 |M| |W| mean=2.440e-02\n",
      "ep 17 batch 24 | loss=0.6418 | alloc=1.10GB reserv=27.45GB\n",
      "grad W_out: 0.20955142378807068\n",
      "grad W: 4.8067522048950195\n",
      "grad W_in: 1.6612435579299927\n",
      "grad W_out: 0.12621137499809265\n",
      "grad W: 3.714482307434082\n",
      "grad W_in: 2.7251691818237305\n",
      "[ep 17 | batch 26] loss=0.5463 |M| |W| mean=2.441e-02\n",
      "ep 17 batch 26 | loss=0.5463 | alloc=1.10GB reserv=27.45GB\n",
      "grad W_out: 0.37779754400253296\n",
      "grad W: 5.572895050048828\n",
      "grad W_in: 1.2250285148620605\n",
      "grad W_out: 0.3939063549041748\n",
      "grad W: 10.671893119812012\n",
      "grad W_in: 3.8716237545013428\n",
      "[ep 17 | batch 28] loss=0.6819 |M| |W| mean=2.441e-02\n",
      "ep 17 batch 28 | loss=0.6819 | alloc=1.10GB reserv=27.45GB\n",
      "grad W_out: 0.21032875776290894\n",
      "grad W: 3.105595827102661\n",
      "grad W_in: 1.8229843378067017\n",
      "grad W_out: 0.31165584921836853\n",
      "grad W: 8.844364166259766\n",
      "grad W_in: 5.958911418914795\n",
      "[ep 17 | batch 30] loss=0.6073 |M| |W| mean=2.441e-02\n",
      "ep 17 batch 30 | loss=0.6073 | alloc=1.10GB reserv=27.46GB\n",
      "grad W_out: 0.2697138786315918\n",
      "grad W: 6.890715599060059\n",
      "grad W_in: 4.49735164642334\n",
      "grad W_out: 0.32106658816337585\n",
      "grad W: 7.895737171173096\n",
      "grad W_in: 7.708156108856201\n",
      "[ep 17 | batch 32] loss=0.6283 |M| |W| mean=2.442e-02\n",
      "ep 17 batch 32 | loss=0.6283 | alloc=1.10GB reserv=27.46GB\n",
      "grad W_out: 0.3906380832195282\n",
      "grad W: 9.6968412399292\n",
      "grad W_in: 4.883725166320801\n",
      "grad W_out: 0.2433781772851944\n",
      "grad W: 4.831908226013184\n",
      "grad W_in: 2.724379062652588\n",
      "[ep 17 | batch 34] loss=0.6511 |M| |W| mean=2.441e-02\n",
      "ep 17 batch 34 | loss=0.6511 | alloc=1.10GB reserv=27.46GB\n",
      "grad W_out: 0.32531988620758057\n",
      "grad W: 7.689543724060059\n",
      "grad W_in: 6.604610919952393\n",
      "grad W_out: 0.12898367643356323\n",
      "grad W: 3.647350311279297\n",
      "grad W_in: 1.2626410722732544\n",
      "[ep 17 | batch 36] loss=0.6018 |M| |W| mean=2.442e-02\n",
      "ep 17 batch 36 | loss=0.6018 | alloc=1.10GB reserv=27.46GB\n",
      "grad W_out: 0.2352515161037445\n",
      "grad W: 7.481969356536865\n",
      "grad W_in: 5.9783830642700195\n",
      "grad W_out: 0.23732438683509827\n",
      "grad W: 5.606671333312988\n",
      "grad W_in: 2.885928153991699\n",
      "[ep 17 | batch 38] loss=0.6316 |M| |W| mean=2.443e-02\n",
      "ep 17 batch 38 | loss=0.6316 | alloc=1.11GB reserv=27.46GB\n",
      "grad W_out: 0.25883498787879944\n",
      "grad W: 6.215915203094482\n",
      "grad W_in: 1.2508281469345093\n",
      "grad W_out: 0.19800572097301483\n",
      "grad W: 3.8762624263763428\n",
      "grad W_in: 1.9143317937850952\n",
      "[ep 17 | batch 40] loss=0.5937 |M| |W| mean=2.444e-02\n",
      "ep 17 batch 40 | loss=0.5937 | alloc=1.11GB reserv=27.46GB\n",
      "grad W_out: 0.21779485046863556\n",
      "grad W: 4.453558921813965\n",
      "grad W_in: 1.6657088994979858\n",
      "grad W_out: 0.26577460765838623\n",
      "grad W: 4.070894241333008\n",
      "grad W_in: 1.1099896430969238\n",
      "[ep 17 | batch 42] loss=0.5784 |M| |W| mean=2.444e-02\n",
      "ep 17 batch 42 | loss=0.5784 | alloc=1.11GB reserv=27.47GB\n",
      "grad W_out: 0.30949297547340393\n",
      "grad W: 5.722297668457031\n",
      "grad W_in: 1.4499322175979614\n",
      "grad W_out: 0.30804792046546936\n",
      "grad W: 7.26352071762085\n",
      "grad W_in: 3.251232624053955\n",
      "[ep 17 | batch 44] loss=0.6069 |M| |W| mean=2.445e-02\n",
      "ep 17 batch 44 | loss=0.6069 | alloc=1.11GB reserv=27.47GB\n",
      "grad W_out: 0.3347834050655365\n",
      "grad W: 11.662805557250977\n",
      "grad W_in: 9.433757781982422\n",
      "grad W_out: 0.2702426612377167\n",
      "grad W: 5.887253284454346\n",
      "grad W_in: 3.1167237758636475\n",
      "[ep 17 | batch 46] loss=0.5944 |M| |W| mean=2.445e-02\n",
      "ep 17 batch 46 | loss=0.5944 | alloc=1.11GB reserv=27.47GB\n",
      "grad W_out: 0.370254784822464\n",
      "grad W: 5.720816612243652\n",
      "grad W_in: 1.1019397974014282\n",
      "grad W_out: 0.32789841294288635\n",
      "grad W: 8.38830280303955\n",
      "grad W_in: 2.5132527351379395\n",
      "[ep 17 | batch 48] loss=0.6904 |M| |W| mean=2.445e-02\n",
      "ep 17 batch 48 | loss=0.6904 | alloc=1.11GB reserv=27.47GB\n",
      "grad W_out: 0.17238344252109528\n",
      "grad W: 7.076862335205078\n",
      "grad W_in: 6.112940311431885\n",
      "grad W_out: 0.36089617013931274\n",
      "grad W: 9.402989387512207\n",
      "grad W_in: 8.90766716003418\n",
      "[ep 17 | batch 50] loss=0.6260 |M| |W| mean=2.446e-02\n",
      "ep 17 batch 50 | loss=0.6260 | alloc=1.11GB reserv=27.47GB\n",
      "grad W_out: 0.39487597346305847\n",
      "grad W: 7.622769832611084\n",
      "grad W_in: 2.5891056060791016\n",
      "grad W_out: 0.34071213006973267\n",
      "grad W: 7.723601341247559\n",
      "grad W_in: 4.020167350769043\n",
      "[ep 17 | batch 52] loss=0.6522 |M| |W| mean=2.446e-02\n",
      "ep 17 batch 52 | loss=0.6522 | alloc=1.12GB reserv=27.48GB\n",
      "grad W_out: 0.2629005014896393\n",
      "grad W: 8.181068420410156\n",
      "grad W_in: 8.393728256225586\n",
      "grad W_out: 0.3758854866027832\n",
      "grad W: 10.159958839416504\n",
      "grad W_in: 8.633655548095703\n",
      "[ep 17 | batch 54] loss=0.6143 |M| |W| mean=2.446e-02\n",
      "ep 17 batch 54 | loss=0.6143 | alloc=1.12GB reserv=27.48GB\n",
      "grad W_out: 0.2935243248939514\n",
      "grad W: 8.10611343383789\n",
      "grad W_in: 1.9963198900222778\n",
      "grad W_out: 0.42678961157798767\n",
      "grad W: 6.5136284828186035\n",
      "grad W_in: 2.1217308044433594\n",
      "[ep 17 | batch 56] loss=0.6764 |M| |W| mean=2.446e-02\n",
      "ep 17 batch 56 | loss=0.6764 | alloc=1.12GB reserv=27.48GB\n",
      "grad W_out: 0.22675788402557373\n",
      "grad W: 6.132166385650635\n",
      "grad W_in: 2.723864793777466\n",
      "grad W_out: 0.285035103559494\n",
      "grad W: 5.139403343200684\n",
      "grad W_in: 0.9358723759651184\n",
      "[ep 17 | batch 58] loss=0.6315 |M| |W| mean=2.446e-02\n",
      "ep 17 batch 58 | loss=0.6315 | alloc=1.12GB reserv=27.48GB\n",
      "grad W_out: 0.3867778182029724\n",
      "grad W: 8.176820755004883\n",
      "grad W_in: 4.517633438110352\n",
      "grad W_out: 0.19120806455612183\n",
      "grad W: 3.5510916709899902\n",
      "grad W_in: 1.4754294157028198\n",
      "[ep 17 | batch 60] loss=0.6301 |M| |W| mean=2.446e-02\n",
      "ep 17 batch 60 | loss=0.6301 | alloc=1.12GB reserv=27.48GB\n",
      "grad W_out: 0.32179954648017883\n",
      "grad W: 8.198469161987305\n",
      "grad W_in: 3.016403913497925\n",
      "grad W_out: 0.2820601761341095\n",
      "grad W: 6.971673965454102\n",
      "grad W_in: 6.0856757164001465\n",
      "[ep 17 | batch 62] loss=0.6386 |M| |W| mean=2.447e-02\n",
      "ep 17 batch 62 | loss=0.6386 | alloc=1.12GB reserv=27.49GB\n",
      "grad W_out: 0.19671013951301575\n",
      "grad W: 4.130671501159668\n",
      "grad W_in: 1.473002314567566\n",
      "grad W_out: 0.25355032086372375\n",
      "grad W: 8.121871948242188\n",
      "grad W_in: 5.596417427062988\n",
      "[ep 17 | batch 64] loss=0.6475 |M| |W| mean=2.448e-02\n",
      "ep 17 batch 64 | loss=0.6475 | alloc=1.12GB reserv=27.49GB\n",
      "grad W_out: 0.3507220447063446\n",
      "grad W: 7.568041801452637\n",
      "grad W_in: 5.34444522857666\n",
      "grad W_out: 0.36030668020248413\n",
      "grad W: 8.397432327270508\n",
      "grad W_in: 5.719834804534912\n",
      "[ep 17 | batch 66] loss=0.6700 |M| |W| mean=2.447e-02\n",
      "ep 17 batch 66 | loss=0.6700 | alloc=1.13GB reserv=27.49GB\n",
      "grad W_out: 0.3307369351387024\n",
      "grad W: 10.397711753845215\n",
      "grad W_in: 9.321693420410156\n",
      "grad W_out: 0.30194932222366333\n",
      "grad W: 6.686605453491211\n",
      "grad W_in: 4.350629806518555\n",
      "[ep 17 | batch 68] loss=0.7072 |M| |W| mean=2.448e-02\n",
      "ep 17 batch 68 | loss=0.7072 | alloc=1.13GB reserv=27.49GB\n",
      "grad W_out: 0.3208266496658325\n",
      "grad W: 8.169828414916992\n",
      "grad W_in: 6.79749870300293\n",
      "grad W_out: 0.2652047574520111\n",
      "grad W: 4.589563846588135\n",
      "grad W_in: 1.4734100103378296\n",
      "[ep 17 | batch 70] loss=0.6165 |M| |W| mean=2.448e-02\n",
      "ep 17 batch 70 | loss=0.6165 | alloc=1.13GB reserv=27.49GB\n",
      "grad W_out: 0.3009670078754425\n",
      "grad W: 6.059237957000732\n",
      "grad W_in: 6.315316200256348\n",
      "grad W_out: 0.3410319685935974\n",
      "grad W: 7.032501220703125\n",
      "grad W_in: 6.289477825164795\n",
      "[ep 17 | batch 72] loss=0.6265 |M| |W| mean=2.448e-02\n",
      "ep 17 batch 72 | loss=0.6265 | alloc=1.13GB reserv=27.50GB\n",
      "grad W_out: 0.2660168409347534\n",
      "grad W: 9.69982624053955\n",
      "grad W_in: 8.934686660766602\n",
      "grad W_out: 0.23822064697742462\n",
      "grad W: 4.562657833099365\n",
      "grad W_in: 1.3159204721450806\n",
      "[ep 17 | batch 74] loss=0.6736 |M| |W| mean=2.448e-02\n",
      "ep 17 batch 74 | loss=0.6736 | alloc=1.13GB reserv=27.50GB\n",
      "grad W_out: 0.2689610719680786\n",
      "grad W: 4.143026351928711\n",
      "grad W_in: 3.131495237350464\n",
      "grad W_out: 0.24124109745025635\n",
      "grad W: 7.908329486846924\n",
      "grad W_in: 6.768218517303467\n",
      "[ep 17 | batch 76] loss=0.5937 |M| |W| mean=2.449e-02\n",
      "ep 17 batch 76 | loss=0.5937 | alloc=1.13GB reserv=27.50GB\n",
      "grad W_out: 0.29897359013557434\n",
      "grad W: 7.782365798950195\n",
      "grad W_in: 7.81981086730957\n",
      "grad W_out: 0.24585466086864471\n",
      "grad W: 4.589145660400391\n",
      "grad W_in: 2.4843223094940186\n",
      "[ep 17 | batch 78] loss=0.5906 |M| |W| mean=2.449e-02\n",
      "ep 17 batch 78 | loss=0.5906 | alloc=1.13GB reserv=27.50GB\n",
      "grad W_out: 0.1902478188276291\n",
      "grad W: 4.59147834777832\n",
      "grad W_in: 1.3988878726959229\n",
      "grad W_out: 0.23677147924900055\n",
      "grad W: 7.230912208557129\n",
      "grad W_in: 5.806838512420654\n",
      "[ep 17 | batch 80] loss=0.6703 |M| |W| mean=2.449e-02\n",
      "ep 17 batch 80 | loss=0.6703 | alloc=1.14GB reserv=27.50GB\n",
      "grad W_out: 0.2524521052837372\n",
      "grad W: 4.972906589508057\n",
      "grad W_in: 3.813420295715332\n",
      "grad W_out: 0.457858145236969\n",
      "grad W: 12.315020561218262\n",
      "grad W_in: 7.331439018249512\n",
      "[ep 17 | batch 82] loss=0.6822 |M| |W| mean=2.450e-02\n",
      "ep 17 batch 82 | loss=0.6822 | alloc=1.14GB reserv=27.51GB\n",
      "grad W_out: 0.30454221367836\n",
      "grad W: 6.051701545715332\n",
      "grad W_in: 2.3034520149230957\n",
      "grad W_out: 0.21854807436466217\n",
      "grad W: 5.530678749084473\n",
      "grad W_in: 1.3376636505126953\n",
      "[ep 17 | batch 84] loss=0.6206 |M| |W| mean=2.450e-02\n",
      "ep 17 batch 84 | loss=0.6206 | alloc=1.14GB reserv=27.51GB\n",
      "grad W_out: 0.394904226064682\n",
      "grad W: 10.771157264709473\n",
      "grad W_in: 8.715583801269531\n",
      "grad W_out: 0.23359858989715576\n",
      "grad W: 5.752880096435547\n",
      "grad W_in: 2.1838719844818115\n",
      "[ep 17 | batch 86] loss=0.6022 |M| |W| mean=2.451e-02\n",
      "ep 17 batch 86 | loss=0.6022 | alloc=1.14GB reserv=27.51GB\n",
      "grad W_out: 0.48617759346961975\n",
      "grad W: 9.4960298538208\n",
      "grad W_in: 9.363360404968262\n",
      "grad W_out: 0.3692218065261841\n",
      "grad W: 8.036356925964355\n",
      "grad W_in: 5.739871025085449\n",
      "[ep 17 | batch 88] loss=0.7029 |M| |W| mean=2.451e-02\n",
      "ep 17 batch 88 | loss=0.7029 | alloc=1.14GB reserv=27.51GB\n",
      "grad W_out: 0.28589928150177\n",
      "grad W: 5.759888648986816\n",
      "grad W_in: 3.55753493309021\n",
      "grad W_out: 0.24719466269016266\n",
      "grad W: 6.047872066497803\n",
      "grad W_in: 6.09499979019165\n",
      "[ep 17 | batch 90] loss=0.6438 |M| |W| mean=2.452e-02\n",
      "ep 17 batch 90 | loss=0.6438 | alloc=1.14GB reserv=27.51GB\n",
      "grad W_out: 0.35843077301979065\n",
      "grad W: 8.249844551086426\n",
      "grad W_in: 5.99869966506958\n",
      "grad W_out: 0.2734664976596832\n",
      "grad W: 5.522425651550293\n",
      "grad W_in: 1.2804076671600342\n",
      "[ep 17 | batch 92] loss=0.5797 |M| |W| mean=2.453e-02\n",
      "ep 17 batch 92 | loss=0.5797 | alloc=1.14GB reserv=27.52GB\n",
      "grad W_out: 0.6028457283973694\n",
      "grad W: 18.169567108154297\n",
      "grad W_in: 18.783966064453125\n",
      "Epoch 17/20 | lr=1.00e-03 | loss=0.6342 | test_acc=78.65% | dt=1178.2s\n",
      "grad W_out: 0.26530611515045166\n",
      "grad W: 4.980906009674072\n",
      "grad W_in: 3.5869863033294678\n",
      "[ep 18 | batch 0] loss=0.6132 |M| |W| mean=2.452e-02\n",
      "ep 18 batch 0 | loss=0.6132 | alloc=1.15GB reserv=27.52GB\n",
      "grad W_out: 0.43107661604881287\n",
      "grad W: 12.076864242553711\n",
      "grad W_in: 10.330206871032715\n",
      "grad W_out: 0.41207969188690186\n",
      "grad W: 7.083247184753418\n",
      "grad W_in: 3.3955280780792236\n",
      "[ep 18 | batch 2] loss=0.7118 |M| |W| mean=2.453e-02\n",
      "ep 18 batch 2 | loss=0.7118 | alloc=1.15GB reserv=27.52GB\n",
      "grad W_out: 0.32628750801086426\n",
      "grad W: 7.408005237579346\n",
      "grad W_in: 4.007821559906006\n",
      "grad W_out: 0.45747408270835876\n",
      "grad W: 7.783627986907959\n",
      "grad W_in: 4.6534199714660645\n",
      "[ep 18 | batch 4] loss=0.6368 |M| |W| mean=2.454e-02\n",
      "ep 18 batch 4 | loss=0.6368 | alloc=1.15GB reserv=27.52GB\n",
      "grad W_out: 0.2386433184146881\n",
      "grad W: 6.647157192230225\n",
      "grad W_in: 4.669628620147705\n",
      "grad W_out: 0.40222638845443726\n",
      "grad W: 7.341965675354004\n",
      "grad W_in: 4.109071254730225\n",
      "[ep 18 | batch 6] loss=0.6418 |M| |W| mean=2.454e-02\n",
      "ep 18 batch 6 | loss=0.6418 | alloc=1.15GB reserv=27.52GB\n",
      "grad W_out: 0.40830883383750916\n",
      "grad W: 8.13387680053711\n",
      "grad W_in: 4.05116605758667\n",
      "grad W_out: 0.221156045794487\n",
      "grad W: 6.7964768409729\n",
      "grad W_in: 3.4325127601623535\n",
      "[ep 18 | batch 8] loss=0.6301 |M| |W| mean=2.455e-02\n",
      "ep 18 batch 8 | loss=0.6301 | alloc=1.15GB reserv=27.53GB\n",
      "grad W_out: 0.3212203085422516\n",
      "grad W: 5.583774089813232\n",
      "grad W_in: 1.4043840169906616\n",
      "grad W_out: 0.3476525545120239\n",
      "grad W: 8.38832950592041\n",
      "grad W_in: 7.021212577819824\n",
      "[ep 18 | batch 10] loss=0.6989 |M| |W| mean=2.455e-02\n",
      "ep 18 batch 10 | loss=0.6989 | alloc=1.15GB reserv=27.53GB\n",
      "grad W_out: 0.3779070973396301\n",
      "grad W: 8.164692878723145\n",
      "grad W_in: 7.154730796813965\n",
      "grad W_out: 0.28470316529273987\n",
      "grad W: 6.1477813720703125\n",
      "grad W_in: 5.485084533691406\n",
      "[ep 18 | batch 12] loss=0.6414 |M| |W| mean=2.455e-02\n",
      "ep 18 batch 12 | loss=0.6414 | alloc=1.15GB reserv=27.53GB\n",
      "grad W_out: 0.413609117269516\n",
      "grad W: 12.37115478515625\n",
      "grad W_in: 11.75088882446289\n",
      "grad W_out: 0.19343604147434235\n",
      "grad W: 4.072033405303955\n",
      "grad W_in: 3.7452242374420166\n",
      "[ep 18 | batch 14] loss=0.6027 |M| |W| mean=2.456e-02\n",
      "ep 18 batch 14 | loss=0.6027 | alloc=1.16GB reserv=27.53GB\n",
      "grad W_out: 0.2854221761226654\n",
      "grad W: 6.620673656463623\n",
      "grad W_in: 6.126197338104248\n",
      "grad W_out: 0.36900418996810913\n",
      "grad W: 11.413606643676758\n",
      "grad W_in: 11.716038703918457\n",
      "[ep 18 | batch 16] loss=0.6812 |M| |W| mean=2.457e-02\n",
      "ep 18 batch 16 | loss=0.6812 | alloc=1.16GB reserv=27.53GB\n",
      "grad W_out: 0.3256399631500244\n",
      "grad W: 6.7424116134643555\n",
      "grad W_in: 3.4228477478027344\n",
      "grad W_out: 0.5200303792953491\n",
      "grad W: 8.243208885192871\n",
      "grad W_in: 4.2411956787109375\n",
      "[ep 18 | batch 18] loss=0.6448 |M| |W| mean=2.456e-02\n",
      "ep 18 batch 18 | loss=0.6448 | alloc=1.16GB reserv=27.54GB\n",
      "grad W_out: 0.5857527852058411\n",
      "grad W: 11.769028663635254\n",
      "grad W_in: 9.730751037597656\n",
      "grad W_out: 0.4011158347129822\n",
      "grad W: 7.203792095184326\n",
      "grad W_in: 1.761044979095459\n",
      "[ep 18 | batch 20] loss=0.7002 |M| |W| mean=2.457e-02\n",
      "ep 18 batch 20 | loss=0.7002 | alloc=1.16GB reserv=27.54GB\n",
      "grad W_out: 0.3199765086174011\n",
      "grad W: 5.762780666351318\n",
      "grad W_in: 1.8990695476531982\n",
      "grad W_out: 0.4730662405490875\n",
      "grad W: 10.004091262817383\n",
      "grad W_in: 5.714801788330078\n",
      "[ep 18 | batch 22] loss=0.7296 |M| |W| mean=2.457e-02\n",
      "ep 18 batch 22 | loss=0.7296 | alloc=1.16GB reserv=27.54GB\n",
      "grad W_out: 0.5118537545204163\n",
      "grad W: 10.86284351348877\n",
      "grad W_in: 8.469919204711914\n",
      "grad W_out: 0.33331191539764404\n",
      "grad W: 7.000716209411621\n",
      "grad W_in: 3.907438278198242\n",
      "[ep 18 | batch 24] loss=0.6356 |M| |W| mean=2.456e-02\n",
      "ep 18 batch 24 | loss=0.6356 | alloc=1.16GB reserv=27.54GB\n",
      "grad W_out: 0.4123325049877167\n",
      "grad W: 10.490699768066406\n",
      "grad W_in: 6.207592010498047\n",
      "grad W_out: 0.2978154718875885\n",
      "grad W: 7.116765975952148\n",
      "grad W_in: 4.836878776550293\n",
      "[ep 18 | batch 26] loss=0.6823 |M| |W| mean=2.457e-02\n",
      "ep 18 batch 26 | loss=0.6823 | alloc=1.16GB reserv=27.54GB\n",
      "grad W_out: 0.1878899484872818\n",
      "grad W: 5.077204704284668\n",
      "grad W_in: 1.9740984439849854\n",
      "grad W_out: 0.28839045763015747\n",
      "grad W: 5.6072893142700195\n",
      "grad W_in: 1.6552162170410156\n",
      "[ep 18 | batch 28] loss=0.6218 |M| |W| mean=2.457e-02\n",
      "ep 18 batch 28 | loss=0.6218 | alloc=1.17GB reserv=27.54GB\n",
      "grad W_out: 0.4596860110759735\n",
      "grad W: 8.472094535827637\n",
      "grad W_in: 6.180509567260742\n",
      "grad W_out: 0.4021157920360565\n",
      "grad W: 8.392328262329102\n",
      "grad W_in: 4.977962017059326\n",
      "[ep 18 | batch 30] loss=0.6665 |M| |W| mean=2.457e-02\n",
      "ep 18 batch 30 | loss=0.6665 | alloc=1.17GB reserv=27.55GB\n",
      "grad W_out: 0.31938523054122925\n",
      "grad W: 3.799947738647461\n",
      "grad W_in: 1.0787745714187622\n",
      "grad W_out: 0.31363576650619507\n",
      "grad W: 5.153351306915283\n",
      "grad W_in: 3.7525179386138916\n",
      "[ep 18 | batch 32] loss=0.5890 |M| |W| mean=2.457e-02\n",
      "ep 18 batch 32 | loss=0.5890 | alloc=1.17GB reserv=27.55GB\n",
      "grad W_out: 0.5155084133148193\n",
      "grad W: 7.9574503898620605\n",
      "grad W_in: 2.3967230319976807\n",
      "grad W_out: 0.45904073119163513\n",
      "grad W: 7.092011451721191\n",
      "grad W_in: 2.5333783626556396\n",
      "[ep 18 | batch 34] loss=0.7063 |M| |W| mean=2.457e-02\n",
      "ep 18 batch 34 | loss=0.7063 | alloc=1.17GB reserv=27.55GB\n",
      "grad W_out: 0.21405193209648132\n",
      "grad W: 4.970545768737793\n",
      "grad W_in: 2.886446237564087\n",
      "grad W_out: 0.37356922030448914\n",
      "grad W: 6.528626918792725\n",
      "grad W_in: 1.7075763940811157\n",
      "[ep 18 | batch 36] loss=0.6711 |M| |W| mean=2.457e-02\n",
      "ep 18 batch 36 | loss=0.6711 | alloc=1.17GB reserv=27.55GB\n",
      "grad W_out: 0.20351891219615936\n",
      "grad W: 5.978248596191406\n",
      "grad W_in: 1.6282974481582642\n",
      "grad W_out: 0.23743261396884918\n",
      "grad W: 3.615309715270996\n",
      "grad W_in: 1.1193091869354248\n",
      "[ep 18 | batch 38] loss=0.6379 |M| |W| mean=2.457e-02\n",
      "ep 18 batch 38 | loss=0.6379 | alloc=1.17GB reserv=27.55GB\n",
      "grad W_out: 0.24187766015529633\n",
      "grad W: 3.86033034324646\n",
      "grad W_in: 1.3210991621017456\n",
      "grad W_out: 0.3332512378692627\n",
      "grad W: 6.077188014984131\n",
      "grad W_in: 2.402961254119873\n",
      "[ep 18 | batch 40] loss=0.6051 |M| |W| mean=2.458e-02\n",
      "ep 18 batch 40 | loss=0.6051 | alloc=1.17GB reserv=27.56GB\n",
      "grad W_out: 0.2473698854446411\n",
      "grad W: 4.193197727203369\n",
      "grad W_in: 2.78027606010437\n",
      "grad W_out: 0.22209852933883667\n",
      "grad W: 4.809459209442139\n",
      "grad W_in: 3.8500661849975586\n",
      "[ep 18 | batch 42] loss=0.6851 |M| |W| mean=2.459e-02\n",
      "ep 18 batch 42 | loss=0.6851 | alloc=1.18GB reserv=27.56GB\n",
      "grad W_out: 0.28826865553855896\n",
      "grad W: 5.111398220062256\n",
      "grad W_in: 1.7017269134521484\n",
      "grad W_out: 0.2326389104127884\n",
      "grad W: 7.511092662811279\n",
      "grad W_in: 5.676386833190918\n",
      "[ep 18 | batch 44] loss=0.6001 |M| |W| mean=2.459e-02\n",
      "ep 18 batch 44 | loss=0.6001 | alloc=1.18GB reserv=27.56GB\n",
      "grad W_out: 0.12405947595834732\n",
      "grad W: 4.9995622634887695\n",
      "grad W_in: 3.302105188369751\n",
      "grad W_out: 0.2674041986465454\n",
      "grad W: 3.801597833633423\n",
      "grad W_in: 1.0685207843780518\n",
      "[ep 18 | batch 46] loss=0.6008 |M| |W| mean=2.459e-02\n",
      "ep 18 batch 46 | loss=0.6008 | alloc=1.18GB reserv=27.56GB\n",
      "grad W_out: 0.2646566331386566\n",
      "grad W: 4.451648235321045\n",
      "grad W_in: 0.8163860440254211\n",
      "grad W_out: 0.18463848531246185\n",
      "grad W: 4.170426368713379\n",
      "grad W_in: 0.6894518733024597\n",
      "[ep 18 | batch 48] loss=0.6413 |M| |W| mean=2.459e-02\n",
      "ep 18 batch 48 | loss=0.6413 | alloc=1.18GB reserv=27.56GB\n",
      "grad W_out: 0.2009042352437973\n",
      "grad W: 2.7398757934570312\n",
      "grad W_in: 1.5829007625579834\n",
      "grad W_out: 0.23275718092918396\n",
      "grad W: 2.756861448287964\n",
      "grad W_in: 1.294891357421875\n",
      "[ep 18 | batch 50] loss=0.6426 |M| |W| mean=2.459e-02\n",
      "ep 18 batch 50 | loss=0.6426 | alloc=1.18GB reserv=27.57GB\n",
      "grad W_out: 0.2836272120475769\n",
      "grad W: 4.969923496246338\n",
      "grad W_in: 2.0137503147125244\n",
      "grad W_out: 0.2637796700000763\n",
      "grad W: 4.686538219451904\n",
      "grad W_in: 1.472847580909729\n",
      "[ep 18 | batch 52] loss=0.5745 |M| |W| mean=2.459e-02\n",
      "ep 18 batch 52 | loss=0.5745 | alloc=1.18GB reserv=27.57GB\n",
      "grad W_out: 0.21689316630363464\n",
      "grad W: 5.36716365814209\n",
      "grad W_in: 3.7507448196411133\n",
      "grad W_out: 0.20774781703948975\n",
      "grad W: 4.104865550994873\n",
      "grad W_in: 1.006371021270752\n",
      "[ep 18 | batch 54] loss=0.6172 |M| |W| mean=2.460e-02\n",
      "ep 18 batch 54 | loss=0.6172 | alloc=1.18GB reserv=27.57GB\n",
      "grad W_out: 0.3614881932735443\n",
      "grad W: 5.434718132019043\n",
      "grad W_in: 1.5523176193237305\n",
      "grad W_out: 0.2526566684246063\n",
      "grad W: 4.917201519012451\n",
      "grad W_in: 2.8549137115478516\n",
      "[ep 18 | batch 56] loss=0.6317 |M| |W| mean=2.461e-02\n",
      "ep 18 batch 56 | loss=0.6317 | alloc=1.19GB reserv=27.57GB\n",
      "grad W_out: 0.2545333206653595\n",
      "grad W: 4.647069931030273\n",
      "grad W_in: 1.6119779348373413\n",
      "grad W_out: 0.28105005621910095\n",
      "grad W: 5.774705410003662\n",
      "grad W_in: 6.559417247772217\n",
      "[ep 18 | batch 58] loss=0.5948 |M| |W| mean=2.461e-02\n",
      "ep 18 batch 58 | loss=0.5948 | alloc=1.19GB reserv=27.57GB\n",
      "grad W_out: 0.2654099464416504\n",
      "grad W: 4.952955722808838\n",
      "grad W_in: 4.425377368927002\n",
      "grad W_out: 0.2168140858411789\n",
      "grad W: 5.5805253982543945\n",
      "grad W_in: 4.713019847869873\n",
      "[ep 18 | batch 60] loss=0.5891 |M| |W| mean=2.460e-02\n",
      "ep 18 batch 60 | loss=0.5891 | alloc=1.19GB reserv=27.58GB\n",
      "grad W_out: 0.22605788707733154\n",
      "grad W: 2.6053926944732666\n",
      "grad W_in: 2.1295664310455322\n",
      "grad W_out: 0.20401614904403687\n",
      "grad W: 5.7653889656066895\n",
      "grad W_in: 5.894836902618408\n",
      "[ep 18 | batch 62] loss=0.6309 |M| |W| mean=2.461e-02\n",
      "ep 18 batch 62 | loss=0.6309 | alloc=1.19GB reserv=27.58GB\n",
      "grad W_out: 0.23401190340518951\n",
      "grad W: 3.6697800159454346\n",
      "grad W_in: 1.7448934316635132\n",
      "grad W_out: 0.1495456099510193\n",
      "grad W: 6.959069728851318\n",
      "grad W_in: 5.203727722167969\n",
      "[ep 18 | batch 64] loss=0.5641 |M| |W| mean=2.462e-02\n",
      "ep 18 batch 64 | loss=0.5641 | alloc=1.19GB reserv=27.58GB\n",
      "grad W_out: 0.14980155229568481\n",
      "grad W: 2.8818535804748535\n",
      "grad W_in: 0.8088744878768921\n",
      "grad W_out: 0.2648632526397705\n",
      "grad W: 4.602250576019287\n",
      "grad W_in: 2.704324960708618\n",
      "[ep 18 | batch 66] loss=0.6373 |M| |W| mean=2.462e-02\n",
      "ep 18 batch 66 | loss=0.6373 | alloc=1.19GB reserv=27.58GB\n",
      "grad W_out: 0.29899513721466064\n",
      "grad W: 8.207036018371582\n",
      "grad W_in: 1.8743258714675903\n",
      "grad W_out: 0.3840634822845459\n",
      "grad W: 9.720230102539062\n",
      "grad W_in: 6.38370943069458\n",
      "[ep 18 | batch 68] loss=0.6028 |M| |W| mean=2.463e-02\n",
      "ep 18 batch 68 | loss=0.6028 | alloc=1.19GB reserv=27.58GB\n",
      "grad W_out: 0.2465108335018158\n",
      "grad W: 5.300703525543213\n",
      "grad W_in: 1.5113967657089233\n",
      "grad W_out: 0.19316765666007996\n",
      "grad W: 4.92282247543335\n",
      "grad W_in: 3.1127102375030518\n",
      "[ep 18 | batch 70] loss=0.5383 |M| |W| mean=2.463e-02\n",
      "ep 18 batch 70 | loss=0.5383 | alloc=1.20GB reserv=27.59GB\n",
      "grad W_out: 0.21851301193237305\n",
      "grad W: 4.289576530456543\n",
      "grad W_in: 2.8731515407562256\n",
      "grad W_out: 0.22493726015090942\n",
      "grad W: 5.492326259613037\n",
      "grad W_in: 3.627429962158203\n",
      "[ep 18 | batch 72] loss=0.6120 |M| |W| mean=2.463e-02\n",
      "ep 18 batch 72 | loss=0.6120 | alloc=1.20GB reserv=27.59GB\n",
      "grad W_out: 0.43186262249946594\n",
      "grad W: 9.123185157775879\n",
      "grad W_in: 2.603787899017334\n",
      "grad W_out: 0.1894274801015854\n",
      "grad W: 3.9700262546539307\n",
      "grad W_in: 1.0982027053833008\n",
      "[ep 18 | batch 74] loss=0.5833 |M| |W| mean=2.463e-02\n",
      "ep 18 batch 74 | loss=0.5833 | alloc=1.20GB reserv=27.59GB\n",
      "grad W_out: 0.23805634677410126\n",
      "grad W: 6.387317180633545\n",
      "grad W_in: 5.439587116241455\n",
      "grad W_out: 0.3025408387184143\n",
      "grad W: 7.839474201202393\n",
      "grad W_in: 5.262511730194092\n",
      "[ep 18 | batch 76] loss=0.6420 |M| |W| mean=2.463e-02\n",
      "ep 18 batch 76 | loss=0.6420 | alloc=1.20GB reserv=27.59GB\n",
      "grad W_out: 0.1720041036605835\n",
      "grad W: 4.06353235244751\n",
      "grad W_in: 3.642899751663208\n",
      "grad W_out: 0.2356662154197693\n",
      "grad W: 5.330959796905518\n",
      "grad W_in: 3.702986717224121\n",
      "[ep 18 | batch 78] loss=0.6116 |M| |W| mean=2.464e-02\n",
      "ep 18 batch 78 | loss=0.6116 | alloc=1.20GB reserv=27.59GB\n",
      "grad W_out: 0.3209248185157776\n",
      "grad W: 6.676743984222412\n",
      "grad W_in: 4.971630573272705\n",
      "grad W_out: 0.4037094712257385\n",
      "grad W: 10.797613143920898\n",
      "grad W_in: 3.7182910442352295\n",
      "[ep 18 | batch 80] loss=0.5996 |M| |W| mean=2.463e-02\n",
      "ep 18 batch 80 | loss=0.5996 | alloc=1.20GB reserv=27.60GB\n",
      "grad W_out: 0.3420538902282715\n",
      "grad W: 6.218946933746338\n",
      "grad W_in: 5.462762355804443\n",
      "grad W_out: 0.45859768986701965\n",
      "grad W: 9.718364715576172\n",
      "grad W_in: 4.101043701171875\n",
      "[ep 18 | batch 82] loss=0.6183 |M| |W| mean=2.464e-02\n",
      "ep 18 batch 82 | loss=0.6183 | alloc=1.20GB reserv=27.60GB\n",
      "grad W_out: 0.2925446629524231\n",
      "grad W: 6.7806477546691895\n",
      "grad W_in: 2.888939142227173\n",
      "grad W_out: 0.2876507341861725\n",
      "grad W: 5.936039447784424\n",
      "grad W_in: 1.3569839000701904\n",
      "[ep 18 | batch 84] loss=0.6304 |M| |W| mean=2.464e-02\n",
      "ep 18 batch 84 | loss=0.6304 | alloc=1.21GB reserv=27.60GB\n",
      "grad W_out: 0.2980239987373352\n",
      "grad W: 6.967369556427002\n",
      "grad W_in: 2.8876917362213135\n",
      "grad W_out: 0.33312544226646423\n",
      "grad W: 5.200942516326904\n",
      "grad W_in: 1.7092411518096924\n",
      "[ep 18 | batch 86] loss=0.5840 |M| |W| mean=2.464e-02\n",
      "ep 18 batch 86 | loss=0.5840 | alloc=1.21GB reserv=27.60GB\n",
      "grad W_out: 0.3612714409828186\n",
      "grad W: 5.694718837738037\n",
      "grad W_in: 2.1463983058929443\n",
      "grad W_out: 0.27598837018013\n",
      "grad W: 5.964221000671387\n",
      "grad W_in: 3.2658512592315674\n",
      "[ep 18 | batch 88] loss=0.6684 |M| |W| mean=2.465e-02\n",
      "ep 18 batch 88 | loss=0.6684 | alloc=1.21GB reserv=27.60GB\n",
      "grad W_out: 0.2104276865720749\n",
      "grad W: 4.517999649047852\n",
      "grad W_in: 1.3811317682266235\n",
      "grad W_out: 0.23004843294620514\n",
      "grad W: 5.838082790374756\n",
      "grad W_in: 3.245426893234253\n",
      "[ep 18 | batch 90] loss=0.5634 |M| |W| mean=2.465e-02\n",
      "ep 18 batch 90 | loss=0.5634 | alloc=1.21GB reserv=27.61GB\n",
      "grad W_out: 0.3174838125705719\n",
      "grad W: 4.2944722175598145\n",
      "grad W_in: 1.6192368268966675\n",
      "grad W_out: 0.262315034866333\n",
      "grad W: 4.785010814666748\n",
      "grad W_in: 0.6460176110267639\n",
      "[ep 18 | batch 92] loss=0.6126 |M| |W| mean=2.465e-02\n",
      "ep 18 batch 92 | loss=0.6126 | alloc=1.21GB reserv=27.61GB\n",
      "grad W_out: 0.31227561831474304\n",
      "grad W: 5.758593559265137\n",
      "grad W_in: 1.706233024597168\n",
      "Epoch 18/20 | lr=1.00e-03 | loss=0.6299 | test_acc=80.06% | dt=1178.2s\n",
      "grad W_out: 0.30348411202430725\n",
      "grad W: 6.1963019371032715\n",
      "grad W_in: 2.7814390659332275\n",
      "[ep 19 | batch 0] loss=0.6070 |M| |W| mean=2.465e-02\n",
      "ep 19 batch 0 | loss=0.6070 | alloc=1.21GB reserv=27.61GB\n",
      "grad W_out: 0.2876383066177368\n",
      "grad W: 5.843943119049072\n",
      "grad W_in: 2.0255682468414307\n",
      "grad W_out: 0.21280653774738312\n",
      "grad W: 4.5965895652771\n",
      "grad W_in: 0.8672080636024475\n",
      "[ep 19 | batch 2] loss=0.5293 |M| |W| mean=2.465e-02\n",
      "ep 19 batch 2 | loss=0.5293 | alloc=1.21GB reserv=27.61GB\n",
      "grad W_out: 0.22843961417675018\n",
      "grad W: 4.965145587921143\n",
      "grad W_in: 4.208622932434082\n",
      "grad W_out: 0.2541627883911133\n",
      "grad W: 4.782008171081543\n",
      "grad W_in: 2.127732515335083\n",
      "[ep 19 | batch 4] loss=0.6516 |M| |W| mean=2.465e-02\n",
      "ep 19 batch 4 | loss=0.6516 | alloc=1.22GB reserv=27.61GB\n",
      "grad W_out: 0.1869897097349167\n",
      "grad W: 4.471061706542969\n",
      "grad W_in: 1.3733673095703125\n",
      "grad W_out: 0.3553682267665863\n",
      "grad W: 8.345803260803223\n",
      "grad W_in: 7.232869625091553\n",
      "[ep 19 | batch 6] loss=0.6218 |M| |W| mean=2.465e-02\n",
      "ep 19 batch 6 | loss=0.6218 | alloc=1.22GB reserv=27.62GB\n",
      "grad W_out: 0.2648722231388092\n",
      "grad W: 5.610409259796143\n",
      "grad W_in: 1.69534170627594\n",
      "grad W_out: 0.21372775733470917\n",
      "grad W: 4.448864936828613\n",
      "grad W_in: 1.655609130859375\n",
      "[ep 19 | batch 8] loss=0.5727 |M| |W| mean=2.465e-02\n",
      "ep 19 batch 8 | loss=0.5727 | alloc=1.22GB reserv=27.62GB\n",
      "grad W_out: 0.18447257578372955\n",
      "grad W: 4.919414043426514\n",
      "grad W_in: 1.7646598815917969\n",
      "grad W_out: 0.3107525706291199\n",
      "grad W: 6.055397033691406\n",
      "grad W_in: 5.265992164611816\n",
      "[ep 19 | batch 10] loss=0.6099 |M| |W| mean=2.466e-02\n",
      "ep 19 batch 10 | loss=0.6099 | alloc=1.22GB reserv=27.62GB\n",
      "grad W_out: 0.22929225862026215\n",
      "grad W: 3.9102580547332764\n",
      "grad W_in: 0.8698946237564087\n",
      "grad W_out: 0.2921565771102905\n",
      "grad W: 4.986553192138672\n",
      "grad W_in: 2.505479335784912\n",
      "[ep 19 | batch 12] loss=0.5699 |M| |W| mean=2.466e-02\n",
      "ep 19 batch 12 | loss=0.5699 | alloc=1.22GB reserv=27.62GB\n",
      "grad W_out: 0.23399962484836578\n",
      "grad W: 5.0995259284973145\n",
      "grad W_in: 2.4374561309814453\n",
      "grad W_out: 0.17571331560611725\n",
      "grad W: 5.244332790374756\n",
      "grad W_in: 3.0264596939086914\n",
      "[ep 19 | batch 14] loss=0.5984 |M| |W| mean=2.467e-02\n",
      "ep 19 batch 14 | loss=0.5984 | alloc=1.22GB reserv=27.62GB\n",
      "grad W_out: 0.21389323472976685\n",
      "grad W: 4.979435443878174\n",
      "grad W_in: 4.474334239959717\n",
      "grad W_out: 0.32562190294265747\n",
      "grad W: 5.96917724609375\n",
      "grad W_in: 2.22615122795105\n",
      "[ep 19 | batch 16] loss=0.6062 |M| |W| mean=2.467e-02\n",
      "ep 19 batch 16 | loss=0.6062 | alloc=1.22GB reserv=27.62GB\n",
      "grad W_out: 0.2349167764186859\n",
      "grad W: 3.540821075439453\n",
      "grad W_in: 1.029861330986023\n",
      "grad W_out: 0.24693334102630615\n",
      "grad W: 4.536884307861328\n",
      "grad W_in: 3.7714409828186035\n",
      "[ep 19 | batch 18] loss=0.6519 |M| |W| mean=2.467e-02\n",
      "ep 19 batch 18 | loss=0.6519 | alloc=1.23GB reserv=27.63GB\n",
      "grad W_out: 0.25673842430114746\n",
      "grad W: 5.268150329589844\n",
      "grad W_in: 5.37132453918457\n",
      "grad W_out: 0.2065918743610382\n",
      "grad W: 4.543949604034424\n",
      "grad W_in: 1.545818567276001\n",
      "[ep 19 | batch 20] loss=0.5357 |M| |W| mean=2.468e-02\n",
      "ep 19 batch 20 | loss=0.5357 | alloc=1.23GB reserv=27.63GB\n",
      "grad W_out: 0.2058837115764618\n",
      "grad W: 5.772686004638672\n",
      "grad W_in: 5.1496992111206055\n",
      "grad W_out: 0.21613049507141113\n",
      "grad W: 5.396085262298584\n",
      "grad W_in: 6.031882286071777\n",
      "[ep 19 | batch 22] loss=0.6313 |M| |W| mean=2.468e-02\n",
      "ep 19 batch 22 | loss=0.6313 | alloc=1.23GB reserv=27.63GB\n",
      "grad W_out: 0.17029604315757751\n",
      "grad W: 4.361085414886475\n",
      "grad W_in: 2.047762870788574\n",
      "grad W_out: 0.15916483104228973\n",
      "grad W: 2.8417723178863525\n",
      "grad W_in: 1.8333817720413208\n",
      "[ep 19 | batch 24] loss=0.6469 |M| |W| mean=2.468e-02\n",
      "ep 19 batch 24 | loss=0.6469 | alloc=1.23GB reserv=27.63GB\n",
      "grad W_out: 0.1908390372991562\n",
      "grad W: 5.864825248718262\n",
      "grad W_in: 6.48661470413208\n",
      "grad W_out: 0.2770746052265167\n",
      "grad W: 3.5965776443481445\n",
      "grad W_in: 0.5308374166488647\n",
      "[ep 19 | batch 26] loss=0.5638 |M| |W| mean=2.469e-02\n",
      "ep 19 batch 26 | loss=0.5638 | alloc=1.23GB reserv=27.63GB\n",
      "grad W_out: 0.2926061749458313\n",
      "grad W: 4.672524929046631\n",
      "grad W_in: 1.6381343603134155\n",
      "grad W_out: 0.2899076044559479\n",
      "grad W: 4.805739402770996\n",
      "grad W_in: 4.094099521636963\n",
      "[ep 19 | batch 28] loss=0.5899 |M| |W| mean=2.469e-02\n",
      "ep 19 batch 28 | loss=0.5899 | alloc=1.23GB reserv=27.64GB\n",
      "grad W_out: 0.2387007623910904\n",
      "grad W: 4.779720306396484\n",
      "grad W_in: 2.5898351669311523\n",
      "grad W_out: 0.21542410552501678\n",
      "grad W: 3.639843463897705\n",
      "grad W_in: 1.388248324394226\n",
      "[ep 19 | batch 30] loss=0.5824 |M| |W| mean=2.468e-02\n",
      "ep 19 batch 30 | loss=0.5824 | alloc=1.23GB reserv=27.64GB\n",
      "grad W_out: 0.21997785568237305\n",
      "grad W: 6.119857311248779\n",
      "grad W_in: 4.412158489227295\n",
      "grad W_out: 0.2441423535346985\n",
      "grad W: 7.225601673126221\n",
      "grad W_in: 4.0635480880737305\n",
      "[ep 19 | batch 32] loss=0.5895 |M| |W| mean=2.469e-02\n",
      "ep 19 batch 32 | loss=0.5895 | alloc=1.24GB reserv=27.64GB\n",
      "grad W_out: 0.18718382716178894\n",
      "grad W: 3.3620824813842773\n",
      "grad W_in: 0.6631708741188049\n",
      "grad W_out: 0.17657704651355743\n",
      "grad W: 7.785002708435059\n",
      "grad W_in: 6.644140720367432\n",
      "[ep 19 | batch 34] loss=0.5544 |M| |W| mean=2.469e-02\n",
      "ep 19 batch 34 | loss=0.5544 | alloc=1.24GB reserv=27.64GB\n",
      "grad W_out: 0.2740442156791687\n",
      "grad W: 6.6563639640808105\n",
      "grad W_in: 1.5943585634231567\n",
      "grad W_out: 0.21155817806720734\n",
      "grad W: 2.6240158081054688\n",
      "grad W_in: 0.5837296843528748\n",
      "[ep 19 | batch 36] loss=0.5704 |M| |W| mean=2.469e-02\n",
      "ep 19 batch 36 | loss=0.5704 | alloc=1.24GB reserv=27.64GB\n",
      "grad W_out: 0.37787652015686035\n",
      "grad W: 4.150403022766113\n",
      "grad W_in: 1.7008782625198364\n",
      "grad W_out: 0.21968688070774078\n",
      "grad W: 4.303865432739258\n",
      "grad W_in: 2.3751776218414307\n",
      "[ep 19 | batch 38] loss=0.5418 |M| |W| mean=2.469e-02\n",
      "ep 19 batch 38 | loss=0.5418 | alloc=1.24GB reserv=27.65GB\n",
      "grad W_out: 0.2366655170917511\n",
      "grad W: 5.367276668548584\n",
      "grad W_in: 1.795793890953064\n",
      "grad W_out: 0.2065916806459427\n",
      "grad W: 3.7385010719299316\n",
      "grad W_in: 1.8222744464874268\n",
      "[ep 19 | batch 40] loss=0.5723 |M| |W| mean=2.469e-02\n",
      "ep 19 batch 40 | loss=0.5723 | alloc=1.24GB reserv=27.65GB\n",
      "grad W_out: 0.26295244693756104\n",
      "grad W: 7.1922383308410645\n",
      "grad W_in: 6.335449695587158\n",
      "grad W_out: 0.2746780216693878\n",
      "grad W: 5.340255260467529\n",
      "grad W_in: 2.7277004718780518\n",
      "[ep 19 | batch 42] loss=0.6328 |M| |W| mean=2.470e-02\n",
      "ep 19 batch 42 | loss=0.6328 | alloc=1.24GB reserv=27.65GB\n",
      "grad W_out: 0.42196065187454224\n",
      "grad W: 10.03490924835205\n",
      "grad W_in: 3.504051923751831\n",
      "grad W_out: 0.2128528207540512\n",
      "grad W: 4.406597137451172\n",
      "grad W_in: 1.0971882343292236\n",
      "[ep 19 | batch 44] loss=0.5377 |M| |W| mean=2.470e-02\n",
      "ep 19 batch 44 | loss=0.5377 | alloc=1.24GB reserv=27.65GB\n",
      "grad W_out: 0.29567667841911316\n",
      "grad W: 4.926593780517578\n",
      "grad W_in: 2.7634806632995605\n",
      "grad W_out: 0.40581178665161133\n",
      "grad W: 9.472959518432617\n",
      "grad W_in: 2.782566785812378\n",
      "[ep 19 | batch 46] loss=0.6755 |M| |W| mean=2.471e-02\n",
      "ep 19 batch 46 | loss=0.6755 | alloc=1.25GB reserv=27.65GB\n",
      "grad W_out: 0.2717026174068451\n",
      "grad W: 7.153681755065918\n",
      "grad W_in: 1.0129694938659668\n",
      "grad W_out: 0.3891758620738983\n",
      "grad W: 6.139472007751465\n",
      "grad W_in: 2.1500890254974365\n",
      "[ep 19 | batch 48] loss=0.5852 |M| |W| mean=2.471e-02\n",
      "ep 19 batch 48 | loss=0.5852 | alloc=1.25GB reserv=27.66GB\n",
      "grad W_out: 0.22068454325199127\n",
      "grad W: 4.896251678466797\n",
      "grad W_in: 0.519864559173584\n",
      "grad W_out: 0.32525956630706787\n",
      "grad W: 7.8450212478637695\n",
      "grad W_in: 2.0561635494232178\n",
      "[ep 19 | batch 50] loss=0.6717 |M| |W| mean=2.471e-02\n",
      "ep 19 batch 50 | loss=0.6717 | alloc=1.25GB reserv=27.66GB\n",
      "grad W_out: 0.3714219927787781\n",
      "grad W: 6.247157573699951\n",
      "grad W_in: 1.4125746488571167\n",
      "grad W_out: 0.1976819932460785\n",
      "grad W: 5.265642166137695\n",
      "grad W_in: 0.9060548543930054\n",
      "[ep 19 | batch 52] loss=0.5218 |M| |W| mean=2.471e-02\n",
      "ep 19 batch 52 | loss=0.5218 | alloc=1.25GB reserv=27.66GB\n",
      "grad W_out: 0.3092741072177887\n",
      "grad W: 6.47733211517334\n",
      "grad W_in: 1.782056212425232\n",
      "grad W_out: 0.15741945803165436\n",
      "grad W: 6.635836601257324\n",
      "grad W_in: 3.7122812271118164\n",
      "[ep 19 | batch 54] loss=0.6710 |M| |W| mean=2.472e-02\n",
      "ep 19 batch 54 | loss=0.6710 | alloc=1.25GB reserv=27.66GB\n",
      "grad W_out: 0.3187433183193207\n",
      "grad W: 5.483707427978516\n",
      "grad W_in: 2.6046032905578613\n",
      "grad W_out: 0.31303170323371887\n",
      "grad W: 5.3767409324646\n",
      "grad W_in: 1.8491997718811035\n",
      "[ep 19 | batch 56] loss=0.6214 |M| |W| mean=2.473e-02\n",
      "ep 19 batch 56 | loss=0.6214 | alloc=1.25GB reserv=27.66GB\n",
      "grad W_out: 0.27154624462127686\n",
      "grad W: 5.085184574127197\n",
      "grad W_in: 3.2983551025390625\n",
      "grad W_out: 0.27856966853141785\n",
      "grad W: 7.474113464355469\n",
      "grad W_in: 4.052816867828369\n",
      "[ep 19 | batch 58] loss=0.6062 |M| |W| mean=2.473e-02\n",
      "ep 19 batch 58 | loss=0.6062 | alloc=1.25GB reserv=27.67GB\n",
      "grad W_out: 0.12252013385295868\n",
      "grad W: 4.059247970581055\n",
      "grad W_in: 0.7885809540748596\n",
      "grad W_out: 0.2867693305015564\n",
      "grad W: 5.091325759887695\n",
      "grad W_in: 2.2117068767547607\n",
      "[ep 19 | batch 60] loss=0.6446 |M| |W| mean=2.473e-02\n",
      "ep 19 batch 60 | loss=0.6446 | alloc=1.26GB reserv=27.67GB\n",
      "grad W_out: 0.14672145247459412\n",
      "grad W: 4.389902114868164\n",
      "grad W_in: 1.2992908954620361\n",
      "grad W_out: 0.19526369869709015\n",
      "grad W: 4.336451530456543\n",
      "grad W_in: 1.479669451713562\n",
      "[ep 19 | batch 62] loss=0.5578 |M| |W| mean=2.474e-02\n",
      "ep 19 batch 62 | loss=0.5578 | alloc=1.26GB reserv=27.67GB\n",
      "grad W_out: 0.2604323625564575\n",
      "grad W: 5.78595495223999\n",
      "grad W_in: 4.185903549194336\n",
      "grad W_out: 0.24922162294387817\n",
      "grad W: 4.641575336456299\n",
      "grad W_in: 0.7096747756004333\n",
      "[ep 19 | batch 64] loss=0.6488 |M| |W| mean=2.474e-02\n",
      "ep 19 batch 64 | loss=0.6488 | alloc=1.26GB reserv=27.67GB\n",
      "grad W_out: 0.27779072523117065\n",
      "grad W: 7.9311041831970215\n",
      "grad W_in: 4.257753849029541\n",
      "grad W_out: 0.24872000515460968\n",
      "grad W: 6.893945693969727\n",
      "grad W_in: 7.116252899169922\n",
      "[ep 19 | batch 66] loss=0.6188 |M| |W| mean=2.474e-02\n",
      "ep 19 batch 66 | loss=0.6188 | alloc=1.26GB reserv=27.67GB\n",
      "grad W_out: 0.28938156366348267\n",
      "grad W: 7.6730451583862305\n",
      "grad W_in: 3.610927104949951\n",
      "grad W_out: 0.3629222810268402\n",
      "grad W: 7.696142673492432\n",
      "grad W_in: 5.461863040924072\n",
      "[ep 19 | batch 68] loss=0.5832 |M| |W| mean=2.474e-02\n",
      "ep 19 batch 68 | loss=0.5832 | alloc=1.26GB reserv=27.68GB\n",
      "grad W_out: 0.30056384205818176\n",
      "grad W: 4.86444091796875\n",
      "grad W_in: 0.7638286352157593\n",
      "grad W_out: 0.2387581467628479\n",
      "grad W: 5.588761329650879\n",
      "grad W_in: 2.3513810634613037\n",
      "[ep 19 | batch 70] loss=0.5615 |M| |W| mean=2.475e-02\n",
      "ep 19 batch 70 | loss=0.5615 | alloc=1.26GB reserv=27.68GB\n",
      "grad W_out: 0.23943327367305756\n",
      "grad W: 5.398739814758301\n",
      "grad W_in: 3.277900457382202\n",
      "grad W_out: 0.2103693187236786\n",
      "grad W: 4.440603733062744\n",
      "grad W_in: 1.4621477127075195\n",
      "[ep 19 | batch 72] loss=0.5704 |M| |W| mean=2.475e-02\n",
      "ep 19 batch 72 | loss=0.5704 | alloc=1.26GB reserv=27.68GB\n",
      "grad W_out: 0.21800953149795532\n",
      "grad W: 5.025387763977051\n",
      "grad W_in: 3.9585723876953125\n",
      "grad W_out: 0.33219337463378906\n",
      "grad W: 7.489019393920898\n",
      "grad W_in: 2.33785343170166\n",
      "[ep 19 | batch 74] loss=0.6559 |M| |W| mean=2.476e-02\n",
      "ep 19 batch 74 | loss=0.6559 | alloc=1.27GB reserv=27.68GB\n",
      "grad W_out: 0.25160181522369385\n",
      "grad W: 6.210422039031982\n",
      "grad W_in: 2.5408482551574707\n",
      "grad W_out: 0.24148796498775482\n",
      "grad W: 4.954643726348877\n",
      "grad W_in: 2.091963291168213\n",
      "[ep 19 | batch 76] loss=0.5678 |M| |W| mean=2.476e-02\n",
      "ep 19 batch 76 | loss=0.5678 | alloc=1.27GB reserv=27.68GB\n",
      "grad W_out: 0.28155794739723206\n",
      "grad W: 8.991124153137207\n",
      "grad W_in: 2.1907646656036377\n",
      "grad W_out: 0.3175738751888275\n",
      "grad W: 5.847758769989014\n",
      "grad W_in: 2.2464957237243652\n",
      "[ep 19 | batch 78] loss=0.6130 |M| |W| mean=2.476e-02\n",
      "ep 19 batch 78 | loss=0.6130 | alloc=1.27GB reserv=27.69GB\n",
      "grad W_out: 0.17021578550338745\n",
      "grad W: 6.708371162414551\n",
      "grad W_in: 3.2877471446990967\n",
      "grad W_out: 0.26894500851631165\n",
      "grad W: 9.424589157104492\n",
      "grad W_in: 7.213649272918701\n",
      "[ep 19 | batch 80] loss=0.6443 |M| |W| mean=2.476e-02\n",
      "ep 19 batch 80 | loss=0.6443 | alloc=1.27GB reserv=27.69GB\n",
      "grad W_out: 0.34257522225379944\n",
      "grad W: 6.683762550354004\n",
      "grad W_in: 4.095902442932129\n",
      "grad W_out: 0.23250508308410645\n",
      "grad W: 7.061312675476074\n",
      "grad W_in: 4.594206809997559\n",
      "[ep 19 | batch 82] loss=0.6205 |M| |W| mean=2.476e-02\n",
      "ep 19 batch 82 | loss=0.6205 | alloc=1.27GB reserv=27.69GB\n",
      "grad W_out: 0.32859867811203003\n",
      "grad W: 10.366684913635254\n",
      "grad W_in: 9.498575210571289\n",
      "grad W_out: 0.20538461208343506\n",
      "grad W: 3.439635992050171\n",
      "grad W_in: 0.8856112360954285\n",
      "[ep 19 | batch 84] loss=0.5646 |M| |W| mean=2.477e-02\n",
      "ep 19 batch 84 | loss=0.5646 | alloc=1.27GB reserv=27.69GB\n",
      "grad W_out: 0.31916093826293945\n",
      "grad W: 7.2144269943237305\n",
      "grad W_in: 6.041890621185303\n",
      "grad W_out: 0.34763795137405396\n",
      "grad W: 8.28514575958252\n",
      "grad W_in: 8.668049812316895\n",
      "[ep 19 | batch 86] loss=0.6717 |M| |W| mean=2.478e-02\n",
      "ep 19 batch 86 | loss=0.6717 | alloc=1.27GB reserv=27.69GB\n",
      "grad W_out: 0.36824652552604675\n",
      "grad W: 6.230506896972656\n",
      "grad W_in: 1.1279919147491455\n",
      "grad W_out: 0.43377685546875\n",
      "grad W: 7.076704978942871\n",
      "grad W_in: 3.5548489093780518\n",
      "[ep 19 | batch 88] loss=0.6438 |M| |W| mean=2.477e-02\n",
      "ep 19 batch 88 | loss=0.6438 | alloc=1.28GB reserv=27.70GB\n",
      "grad W_out: 0.3062402904033661\n",
      "grad W: 9.161842346191406\n",
      "grad W_in: 6.27601432800293\n",
      "grad W_out: 0.19848822057247162\n",
      "grad W: 3.8549814224243164\n",
      "grad W_in: 2.8601362705230713\n",
      "[ep 19 | batch 90] loss=0.6549 |M| |W| mean=2.478e-02\n",
      "ep 19 batch 90 | loss=0.6549 | alloc=1.28GB reserv=27.70GB\n",
      "grad W_out: 0.27642351388931274\n",
      "grad W: 7.797215938568115\n",
      "grad W_in: 4.771202087402344\n",
      "grad W_out: 0.5622872710227966\n",
      "grad W: 11.298761367797852\n",
      "grad W_in: 7.263064384460449\n",
      "[ep 19 | batch 92] loss=0.6786 |M| |W| mean=2.478e-02\n",
      "ep 19 batch 92 | loss=0.6786 | alloc=1.28GB reserv=27.70GB\n",
      "grad W_out: 0.3162270188331604\n",
      "grad W: 5.772787094116211\n",
      "grad W_in: 1.0794639587402344\n",
      "Epoch 19/20 | lr=1.00e-03 | loss=0.6022 | test_acc=79.32% | dt=1178.7s\n",
      "grad W_out: 0.2497512251138687\n",
      "grad W: 5.093100070953369\n",
      "grad W_in: 3.107799530029297\n",
      "[ep 20 | batch 0] loss=0.6900 |M| |W| mean=2.478e-02\n",
      "ep 20 batch 0 | loss=0.6900 | alloc=1.28GB reserv=27.70GB\n",
      "grad W_out: 0.41766247153282166\n",
      "grad W: 9.325919151306152\n",
      "grad W_in: 9.424188613891602\n",
      "grad W_out: 0.32670170068740845\n",
      "grad W: 8.998465538024902\n",
      "grad W_in: 6.251169204711914\n",
      "[ep 20 | batch 2] loss=0.5836 |M| |W| mean=2.479e-02\n",
      "ep 20 batch 2 | loss=0.5836 | alloc=1.28GB reserv=27.70GB\n",
      "grad W_out: 0.3816591799259186\n",
      "grad W: 8.718144416809082\n",
      "grad W_in: 6.060944080352783\n",
      "grad W_out: 0.4979354739189148\n",
      "grad W: 9.036951065063477\n",
      "grad W_in: 6.682340621948242\n",
      "[ep 20 | batch 4] loss=0.6103 |M| |W| mean=2.480e-02\n",
      "ep 20 batch 4 | loss=0.6103 | alloc=1.28GB reserv=27.71GB\n",
      "grad W_out: 0.2801045775413513\n",
      "grad W: 7.696599006652832\n",
      "grad W_in: 6.353481292724609\n",
      "grad W_out: 0.2836814522743225\n",
      "grad W: 5.549910545349121\n",
      "grad W_in: 2.5222866535186768\n",
      "[ep 20 | batch 6] loss=0.6334 |M| |W| mean=2.479e-02\n",
      "ep 20 batch 6 | loss=0.6334 | alloc=1.28GB reserv=27.71GB\n",
      "grad W_out: 0.4590000510215759\n",
      "grad W: 8.775249481201172\n",
      "grad W_in: 5.768972396850586\n",
      "grad W_out: 0.4055385887622833\n",
      "grad W: 7.833630084991455\n",
      "grad W_in: 2.492271900177002\n",
      "[ep 20 | batch 8] loss=0.5970 |M| |W| mean=2.479e-02\n",
      "ep 20 batch 8 | loss=0.5970 | alloc=1.29GB reserv=27.71GB\n",
      "grad W_out: 0.23636585474014282\n",
      "grad W: 4.934503555297852\n",
      "grad W_in: 2.678835391998291\n",
      "grad W_out: 0.25070416927337646\n",
      "grad W: 5.079537868499756\n",
      "grad W_in: 2.034824848175049\n",
      "[ep 20 | batch 10] loss=0.5711 |M| |W| mean=2.480e-02\n",
      "ep 20 batch 10 | loss=0.5711 | alloc=1.29GB reserv=27.71GB\n",
      "grad W_out: 0.47277382016181946\n",
      "grad W: 6.0634965896606445\n",
      "grad W_in: 2.5299320220947266\n",
      "grad W_out: 0.3280230462551117\n",
      "grad W: 7.986555576324463\n",
      "grad W_in: 3.2105679512023926\n",
      "[ep 20 | batch 12] loss=0.5786 |M| |W| mean=2.480e-02\n",
      "ep 20 batch 12 | loss=0.5786 | alloc=1.29GB reserv=27.71GB\n",
      "grad W_out: 0.1717979907989502\n",
      "grad W: 5.285994529724121\n",
      "grad W_in: 1.752398133277893\n",
      "grad W_out: 0.23044124245643616\n",
      "grad W: 4.65669584274292\n",
      "grad W_in: 2.2639000415802\n",
      "[ep 20 | batch 14] loss=0.6260 |M| |W| mean=2.480e-02\n",
      "ep 20 batch 14 | loss=0.6260 | alloc=1.29GB reserv=27.71GB\n",
      "grad W_out: 0.6002780795097351\n",
      "grad W: 16.346942901611328\n",
      "grad W_in: 9.123141288757324\n",
      "grad W_out: 0.2864803075790405\n",
      "grad W: 5.591577529907227\n",
      "grad W_in: 3.6344637870788574\n",
      "[ep 20 | batch 16] loss=0.6189 |M| |W| mean=2.482e-02\n",
      "ep 20 batch 16 | loss=0.6189 | alloc=1.29GB reserv=27.72GB\n",
      "grad W_out: 0.3662377893924713\n",
      "grad W: 9.60886001586914\n",
      "grad W_in: 8.382636070251465\n",
      "grad W_out: 0.3628714978694916\n",
      "grad W: 9.10960578918457\n",
      "grad W_in: 7.139279842376709\n",
      "[ep 20 | batch 18] loss=0.5831 |M| |W| mean=2.481e-02\n",
      "ep 20 batch 18 | loss=0.5831 | alloc=1.29GB reserv=27.72GB\n",
      "grad W_out: 0.42164745926856995\n",
      "grad W: 7.179978847503662\n",
      "grad W_in: 6.549225807189941\n",
      "grad W_out: 0.340129554271698\n",
      "grad W: 12.769196510314941\n",
      "grad W_in: 11.721715927124023\n",
      "[ep 20 | batch 20] loss=0.5738 |M| |W| mean=2.482e-02\n",
      "ep 20 batch 20 | loss=0.5738 | alloc=1.29GB reserv=27.72GB\n",
      "grad W_out: 0.22911208868026733\n",
      "grad W: 4.496045112609863\n",
      "grad W_in: 2.41042423248291\n",
      "grad W_out: 0.15306107699871063\n",
      "grad W: 7.620162010192871\n",
      "grad W_in: 6.96615743637085\n",
      "[ep 20 | batch 22] loss=0.5801 |M| |W| mean=2.483e-02\n",
      "ep 20 batch 22 | loss=0.5801 | alloc=1.30GB reserv=27.72GB\n",
      "grad W_out: 0.3399818539619446\n",
      "grad W: 7.854950428009033\n",
      "grad W_in: 2.0698347091674805\n",
      "grad W_out: 0.297421932220459\n",
      "grad W: 5.56918478012085\n",
      "grad W_in: 2.6501784324645996\n",
      "[ep 20 | batch 24] loss=0.6208 |M| |W| mean=2.483e-02\n",
      "ep 20 batch 24 | loss=0.6208 | alloc=1.30GB reserv=27.72GB\n",
      "grad W_out: 0.21592381596565247\n",
      "grad W: 4.442327976226807\n",
      "grad W_in: 4.057775497436523\n",
      "grad W_out: 0.283830463886261\n",
      "grad W: 6.224580764770508\n",
      "grad W_in: 2.6400396823883057\n",
      "[ep 20 | batch 26] loss=0.5467 |M| |W| mean=2.483e-02\n",
      "ep 20 batch 26 | loss=0.5467 | alloc=1.30GB reserv=27.73GB\n",
      "grad W_out: 0.2967437207698822\n",
      "grad W: 5.398521423339844\n",
      "grad W_in: 1.1306084394454956\n",
      "grad W_out: 0.27140533924102783\n",
      "grad W: 4.976353645324707\n",
      "grad W_in: 1.0242005586624146\n",
      "[ep 20 | batch 28] loss=0.5683 |M| |W| mean=2.483e-02\n",
      "ep 20 batch 28 | loss=0.5683 | alloc=1.30GB reserv=27.73GB\n",
      "grad W_out: 0.345465749502182\n",
      "grad W: 5.362764358520508\n",
      "grad W_in: 2.3175063133239746\n",
      "grad W_out: 0.21955245733261108\n",
      "grad W: 5.802961826324463\n",
      "grad W_in: 0.6954477429389954\n",
      "[ep 20 | batch 30] loss=0.5337 |M| |W| mean=2.484e-02\n",
      "ep 20 batch 30 | loss=0.5337 | alloc=1.30GB reserv=27.73GB\n",
      "grad W_out: 0.2708350718021393\n",
      "grad W: 6.845729827880859\n",
      "grad W_in: 3.5591344833374023\n",
      "grad W_out: 0.236810103058815\n",
      "grad W: 4.890606880187988\n",
      "grad W_in: 2.1680684089660645\n",
      "[ep 20 | batch 32] loss=0.6123 |M| |W| mean=2.484e-02\n",
      "ep 20 batch 32 | loss=0.6123 | alloc=1.30GB reserv=27.73GB\n",
      "grad W_out: 0.23695997893810272\n",
      "grad W: 3.8807497024536133\n",
      "grad W_in: 0.6083594560623169\n",
      "grad W_out: 0.1988597959280014\n",
      "grad W: 4.164305686950684\n",
      "grad W_in: 1.1457804441452026\n",
      "[ep 20 | batch 34] loss=0.5409 |M| |W| mean=2.485e-02\n",
      "ep 20 batch 34 | loss=0.5409 | alloc=1.30GB reserv=27.73GB\n",
      "grad W_out: 0.39761048555374146\n",
      "grad W: 6.944180011749268\n",
      "grad W_in: 5.455191612243652\n",
      "grad W_out: 0.28002527356147766\n",
      "grad W: 3.161952257156372\n",
      "grad W_in: 1.5115180015563965\n",
      "[ep 20 | batch 36] loss=0.6702 |M| |W| mean=2.484e-02\n",
      "ep 20 batch 36 | loss=0.6702 | alloc=1.31GB reserv=27.74GB\n",
      "grad W_out: 0.2873557507991791\n",
      "grad W: 4.776119232177734\n",
      "grad W_in: 3.7245943546295166\n",
      "grad W_out: 0.2705132067203522\n",
      "grad W: 5.684413433074951\n",
      "grad W_in: 5.521752834320068\n",
      "[ep 20 | batch 38] loss=0.6143 |M| |W| mean=2.485e-02\n",
      "ep 20 batch 38 | loss=0.6143 | alloc=1.31GB reserv=27.74GB\n",
      "grad W_out: 0.13248999416828156\n",
      "grad W: 3.1471235752105713\n",
      "grad W_in: 1.438877820968628\n",
      "grad W_out: 0.28899145126342773\n",
      "grad W: 6.787150859832764\n",
      "grad W_in: 4.809317588806152\n",
      "[ep 20 | batch 40] loss=0.5715 |M| |W| mean=2.485e-02\n",
      "ep 20 batch 40 | loss=0.5715 | alloc=1.31GB reserv=27.74GB\n",
      "grad W_out: 0.29589393734931946\n",
      "grad W: 8.305510520935059\n",
      "grad W_in: 8.081619262695312\n",
      "grad W_out: 0.4969629645347595\n",
      "grad W: 11.637336730957031\n",
      "grad W_in: 11.444830894470215\n",
      "[ep 20 | batch 42] loss=0.7100 |M| |W| mean=2.485e-02\n",
      "ep 20 batch 42 | loss=0.7100 | alloc=1.31GB reserv=27.74GB\n",
      "grad W_out: 0.3431893289089203\n",
      "grad W: 7.6928253173828125\n",
      "grad W_in: 6.050350666046143\n",
      "grad W_out: 0.5246144533157349\n",
      "grad W: 11.529023170471191\n",
      "grad W_in: 8.671520233154297\n",
      "[ep 20 | batch 44] loss=0.6163 |M| |W| mean=2.485e-02\n",
      "ep 20 batch 44 | loss=0.6163 | alloc=1.31GB reserv=27.74GB\n",
      "grad W_out: 0.415809690952301\n",
      "grad W: 9.37586498260498\n",
      "grad W_in: 7.528688907623291\n",
      "grad W_out: 0.4776345491409302\n",
      "grad W: 7.976546287536621\n",
      "grad W_in: 4.180506229400635\n",
      "[ep 20 | batch 46] loss=0.6362 |M| |W| mean=2.486e-02\n",
      "ep 20 batch 46 | loss=0.6362 | alloc=1.31GB reserv=27.75GB\n",
      "grad W_out: 0.34687942266464233\n",
      "grad W: 8.409128189086914\n",
      "grad W_in: 3.5524425506591797\n",
      "grad W_out: 0.21498745679855347\n",
      "grad W: 3.6877167224884033\n",
      "grad W_in: 1.2728453874588013\n",
      "[ep 20 | batch 48] loss=0.5040 |M| |W| mean=2.486e-02\n",
      "ep 20 batch 48 | loss=0.5040 | alloc=1.31GB reserv=27.75GB\n",
      "grad W_out: 0.39759233593940735\n",
      "grad W: 6.442045211791992\n",
      "grad W_in: 2.5790233612060547\n",
      "grad W_out: 0.4913563132286072\n",
      "grad W: 8.184303283691406\n",
      "grad W_in: 1.5895185470581055\n",
      "[ep 20 | batch 50] loss=0.6677 |M| |W| mean=2.486e-02\n",
      "ep 20 batch 50 | loss=0.6677 | alloc=1.32GB reserv=27.75GB\n",
      "grad W_out: 0.275429904460907\n",
      "grad W: 3.939005136489868\n",
      "grad W_in: 0.777166485786438\n",
      "grad W_out: 0.13467751443386078\n",
      "grad W: 3.2319703102111816\n",
      "grad W_in: 0.4850858747959137\n",
      "[ep 20 | batch 52] loss=0.6381 |M| |W| mean=2.486e-02\n",
      "ep 20 batch 52 | loss=0.6381 | alloc=1.32GB reserv=27.75GB\n",
      "grad W_out: 0.3736914098262787\n",
      "grad W: 7.9488115310668945\n",
      "grad W_in: 2.860530376434326\n",
      "grad W_out: 0.2551284730434418\n",
      "grad W: 4.36334228515625\n",
      "grad W_in: 2.8848021030426025\n",
      "[ep 20 | batch 54] loss=0.5354 |M| |W| mean=2.485e-02\n",
      "ep 20 batch 54 | loss=0.5354 | alloc=1.32GB reserv=27.75GB\n",
      "grad W_out: 0.25930255651474\n",
      "grad W: 4.132447719573975\n",
      "grad W_in: 3.5675251483917236\n",
      "grad W_out: 0.28002282977104187\n",
      "grad W: 4.106810569763184\n",
      "grad W_in: 0.835157036781311\n",
      "[ep 20 | batch 56] loss=0.5544 |M| |W| mean=2.486e-02\n",
      "ep 20 batch 56 | loss=0.5544 | alloc=1.32GB reserv=27.76GB\n",
      "grad W_out: 0.23972421884536743\n",
      "grad W: 3.548959493637085\n",
      "grad W_in: 1.7560652494430542\n",
      "grad W_out: 0.22416090965270996\n",
      "grad W: 4.328729152679443\n",
      "grad W_in: 1.0140278339385986\n",
      "[ep 20 | batch 58] loss=0.6264 |M| |W| mean=2.486e-02\n",
      "ep 20 batch 58 | loss=0.6264 | alloc=1.32GB reserv=27.76GB\n",
      "grad W_out: 0.25505104660987854\n",
      "grad W: 3.7321836948394775\n",
      "grad W_in: 1.5862435102462769\n",
      "grad W_out: 0.319205105304718\n",
      "grad W: 6.241695404052734\n",
      "grad W_in: 2.481920003890991\n",
      "[ep 20 | batch 60] loss=0.5760 |M| |W| mean=2.487e-02\n",
      "ep 20 batch 60 | loss=0.5760 | alloc=1.32GB reserv=27.76GB\n",
      "grad W_out: 0.23306752741336823\n",
      "grad W: 3.783186674118042\n",
      "grad W_in: 2.637662172317505\n",
      "grad W_out: 0.1681750863790512\n",
      "grad W: 3.966937780380249\n",
      "grad W_in: 0.909814715385437\n",
      "[ep 20 | batch 62] loss=0.6002 |M| |W| mean=2.487e-02\n",
      "ep 20 batch 62 | loss=0.6002 | alloc=1.32GB reserv=27.76GB\n",
      "grad W_out: 0.14214184880256653\n",
      "grad W: 3.1577563285827637\n",
      "grad W_in: 0.9181190729141235\n",
      "grad W_out: 0.2628263533115387\n",
      "grad W: 3.998587131500244\n",
      "grad W_in: 2.4532620906829834\n",
      "[ep 20 | batch 64] loss=0.5441 |M| |W| mean=2.487e-02\n",
      "ep 20 batch 64 | loss=0.5441 | alloc=1.33GB reserv=27.76GB\n",
      "grad W_out: 0.2300020307302475\n",
      "grad W: 5.772463321685791\n",
      "grad W_in: 3.0744857788085938\n",
      "grad W_out: 0.36172205209732056\n",
      "grad W: 5.946996688842773\n",
      "grad W_in: 1.1778236627578735\n",
      "[ep 20 | batch 66] loss=0.5739 |M| |W| mean=2.488e-02\n",
      "ep 20 batch 66 | loss=0.5739 | alloc=1.33GB reserv=27.77GB\n",
      "grad W_out: 0.25159916281700134\n",
      "grad W: 5.509895324707031\n",
      "grad W_in: 2.2679147720336914\n",
      "grad W_out: 0.3185037672519684\n",
      "grad W: 5.707669734954834\n",
      "grad W_in: 3.7454164028167725\n",
      "[ep 20 | batch 68] loss=0.5908 |M| |W| mean=2.488e-02\n",
      "ep 20 batch 68 | loss=0.5908 | alloc=1.33GB reserv=27.77GB\n",
      "grad W_out: 0.37874624133110046\n",
      "grad W: 7.241617202758789\n",
      "grad W_in: 2.3059027194976807\n",
      "grad W_out: 0.2556639313697815\n",
      "grad W: 5.566490173339844\n",
      "grad W_in: 1.965934157371521\n",
      "[ep 20 | batch 70] loss=0.5086 |M| |W| mean=2.488e-02\n",
      "ep 20 batch 70 | loss=0.5086 | alloc=1.33GB reserv=27.77GB\n",
      "grad W_out: 0.34300997853279114\n",
      "grad W: 4.170348644256592\n",
      "grad W_in: 1.6726369857788086\n",
      "grad W_out: 0.2920304536819458\n",
      "grad W: 5.439347743988037\n",
      "grad W_in: 0.9517133235931396\n",
      "[ep 20 | batch 72] loss=0.4734 |M| |W| mean=2.489e-02\n",
      "ep 20 batch 72 | loss=0.4734 | alloc=1.33GB reserv=27.77GB\n",
      "grad W_out: 0.3168705403804779\n",
      "grad W: 6.307914733886719\n",
      "grad W_in: 1.216984748840332\n",
      "grad W_out: 0.2982218861579895\n",
      "grad W: 6.970993995666504\n",
      "grad W_in: 1.6971542835235596\n",
      "[ep 20 | batch 74] loss=0.6429 |M| |W| mean=2.490e-02\n",
      "ep 20 batch 74 | loss=0.6429 | alloc=1.33GB reserv=27.77GB\n",
      "grad W_out: 0.33181366324424744\n",
      "grad W: 2.728274345397949\n",
      "grad W_in: 0.839737057685852\n",
      "grad W_out: 0.1457502394914627\n",
      "grad W: 4.794709205627441\n",
      "grad W_in: 2.5095934867858887\n",
      "[ep 20 | batch 76] loss=0.6046 |M| |W| mean=2.490e-02\n",
      "ep 20 batch 76 | loss=0.6046 | alloc=1.33GB reserv=27.78GB\n",
      "grad W_out: 0.32020169496536255\n",
      "grad W: 6.011776924133301\n",
      "grad W_in: 1.4985957145690918\n",
      "grad W_out: 0.19722820818424225\n",
      "grad W: 4.687243461608887\n",
      "grad W_in: 2.4734625816345215\n",
      "[ep 20 | batch 78] loss=0.6004 |M| |W| mean=2.490e-02\n",
      "ep 20 batch 78 | loss=0.6004 | alloc=1.34GB reserv=27.78GB\n",
      "grad W_out: 0.17397570610046387\n",
      "grad W: 3.1203651428222656\n",
      "grad W_in: 1.4393680095672607\n",
      "grad W_out: 0.21279314160346985\n",
      "grad W: 3.909113883972168\n",
      "grad W_in: 1.1585664749145508\n",
      "[ep 20 | batch 80] loss=0.5808 |M| |W| mean=2.491e-02\n",
      "ep 20 batch 80 | loss=0.5808 | alloc=1.34GB reserv=27.78GB\n",
      "grad W_out: 0.21886558830738068\n",
      "grad W: 5.424837112426758\n",
      "grad W_in: 1.1703486442565918\n",
      "grad W_out: 0.27015620470046997\n",
      "grad W: 3.8768973350524902\n",
      "grad W_in: 1.0706062316894531\n",
      "[ep 20 | batch 82] loss=0.6081 |M| |W| mean=2.491e-02\n",
      "ep 20 batch 82 | loss=0.6081 | alloc=1.34GB reserv=27.78GB\n",
      "grad W_out: 0.11724880337715149\n",
      "grad W: 2.620845317840576\n",
      "grad W_in: 0.47671523690223694\n",
      "grad W_out: 0.28559914231300354\n",
      "grad W: 5.954767227172852\n",
      "grad W_in: 1.6172728538513184\n",
      "[ep 20 | batch 84] loss=0.5983 |M| |W| mean=2.492e-02\n",
      "ep 20 batch 84 | loss=0.5983 | alloc=1.34GB reserv=27.78GB\n",
      "grad W_out: 0.18500708043575287\n",
      "grad W: 5.719142436981201\n",
      "grad W_in: 2.7719061374664307\n",
      "grad W_out: 0.2284657210111618\n",
      "grad W: 5.408141136169434\n",
      "grad W_in: 2.208888530731201\n",
      "[ep 20 | batch 86] loss=0.5650 |M| |W| mean=2.493e-02\n",
      "ep 20 batch 86 | loss=0.5650 | alloc=1.34GB reserv=27.79GB\n",
      "grad W_out: 0.23865541815757751\n",
      "grad W: 5.836228370666504\n",
      "grad W_in: 2.517122507095337\n",
      "grad W_out: 0.25589463114738464\n",
      "grad W: 5.580481052398682\n",
      "grad W_in: 1.827955722808838\n",
      "[ep 20 | batch 88] loss=0.5773 |M| |W| mean=2.493e-02\n",
      "ep 20 batch 88 | loss=0.5773 | alloc=1.34GB reserv=27.79GB\n",
      "grad W_out: 0.25764697790145874\n",
      "grad W: 3.8359947204589844\n",
      "grad W_in: 0.668436586856842\n",
      "grad W_out: 0.19865810871124268\n",
      "grad W: 2.666999340057373\n",
      "grad W_in: 1.2759557962417603\n",
      "[ep 20 | batch 90] loss=0.5925 |M| |W| mean=2.493e-02\n",
      "ep 20 batch 90 | loss=0.5925 | alloc=1.34GB reserv=27.79GB\n",
      "grad W_out: 0.27444228529930115\n",
      "grad W: 6.819700241088867\n",
      "grad W_in: 3.653280019760132\n",
      "grad W_out: 0.30072319507598877\n",
      "grad W: 8.428208351135254\n",
      "grad W_in: 1.7067679166793823\n",
      "[ep 20 | batch 92] loss=0.5434 |M| |W| mean=2.493e-02\n",
      "ep 20 batch 92 | loss=0.5434 | alloc=1.35GB reserv=27.79GB\n",
      "grad W_out: 0.3411828875541687\n",
      "grad W: 6.819823741912842\n",
      "grad W_in: 1.8572200536727905\n",
      "Epoch 20/20 | lr=1.00e-03 | loss=0.5949 | test_acc=79.34% | dt=1178.6s\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trained_net, params \u001b[38;5;241m=\u001b[39m train_sequential_mnist(\n\u001b[1;32m      2\u001b[0m                 device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m                 hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m,\n\u001b[1;32m      4\u001b[0m                 batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m640\u001b[39m,\n\u001b[1;32m      5\u001b[0m                 eta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[1;32m      6\u001b[0m                 lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m,\n\u001b[1;32m      7\u001b[0m                 epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "trained_net, params = train_sequential_mnist(\n",
    "                device=\"cuda\",\n",
    "                hidden_dim=300,\n",
    "                batch_size=640,\n",
    "                eta=0.9,\n",
    "                lr=1e-3,\n",
    "                epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TorchOnGPUs",
   "language": "python",
   "name": "torchongpus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
