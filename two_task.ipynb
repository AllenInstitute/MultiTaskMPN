{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "374fb8f2-9b83-44ce-821b-8917a114c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "import copy \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# PyTorch Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Data Handling and Image Processing\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Style for Matplotlib\n",
    "import scienceplots\n",
    "plt.style.use('science')\n",
    "plt.style.use(['no-latex'])\n",
    "\n",
    "# Scientific Computing and Machine Learning\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.linalg import subspace_angles\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Custom Modules and Extensions\n",
    "sys.path.append(\"../netrep/\")\n",
    "sys.path.append(\"../svcca/\")\n",
    "import cca_core\n",
    "from netrep.metrics import LinearMetric\n",
    "import networks as nets  # Contains RNNs\n",
    "import net_helpers\n",
    "import mpn_tasks\n",
    "import helper\n",
    "import mpn\n",
    "\n",
    "import scienceplots\n",
    "plt.style.use('science')\n",
    "plt.style.use(['no-latex'])\n",
    "\n",
    "# Memory Optimization\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd1556c8-a4b6-434b-a60f-37035980bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 Red, 1 blue, 2 green, 3 purple, 4 orange, 5 teal, 6 gray, 7 pink, 8 yellow\n",
    "c_vals = ['#e53e3e', '#3182ce', '#38a169', '#805ad5','#dd6b20', '#319795', '#718096', '#d53f8c', '#d69e2e',]\n",
    "c_vals_l = ['#feb2b2', '#90cdf4', '#9ae6b4', '#d6bcfa', '#fbd38d', '#81e6d9', '#e2e8f0', '#fbb6ce', '#faf089',]\n",
    "c_vals_d = ['#9b2c2c', '#2c5282', '#276749', '#553c9a', '#9c4221', '#285e61', '#2d3748', '#97266d', '#975a16',]\n",
    "l_vals = ['solid', 'dashed', 'dotted', 'dashdot', '-', '--', '-.', ':', (0, (3, 1, 1, 1)), (0, (5, 10))]\n",
    "markers_vals = ['o', 'v', '^', '<', '>', '1', '2', '3', '4', 's', 'p', '*', 'h', 'H', '+', 'x', 'D', 'd', '|', '_']\n",
    "hyp_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34978bf6-67b8-41bd-a022-a7b46a320686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set seed 603\n",
      "Fixation_off: True; Task_info: True\n",
      "Rules: ['delaygo', 'delayanti']\n",
      "  Input size 8, Output size 3\n",
      "Using CUDA...\n"
     ]
    }
   ],
   "source": [
    "# Reload modules if changes have been made to them\n",
    "from importlib import reload\n",
    "\n",
    "reload(nets)\n",
    "reload(net_helpers)\n",
    "\n",
    "fixseed = False # randomize setting the seed may lead to not perfectly solved results\n",
    "seed = random.randint(1,1000) if not fixseed else 8 # random set the seed to test robustness by default\n",
    "print(f\"Set seed {seed}\")\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "hyp_dict['task_type'] = 'multitask' # int, NeuroGym, multitask\n",
    "hyp_dict['mode_for_all'] = \"random_batch\"\n",
    "hyp_dict['ruleset'] = 'delaygofamily' # low_dim, all, test\n",
    "\n",
    "accept_rules = ('fdgo', 'fdanti', 'delaygo', 'delayanti', 'reactgo', 'reactanti', \n",
    "                'delaydm1', 'delaydm2', 'dmsgo', 'dmcgo', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm')\n",
    "\n",
    "\n",
    "rules_dict = \\\n",
    "    {'all' : ['fdgo', 'reactgo', 'delaygo', 'fdanti', 'reactanti', 'delayanti',\n",
    "              'dm1', 'dm2', 'contextdm1', 'contextdm2', 'multidm',\n",
    "              'delaydm1', 'delaydm2', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm',\n",
    "              'dmsgo', 'dmsnogo', 'dmcgo', 'dmcnogo'],\n",
    "     'low_dim' : ['fdgo', 'reactgo', 'delaygo', 'fdanti', 'reactanti', 'delayanti',\n",
    "                 'delaydm1', 'delaydm2', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm',\n",
    "                 'dmsgo', 'dmsnogo', 'dmcgo', 'dmcnogo'],\n",
    "     'gofamily': ['fdgo', 'fdanti', 'reactgo', 'reactanti', 'delaygo', 'delayanti'],\n",
    "     'delaygo': ['delaygo'],\n",
    "     'delaygofamily': ['delaygo', 'delayanti'],\n",
    "     'fdgo': ['fdgo'],\n",
    "     'fdfamily': ['fdgo', 'fdanti'],\n",
    "     'reactgo': ['reactgo'],\n",
    "     'reactfamily': ['reactgo', 'reactanti'],\n",
    "     'delaydm1': ['delaydm1'],\n",
    "     'delaydmfamily': ['delaydm1', 'delaydm2'],\n",
    "     'dmsgofamily': ['dmsgo', 'dmsnogo'],\n",
    "     'dmsgo': ['dmsgo'],\n",
    "     'dmcgo': ['dmcgo'],\n",
    "     'contextdelayfamily': ['contextdelaydm1', 'contextdelaydm2'],\n",
    "    }\n",
    "    \n",
    "\n",
    "# This can either be used to set parameters OR set parameters and train\n",
    "train = True # whether or not to train the network\n",
    "verbose = True\n",
    "hyp_dict['run_mode'] = 'minimal' # minimal, debug\n",
    "hyp_dict['chosen_network'] = \"dmpn\"\n",
    "\n",
    "# suffix for saving images\n",
    "# inputadd, Wfix, WL2, hL2\n",
    "# inputrandom, Wtrain\n",
    "hyp_dict['addon_name'] = \"inputadd+Wfix+WL2+hL2\"\n",
    "\n",
    "mpn_depth = 1\n",
    "n_hidden = 100\n",
    "\n",
    "# for coding \n",
    "if hyp_dict['chosen_network'] in (\"gru\", \"vanilla\"):\n",
    "    mpn_depth = 1\n",
    "\n",
    "def current_basic_params():\n",
    "    task_params = {\n",
    "        'task_type': hyp_dict['task_type'],\n",
    "        'rules': rules_dict[hyp_dict['ruleset']],\n",
    "        'dt': 40, # ms, directly influence sequence lengths,\n",
    "        'ruleset': hyp_dict['ruleset'],\n",
    "        'n_eachring': 8, # Number of distinct possible inputs on each ring\n",
    "        'in_out_mode': 'low_dim',  # high_dim or low_dim or low_dim_pos (Robert vs. Laura's paper, resp)\n",
    "        'sigma_x': 0.01, # Laura raised to 0.1 to prevent overfitting (Robert uses 0.01)\n",
    "        'mask_type': 'cost', # 'cost', None\n",
    "        'fixate_off': True, # Second fixation signal goes on when first is off\n",
    "        'task_info': True, \n",
    "        'randomize_inputs': False,\n",
    "        'n_input': 20, # Only used if inputs are randomized,\n",
    "        'modality_diff': False,\n",
    "        'label_strength': False, \n",
    "        'long_delay': 'normal' \n",
    "    }\n",
    "\n",
    "    print(f\"Fixation_off: {task_params['fixate_off']}; Task_info: {task_params['task_info']}\")\n",
    "\n",
    "    train_params = {\n",
    "        'lr': 1e-3,\n",
    "        'n_batches': 128,\n",
    "        'batch_size': 128,\n",
    "        'gradient_clip': 10,\n",
    "        'valid_n_batch': 200,\n",
    "        'n_datasets': 100, # Number of distinct batches\n",
    "        'n_epochs_per_set': 50, # longer/shorter training\n",
    "        # 'weight_reg': 'L2',\n",
    "        # 'activity_reg': 'L2', \n",
    "        # 'reg_lambda': 1e-4,\n",
    "    }\n",
    "\n",
    "    if not train: # some \n",
    "        assert train_params['n_epochs_per_set'] == 0\n",
    "\n",
    "    net_params = {\n",
    "        'net_type': hyp_dict['chosen_network'], # mpn1, dmpn, vanilla\n",
    "        'n_neurons': [1] + [n_hidden] * mpn_depth + [1],\n",
    "        'output_bias': False, # Turn off biases for easier interpretation\n",
    "        'loss_type': 'MSE', # XE, MSE\n",
    "        'activation': 'tanh', # linear, ReLU, sigmoid, tanh, tanh_re, tukey, heaviside\n",
    "        'cuda': True,\n",
    "        'monitor_freq': 100,\n",
    "        'monitor_valid_out': True, # Whether or not to save validation output throughout training\n",
    "        'output_matrix': '',# \"\" (default); \"untrained\", or \"orthogonal\"\n",
    "        'input_layer_add': True, \n",
    "        'input_layer_add_trainable': False, # revise this is effectively to [randomize_inputs], tune this\n",
    "        'input_layer_bias': False, \n",
    "        'input_layer': \"trainable\", # for RNN only\n",
    "        \n",
    "        # for one-layer MPN, GRU or Vanilla\n",
    "        'ml_params': {\n",
    "            'bias': True, # Bias of layer\n",
    "            'mp_type': 'mult',\n",
    "            'm_update_type': 'hebb_assoc', # hebb_assoc, hebb_pre\n",
    "            'eta_type': 'scalar', # scalar, pre_vector, post_vector, matrix\n",
    "            'eta_train': False,\n",
    "            # 'eta_init': 'mirror_gaussian', #0.0,\n",
    "            'lam_type': 'scalar', # scalar, pre_vector, post_vector, matrix\n",
    "            'm_time_scale': 400, # ms, sets lambda\n",
    "            'lam_train': False,\n",
    "            'W_freeze': False, # different combination with [input_layer_add_trainable]\n",
    "        },\n",
    "\n",
    "        # Vanilla RNN params\n",
    "        'leaky': True,\n",
    "        'alpha': 0.2,\n",
    "    }\n",
    "\n",
    "    # Ensure the two options are *not* activated at the same time\n",
    "    assert not (task_params[\"randomize_inputs\"] and net_params[\"input_layer_add\"]), (\n",
    "        \"task_params['randomize_inputs'] and net_params['input_layer_add'] cannot both be True.\"\n",
    "    )\n",
    "\n",
    "    # for multiple MPN layers, assert \n",
    "    if mpn_depth > 1:\n",
    "        for mpl_idx in range(mpn_depth - 1):\n",
    "            assert f'ml_params{mpl_idx}' in net_params.keys()\n",
    "\n",
    "    # actually I don't think it is needed\n",
    "    # putting here to warn the parameter checking every time \n",
    "    # when switching network\n",
    "    if hyp_dict['chosen_network'] in (\"gru\", \"vanilla\"):\n",
    "        assert f'ml_params' in net_params.keys()\n",
    "\n",
    "    return task_params, train_params, net_params\n",
    "\n",
    "task_params, train_params, net_params = current_basic_params()\n",
    "\n",
    "shift_index = 1 if not task_params['fixate_off'] else 0\n",
    "\n",
    "if hyp_dict['task_type'] in ('multitask',):\n",
    "    task_params, train_params, net_params = mpn_tasks.convert_and_init_multitask_params(\n",
    "        (task_params, train_params, net_params)\n",
    "    )\n",
    "\n",
    "    net_params['prefs'] = mpn_tasks.get_prefs(task_params['hp'])\n",
    "\n",
    "    print('Rules: {}'.format(task_params['rules']))\n",
    "    print('  Input size {}, Output size {}'.format(\n",
    "        task_params['n_input'], task_params['n_output'],\n",
    "    ))\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "if net_params['cuda']:\n",
    "    print('Using CUDA...')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('Using CPU...')\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# how many epoch each dataset will be trained on\n",
    "epoch_multiply = train_params[\"n_epochs_per_set\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a341b36-dc7a-42b0-bffb-cb17d380041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_dict[\"mess_with_training\"] = False\n",
    "\n",
    "if hyp_dict['mess_with_training']:\n",
    "    hyp_dict['addon_name'] += \"messwithtraining\"\n",
    "\n",
    "params = task_params, train_params, net_params\n",
    "\n",
    "if net_params['net_type'] == 'mpn1':\n",
    "    netFunction = mpn.MultiPlasticNet\n",
    "elif net_params['net_type'] == 'dmpn':\n",
    "    netFunction = mpn.DeepMultiPlasticNet\n",
    "elif net_params['net_type'] == 'vanilla':\n",
    "    netFunction = nets.VanillaRNN\n",
    "elif net_params['net_type'] == 'gru':\n",
    "    netFunction = nets.GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07e4fe48-2af6-4741-8d42-01803ba0abf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Align ['delaygo', 'delayanti'] With Same Time\n",
      "rng reset with seed 1077\n",
      "rng reset with seed 1077\n",
      "rng reset with seed 1077\n",
      "rng reset with seed 1077\n",
      "delaygo\n",
      "delayanti\n"
     ]
    }
   ],
   "source": [
    "test_n_batch = train_params[\"valid_n_batch\"]\n",
    "color_by = \"stim\" # or \"resp\" \n",
    "\n",
    "task_random_fix = True\n",
    "if task_random_fix:\n",
    "    print(f\"Align {task_params['rules']} With Same Time\")\n",
    "\n",
    "if task_params['task_type'] in ('multitask',): # Test batch consists of all the rules\n",
    "    task_params['hp']['batch_size_train'] = test_n_batch\n",
    "    # using homogeneous cutting off\n",
    "    test_mode_for_all = \"random\"\n",
    "    # ZIHAN\n",
    "    # generate test data using \"random\"\n",
    "    test_data, test_trials_extra = mpn_tasks.generate_trials_wrap(task_params, test_n_batch, \\\n",
    "                rules=task_params['rules'], mode_input=test_mode_for_all, fix=task_random_fix\n",
    "    )\n",
    "    _, test_trials, test_rule_idxs = test_trials_extra\n",
    "\n",
    "    task_params_attractor = copy.deepcopy(task_params)\n",
    "    task_params_attractor[\"long_delay\"] = \"long\"\n",
    "    test_data_attractor, test_trials_extra_attractor = mpn_tasks.generate_trials_wrap(task_params_attractor, test_n_batch, \\\n",
    "                                                                                      rules=task_params_attractor['rules'], \\\n",
    "                                                                                      mode_input=test_mode_for_all, fix=task_random_fix\n",
    "    )\n",
    "    _, test_trials_attractor, test_rule_idxs_attractor = test_trials_extra_attractor\n",
    "\n",
    "    task_params['dataset_name'] = 'multitask'\n",
    "\n",
    "    if task_params['in_out_mode'] in ('low_dim_pos',):\n",
    "        output_dim_labels = ('Fixate', 'Cos', '-Cos', 'Sin', '-Sin')\n",
    "    elif task_params['in_out_mode'] in ('low_dim',):\n",
    "        output_dim_labels = ('Fixate', 'Cos', 'Sin')\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def generate_response_stimulus(task_params, test_trials): \n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        labels_resp, labels_stim = [], []\n",
    "        for rule_idx, rule in enumerate(task_params['rules']):\n",
    "            print(rule)\n",
    "            if rule in accept_rules:\n",
    "                if hyp_dict['ruleset'] in ('dmsgo', 'dmcgo'):\n",
    "                    labels.append(test_trials[rule_idx].meta['matches'])\n",
    "                else:\n",
    "                    labels_resp.append(test_trials[rule_idx].meta['resp1'])\n",
    "                    labels_stim.append(test_trials[rule_idx].meta['stim1']) \n",
    "    \n",
    "            else:\n",
    "                raise NotImplementedError()\n",
    "                \n",
    "        labels_resp = np.concatenate(labels_resp, axis=0).reshape(-1,1)\n",
    "        labels_stim = np.concatenate(labels_stim, axis=0).reshape(-1,1)\n",
    "\n",
    "        return labels_resp, labels_stim\n",
    "\n",
    "    labels_resp, labels_stim = generate_response_stimulus(task_params, test_trials)\n",
    "\n",
    "labels = labels_stim if color_by == \"stim\" else labels_resp\n",
    "    \n",
    "test_input, test_output, test_mask = test_data\n",
    "test_input_attractor, test_output_attractor, test_mask_attractor = test_data_attractor\n",
    "\n",
    "permutation = np.random.permutation(test_input.shape[0])\n",
    "test_input = test_input[permutation]\n",
    "test_output = test_output[permutation]\n",
    "test_mask = test_mask[permutation]\n",
    "labels = labels[permutation]\n",
    "\n",
    "test_input_np = test_input.detach().cpu().numpy()\n",
    "test_output_np = test_output.detach().cpu().numpy()\n",
    "\n",
    "# Total number of batches, might be different than test_n_batch\n",
    "# this should be the same regardless of variety of test_input\n",
    "n_batch_all = test_input_np.shape[0] \n",
    "\n",
    "def find_task(task_params, test_input_np, shift_index):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    test_task = [] # which task\n",
    "    for batch_idx in range(test_input_np.shape[0]):\n",
    "        \n",
    "        if task_params[\"randomize_inputs\"]: \n",
    "            test_input_np_ = test_input_np @ np.linalg.pinv(task_params[\"randomize_matrix\"])\n",
    "        else: \n",
    "            test_input_np_ = test_input_np\n",
    "            \n",
    "        task_label = test_input_np_[batch_idx, 0, 6-shift_index:]\n",
    "        # task_label_index = np.where(task_label == 1)[0][0]\n",
    "        \n",
    "        # tol = 1e-3      \n",
    "        # mask = np.isclose(task_label, 1, atol=tol)\n",
    "        task_label = np.asarray(task_label)       \n",
    "        dist = np.abs(task_label - 1)     \n",
    "        mask = dist == dist.min() \n",
    "        \n",
    "        indices = np.where(mask)[0]\n",
    "        \n",
    "        if indices.size:                \n",
    "            task_label_index = indices[0]   \n",
    "        else:\n",
    "            raise ValueError(\"No entry close enough to 1 found\")\n",
    "            \n",
    "        test_task.append(task_label_index)\n",
    "\n",
    "    return test_task  \n",
    "\n",
    "test_task = find_task(task_params, test_input_np, shift_index)\n",
    "test_task_attractor = find_task(task_params_attractor, test_input_attractor.detach().cpu().numpy(), shift_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78b44e9-b6e3-4c1c-a5c7-5a608a6d090b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 100, 100, 3]\n",
      "MultiPlastic Net:\n",
      "  output neurons: 3\n",
      "  Act: tanh\n",
      "\n",
      "Input Layer Frozen\n",
      "=== Layer Universal Setup ===\n",
      "  MP Layer1 parameters:\n",
      "    n_neurons - input: 100, output: 100\n",
      "    M matrix parameters:    update bounds - Max mult: 1.0, Min mult: -1.0\n",
      "      type: mult // Update - type: hebb_assoc // Act fn: linear\n",
      "      Eta: scalar (fixed) // Lambda: scalar (fixed) // Lambda_max: 0.90 (tau: 4.0e+02)\n",
      "How about Test Data at dataset 0\n",
      "Train parameters:\n",
      "  Loss: MSE // LR: 1.00e-03 // Optim: adam\n",
      "  Grad type: backprop // Gradient clip: 1.0e+01\n",
      "Weight reg: None\n",
      "Activity reg: None\n",
      "Iter: 0, LR: 1.000e-03 - train_loss:3.836e-01, rounded train_acc:0.110, valid_loss:3.506e-01, rounded valid_acc:0.159\n",
      "How about Test Data at dataset 1\n",
      "Iter: 100, LR: 1.000e-03 - train_loss:4.076e-02, rounded train_acc:0.990, valid_loss:1.883e-01, rounded valid_acc:0.566\n",
      "How about Test Data at dataset 2\n",
      "Iter: 200, LR: 1.000e-03 - train_loss:1.484e-02, rounded train_acc:0.999, valid_loss:6.986e-02, rounded valid_acc:0.770\n",
      "How about Test Data at dataset 4\n",
      "Iter: 300, LR: 1.000e-03 - train_loss:9.483e-03, rounded train_acc:1.000, valid_loss:4.315e-02, rounded valid_acc:0.833\n",
      "Iter: 400, LR: 1.000e-03 - train_loss:5.531e-03, rounded train_acc:1.000, valid_loss:5.367e-02, rounded valid_acc:0.736\n",
      "How about Test Data at dataset 8\n",
      "Iter: 500, LR: 1.000e-03 - train_loss:6.459e-03, rounded train_acc:1.000, valid_loss:2.615e-02, rounded valid_acc:0.886\n",
      "Iter: 600, LR: 1.000e-03 - train_loss:3.610e-03, rounded train_acc:1.000, valid_loss:3.134e-02, rounded valid_acc:0.862\n",
      "Iter: 700, LR: 1.000e-03 - train_loss:5.612e-03, rounded train_acc:0.999, valid_loss:1.010e-02, rounded valid_acc:0.985\n",
      "Iter: 800, LR: 1.000e-03 - train_loss:2.768e-03, rounded train_acc:1.000, valid_loss:1.002e-02, rounded valid_acc:0.983\n",
      "How about Test Data at dataset 16\n",
      "Iter: 900, LR: 1.000e-03 - train_loss:2.537e-03, rounded train_acc:1.000, valid_loss:5.224e-03, rounded valid_acc:0.998\n",
      "Iter: 1000, LR: 1.000e-03 - train_loss:2.542e-03, rounded train_acc:1.000, valid_loss:4.158e-03, rounded valid_acc:0.996\n",
      "Iter: 1100, LR: 1.000e-03 - train_loss:2.253e-03, rounded train_acc:1.000, valid_loss:3.489e-03, rounded valid_acc:1.000\n",
      "Iter: 1200, LR: 1.000e-03 - train_loss:2.211e-03, rounded train_acc:1.000, valid_loss:3.346e-03, rounded valid_acc:0.994\n",
      "Iter: 1300, LR: 1.000e-03 - train_loss:1.805e-03, rounded train_acc:1.000, valid_loss:3.601e-03, rounded valid_acc:0.996\n",
      "Iter: 1400, LR: 1.000e-03 - train_loss:2.752e-03, rounded train_acc:1.000, valid_loss:4.958e-03, rounded valid_acc:1.000\n",
      "Iter: 1500, LR: 1.000e-03 - train_loss:1.465e-03, rounded train_acc:1.000, valid_loss:3.385e-03, rounded valid_acc:0.998\n",
      "Iter: 1600, LR: 1.000e-03 - train_loss:1.483e-03, rounded train_acc:1.000, valid_loss:2.809e-03, rounded valid_acc:0.997\n",
      "How about Test Data at dataset 32\n",
      "Iter: 1700, LR: 1.000e-03 - train_loss:1.242e-03, rounded train_acc:1.000, valid_loss:2.731e-03, rounded valid_acc:0.997\n",
      "Iter: 1800, LR: 1.000e-03 - train_loss:1.162e-03, rounded train_acc:1.000, valid_loss:3.097e-03, rounded valid_acc:0.995\n",
      "Iter: 1900, LR: 1.000e-03 - train_loss:1.302e-03, rounded train_acc:1.000, valid_loss:4.277e-03, rounded valid_acc:0.996\n",
      "Iter: 2000, LR: 1.000e-03 - train_loss:1.047e-03, rounded train_acc:1.000, valid_loss:3.196e-03, rounded valid_acc:0.998\n",
      "Iter: 2100, LR: 1.000e-03 - train_loss:1.235e-03, rounded train_acc:1.000, valid_loss:2.320e-03, rounded valid_acc:1.000\n",
      "Iter: 2200, LR: 1.000e-03 - train_loss:9.655e-04, rounded train_acc:1.000, valid_loss:2.703e-03, rounded valid_acc:0.996\n",
      "Iter: 2300, LR: 1.000e-03 - train_loss:1.022e-03, rounded train_acc:1.000, valid_loss:2.460e-03, rounded valid_acc:0.997\n",
      "Iter: 2400, LR: 1.000e-03 - train_loss:1.007e-03, rounded train_acc:1.000, valid_loss:2.276e-03, rounded valid_acc:0.995\n",
      "Iter: 2500, LR: 1.000e-03 - train_loss:8.336e-04, rounded train_acc:1.000, valid_loss:2.275e-03, rounded valid_acc:0.995\n",
      "Iter: 2600, LR: 1.000e-03 - train_loss:3.036e-03, rounded train_acc:0.979, valid_loss:3.782e-03, rounded valid_acc:0.991\n",
      "Iter: 2700, LR: 1.000e-03 - train_loss:8.705e-04, rounded train_acc:1.000, valid_loss:2.170e-03, rounded valid_acc:0.997\n",
      "Iter: 2800, LR: 1.000e-03 - train_loss:8.280e-04, rounded train_acc:1.000, valid_loss:2.495e-03, rounded valid_acc:1.000\n",
      "Iter: 2900, LR: 1.000e-03 - train_loss:6.313e-04, rounded train_acc:1.000, valid_loss:2.544e-03, rounded valid_acc:1.000\n",
      "Iter: 3000, LR: 1.000e-03 - train_loss:7.306e-04, rounded train_acc:1.000, valid_loss:2.229e-03, rounded valid_acc:0.998\n",
      "Iter: 3100, LR: 1.000e-03 - train_loss:7.575e-04, rounded train_acc:1.000, valid_loss:4.336e-03, rounded valid_acc:0.987\n",
      "Iter: 3200, LR: 1.000e-03 - train_loss:8.106e-04, rounded train_acc:1.000, valid_loss:1.965e-03, rounded valid_acc:1.000\n",
      "How about Test Data at dataset 64\n"
     ]
    }
   ],
   "source": [
    "# we use net at different training stage on the same test_input\n",
    "net, _, (counter_lst, netout_lst, db_lst, Winput_lst, Winputbias_lst,\\\n",
    "         Woutput_lst, Wall_lst, marker_lst, loss_lst, acc_lst) = net_helpers.train_network(params, device=device, verbose=verbose,\\\n",
    "                                                                                           train=train, hyp_dict=hyp_dict,\\\n",
    "                                                                                           netFunction=netFunction,\\\n",
    "                                                                                           test_input=[test_input, test_input_attractor])\n",
    "counter_lst = [x * epoch_multiply + 1 for x in counter_lst] # avoid log plot issue    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07eee03-d879-44bc-89ef-60f11bdf721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyp_dict['chosen_network'] == \"dmpn\":\n",
    "    if net_params[\"input_layer_add\"]:\n",
    "        fignorm, axsnorm = plt.subplots(1,1,figsize=(4,4))\n",
    "        axsnorm.plot(counter_lst, [np.linalg.norm(Winput_matrix) for Winput_matrix in Winput_lst], \"-o\")\n",
    "        axsnorm.set_xscale(\"log\")\n",
    "        axsnorm.set_ylabel(\"Frobenius Norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f0805a-0938-48e5-976d-3682d45b4448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check, if W_freeze, then the recorded W matrix for the modulation layer should not be changed\n",
    "if net_params[\"ml_params\"][\"W_freeze\"]: \n",
    "    assert np.allclose(Wall_lst[-1][0], Wall_lst[0][0])\n",
    "\n",
    "if net_params[\"input_layer_bias\"]: \n",
    "    assert net_params[\"input_layer_add\"] is True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47256480-5b47-496e-887e-b8de35dcc8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    fig, ax = plt.subplots(1,1,figsize=(3,3))\n",
    "    ax.plot(net.hist['iters_monitor'][1:], net.hist['train_acc'][1:], color=c_vals[0], label='Full train accuracy')\n",
    "    ax.plot(net.hist['iters_monitor'][1:], net.hist['valid_acc'][1:], color=c_vals[1], label='Full valid accuracy')\n",
    "    if net.weight_reg is not None:\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['train_loss_output_label'], color=c_vals_l[0], zorder=-1, label='Output label')\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['train_loss_reg_term'], color=c_vals_l[0], zorder=-1, label='Reg term', linestyle='dashed')\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['valid_loss_output_label'], color=c_vals_l[1], zorder=-1, label='Output valid label')\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['valid_loss_reg_term'], color=c_vals_l[1], zorder=-1, label='Reg valid term', linestyle='dashed')\n",
    "    \n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    # ax.set_ylabel('Loss ({})'.format(net.loss_type))\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_xlabel('# Batches')\n",
    "    plt.savefig(f\"./twotasks/loss_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "    \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e544aca8-271f-49d3-9ed3-ab7023f23600",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    net_helpers.net_eta_lambda_analysis(net, net_params, hyp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c10ab17-bab6-4228-9096-af042e9ac385",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_finalstage = False\n",
    "if use_finalstage:\n",
    "    # plotting output in the validation set\n",
    "    net_out, db = net.iterate_sequence_batch(test_input, run_mode='track_states')\n",
    "    W_output = net.W_output.detach().cpu().numpy()\n",
    "\n",
    "    W_all_ = []\n",
    "    for i in range(len(net.mp_layers)):\n",
    "        W_all_.append(net.mp_layers[i].W.detach().cpu().numpy())\n",
    "    W_ = W_all_[0]\n",
    "    \n",
    "else:\n",
    "    ind = len(marker_lst)-1 \n",
    "    # ind = 0\n",
    "    network_at_percent = (marker_lst[ind]+1)/train_params['n_datasets']*100\n",
    "    print(f\"Using network at {network_at_percent}%\")\n",
    "    # by default using the first test_input \n",
    "    net_out = netout_lst[0][ind]\n",
    "    db = db_lst[0][ind]\n",
    "    W_output = Woutput_lst[ind]\n",
    "    W_ = Wall_lst[ind][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8feb8c0-4029-49b1-b6e9-a5fd01ab5efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_input_output(test_input_np, net_out, test_output_np, test_task, tag=\"\"):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    test_input_np = helper.to_ndarray(test_input_np)\n",
    "    net_out = helper.to_ndarray(net_out)\n",
    "    test_output_np = helper.to_ndarray(test_output_np)\n",
    "    \n",
    "    fig_all, axs_all = plt.subplots(5, 2, figsize=(4*2, 5*2))\n",
    "    \n",
    "    if test_output_np.shape[-1] == 1:\n",
    "        for batch_idx, ax in enumerate(axs):\n",
    "            ax.plot(net_out[batch_idx, :, 0], color=c_vals[batch_idx])\n",
    "            ax.plot(test_output_np[batch_idx, :, 0], color=c_vals_l[batch_idx])\n",
    "    \n",
    "    else:\n",
    "        for batch_idx in range(5):\n",
    "            for out_idx in range(test_output_np.shape[-1]):\n",
    "                axs_all[batch_idx,0].plot(net_out[batch_idx, :, out_idx], color=c_vals[out_idx], label=out_idx)\n",
    "                axs_all[batch_idx,0].plot(test_output_np[batch_idx, :, out_idx], color=c_vals_l[out_idx], linewidth=5, alpha=0.5)\n",
    "                axs_all[batch_idx,0].set_title(f\"{task_params['rules'][test_task[batch_idx]]}\")\n",
    "                axs_all[batch_idx,0].legend()\n",
    "    \n",
    "            input_batch = test_input_np[batch_idx,:,:]\n",
    "            if task_params[\"randomize_inputs\"]: \n",
    "                input_batch = input_batch @ np.linalg.pinv(task_params[\"randomize_matrix\"])\n",
    "            for inp_idx in range(input_batch.shape[-1]):\n",
    "                axs_all[batch_idx,1].plot(input_batch[:,inp_idx], color=c_vals[inp_idx], label=inp_idx)\n",
    "                axs_all[batch_idx,1].set_title(f\"{task_params['rules'][test_task[batch_idx]]}\")\n",
    "                axs_all[batch_idx,1].legend()\n",
    "\n",
    "    for ax in axs_all.flatten(): \n",
    "        ax.set_ylim([-2, 2])\n",
    "    fig_all.tight_layout()\n",
    "    fig_all.savefig(f\"./twotasks/lowD_{hyp_dict['ruleset']}_{hyp_dict['chosen_network']}_seed{seed}_{hyp_dict['addon_name']}_{tag}.png\", dpi=300)\n",
    "\n",
    "plot_input_output(test_input_np, net_out, test_output_np, test_task, tag=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891afd4c-f9dc-4341-8b42-c79f05af55c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # analyze about the attractor \n",
    "# task_params_attractor = copy.deepcopy(task_params)\n",
    "# task_params_attractor[\"long_delay\"] = \"long\" \n",
    "\n",
    "# test_data_attractor, test_trials_extra_attractor = mpn_tasks.generate_trials_wrap(task_params_attractor, test_n_batch, \\\n",
    "#             rules=task_params_attractor['rules'], mode_input=test_mode_for_all, fix=task_random_fix\n",
    "# )\n",
    "# test_input_attractor, test_output_attractor, test_mask_attractor = test_data_attractor \n",
    "# _, test_trials_attractor, test_rule_idxs_attractor = test_trials_extra\n",
    "# print(test_input_attractor.shape)\n",
    "\n",
    "# # because the sequence is too long (for the long delay period)\n",
    "# batch_cutoff = [int(test_input_attractor.shape[0] * (i + 1) / 10) for i in range(10)] \n",
    "# batch_cutoff.insert(0, 0)\n",
    "\n",
    "# net_out_attractor_all, net_hidden_attractor_all, db_attractor_all = [], [], [] \n",
    "\n",
    "# for net_ in net_lst: \n",
    "#     net_out_attractor, net_hidden_attractor, db_attractor_lst = [], [], []\n",
    "#     for ct in range(0, len(batch_cutoff)-1):\n",
    "#         net_out_attractor_, net_hidden_attractor_, db_attractor_ = net_.iterate_sequence_batch(test_input_attractor[batch_cutoff[ct]:batch_cutoff[ct+1],:,:], run_mode='track_states')\n",
    "#         net_out_attractor.append(net_out_attractor_)\n",
    "#         net_hidden_attractor.append(net_hidden_attractor_)\n",
    "#         db_attractor_lst.append(db_attractor_)\n",
    "    \n",
    "#     db_attractor = {} \n",
    "#     for key in db_attractor_lst[0].keys(): \n",
    "#         entry_lst = [db_att[key] for db_att in db_attractor_lst]\n",
    "#         db_attractor[key] = torch.cat(entry_lst, dim=0)\n",
    "\n",
    "#     net_out_attractor_all.append(torch.cat(net_out_attractor, dim=0))\n",
    "#     net_hidden_attractor_all.append(torch.cat(net_hidden_attractor, dim=0))\n",
    "#     db_attractor_all.append(db_attractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9a59c2-d2f4-429c-a82c-11a25703e9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, test_trials_attractor, test_rule_idxs_attractor = test_trials_extra_attractor\n",
    "\n",
    "# test_input_attractor_np = test_input_attractor.detach().cpu().numpy()\n",
    "# test_output_attractor_np = test_output_attractor.detach().cpu().numpy()\n",
    "# test_task_attractor = find_task(task_params_attractor, test_input_attractor_np, shift_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a24480-1cee-46ca-9210-6fb310457468",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_input_output(test_input_attractor, netout_lst[1][ind], test_output_attractor, test_task_attractor, tag=\"long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b78d25-9681-4c92-9fff-0cf68f6d1b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_resp, labels_stim = generate_response_stimulus(task_params_attractor, test_trials_attractor)\n",
    "labels_attractor = labels_stim if color_by == \"stim\" else labels_resp\n",
    "\n",
    "label_task_comb_attractor = []\n",
    "for i in range(len(labels)):\n",
    "    label_task_comb_attractor.append([labels[i][0], test_task[i]])\n",
    "label_task_comb_attractor = np.array(label_task_comb_attractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c23ea03-ed43-4b9c-a4ca-b922020fa4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_similarity(Ms_orig, hs, net_params, label_task_comb, checktime, compare=\"modulation\"): \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    inverse_modulation_ss_dt = []\n",
    "    inverse_modulation_sr_dt = []\n",
    "    inverse_modulation_st_ds = [[], []]\n",
    "    modulation_save = [[],[]]\n",
    "\n",
    "    # same stimulus (effectively anti-response), different task\n",
    "    for k in range(8):\n",
    "        ind1 = [i for i, lst in enumerate(label_task_comb) if np.array_equal(lst, [k, 0])]\n",
    "        ind2 = [i for i, lst in enumerate(label_task_comb) if np.array_equal(lst, [k, 1])]\n",
    "\n",
    "        if net_params[\"input_layer_add\"]:\n",
    "            win = net.W_initial_linear.weight.data.detach().cpu().numpy()\n",
    "        else: \n",
    "            win = None \n",
    "        \n",
    "        if compare == \"modulation\": \n",
    "            Ms1_change_stimulus = ((Ms_orig[ind1[0],checktime,:,:]) @ win)[:,0].flatten() if win is not None else (Ms_orig[ind1[0],checktime,:,:])[:,0].flatten()\n",
    "            Ms2_change_stimulus = ((Ms_orig[ind2[0],checktime,:,:]) @ win)[:,0].flatten() if win is not None else (Ms_orig[ind2[0],checktime,:,:])[:,0].flatten()\n",
    "        elif compare == \"hidden\": \n",
    "            Ms1_change_stimulus = hs[ind1[0],checktime,:].flatten()\n",
    "            Ms2_change_stimulus = hs[ind2[0],checktime,:].flatten()\n",
    "        \n",
    "        inverse_modulation_ss_dt.append(1 - cosine(Ms1_change_stimulus, Ms2_change_stimulus))\n",
    "\n",
    "        modulation_save[0].append(Ms1_change_stimulus)\n",
    "        modulation_save[1].append(Ms2_change_stimulus)\n",
    "\n",
    "    # same response, different task \n",
    "    for k in range(8):\n",
    "        ind1 = [i for i, lst in enumerate(label_task_comb) if np.array_equal(lst, [k, 0])]\n",
    "        ind2 = [i for i, lst in enumerate(label_task_comb) if np.array_equal(lst, [(k + 4) % 8, 1])]\n",
    "\n",
    "        if compare == \"modulation\": \n",
    "            Ms1_change_stimulus = ((Ms_orig[ind1[0],checktime,:,:]) @ win)[:,0].flatten() if win is not None else ((Ms_orig[ind1[0],checktime,:,:]))[:,0].flatten()\n",
    "            Ms2_change_stimulus = ((Ms_orig[ind2[0],checktime,:,:]) @ win)[:,0].flatten() if win is not None else ((Ms_orig[ind2[0],checktime,:,:]))[:,0].flatten()\n",
    "        elif compare == \"hidden\": \n",
    "            Ms1_change_stimulus = hs[ind1[0],checktime,:].flatten()\n",
    "            Ms2_change_stimulus = hs[ind2[0],checktime,:].flatten()\n",
    "        \n",
    "        inverse_modulation_sr_dt.append(1 - cosine(Ms1_change_stimulus, Ms2_change_stimulus))\n",
    "\n",
    "    # same task, different stimulus \n",
    "    modulation_matrices = [\n",
    "        np.full((len(modulation_save[0]), len(modulation_save[0])), np.nan), \n",
    "        np.full((len(modulation_save[0]), len(modulation_save[0])), np.nan)\n",
    "    ]\n",
    "    for i in range(len(modulation_save[0])):\n",
    "        for j in range(i+1, len(modulation_save[0])):\n",
    "            modulation_matrices[0][i,j] = 1 - cosine(modulation_save[0][i], modulation_save[0][j]) \n",
    "            modulation_matrices[1][i,j] = 1 - cosine(modulation_save[1][i], modulation_save[1][j])\n",
    "\n",
    "    result = [[np.mean(inverse_modulation_ss_dt), np.std(inverse_modulation_ss_dt)], \\\n",
    "                           [np.mean(inverse_modulation_sr_dt), np.std(inverse_modulation_sr_dt)], \\\n",
    "                           helper.sample_upper_means(modulation_matrices[0], k=8, n_iter=10), \\\n",
    "                           helper.sample_upper_means(modulation_matrices[1], k=8, n_iter=10)]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1f92a0-c04f-4a7a-8b08-e753efb1b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here db is selected based on learning stage selection \n",
    "\n",
    "layer_index = 0 # 1 layer MPN \n",
    "if net_params[\"input_layer_add\"]:\n",
    "    layer_index += 1 \n",
    "    \n",
    "def modulation_extraction(test_input, db, layer_index):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    max_seq_len = test_input.shape[1] \n",
    "    \n",
    "    Ms = np.concatenate((\n",
    "        db[f'M{layer_index}'].detach().cpu().numpy().reshape(n_batch_all, max_seq_len, -1),\n",
    "    ), axis=-1)\n",
    "\n",
    "    Ms_orig = np.concatenate((\n",
    "        db[f'M{layer_index}'].detach().cpu().numpy(),\n",
    "    ), axis=-1)\n",
    "\n",
    "    bs = np.concatenate((\n",
    "        db[f'b{layer_index}'].detach().cpu().numpy(),\n",
    "    ), axis=-1) \n",
    "\n",
    "    hs = np.concatenate((\n",
    "        db[f'hidden{layer_index}'].detach().cpu().numpy().reshape(n_batch_all, max_seq_len, -1),\n",
    "    ), axis=-1)\n",
    "\n",
    "    return Ms, Ms_orig, hs, bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a350fae9-ae28-4341-a5e9-a6d98bd227f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for db_attractor in db_lst[1]:\n",
    "    _, M_attractor, h_attractor, _ = modulation_extraction(test_input_attractor, db_attractor, layer_index)\n",
    "    \n",
    "    checktime_sample = test_input_attractor[0,:,0].detach().cpu()\n",
    "\n",
    "    # to handle noise    \n",
    "    mask = checktime_sample < 0.5                          \n",
    "    idx = torch.nonzero(mask, as_tuple=False) \n",
    "    checktime_attractor = idx[0].item()  \n",
    "    \n",
    "    result_attractor = analyze_similarity(M_attractor, h_attractor, net_params, label_task_comb_attractor, checktime=checktime_attractor, compare=\"hidden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9efbd2f-a48d-4f8e-ba55-b0b102463546",
   "metadata": {},
   "outputs": [],
   "source": [
    "if net_params[\"net_type\"] in (\"dmpn\", ):\n",
    "    if mpn_depth == 1:\n",
    "        Ms, Ms_orig, hs, bs = modulation_extraction(test_input, db, layer_index)\n",
    "    else:\n",
    "        modulations, hiddens = [], []\n",
    "        for i in range(mpn_depth):\n",
    "            max_seq_len = test_input.shape[1]\n",
    "            modulations.append(db[f'M{i}'].detach().cpu().numpy().reshape(n_batch_all, max_seq_len, -1))\n",
    "            hiddens.append(db[f'hidden{i}'].detach().cpu().numpy().reshape(n_batch_all, max_seq_len, -1),)\n",
    "\n",
    "        Ms = modulations[0]\n",
    "        hs = hiddens[0]\n",
    "        \n",
    "elif net_params[\"net_type\"] in (\"vanilla\", \"gru\"):\n",
    "    hs = db['hidden'].detach().cpu().numpy()\n",
    "\n",
    "pca_type = 'full' # full, cell_types\n",
    "pca_target_lst = ['hs', 'Ms'] # hs, 'Ms' \n",
    "if net_params[\"net_type\"] in (\"vanilla\", \"gru\"):\n",
    "    pca_target_lst = ['hs'] # if not dmpn, no M information effectively\n",
    "\n",
    "# using recorded information\n",
    "recordkyle_all, recordkyle_nameall = [], []\n",
    "for test_subtrial in test_trials:\n",
    "    metaepoch = test_subtrial.epochs\n",
    "    periodname = list(metaepoch.keys())\n",
    "    recordkyle, recordkyle_name = [], []\n",
    "    for keyiter in range(len(periodname)):\n",
    "        try:\n",
    "            recordkyle_name.append(periodname[keyiter])\n",
    "            if test_mode_for_all == \"random\":\n",
    "                recordkyle.append(metaepoch[periodname[keyiter]][1])\n",
    "            elif test_mode_for_all == \"random_batch\":\n",
    "                recordkyle.append(list(metaepoch[periodname[keyiter]][1]))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    if test_mode_for_all in (\"random\",):\n",
    "        fillrecordkyle = []\n",
    "        for timestamp in recordkyle:\n",
    "            fillrecordkyle.append([timestamp for _ in range(hs.shape[0])])\n",
    "        recordkyle = fillrecordkyle\n",
    "\n",
    "    recordkyle.insert(0, [0 for _ in range(len(recordkyle[1]))])\n",
    "    recordkyle = np.array(recordkyle).T.tolist()\n",
    "    recordkyle_all.extend(recordkyle)\n",
    "    recordkyle_nameall.append(recordkyle_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f41fdc5-88a2-459a-8ea5-8177aa1a9393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sep 30th\n",
    "# This part of code should be adaptive for multitask, which may have different breaks and periods for each task\n",
    "unique_lists = set(tuple(lst) for lst in recordkyle_all)\n",
    "# here select task specific information\n",
    "# which maximally should have length of number of mpn_tasks\n",
    "unique_recordkyle_all = [list(lst) for lst in unique_lists]\n",
    "if not task_random_fix:\n",
    "    assert len(unique_recordkyle_all) >= len(rules_dict[hyp_dict['ruleset']])\n",
    "else:\n",
    "    print(\"Test DataSet Random Seed Is Fixed\")\n",
    "\n",
    "all_session_breakdown = []\n",
    "for task_specific_time in unique_recordkyle_all:\n",
    "    session_breakdown = []\n",
    "    for sindex in range(0,len(task_specific_time)-1):\n",
    "        # all sessions should be the same for each task\n",
    "        # but different across mpn_tasks\n",
    "        # though the time of when response period starts might be similar across\n",
    "        session_breakdown.append([task_specific_time[sindex], task_specific_time[sindex+1]]) \n",
    "    session_breakdown.append([task_specific_time[0], task_specific_time[-1]])\n",
    "    all_session_breakdown.append(session_breakdown)\n",
    "\n",
    "# break down time\n",
    "all_breaks = []\n",
    "for session_breakdown in all_session_breakdown:\n",
    "    breaks = [cut[1] for cut in session_breakdown[:-1]]\n",
    "    print(f\"Task {all_session_breakdown.index(session_breakdown)}; breaks: {breaks}\")\n",
    "    all_breaks.append(breaks)\n",
    "\n",
    "# for delay-task\n",
    "assert len(all_breaks)\n",
    "response_start = all_breaks[0][-2]\n",
    "stimulus_start = all_breaks[0][0]\n",
    "stimulus_end = all_breaks[0][1]\n",
    "print(f\"response_start: {response_start}\")\n",
    "print(f\"stimulus_start: {stimulus_start}\")\n",
    "print(f\"stimulus_end: {stimulus_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5adc633-8a73-4e8b-8733-b4ce98482ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages_num = len(Wall_lst) # how many recorded neurons in total\n",
    "break_info = all_breaks[0]\n",
    "\n",
    "input_nums = Ms_orig.shape[-1]\n",
    "batch_nums = Ms_orig.shape[0]\n",
    "neuron_nums = Ms_orig.shape[2]\n",
    "colors = helper.generate_rainbow_colors(Ms_orig.shape[2])\n",
    "\n",
    "def generate_random_orthonormal_matrix(N, num_columns=3):\n",
    "    \"\"\"\n",
    "    generates an N x num_columns random matrix with orthonormal columns.\n",
    "    \"\"\"\n",
    "    random_matrix = np.random.randn(N, num_columns)    \n",
    "    Q, R = np.linalg.qr(random_matrix)    \n",
    "    return Q[:, :num_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49da3c53-fdcf-46d5-a9ec-0ba653c8ec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_task_comb = []\n",
    "for i in range(len(labels)):\n",
    "    label_task_comb.append([labels[i][0], test_task[i]])\n",
    "label_task_comb_arr = np.array(label_task_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e212e03-6fb4-439b-9fd6-5aefd8a09a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check from equation 2-7\n",
    "def plot_trajectory_by_index(label_index, stage_iter, verbose=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    W_ = Wall_lst[stage_iter][0]\n",
    "    W_input = Winput_lst[stage_iter] if net_params[\"input_layer_add\"] else np.ones((test_input.shape[-1], n_hidden))\n",
    "    W_inputbias = Winputbias_lst[stage_iter] if net_params[\"input_layer_bias\"] else np.zeros((W_input.shape[0],1))\n",
    "    W_output = Woutput_lst[stage_iter]\n",
    "    _, Ms_orig, _, bs = modulation_extraction(test_input, db_lst[0][stage_iter], layer_index) # batch * seq_len * hidden_neuron * input_neuron\n",
    "\n",
    "    if verbose:\n",
    "        figsize1, figsize2 = 2, 6\n",
    "        figexh1, axsexh1 = plt.subplots(3,3,figsize=(figsize2*3,figsize1*3))  \n",
    "        figexh2, axsexh2 = plt.subplots(4,3,figsize=(figsize2*3,figsize1*4))  \n",
    "        figdiff, axsdiff = plt.subplots(1,2,figsize=(4*2,2))\n",
    "\n",
    "    task_middle_dict = {}\n",
    "    task_labels_across_batch = []\n",
    "\n",
    "    saver_shape1 = (3,3)\n",
    "    saver1 = np.empty((test_input.shape[0], saver_shape1[0], saver_shape1[1]), dtype=object)\n",
    "    saver_shape2 = (4,3)\n",
    "    saver2 = np.empty((test_input.shape[0], saver_shape2[0]+1, saver_shape2[1]), dtype=object)\n",
    "    saver2_random = np.empty((test_input.shape[0], saver_shape2[0]+1, saver_shape2[1]), dtype=object) # projection to random space\n",
    "\n",
    "    random_output_Y_lst = [generate_random_orthonormal_matrix(W_output.shape[1]) for _ in range(10)]\n",
    "\n",
    "    for batch_iter in range(test_input.shape[0]):\n",
    "        writeon = 0\n",
    "        labels_for_batch = labels[batch_iter,0]\n",
    "        \n",
    "        if labels_for_batch in label_index: # >=0: for all label; ==0, say, for specific label on the ring (regardless on which task is using)\n",
    "            xx = test_input[batch_iter, :, :].cpu().numpy()[0,6-shift_index:]\n",
    "            which_task = np.where(xx)[0][0] # extract here, will repeat later at different time slices\n",
    "            \n",
    "            if labels_for_batch not in task_middle_dict.keys():\n",
    "                task_middle_dict[which_task] = []\n",
    "                writeon = 1\n",
    "                \n",
    "            x_batch_taskinfo = test_input[batch_iter, :, :][:,6-shift_index:].cpu().numpy()[0,:]\n",
    "            # based on the input info, which task is used \n",
    "            task_specific = np.where(x_batch_taskinfo == 1)[0]\n",
    "            # this should not work properly if input includes random noise \n",
    "            # for the function's purpose, the input should not include noise \n",
    "            assert len(task_specific) == 1\n",
    "            task_specific = task_specific[0]\n",
    "            \n",
    "            task_labels_across_batch.append(task_specific) # load task information (which task) across batches\n",
    "        \n",
    "            res_eq26, res_eq8, res_eq11 = [], [], []\n",
    "            res_meta = []\n",
    "\n",
    "            for i in range(saver_shape1[0]):\n",
    "                for j in range(saver_shape1[1]):\n",
    "                    saver1[batch_iter, i, j] = np.array([])\n",
    "        \n",
    "            for i in range(saver_shape2[0]+1):\n",
    "                for j in range(saver_shape2[1]):\n",
    "                    saver2[batch_iter, i, j] = np.array([])\n",
    "                    saver2_random[batch_iter, i, j] = np.array([])\n",
    "        \n",
    "            for time_iter in range(test_input.shape[1]):\n",
    "                x = test_input[batch_iter, time_iter, :].cpu().numpy().reshape(-1,1)\n",
    "                \n",
    "                input_length = len(x)\n",
    "        \n",
    "                x_fixon, x_fixoff, x_stimulus, x_task = [np.zeros((input_length, 1)) for _ in range(4)]\n",
    "                # one-hot encoded vector for fixation\n",
    "                x_fixon[0,0] = x[0,0] \n",
    "                # one-hot encoded vector for fixation off (set to dummy if not presented)\n",
    "                x_fixoff[1,0] = x[1,0] if task_params['fixate_off'] else 0\n",
    "                # one-hot encoded vector for stimulus\n",
    "                x_stimulus[2-shift_index:6-shift_index,0] = x[2-shift_index:6-shift_index,0]\n",
    "                # one-hot encoded vector for task\n",
    "                # task (dynamically setting for all element after the 6th elements)\n",
    "                tasks_info = x[6-shift_index:,0]\n",
    "                x_task[6-shift_index:,0] = tasks_info\n",
    "\n",
    "                which_task = np.where(tasks_info)[0][0]\n",
    "                \n",
    "                Mt = Ms_orig[batch_iter, time_iter, :, :] \n",
    "                bt = bs[batch_iter, time_iter, :].reshape(-1,1) # hidden_neuron * 1\n",
    "                \n",
    "                middle =  W_ + W_ * Mt\n",
    "\n",
    "                if time_iter >= response_start + 1 and len(label_index) == 1:\n",
    "                    if writeon:\n",
    "                        task_middle_dict[which_task].append(middle)\n",
    "                \n",
    "                y_fix = W_output[0,:].reshape(1,-1)\n",
    "                Y_resp1 = W_output[1,:].reshape(1,-1)\n",
    "                Y_resp2 = W_output[2,:].reshape(1,-1)\n",
    "\n",
    "                if task_params['fixate_off']:\n",
    "                    allX1 = [x_fixon+x_task, x_fixoff+x_task, x_stimulus+x_fixon+x_task]\n",
    "                else:\n",
    "                    allX1 = [x_fixon+x_task, x_task, x_stimulus+x_fixon+x_task]\n",
    "\n",
    "                def process_x_add_input(x):\n",
    "                    if net_params[\"input_layer_add\"]:\n",
    "                        x = W_input @ x  \n",
    "                    if net_params[\"input_layer_bias\"]:\n",
    "                        x = x + W_inputbias\n",
    "                    return x\n",
    "                \n",
    "                allX1name = [\"x_fixon+x_task\", \"x_fixoff+x_task\", \"x_stimulus+x_fixon+x_task\"]\n",
    "                allX2 = [x_fixon, x_fixoff, x_stimulus, x_task]\n",
    "                allX2name = [\"x_fixon\", \"x_fixoff\", \"x_stimulus\", \"x_task\"]\n",
    "                allY = [y_fix, Y_resp1, Y_resp2]\n",
    "                allYname = [\"y_fix\", \"Y_resp1\", \"Y_resp2\"]\n",
    "\n",
    "                allX1 = [process_x_add_input(x) for x in allX1]\n",
    "                allX2 = [process_x_add_input(x) for x in allX2]\n",
    "        \n",
    "                for yiter in range(len(allY)):\n",
    "                    for xiter in range(len(allX1)):\n",
    "                        # res1 = helper.to_unit_vector(allY[yiter]) @ helper.to_unit_vector(middle @ allX1[xiter])\n",
    "                        step1 = middle @ allX1[xiter] + bt # adjust according to specific bias \n",
    "                        res1 = allY[yiter] @ step1 \n",
    "                        saver1[batch_iter, xiter, yiter] = np.append(saver1[batch_iter, xiter, yiter], res1[0,0])\n",
    "        \n",
    "                for y1 in range(len(allY)):\n",
    "                    for x1 in range(len(allX2)):\n",
    "                        # res2 = helper.to_unit_vector(allY[yiter]) @ helper.to_unit_vector(middle @ allX2[xiter])\n",
    "                        step1 = middle @ allX2[x1]\n",
    "                        res2 = allY[y1] @ step1\n",
    "                        res2_random = [((random_output_Y[:,y1].reshape(1,-1)) @ middle @ allX2[x1])[0,0] for random_output_Y in random_output_Y_lst]\n",
    "                        \n",
    "                        saver2[batch_iter, x1, y1] = np.append(saver2[batch_iter, x1, y1], res2[0,0])\n",
    "                        saver2_random[batch_iter, x1, y1] = np.append(saver2_random[batch_iter, x1, y1], np.mean(res2_random))\n",
    "\n",
    "                # how about bias projection to output\n",
    "                for y_iter2 in range(len(allY)):\n",
    "                    step1 = bt \n",
    "                    res2 = allY[y_iter2] @ step1\n",
    "                    saver2[batch_iter, len(allX2), y_iter2] = np.append(saver2[batch_iter, len(allX2), y_iter2], res2[0,0])\n",
    "\n",
    "            if verbose:\n",
    "                for i in range(saver_shape1[0]):\n",
    "                    for j in range(saver_shape1[1]):\n",
    "                        axsexh1[i,j].plot(saver1[batch_iter,i,j], color=c_vals[labels_for_batch], linestyle=l_vals[task_specific])\n",
    "            \n",
    "                for i in range(saver_shape2[0]):\n",
    "                    for j in range(saver_shape2[1]):            \n",
    "                        axsexh2[i,j].plot(saver2[batch_iter,i,j], color=c_vals[labels_for_batch], linestyle=l_vals[task_specific])\n",
    "        \n",
    "                # # extract fixon-task information explicitly\n",
    "                axsdiff[0].plot(saver2[batch_iter,0,1] + saver2[batch_iter,3,1], color=c_vals[labels_for_batch], linestyle=l_vals[task_specific])\n",
    "                axsdiff[0].plot(saver2_random[batch_iter,0,1] + saver2_random[batch_iter,3,1], color=c_vals_l[labels_for_batch], linestyle=l_vals[task_specific])\n",
    "                \n",
    "                axsdiff[1].plot(saver2[batch_iter,0,2] + saver2[batch_iter,3,2], color=c_vals[labels_for_batch], linestyle=l_vals[task_specific])\n",
    "                axsdiff[1].plot(saver2_random[batch_iter,0,2] + saver2_random[batch_iter,3,2], color=c_vals_l[labels_for_batch], linestyle=l_vals[task_specific])\n",
    "\n",
    "    if verbose:\n",
    "        # plot fixon/task information for one specific stimulus on one figure\n",
    "        # show perfect cancellation until fixon info goes away (during response period)\n",
    "        figpaper, axspaper = plt.subplots(8,1,figsize=(4, figsize1*8))\n",
    "\n",
    "        temp_saver = []\n",
    "\n",
    "        for batch_iter in range(test_input.shape[0]):\n",
    "            labels_for_batch = labels[batch_iter, 0]\n",
    "            task_for_batch = task_labels_across_batch[batch_iter]\n",
    "            if labels_for_batch in label_index and (labels_for_batch, task_for_batch) not in temp_saver:\n",
    "                f_fixon, f_task, f_bias = saver2[batch_iter, 0, 1], saver2[batch_iter, 3, 1], saver2[batch_iter, -1, 1]\n",
    "                axspaper[labels_for_batch].plot(f_fixon, color=c_vals[0], linestyle=l_vals[task_for_batch], label=f\"Fixon {task_for_batch}\")\n",
    "                axspaper[labels_for_batch].plot(f_task+f_bias, color=c_vals[1], linestyle=l_vals[task_for_batch], label=f\"Task+Bias {task_for_batch}\")\n",
    "                axspaper[labels_for_batch].plot(f_fixon+f_task+f_bias, color=c_vals[2], linestyle=l_vals[task_for_batch], linewidth=3, \\\n",
    "                                                       label=f\"Combine {task_for_batch}\")\n",
    "                axspaper[labels_for_batch].axhline(0, color=c_vals[3])\n",
    "                axspaper[labels_for_batch].set_title(f\"Stimulus Label {labels_for_batch}\")\n",
    "\n",
    "                temp_saver.append((labels_for_batch, task_for_batch))\n",
    "\n",
    "        for axsp in axspaper:\n",
    "            axsp.legend()\n",
    "            axsp.set_ylim([-2.0, 2.0])\n",
    "        figpaper.tight_layout()   \n",
    "        figpaper.savefig(f\"./twotasks/show_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "        \n",
    "        for i in range(saver_shape1[0]):\n",
    "            for j in range(saver_shape1[1]):\n",
    "                axsexh1[i,j].set_ylim([-2.0, 2.0])\n",
    "                axsexh1[i,j].set_title(f\"{allX1name[i]} & {allYname[j]}\")\n",
    "        \n",
    "        for i in range(saver_shape2[0]):\n",
    "            for j in range(saver_shape2[1]):\n",
    "                axsexh2[i,j].set_ylim([-2.0, 2.0])\n",
    "                axsexh2[i,j].set_title(f\"{allX2name[i]} & {allYname[j]}\")\n",
    "        \n",
    "        for ax in np.concatenate((axsexh1.flatten(), axsexh2.flatten())):\n",
    "            for breaks in all_breaks:\n",
    "                for bb in breaks:\n",
    "                    ax.axvline(bb, linestyle=\"--\", c=c_vals[all_breaks.index(breaks)])\n",
    "    \n",
    "        label_index_name = \"all\" if len(label_index) == 8 else label_index\n",
    "        \n",
    "        figexh1.suptitle(f\"Exhaustive Search 1 {color_by} at Stage {stage_iter}\")\n",
    "        figexh1.tight_layout()\n",
    "        # figexh1.savefig(f\"./twotasks/es1_{task_params['fixate_off']}_{network_at_percent}_{label_index_name}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "        \n",
    "        figexh2.suptitle(f\"Exhaustive Search 2 {color_by} Stage {stage_iter}\")\n",
    "        figexh2.tight_layout()\n",
    "        # figexh2.savefig(f\"./twotasks/es2_{task_params['fixate_off']}_{network_at_percent}_{label_index_name}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "\n",
    "        axsdiff[0].set_title(\"Stimulus 1\")\n",
    "        axsdiff[1].set_title(\"Stimulus 2\")\n",
    "        figdiff.suptitle(f\"Fixon-Task at Stage {stage_iter}\")\n",
    "        figdiff.tight_layout()\n",
    "        # figdiff.savefig(f\"./twotasks/diff_{task_params['fixate_off']}_{network_at_percent}_{label_index_name}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "\n",
    "    return task_middle_dict if len(label_index) == 1 else {}, task_labels_across_batch, saver2, saver2_random # only do it for single task learnig for clarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954cbe71-7953-48eb-9f2f-af7d5b3be088",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trajectory, all_trajectory_random = [], []\n",
    "for stage_iter in range(stages_num):\n",
    "    task_middle_dict, task_labels_across_batch, save_trajectory, save_trajectory_random = plot_trajectory_by_index(np.unique(labels), \\\n",
    "                                                                                                                    stage_iter, \\\n",
    "                                                                                                                    verbose=(stage_iter==stages_num-1)\n",
    "                                                                                          )\n",
    "    all_trajectory.append(save_trajectory)\n",
    "    all_trajectory_random.append(save_trajectory_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d7d96a-195a-4b05-adda-ef5af48cc49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def analyze_trajectory(save_trajectory):\n",
    "#     \"\"\"\n",
    "#     Analyze trajectories by calculating mean absolute values for fixations and mpn_tasks.\n",
    "#     \"\"\"\n",
    "#     def process_trajectory(trajectory, ind=False):\n",
    "#         results = []\n",
    "#         for batch in trajectory:\n",
    "#             stim1_fixon = batch[0,1][stimulus_start:response_start]\n",
    "#             stim1_task = batch[3,1][stimulus_start:response_start]\n",
    "#             if ind: \n",
    "#                 bias = batch[4,1][stimulus_start:response_start]\n",
    "#             else:\n",
    "#                 bias = np.zeros_like(stim1_fixon.shape)\n",
    "\n",
    "#             response_beh = batch[0,1][response_start:] + batch[3,1][response_start:] + batch[4,1][response_start:]\n",
    "\n",
    "#             # average over time \n",
    "#             results.append([np.mean(np.abs(stim1_fixon + stim1_task + bias)), \\\n",
    "#                             np.mean(np.abs(stim1_fixon)), np.mean(np.abs(stim1_task)), \\\n",
    "#                             np.mean(np.abs(stim1_fixon + stim1_task + bias)), \\\n",
    "#                             np.mean(np.abs(stim1_fixon)), np.mean(np.abs(stim1_task)), \\\n",
    "#                             np.mean(response_beh)\n",
    "#                            ])\n",
    "#         return np.array(results)\n",
    "\n",
    "#     task_mask_0 = np.array(test_task) == 0 \n",
    "#     task_mask_1 = np.array(test_task) == 1\n",
    "    \n",
    "#     # Process both trajectories\n",
    "#     result = process_trajectory(save_trajectory, True)\n",
    "\n",
    "#     response_cancel = []\n",
    "#     for k in range(8):\n",
    "#         ind1 = [i for i, lst in enumerate(label_task_comb) if lst == [k, 0]]\n",
    "#         ind2 = [i for i, lst in enumerate(label_task_comb) if lst == [k, 1]]\n",
    "#         response_mag = result[:,3]\n",
    "#         response_cancel.append(np.abs(np.mean(response_mag[ind1]) + np.mean(response_mag[ind2])))\n",
    "\n",
    "#     # Return the mean of the computed values\n",
    "#     return np.mean(result[:, 0][task_mask_0]), np.mean(result[:, 1][task_mask_0]), np.mean(result[:,2][task_mask_0]), \\\n",
    "#             np.mean(result[:, 0][task_mask_1]), np.mean(result[:, 1][task_mask_1]), np.mean(result[:,2][task_mask_1]), \\\n",
    "#             np.mean(response_cancel), np.std(response_cancel)\n",
    "\n",
    "# fixon_task_diff = np.array([analyze_trajectory(all_trajectory[i]) for i in range(len(all_trajectory))])\n",
    "\n",
    "# figfixontaskcancel, axsfixontaskcancel = plt.subplots(figsize=(4,2))\n",
    "# axsfixontaskcancel.plot(counter_lst, fixon_task_diff[:,0], \"-o\", c=c_vals[0], linestyle=l_vals[0], label=\"abs(fixon-task) Task 0\")\n",
    "# axsfixontaskcancel.plot(counter_lst, fixon_task_diff[:,1], \"-o\", c=c_vals[1], linestyle=l_vals[0], label=\"abs(fixon) Task 0\")\n",
    "# axsfixontaskcancel.plot(counter_lst, fixon_task_diff[:,2], \"-o\", c=c_vals[2], linestyle=l_vals[0], label=\"abs(task) Task 0\")\n",
    "# axsfixontaskcancel.plot(counter_lst, fixon_task_diff[:,3], \"-o\", c=c_vals[0], linestyle=l_vals[1], label=\"abs(fixon-task) Task 1\")\n",
    "# axsfixontaskcancel.plot(counter_lst, fixon_task_diff[:,4], \"-o\", c=c_vals[1], linestyle=l_vals[1], label=\"abs(fixon) Task 1\")\n",
    "# axsfixontaskcancel.plot(counter_lst, fixon_task_diff[:,5], \"-o\", c=c_vals[2], linestyle=l_vals[1], label=\"abs(task) Task 1\")\n",
    "# # axsfixontaskcancel[0].plot(fixon_task_diff[:,3], \"-o\", c=c_vals_l[0], label=\"abs(fixon-task) random\")\n",
    "# # axsfixontaskcancel[0].plot(fixon_task_diff[:,4], \"-o\", c=c_vals_l[1], label=\"abs(fixon) random\")\n",
    "# # axsfixontaskcancel[0].plot(fixon_task_diff[:,5], \"-o\", c=c_vals_l[2], label=\"abs(task) random\")\n",
    "# axsfixontaskcancel.legend()\n",
    "# axsfixontaskcancel.set_ylabel(\"Projection Magnitude\")\n",
    "# # axsfixontaskcancel.set_title(\"Average Cancellation Effect Before Response Period\")\n",
    "\n",
    "# axsfixontaskcancel.set_xlabel(\"# Epoch\")\n",
    "# axsfixontaskcancel.set_xscale(\"log\")\n",
    "# figfixontaskcancel.tight_layout()\n",
    "# # figfixontaskcancel.savefig(f\"./twotasks/cancel_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "\n",
    "# fig, ax1 = plt.subplots(figsize=(4,2))  # Create a new figure\n",
    "\n",
    "# ax1.plot(counter_lst, loss_lst, \"-o\", c=c_vals[0], label=\"Loss\")\n",
    "# ax1.set_ylabel(\"Loss\", color=c_vals[0])\n",
    "# ax1.tick_params(axis='y', colors=c_vals[0])\n",
    "# ax1.set_yscale(\"log\")  # Keep log scale for loss\n",
    "# ax1.set_xlabel(\"Counter\")\n",
    "\n",
    "# # Create a second y-axis for accuracy (right)\n",
    "# ax2 = ax1.twinx()\n",
    "# ax2.plot(counter_lst, acc_lst, \"-o\", c=c_vals[1], label=\"Accuracy\")\n",
    "# ax2.set_ylabel(\"Accuracy\", color=c_vals[1])\n",
    "# ax2.tick_params(axis='y', colors=c_vals[1])\n",
    "\n",
    "# ax1.set_xlabel(\"# Epoch\")\n",
    "# ax1.set_xscale(\"log\")\n",
    "# fig.tight_layout()\n",
    "# # fig.savefig(f\"./twotasks/loss_acc_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "\n",
    "# figcancel, axcancel = plt.subplots(figsize=(4,2))\n",
    "# axcancel.plot(counter_lst, fixon_task_diff[:,6], \"-o\", c=c_vals[0], label=\"Cancel of Proj in Response\")\n",
    "# axcancel.fill_between(counter_lst, fixon_task_diff[:,6] - fixon_task_diff[:,7], fixon_task_diff[:,6] + fixon_task_diff[:,7], color=c_vals_l[0])\n",
    "# axcancel.set_xlabel(\"# Epoch\")\n",
    "# axcancel.set_xscale(\"log\")\n",
    "# axcancel.set_ylabel(\"Projection Diff\")\n",
    "# # figcancel.savefig(f\"./twotasks/response_cancel_task_anti_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddf8d10-5e6c-420c-b509-9a5bff95373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_cosine_similarity(matrix):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    cos_sim_matrix = cosine_similarity(matrix)\n",
    "    avg_similarity = np.nanmean(np.triu(cos_sim_matrix))\n",
    "    \n",
    "    return avg_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bc21c7-26ec-4123-bfce-ee4d191d42e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modulation of fixon and task should be the same for each task specifically \n",
    "# regardless of the stimulus \n",
    "assert color_by == \"stim\" \n",
    "\n",
    "comparelst = [\"hidden\", \"modulation\"]\n",
    "\n",
    "for compare in comparelst:\n",
    "    all_result = [] \n",
    "    for stage_iter in range(stages_num):\n",
    "        Woutput = Woutput_lst[stage_iter]\n",
    "        W = Wall_lst[stage_iter][0]\n",
    "        _, Ms_orig, hs, bs = modulation_extraction(db_lst[stage_iter], layer_index)\n",
    "\n",
    "        result = analyze_similarity(Ms_orig, hs, net_params, label_task_comb, compare=compare)\n",
    "    \n",
    "        all_result.append(result)\n",
    "    \n",
    "    all_result = np.array(all_result)\n",
    "    \n",
    "    legends = [\"Same stimulus, different task\", \"Same response, different task\", \\\n",
    "               \"Same task 0 different stimulus (Subsampled)\", \"Same task 1 different stimulus (Subsampled)\"]\n",
    "    \n",
    "    figr, axsr = plt.subplots(1,1,figsize=(4,4))\n",
    "    for i in range(all_result.shape[1]):\n",
    "        axsr.plot(counter_lst, all_result[:,i,0], \"-o\", c=c_vals[i], label=legends[i])\n",
    "        axsr.fill_between(counter_lst, all_result[:,i,0]-all_result[:,i,1], all_result[:,i,0]+all_result[:,i,1], \\\n",
    "                         color=c_vals_l[i], alpha=0.1)\n",
    "    # axsr.plot(counter_lst, all_result[:,4], \"-o\", c=c_vals[5], label=\"Task 0/1 Modulation Diff End Fixon Period\")\n",
    "    axsr.legend()\n",
    "    axsr.set_xlabel(\"# Epoch\")\n",
    "    axsr.set_xscale(\"log\")\n",
    "    axsr.set_ylabel(\"Correlation\")\n",
    "    axsr.set_title(compare)\n",
    "    figr.savefig(f\"./twotasks/compare_{compare}_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b170e5bb-a866-40cd-8426-f62f6f364b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = all_breaks[0] \n",
    "tb.insert(0,0)\n",
    "\n",
    "for el in range(len(tb)-1):\n",
    "    \n",
    "    cell_vars_tot = np.var(hs[:,tb[el]:tb[el+1],:], axis=(0, 1)) # Var over batch and sequence\n",
    "    n_rules = len(task_params['rules'])\n",
    "    n_cells = hs.shape[-1]\n",
    "    \n",
    "    cell_vars_rules = np.zeros((n_rules, n_cells,))\n",
    "    cell_vars_rules_norm = np.zeros_like(cell_vars_rules)\n",
    "    \n",
    "    for rule_idx, rule in enumerate(task_params['rules']):\n",
    "        print('Rule {} (idx {})'.format(rule, rule_idx))\n",
    "        rule_hs = hs[test_rule_idxs == rule_idx, tb[el]:tb[el+1], :]\n",
    "        cell_vars_rules[rule_idx] = np.var(rule_hs, axis=(0, 1)) \n",
    "    \n",
    "    # normalize\n",
    "    cell_max_var = np.max(cell_vars_rules, axis=0) # Across rules\n",
    "    for rule_idx, rule in enumerate(task_params['rules']):\n",
    "        cell_vars_rules_norm[rule_idx] = np.where(\n",
    "            cell_max_var > 0., cell_vars_rules[rule_idx] / cell_max_var, 0.\n",
    "        )\n",
    "    \n",
    "    # sort \n",
    "    if n_rules > 1:\n",
    "        rule0_vals = cell_vars_rules_norm[0].tolist()\n",
    "        rule1_vals = cell_vars_rules_norm[1].tolist()\n",
    "    \n",
    "    rule01_vals = np.array(list(zip(rule0_vals, rule1_vals)), dtype=[('rule0', float), ('rule1', float)])\n",
    "    sort_idxs = np.argsort(rule01_vals, order=['rule0', 'rule1'])[::-1]\n",
    "    \n",
    "    cell_vars_rules_sorted_norm = cell_vars_rules_norm[:, sort_idxs]\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "    for rule_idx, rule in enumerate(task_params['rules']):\n",
    "        ax.plot(cell_vars_rules_sorted_norm[rule_idx], color=c_vals[rule_idx],\n",
    "                label=task_params['rules'][rule_idx])\n",
    "    \n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Cell_idx')\n",
    "    ax.set_ylabel('Normalized task variance')\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12,4))\n",
    "    sns.heatmap(cell_vars_rules_sorted_norm, ax=ax, cmap=\"coolwarm\", cbar=True, vmin=0, vmax=1)\n",
    "    ax.set_yticks(np.arange(n_rules) + 0.5)\n",
    "    ax.set_yticklabels(task_params['rules'], rotation=45)\n",
    "    ax.set_xlabel('Cell idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47aacc0-d4d3-4b17-aeec-1ea0ff31c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fighs, axshs = plt.subplots(3,1,figsize=(5*1,5*3))\n",
    "axshs = np.atleast_2d(axshs).T\n",
    "pca_er_stage = []\n",
    "\n",
    "start_stage = stages_num - 1\n",
    "for stage_iter in range(start_stage, stages_num):\n",
    "    PCA_downsample = 3\n",
    "    Ms, Ms_orig, hs, bs = modulation_extraction(db_lst[stage_iter], layer_index)\n",
    "\n",
    "    pca = PCA(n_components = PCA_downsample)\n",
    "    \n",
    "    # Ms_end_of_stimulus = Ms_orig[np.array(test_task) == 0,stimulus_end:stimulus_end+1,:,:]\n",
    "    # Ms_end_of_stimulus_flattened = Ms_end_of_stimulus.reshape(Ms_end_of_stimulus.shape[0] * Ms_end_of_stimulus.shape[1] * Ms_end_of_stimulus.shape[3], Ms_end_of_stimulus.shape[2])\n",
    "    # pca.fit(Ms_end_of_stimulus_flattened)\n",
    "    # Ms_flattened = Ms_orig.reshape(Ms_orig.shape[0] * Ms_orig.shape[1] * Ms_orig.shape[3], Ms_orig.shape[2])\n",
    "    # projected_data = pca.transform(Ms_flattened)\n",
    "\n",
    "    # Ms_reconstructed = projected_data.reshape(Ms_orig.shape[0], Ms_orig.shape[1], Ms_orig.shape[3], PCA_downsample)\n",
    "    # lowd_data_lst = [Ms_reconstructed[:,:,0,:]] # for instance, modulation of fixon\n",
    "\n",
    "    # from Kyle's previous code \n",
    "    n_activity = Ms.shape[-1] \n",
    "    activity_zero = np.zeros((1, n_activity,))\n",
    "\n",
    "    as_flat = Ms.reshape((-1, n_activity))\n",
    "    pca.fit(as_flat) \n",
    "    as_pca = pca.transform(as_flat)\n",
    "    projected_data = as_pca.reshape((Ms.shape[0], Ms.shape[1], -1))\n",
    "    zeros_pca = pca.transform(activity_zero)\n",
    "\n",
    "    lowd_data_lst = [projected_data]\n",
    "    \n",
    "    for i in range(Ms_orig.shape[0]):\n",
    "        for dd in range(len(lowd_data_lst)):\n",
    "            lowd_data = lowd_data_lst[dd]\n",
    "            data_batch = lowd_data[i,:,:]\n",
    "            \n",
    "            axshs[0,stage_iter-start_stage].scatter(data_batch[:stimulus_end,0], data_batch[:stimulus_end,1], marker=markers_vals[dd], c=c_vals[labels[i,0]], alpha=0.2)\n",
    "            axshs[0,stage_iter-start_stage].plot(data_batch[:stimulus_start-1,0], data_batch[:stimulus_start-1,1], c=c_vals[labels[i,0]], linestyle=l_vals[0], alpha=0.2)\n",
    "            axshs[0,stage_iter-start_stage].plot(data_batch[stimulus_start-1:stimulus_end,0], data_batch[stimulus_start-1:stimulus_end,1], c=c_vals[labels[i,0]], linestyle=l_vals[1], alpha=0.2)\n",
    "            axshs[0,stage_iter-start_stage].scatter(zeros_pca[0,0], zeros_pca[0,1])\n",
    "            axshs[0,stage_iter-start_stage].set_xlabel(\"PCA 1\")\n",
    "            axshs[0,stage_iter-start_stage].set_ylabel(\"PCA 2\")\n",
    "            \n",
    "            axshs[1,stage_iter-start_stage].scatter(data_batch[:stimulus_end,0], data_batch[:stimulus_end,2], marker=markers_vals[dd], c=c_vals[labels[i,0]], alpha=0.2)\n",
    "            axshs[1,stage_iter-start_stage].plot(data_batch[:stimulus_start-1,0], data_batch[:stimulus_start-1,2], c=c_vals[labels[i,0]], linestyle=l_vals[0], alpha=0.2)\n",
    "            axshs[1,stage_iter-start_stage].plot(data_batch[stimulus_start-1:stimulus_end,0], data_batch[stimulus_start-1:stimulus_end,2], c=c_vals[labels[i,0]], linestyle=l_vals[1], alpha=0.2)\n",
    "            axshs[1,stage_iter-start_stage].scatter(zeros_pca[0,0], zeros_pca[0,2])\n",
    "            axshs[1,stage_iter-start_stage].set_xlabel(\"PCA 1\")\n",
    "            axshs[1,stage_iter-start_stage].set_ylabel(\"PCA 3\")\n",
    "            \n",
    "            axshs[2,stage_iter-start_stage].scatter(data_batch[:stimulus_end,1], data_batch[:stimulus_end,2], marker=markers_vals[dd], c=c_vals[labels[i,0]], alpha=0.2)\n",
    "            axshs[2,stage_iter-start_stage].plot(data_batch[:stimulus_start-1,1], data_batch[:stimulus_start-1,2], c=c_vals[labels[i,0]], linestyle=l_vals[0], alpha=0.2)\n",
    "            axshs[2,stage_iter-start_stage].plot(data_batch[stimulus_start-1:stimulus_end,1], data_batch[stimulus_start-1:stimulus_end,2], c=c_vals[labels[i,0]], linestyle=l_vals[1], alpha=0.2)\n",
    "            axshs[2,stage_iter-start_stage].scatter(zeros_pca[0,1], zeros_pca[0,2])\n",
    "            axshs[2,stage_iter-start_stage].set_xlabel(\"PCA 2\")\n",
    "            axshs[2,stage_iter-start_stage].set_ylabel(\"PCA 3\")\n",
    "\n",
    "fighs.tight_layout()\n",
    "# fighs.savefig(f\"./twotasks/m_pca_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9771191-9cb1-4ad2-a670-a230e5614cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fc1bb2-f9df-4e97-88c0-8214c5deba61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
