{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "374fb8f2-9b83-44ce-821b-8917a114c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "import copy \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# PyTorch Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Data Handling and Image Processing\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Style for Matplotlib\n",
    "import scienceplots\n",
    "plt.style.use('science')\n",
    "plt.style.use(['no-latex'])\n",
    "\n",
    "# Scientific Computing and Machine Learning\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.linalg import subspace_angles\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Custom Modules and Extensions\n",
    "sys.path.append(\"../netrep/\")\n",
    "sys.path.append(\"../svcca/\")\n",
    "\n",
    "import networks as nets  # Contains RNNs\n",
    "import net_helpers\n",
    "import mpn_tasks\n",
    "import helper\n",
    "import mpn\n",
    "\n",
    "import scienceplots\n",
    "plt.style.use('science')\n",
    "plt.style.use(['no-latex'])\n",
    "\n",
    "# Memory Optimization\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd1556c8-a4b6-434b-a60f-37035980bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 Red, 1 blue, 2 green, 3 purple, 4 orange, 5 teal, 6 gray, 7 pink, 8 yellow\n",
    "c_vals = ['#e53e3e', '#3182ce', '#38a169', '#805ad5','#dd6b20', '#319795', '#718096', '#d53f8c', '#d69e2e',] * 10\n",
    "c_vals_l = ['#feb2b2', '#90cdf4', '#9ae6b4', '#d6bcfa', '#fbd38d', '#81e6d9', '#e2e8f0', '#fbb6ce', '#faf089',] * 10\n",
    "c_vals_d = ['#9b2c2c', '#2c5282', '#276749', '#553c9a', '#9c4221', '#285e61', '#2d3748', '#97266d', '#975a16',] * 10 \n",
    "l_vals = ['solid', 'dashed', 'dotted', 'dashdot', '-', '--', '-.', ':', (0, (3, 1, 1, 1)), (0, (5, 10))]\n",
    "markers_vals = ['o', 'v', '*', '+', '>', '1', '2', '3', '4', 's', 'p', '*', 'h', 'H', '+', 'x', 'D', 'd', '|', '_']\n",
    "linestyles = [\"-\", \"--\", \"-.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58d36397-6fe4-4124-bf57-307f571e9309",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34978bf6-67b8-41bd-a022-a7b46a320686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set seed 114\n",
      "Fixation_off: True; Task_info: True\n",
      "Rules: ['delaygo', 'delayanti']\n",
      "  Input size 8, Output size 3\n",
      "Using CPU...\n"
     ]
    }
   ],
   "source": [
    "# Reload modules if changes have been made to them\n",
    "from importlib import reload\n",
    "\n",
    "reload(nets)\n",
    "reload(net_helpers)\n",
    "\n",
    "fixseed = False # randomize setting the seed may lead to not perfectly solved results\n",
    "seed = random.randint(1,1000) if not fixseed else 8 # random set the seed to test robustness by default\n",
    "print(f\"Set seed {seed}\")\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "hyp_dict['task_type'] = 'multitask' # int, NeuroGym, multitask\n",
    "hyp_dict['mode_for_all'] = \"random_batch\"\n",
    "hyp_dict['ruleset'] = 'delaygofamily' # low_dim, all, test\n",
    "\n",
    "accept_rules = ('fdgo', 'fdanti', 'delaygo', 'delayanti', 'reactgo', 'reactanti', \n",
    "                'delaydm1', 'delaydm2', 'dmsgo', 'dmcgo', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm')\n",
    "\n",
    "\n",
    "rules_dict = \\\n",
    "    {'all' : ['fdgo', 'reactgo', 'delaygo', 'fdanti', 'reactanti', 'delayanti',\n",
    "              'dm1', 'dm2', 'contextdm1', 'contextdm2', 'multidm',\n",
    "              'delaydm1', 'delaydm2', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm',\n",
    "              'dmsgo', 'dmsnogo', 'dmcgo', 'dmcnogo'],\n",
    "     'low_dim' : ['fdgo', 'reactgo', 'delaygo', 'fdanti', 'reactanti', 'delayanti',\n",
    "                 'delaydm1', 'delaydm2', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm',\n",
    "                 'dmsgo', 'dmsnogo', 'dmcgo', 'dmcnogo'],\n",
    "     'gofamily': ['fdgo', 'fdanti', 'reactgo', 'reactanti', 'delaygo', 'delayanti'],\n",
    "     'delaygo': ['delaygo'],\n",
    "     'delaygofamily': ['delaygo', 'delayanti'],\n",
    "     'fdgo': ['fdgo'],\n",
    "     'fdfamily': ['fdgo', 'fdanti'],\n",
    "     'reactgo': ['reactgo'],\n",
    "     'reactfamily': ['reactgo', 'reactanti'],\n",
    "     'delaydm1': ['delaydm1'],\n",
    "     'delaydmfamily': ['delaydm1', 'delaydm2'],\n",
    "     'dmsgofamily': ['dmsgo', 'dmsnogo'],\n",
    "     'dmsgo': ['dmsgo'],\n",
    "     'dmcgo': ['dmcgo'],\n",
    "     'contextdelayfamily': ['contextdelaydm1', 'contextdelaydm2'],\n",
    "    }\n",
    "    \n",
    "\n",
    "# This can either be used to set parameters OR set parameters and train\n",
    "train = True # whether or not to train the network\n",
    "verbose = True\n",
    "hyp_dict['run_mode'] = 'minimal' # minimal, debug\n",
    "hyp_dict['chosen_network'] = \"dmpn\"\n",
    "\n",
    "# suffix for saving images\n",
    "# inputadd, Wfix, WL2, hL2\n",
    "# inputrandom, Wtrain\n",
    "# noise001\n",
    "# largeregularization\n",
    "# trainetalambda\n",
    "\n",
    "mpn_depth = 1\n",
    "n_hidden = 200\n",
    "\n",
    "hyp_dict['addon_name'] = \"inputrandom+Wtrain+WL2+hL2\"\n",
    "hyp_dict['addon_name'] += f\"+hidden{n_hidden}\"\n",
    "\n",
    "# for coding \n",
    "if hyp_dict['chosen_network'] in (\"gru\", \"vanilla\"):\n",
    "    mpn_depth = 1\n",
    "\n",
    "def current_basic_params():\n",
    "    task_params = {\n",
    "        'task_type': hyp_dict['task_type'],\n",
    "        'rules': rules_dict[hyp_dict['ruleset']],\n",
    "        'dt': 40, # ms, directly influence sequence lengths,\n",
    "        'ruleset': hyp_dict['ruleset'],\n",
    "        'n_eachring': 8, # Number of distinct possible inputs on each ring\n",
    "        'in_out_mode': 'low_dim',  # high_dim or low_dim or low_dim_pos (Robert vs. Laura's paper, resp)\n",
    "        'sigma_x': 0.00, # Laura raised to 0.1 to prevent overfitting (Robert uses 0.01)\n",
    "        'mask_type': 'cost', # 'cost', None\n",
    "        'fixate_off': True, # Second fixation signal goes on when first is off\n",
    "        'task_info': True, \n",
    "        'randomize_inputs': False,\n",
    "        'n_input': 20, # Only used if inputs are randomized,\n",
    "        'modality_diff': False,\n",
    "        'label_strength': False, \n",
    "        'long_delay': 'normal',\n",
    "        'long_response': 'normal',\n",
    "        'adjust_task_prop': True,\n",
    "        'adjust_task_decay': 0.9, \n",
    "    }\n",
    "\n",
    "    print(f\"Fixation_off: {task_params['fixate_off']}; Task_info: {task_params['task_info']}\")\n",
    "\n",
    "    train_params = {\n",
    "        'lr': 1e-3,\n",
    "        'n_batches': 128,\n",
    "        'batch_size': 128,\n",
    "        'gradient_clip': 10,\n",
    "        'valid_n_batch': 100,\n",
    "        'n_datasets': 10000, # Number of distinct batches\n",
    "        'valid_check': None, \n",
    "        'n_epochs_per_set': 1, # longer/shorter training\n",
    "        'weight_reg': 'L2',\n",
    "        'activity_reg': 'L2', \n",
    "        'reg_lambda': 1e-4,\n",
    "    }\n",
    "\n",
    "    if not train: # some \n",
    "        assert train_params['n_epochs_per_set'] == 0\n",
    "\n",
    "    net_params = {\n",
    "        'net_type': hyp_dict['chosen_network'], # mpn1, dmpn, vanilla\n",
    "        'n_neurons': [1] + [n_hidden] * mpn_depth + [1],\n",
    "        'output_bias': False, # Turn off biases for easier interpretation\n",
    "        'loss_type': 'MSE', # XE, MSE\n",
    "        'activation': 'tanh', # linear, ReLU, sigmoid, tanh, tanh_re, tukey, heaviside\n",
    "        'cuda': True,\n",
    "        'monitor_freq': train_params[\"n_epochs_per_set\"],\n",
    "        'monitor_valid_out': True, # Whether or not to save validation output throughout training\n",
    "        'output_matrix': '',# \"\" (default); \"untrained\", or \"orthogonal\"\n",
    "        'input_layer_add': True, \n",
    "        'input_layer_add_trainable': False, # revise this is effectively to [randomize_inputs], tune this\n",
    "        'input_layer_bias': False, \n",
    "        'input_layer': \"trainable\", # for RNN only\n",
    "        'acc_measure': 'stimulus', \n",
    "        \n",
    "        # for one-layer MPN, GRU or Vanilla\n",
    "        'ml_params': {\n",
    "            'bias': True, # Bias of layer\n",
    "            'mp_type': 'mult',\n",
    "            'm_update_type': 'hebb_assoc', # hebb_assoc, hebb_pre\n",
    "            'eta_type': 'scalar', # scalar, pre_vector, post_vector, matrix\n",
    "            'eta_train': False,\n",
    "            # 'eta_init': 'mirror_gaussian', #0.0,\n",
    "            'lam_type': 'scalar', # scalar, pre_vector, post_vector, matrix\n",
    "            'm_time_scale': 4000, # ms, sets lambda\n",
    "            'lam_train': False,\n",
    "            'W_freeze': False, # different combination with [input_layer_add_trainable]\n",
    "        },\n",
    "\n",
    "        # Vanilla RNN params\n",
    "        'leaky': True,\n",
    "        'alpha': 0.2,\n",
    "    }\n",
    "\n",
    "    # Ensure the two options are *not* activated at the same time\n",
    "    assert not (task_params[\"randomize_inputs\"] and net_params[\"input_layer_add\"]), (\n",
    "        \"task_params['randomize_inputs'] and net_params['input_layer_add'] cannot both be True.\"\n",
    "    )\n",
    "\n",
    "    # for multiple MPN layers, assert \n",
    "    if mpn_depth > 1:\n",
    "        for mpl_idx in range(mpn_depth - 1):\n",
    "            assert f'ml_params{mpl_idx}' in net_params.keys()\n",
    "\n",
    "    # actually I don't think it is needed\n",
    "    # putting here to warn the parameter checking every time \n",
    "    # when switching network\n",
    "    if hyp_dict['chosen_network'] in (\"gru\", \"vanilla\"):\n",
    "        assert f'ml_params' in net_params.keys()\n",
    "\n",
    "    return task_params, train_params, net_params\n",
    "\n",
    "task_params, train_params, net_params = current_basic_params()\n",
    "\n",
    "shift_index = 1 if not task_params['fixate_off'] else 0\n",
    "\n",
    "if hyp_dict['task_type'] in ('multitask',):\n",
    "    task_params, train_params, net_params = mpn_tasks.convert_and_init_multitask_params(\n",
    "        (task_params, train_params, net_params)\n",
    "    )\n",
    "\n",
    "    net_params['prefs'] = mpn_tasks.get_prefs(task_params['hp'])\n",
    "\n",
    "    print('Rules: {}'.format(task_params['rules']))\n",
    "    print('  Input size {}, Output size {}'.format(\n",
    "        task_params['n_input'], task_params['n_output'],\n",
    "    ))\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "if net_params['cuda']:\n",
    "    print('Using CUDA...')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('Using CPU...')\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# how many epoch each dataset will be trained on\n",
    "epoch_multiply = train_params[\"n_epochs_per_set\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a341b36-dc7a-42b0-bffb-cb17d380041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_dict[\"mess_with_training\"] = False\n",
    "\n",
    "if hyp_dict['mess_with_training']:\n",
    "    hyp_dict['addon_name'] += \"messwithtraining\"\n",
    "\n",
    "params = task_params, train_params, net_params\n",
    "\n",
    "if net_params['net_type'] == 'mpn1':\n",
    "    netFunction = mpn.MultiPlasticNet\n",
    "elif net_params['net_type'] == 'dmpn':\n",
    "    netFunction = mpn.DeepMultiPlasticNet\n",
    "elif net_params['net_type'] == 'vanilla':\n",
    "    netFunction = nets.VanillaRNN\n",
    "elif net_params['net_type'] == 'gru':\n",
    "    netFunction = nets.GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07e4fe48-2af6-4741-8d42-01803ba0abf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Align ['delaygo', 'delayanti'] With Same Time\n",
      "rng reset with seed 9643\n",
      "rng reset with seed 9643\n",
      "rng reset with seed 9643\n",
      "rng reset with seed 9643\n",
      "delaygo\n",
      "delayanti\n",
      "torch.Size([200, 802, 8])\n",
      "torch.Size([200, 802, 3])\n"
     ]
    }
   ],
   "source": [
    "test_n_batch = train_params[\"valid_n_batch\"]\n",
    "color_by = \"stim\" # or \"resp\" \n",
    "\n",
    "task_random_fix = True\n",
    "if task_random_fix:\n",
    "    print(f\"Align {task_params['rules']} With Same Time\")\n",
    "\n",
    "if task_params['task_type'] in ('multitask',): # Test batch consists of all the rules\n",
    "    task_params['hp']['batch_size_train'] = test_n_batch\n",
    "    # using homogeneous cutting off\n",
    "    test_mode_for_all = \"random\"\n",
    "    # ZIHAN\n",
    "    # generate test data using \"random\"\n",
    "    test_data, test_trials_extra = mpn_tasks.generate_trials_wrap(task_params, test_n_batch, \\\n",
    "                rules=task_params['rules'], mode_input=test_mode_for_all, fix=task_random_fix\n",
    "    )\n",
    "    _, test_trials, test_rule_idxs = test_trials_extra\n",
    "\n",
    "    task_params_attractor = copy.deepcopy(task_params)\n",
    "    task_params_attractor[\"long_delay\"] = \"long\"\n",
    "    test_data_attractor, test_trials_extra_attractor = mpn_tasks.generate_trials_wrap(task_params_attractor, test_n_batch, \\\n",
    "                                                                                      rules=task_params_attractor['rules'], \\\n",
    "                                                                                      mode_input=test_mode_for_all, fix=task_random_fix)\n",
    "    \n",
    "    _, test_trials_attractor, test_rule_idxs_attractor = test_trials_extra_attractor\n",
    "\n",
    "    task_params['dataset_name'] = 'multitask'\n",
    "\n",
    "    if task_params['in_out_mode'] in ('low_dim_pos',):\n",
    "        output_dim_labels = ('Fixate', 'Cos', '-Cos', 'Sin', '-Sin')\n",
    "    elif task_params['in_out_mode'] in ('low_dim',):\n",
    "        output_dim_labels = ('Fixate', 'Cos', 'Sin')\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def generate_response_stimulus(task_params, test_trials): \n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        labels_resp, labels_stim = [], []\n",
    "        for rule_idx, rule in enumerate(task_params['rules']):\n",
    "            print(rule)\n",
    "            if rule in accept_rules:\n",
    "                if hyp_dict['ruleset'] in ('dmsgo', 'dmcgo'):\n",
    "                    labels.append(test_trials[rule_idx].meta['matches'])\n",
    "                else:\n",
    "                    labels_resp.append(test_trials[rule_idx].meta['resp1'])\n",
    "                    labels_stim.append(test_trials[rule_idx].meta['stim1']) \n",
    "    \n",
    "            else:\n",
    "                raise NotImplementedError()\n",
    "                \n",
    "        labels_resp = np.concatenate(labels_resp, axis=0).reshape(-1,1)\n",
    "        labels_stim = np.concatenate(labels_stim, axis=0).reshape(-1,1)\n",
    "\n",
    "        return labels_resp, labels_stim\n",
    "\n",
    "    labels_resp, labels_stim = generate_response_stimulus(task_params, test_trials)\n",
    "\n",
    "labels = labels_stim if color_by == \"stim\" else labels_resp\n",
    "    \n",
    "test_input, test_output, test_mask = test_data\n",
    "test_input_attractor, test_output_attractor, test_mask_attractor = test_data_attractor\n",
    "print(test_input_attractor.shape)\n",
    "print(test_output_attractor.shape)\n",
    "\n",
    "permutation = np.random.permutation(test_input.shape[0])\n",
    "test_input = test_input[permutation]\n",
    "test_output = test_output[permutation]\n",
    "test_mask = test_mask[permutation]\n",
    "labels = labels[permutation]\n",
    "\n",
    "test_input_np = test_input.detach().cpu().numpy()\n",
    "test_output_np = test_output.detach().cpu().numpy()\n",
    "\n",
    "# Total number of batches, might be different than test_n_batch\n",
    "# this should be the same regardless of variety of test_input\n",
    "n_batch_all = test_input_np.shape[0] \n",
    "\n",
    "def find_task(task_params, test_input_np, shift_index):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    test_task = [] # which task\n",
    "    for batch_idx in range(test_input_np.shape[0]):\n",
    "        \n",
    "        if task_params[\"randomize_inputs\"]: \n",
    "            test_input_np_ = test_input_np @ np.linalg.pinv(task_params[\"randomize_matrix\"])\n",
    "        else: \n",
    "            test_input_np_ = test_input_np\n",
    "            \n",
    "        task_label = test_input_np_[batch_idx, 0, 6-shift_index:]\n",
    "        # task_label_index = np.where(task_label == 1)[0][0]\n",
    "        \n",
    "        # tol = 1e-3      \n",
    "        # mask = np.isclose(task_label, 1, atol=tol)\n",
    "        task_label = np.asarray(task_label)       \n",
    "        dist = np.abs(task_label - 1)     \n",
    "        mask = dist == dist.min() \n",
    "        \n",
    "        indices = np.where(mask)[0]\n",
    "        \n",
    "        if indices.size:                \n",
    "            task_label_index = indices[0]   \n",
    "        else:\n",
    "            raise ValueError(\"No entry close enough to 1 found\")\n",
    "            \n",
    "        test_task.append(task_label_index)\n",
    "\n",
    "    return test_task  \n",
    "\n",
    "test_task = find_task(task_params, test_input_np, shift_index)\n",
    "test_task_attractor = find_task(task_params_attractor, test_input_attractor.detach().cpu().numpy(), shift_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e78b44e9-b6e3-4c1c-a5c7-5a608a6d090b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiPlastic Net:\n",
      "  output neurons: 3\n",
      "  Act: tanh\n",
      "\n",
      "  Input Layer Frozen.\n",
      "  MP Layer1 parameters:\n",
      "    n_neurons - input: 200, output: 200\n",
      "    M matrix parameters:    update bounds - Max mult: 1.0, Min mult: -1.0\n",
      "      type: mult // Update - type: hebb_assoc // Act fn: linear\n",
      "      Eta: scalar (fixed) // Lambda: scalar (fixed) // Lambda_max: 0.99 (tau: 4.0e+03)\n",
      "  No Hidden Recurrency.\n",
      "Trainable parameters: 40,800\n",
      "W_output: (3, 200)\n",
      "mp_layer1.W: (200, 200)\n",
      "mp_layer1.b: (200,)\n",
      "task_params['rules_probs']: [0.5 0.5]\n",
      "Rule: delaygo\n",
      "Rule delaygo seq_len 103, max_seq_len 103\n",
      "inputs_all paddled: (128, 103, 8)\n",
      "inputs_all: torch.Size([128, 103, 8])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_CUDA_mm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# we use net at different training stage on the same test_input\u001b[39;00m\n\u001b[1;32m      2\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      3\u001b[0m net, _, (counter_lst, netout_lst, db_lst, Winput_lst, Winputbias_lst,\\\n\u001b[0;32m----> 4\u001b[0m          Woutput_lst, Wall_lst, marker_lst, loss_lst, acc_lst), _ \u001b[38;5;241m=\u001b[39m \u001b[43mnet_helpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                                                                              \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyp_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyp_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                                                                              \u001b[49m\u001b[43mnetFunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetFunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                                                                              \u001b[49m\u001b[43mtest_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_input_attractor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                                                                              \u001b[49m\u001b[43mprint_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/allen/programs/mindscope/workgroups/auto-model/zihan.zhang/MultiTaskMPN/net_helpers.py:307\u001b[0m, in \u001b[0;36mtrain_network\u001b[0;34m(params, net, device, verbose, train, hyp_dict, netFunction, test_input, pretraining_shift, pretraining_shift_pre, print_frequency)\u001b[0m\n\u001b[1;32m    305\u001b[0m test_input_batch \u001b[38;5;241m=\u001b[39m test_input_[start:end]\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# both are in cuda\u001b[39;00m\n\u001b[0;32m--> 307\u001b[0m net_out_batch, _, db_batch \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterate_sequence_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_input_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrack_states\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# register the values\u001b[39;00m\n\u001b[1;32m    309\u001b[0m net_out_np\u001b[38;5;241m.\u001b[39mappend(net_out_batch\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m/allen/programs/mindscope/workgroups/auto-model/zihan.zhang/MultiTaskMPN/net_helpers.py:1234\u001b[0m, in \u001b[0;36mBaseNetwork.iterate_sequence_batch\u001b[0;34m(self, batch_inputs, batch_labels, batch_masks, run_mode)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;66;03m# Now iterate through the full input sequence, note that network_step of children classes will\u001b[39;00m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;66;03m# update the internal state unless told otherwise. \u001b[39;00m\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seq_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m-> 1234\u001b[0m     step_output, step_activity, db \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1236\u001b[0m     batch_output[:, seq_idx, :] \u001b[38;5;241m=\u001b[39m step_output\n\u001b[1;32m   1237\u001b[0m     batch_hidden[:, seq_idx, :] \u001b[38;5;241m=\u001b[39m step_activity[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# assume 1-layer MPN for now\u001b[39;00m\n",
      "File \u001b[0;32m/allen/programs/mindscope/workgroups/auto-model/zihan.zhang/MultiTaskMPN/mpn.py:769\u001b[0m, in \u001b[0;36mDeepMultiPlasticNet.network_step\u001b[0;34m(self, current_input, run_mode, verbose, seq_idx)\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh_recurrent \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    768\u001b[0m \u001b[38;5;66;03m# current_input is per-time input, so has shape (batch_size, input_size)\u001b[39;00m\n\u001b[0;32m--> 769\u001b[0m output, mpl_activities, db \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;66;03m# M updated internally when this is called\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mpl_idx, mp_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmp_layers):\n",
      "File \u001b[0;32m/allen/programs/mindscope/workgroups/auto-model/zihan.zhang/MultiTaskMPN/mpn.py:700\u001b[0m, in \u001b[0;36mDeepMultiPlasticNet.forward\u001b[0;34m(self, inputs, run_mode, verbose)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;66;03m# 2025-11-19: the x_t in the MPN paper is the \"input to MPN layer\"\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;66;03m# namely after the MLP \u001b[39;00m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layer_active:\n\u001b[0;32m--> 700\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW_initial_linear\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    701\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(x)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/mpn/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mpn/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/mpn/lib/python3.9/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_CUDA_mm)"
     ]
    }
   ],
   "source": [
    "# we use net at different training stage on the same test_input\n",
    "start_time = time.time()\n",
    "net, _, (counter_lst, netout_lst, db_lst, Winput_lst, Winputbias_lst,\\\n",
    "         Woutput_lst, Wall_lst, marker_lst, loss_lst, acc_lst), _ = net_helpers.train_network(params, device=device, verbose=verbose,\n",
    "                                                                                              train=train, hyp_dict=hyp_dict,\\\n",
    "                                                                                              netFunction=netFunction,\\\n",
    "                                                                                              test_input=[test_input, test_input_attractor],\n",
    "                                                                                              print_frequency=100)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Running Time: {end_time - start_time}\")\n",
    "counter_lst = [x * epoch_multiply + 1 for x in counter_lst] # avoid log plot issue    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07eee03-d879-44bc-89ef-60f11bdf721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyp_dict['chosen_network'] == \"dmpn\":\n",
    "    if net_params[\"input_layer_add\"]:\n",
    "        fignorm, axsnorm = plt.subplots(1,1,figsize=(4,4))\n",
    "        axsnorm.plot(counter_lst, [np.linalg.norm(Winput_matrix) for Winput_matrix in Winput_lst], \"-o\")\n",
    "        axsnorm.set_xscale(\"log\")\n",
    "        axsnorm.set_ylabel(\"Frobenius Norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f0805a-0938-48e5-976d-3682d45b4448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check, if W_freeze, then the recorded W matrix for the modulation layer should not be changed\n",
    "if net_params[\"ml_params\"][\"W_freeze\"]: \n",
    "    assert np.allclose(Wall_lst[-1][0], Wall_lst[0][0])\n",
    "\n",
    "if net_params[\"input_layer_bias\"]: \n",
    "    assert net_params[\"input_layer_add\"] is True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47256480-5b47-496e-887e-b8de35dcc8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    fig, ax = plt.subplots(1,1,figsize=(3,3))\n",
    "    ax.plot(net.hist['iters_monitor'][1:], net.hist['train_acc'][1:], color=c_vals[0], label='Full train accuracy')\n",
    "    ax.plot(net.hist['iters_monitor'][1:], net.hist['valid_acc'][1:], color=c_vals[1], label='Full valid accuracy')\n",
    "    if net.weight_reg is not None:\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['train_loss_output_label'], color=c_vals_l[0], zorder=-1, label='Output label')\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['train_loss_reg_term'], color=c_vals_l[0], zorder=-1, label='Reg term', linestyle='dashed')\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['valid_loss_output_label'], color=c_vals_l[1], zorder=-1, label='Output valid label')\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['valid_loss_reg_term'], color=c_vals_l[1], zorder=-1, label='Reg valid term', linestyle='dashed')\n",
    "    \n",
    "    # ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    # ax.set_ylabel('Loss ({})'.format(net.loss_type))\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_xlabel('# Batches')\n",
    "    plt.savefig(f\"./twotasks/loss_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "    \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e544aca8-271f-49d3-9ed3-ab7023f23600",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    net_helpers.net_eta_lambda_analysis(net, net_params, hyp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c10ab17-bab6-4228-9096-af042e9ac385",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_finalstage = False\n",
    "if use_finalstage:\n",
    "    # plotting output in the validation set\n",
    "    net_out, db = net.iterate_sequence_batch(test_input, run_mode='track_states')\n",
    "    W_output = net.W_output.detach().cpu().numpy()\n",
    "\n",
    "    W_all_ = []\n",
    "    for i in range(len(net.mp_layers)):\n",
    "        W_all_.append(net.mp_layers[i].W.detach().cpu().numpy())\n",
    "    W_ = W_all_[0]\n",
    "    \n",
    "else:\n",
    "    ind = len(marker_lst)-1 \n",
    "    # ind = 0\n",
    "    network_at_percent = (marker_lst[ind]+1)/train_params['n_datasets']*100\n",
    "    print(f\"Using network at {network_at_percent}%\")\n",
    "    # by default using the first test_input \n",
    "    net_out = netout_lst[0][ind]\n",
    "    db = db_lst[0][ind]\n",
    "    W_output = Woutput_lst[ind]\n",
    "    W_ = Wall_lst[ind][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8feb8c0-4029-49b1-b6e9-a5fd01ab5efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_input_output(test_input_np, net_out, test_output_np, test_task=None, tag=\"\", batch_num=5):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    test_input_np = helper.to_ndarray(test_input_np)\n",
    "    net_out = helper.to_ndarray(net_out)\n",
    "    test_output_np = helper.to_ndarray(test_output_np)\n",
    "    \n",
    "    fig_all, axs_all = plt.subplots(batch_num,2,figsize=(4*2,batch_num*2))\n",
    "    \n",
    "    if test_output_np.shape[-1] == 1:\n",
    "        for batch_idx, ax in enumerate(axs):\n",
    "            ax.plot(net_out[batch_idx, :, 0], color=c_vals[batch_idx])\n",
    "            ax.plot(test_output_np[batch_idx, :, 0], color=c_vals_l[batch_idx])\n",
    "    \n",
    "    else:\n",
    "        for batch_idx in range(batch_num):\n",
    "            for out_idx in range(test_output_np.shape[-1]):\n",
    "                axs_all[batch_idx,0].plot(net_out[batch_idx, :, out_idx], color=c_vals[out_idx], label=out_idx)\n",
    "                axs_all[batch_idx,0].plot(test_output_np[batch_idx, :, out_idx], color=c_vals_l[out_idx], linewidth=5, alpha=0.5)\n",
    "                if test_task is not None: \n",
    "                    axs_all[batch_idx,0].set_title(f\"{task_params['rules'][test_task[batch_idx]]}\")\n",
    "                axs_all[batch_idx,0].legend()\n",
    "    \n",
    "            input_batch = test_input_np[batch_idx,:,:]\n",
    "            if task_params[\"randomize_inputs\"]: \n",
    "                input_batch = input_batch @ np.linalg.pinv(task_params[\"randomize_matrix\"])\n",
    "            for inp_idx in range(input_batch.shape[-1]):\n",
    "                axs_all[batch_idx,1].plot(input_batch[:,inp_idx], color=c_vals[inp_idx], label=inp_idx)\n",
    "                if test_task is not None: \n",
    "                    axs_all[batch_idx,1].set_title(f\"{task_params['rules'][test_task[batch_idx]]}\")\n",
    "                axs_all[batch_idx,1].legend()\n",
    "\n",
    "    for ax in axs_all.flatten(): \n",
    "        ax.set_ylim([-2, 2])\n",
    "    fig_all.tight_layout()\n",
    "    fig_all.savefig(f\"./twotasks/lowD_{hyp_dict['ruleset']}_{hyp_dict['chosen_network']}_seed{seed}_{hyp_dict['addon_name']}_{tag}.png\", dpi=300)\n",
    "\n",
    "plot_input_output(test_input_np, net_out, test_output_np, test_task, tag=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a24480-1cee-46ca-9210-6fb310457468",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_input_output(test_input_attractor, netout_lst[1][ind], test_output_attractor, test_task_attractor, tag=\"long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beee7f6-5c64-48fb-9f21-697e5beeea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimensionality_measure(W):\n",
    "    \"\"\"\n",
    "    Dimensionality in recurrent spiking networks: Global trends in activity and local origins in\n",
    "    connectivity (Equation 3)\n",
    "    \"The dimensionality is a weighted measure of the number of axes explored by that cloud\"\n",
    "    Recanatesi, et al., 2019\n",
    "    return value in range of (0, 1]\n",
    "    \"\"\"\n",
    "    covW = np.cov(W)\n",
    "    assert covW.shape[0] == n_hidden\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(covW)\n",
    "    numerator = np.sum(eigenvalues) ** 2\n",
    "    denominator = np.sum(eigenvalues ** 2)\n",
    "    return (numerator / denominator) / W.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b78d25-9681-4c92-9fff-0cf68f6d1b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_resp, labels_stim = generate_response_stimulus(task_params_attractor, test_trials_attractor)\n",
    "labels_attractor = labels_stim if color_by == \"stim\" else labels_resp\n",
    "\n",
    "label_task_comb_attractor = []\n",
    "for i in range(len(labels_attractor)):\n",
    "    label_task_comb_attractor.append([labels_attractor[i][0], test_task_attractor[i]])\n",
    "label_task_comb_attractor = np.array(label_task_comb_attractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b513939c-1e16-45f1-97ea-8f71cf5dc4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_non_nan(arr, k):\n",
    "    \"\"\"\n",
    "    Pick `k` distinct (non-NaN) numbers from a 2-D NumPy array.\n",
    "    \"\"\"\n",
    "    pool = arr[~np.isnan(arr)]            # flatten & keep only real numbers\n",
    "    if k > pool.size:                     # ensure enough unique values\n",
    "        raise ValueError(\"k exceeds number of non-NaN entries.\")\n",
    "    return np.random.choice(pool, k, replace=False).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c23ea03-ed43-4b9c-a4ca-b922020fa4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_similarity(Ms_orig, hs, net_params, label_task_comb, checktime, compare=\"modulation\"): \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    inverse_modulation_ss_dt = []\n",
    "    inverse_modulation_sr_dt = []\n",
    "    inverse_modulation_st_ds = [[], []]\n",
    "    modulation_save = [[],[]]\n",
    "\n",
    "    # same stimulus (effectively anti-response), different task\n",
    "    for k in range(8):\n",
    "        ind1 = [i for i, lst in enumerate(label_task_comb) if np.array_equal(lst, [k, 0])]\n",
    "        ind2 = [i for i, lst in enumerate(label_task_comb) if np.array_equal(lst, [k, 1])]\n",
    "        ll = min(len(ind1), len(ind2))\n",
    "\n",
    "        if net_params[\"input_layer_add\"]:\n",
    "            win = net.W_initial_linear.weight.data.detach().cpu().numpy()\n",
    "        else: \n",
    "            win = None \n",
    "        \n",
    "        if compare == \"modulation\": \n",
    "            Ms1_change_stimulus = [((Ms_orig[ind1[i],checktime,:,:]) @ win)[:,0].flatten() if win is not None else (Ms_orig[ind1[i],checktime,:,:])[:,0].flatten() for i in range(ll)]\n",
    "            Ms2_change_stimulus = [((Ms_orig[ind2[i],checktime,:,:]) @ win)[:,0].flatten() if win is not None else (Ms_orig[ind2[i],checktime,:,:])[:,0].flatten() for i in range(ll)]\n",
    "        elif compare == \"hidden\": \n",
    "            Ms1_change_stimulus = [hs[ind1[i],checktime,:].flatten() for i in range(ll)]\n",
    "            Ms2_change_stimulus = [hs[ind2[i],checktime,:].flatten() for i in range(ll)]\n",
    "        \n",
    "        inverse_modulation_ss_dt.append(np.mean(\n",
    "            [1 - cosine(Ms1_change_stimulus[i], Ms2_change_stimulus[j]) for i in range(len(Ms1_change_stimulus)) for j in range(len(Ms2_change_stimulus))]\n",
    "        ))\n",
    "\n",
    "        modulation_save[0].append(Ms1_change_stimulus)\n",
    "        modulation_save[1].append(Ms2_change_stimulus)\n",
    "\n",
    "    # same response, different task \n",
    "    for k in range(8):\n",
    "        ind1 = [i for i, lst in enumerate(label_task_comb) if np.array_equal(lst, [k, 0])]\n",
    "        ind2 = [i for i, lst in enumerate(label_task_comb) if np.array_equal(lst, [(k + 4) % 8, 1])]\n",
    "        ll = min(len(ind1), len(ind2))\n",
    "\n",
    "        if compare == \"modulation\": \n",
    "            Ms1_change_stimulus = [((Ms_orig[ind1[i],checktime,:,:]) @ win)[:,0].flatten() if win is not None else ((Ms_orig[ind1[i],checktime,:,:]))[:,0].flatten() for i in range(ll)]\n",
    "            Ms2_change_stimulus = [((Ms_orig[ind2[i],checktime,:,:]) @ win)[:,0].flatten() if win is not None else ((Ms_orig[ind2[i],checktime,:,:]))[:,0].flatten() for i in range(ll)]\n",
    "        elif compare == \"hidden\": \n",
    "            Ms1_change_stimulus = [hs[ind1[i],checktime,:].flatten() for i in range(ll)]\n",
    "            Ms2_change_stimulus = [hs[ind2[i],checktime,:].flatten() for i in range(ll)]\n",
    "        \n",
    "        inverse_modulation_sr_dt.append(np.mean(\n",
    "            [1 - cosine(Ms1_change_stimulus[i], Ms2_change_stimulus[j]) for i in range(len(Ms1_change_stimulus)) for j in range(len(Ms2_change_stimulus))]\n",
    "        ))\n",
    "\n",
    "    # same task, different stimulus \n",
    "    repeat = 100\n",
    "    modulation_matrices_all = [] \n",
    "    for _ in range(repeat): \n",
    "        modulation_matrices = [\n",
    "            np.full((len(modulation_save[0]), len(modulation_save[0])), np.nan), \n",
    "            np.full((len(modulation_save[0]), len(modulation_save[0])), np.nan)\n",
    "        ]\n",
    "        for i in range(len(modulation_save[0])):\n",
    "            for j in range(i+1, len(modulation_save[0])):\n",
    "                modulation_matrices[0][i,j] = 1 - cosine(random.choice(modulation_save[0][i]), random.choice(modulation_save[0][j]))\n",
    "                modulation_matrices[1][i,j] = 1 - cosine(random.choice(modulation_save[1][i]), random.choice(modulation_save[1][j]))\n",
    "                \n",
    "        modulation_matrices_all.append([np.nanmean(sample_non_nan(modulation_matrices[0], 8)),\n",
    "                                        np.nanmean(sample_non_nan(modulation_matrices[1], 8))\n",
    "                                       ])\n",
    "\n",
    "    modulation_matrices_all = np.array(modulation_matrices_all)\n",
    "\n",
    "    result = [[np.mean(inverse_modulation_ss_dt), np.std(inverse_modulation_ss_dt)], \\\n",
    "              [np.mean(inverse_modulation_sr_dt), np.std(inverse_modulation_sr_dt)], \\\n",
    "               # helper.sample_upper_means(modulation_matrices[0], k=8, n_iter=10), \\\n",
    "               # helper.sample_upper_means(modulation_matrices[1], k=8, n_iter=10)\n",
    "               [np.mean(modulation_matrices_all[:,0]), np.std(modulation_matrices_all[:,0])], \n",
    "               [np.mean(modulation_matrices_all[:,1]), np.std(modulation_matrices_all[:,1])]\n",
    "              \n",
    "             ]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1f92a0-c04f-4a7a-8b08-e753efb1b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here db is selected based on learning stage selection \n",
    "\n",
    "layer_index = 0 # 1 layer MPN \n",
    "if net_params[\"input_layer_add\"]:\n",
    "    layer_index += 1 \n",
    "    \n",
    "def modulation_extraction(test_input, db, layer_index, cuda=False):\n",
    "    \"\"\"\n",
    "    Extracts modulation tensors from `db` and returns:\n",
    "        Ms:      (batch, seq, features) reshaped version of M\n",
    "        Ms_orig: original M (no reshape)\n",
    "        hs:      (batch, seq, features) reshaped version of hidden\n",
    "        bs:      bias vector/matrix as-is (or concatenated if list)\n",
    "    \"\"\"\n",
    "\n",
    "    def _to_numpy(x):\n",
    "        # Convert torch.Tensor -> numpy, otherwise np.asarray\n",
    "        try:\n",
    "            import torch\n",
    "            if isinstance(x, torch.Tensor):\n",
    "                return x.detach().cpu().numpy()\n",
    "        except Exception:\n",
    "            pass\n",
    "        return np.asarray(x)\n",
    "\n",
    "    def _concat_last(x):\n",
    "        # If list/tuple of arrays: concatenate on last axis; else return as-is\n",
    "        return np.concatenate(x, axis=-1) if isinstance(x, (list, tuple)) else x\n",
    "\n",
    "    n_batch, max_seq_len = test_input.shape[0], test_input.shape[1]\n",
    "\n",
    "    # M\n",
    "    M_raw = _concat_last(_to_numpy(db[f\"M{layer_index}\"]))\n",
    "    Ms = M_raw.reshape(n_batch, max_seq_len, -1)\n",
    "    Ms_orig = M_raw  # unreshaped\n",
    "\n",
    "    # b\n",
    "    bs = _concat_last(_to_numpy(db[f\"b{layer_index}\"]))\n",
    "\n",
    "    # hidden\n",
    "    H_raw = _concat_last(_to_numpy(db[f\"hidden{layer_index}\"]))\n",
    "    hs = H_raw.reshape(n_batch, max_seq_len, -1)\n",
    "\n",
    "    return Ms, Ms_orig, hs, bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a350fae9-ae28-4341-a5e9-a6d98bd227f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# across training stage\n",
    "result_attractor_all = [] \n",
    "pr_all = [] \n",
    "time_stamps = {} \n",
    "for db_attractor in db_lst[1]:\n",
    "    _, M_attractor, h_attractor, _, = modulation_extraction(test_input_attractor, db_attractor, layer_index)\n",
    "\n",
    "    prs = [dimensionality_measure(h_attractor[i,:,:].T) for i in range(h_attractor.shape[0])]\n",
    "    pr_all.append([np.mean(prs), np.std(prs)])\n",
    "    \n",
    "    # to handle noise, find the time when fixation is off\n",
    "    checktime_sample = test_input_attractor[0,:,0].detach().cpu()\n",
    "    mask = checktime_sample < 0.5                          \n",
    "    idx = torch.nonzero(mask, as_tuple=False) \n",
    "    checktime_attractor = idx[0].item()  \n",
    "\n",
    "    time_stamps[\"delay_end\"] = checktime_attractor - 2 # a little bit before the fixation off\n",
    "    \n",
    "    result_attractor = analyze_similarity(M_attractor, h_attractor, net_params, label_task_comb_attractor, \\\n",
    "                                          checktime=checktime_attractor, compare=\"hidden\")\n",
    "\n",
    "    result_attractor_all.append(result_attractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77604aca-9117-47ac-897e-f917df3bb60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_all = np.array(pr_all)\n",
    "figpr, axspr = plt.subplots(1,1,figsize=(6,3))\n",
    "axspr.plot(counter_lst, pr_all[:,0], \"-o\", color=c_vals[0])\n",
    "axspr.fill_between(counter_lst, pr_all[:,0]-pr_all[:,1], pr_all[:,0]+pr_all[:,1], color=c_vals_l[0], alpha=0.5)\n",
    "axspr.set_xscale(\"log\")\n",
    "axspr.set_xlabel(\"# Dataset\", fontsize=15)\n",
    "axspr.set_ylabel(\"Normalized \\nParticipation Ratio\", fontsize=15)\n",
    "axspr.tick_params(axis=\"y\", labelsize=12)\n",
    "axspr.tick_params(axis=\"x\", labelsize=12)\n",
    "figpr.savefig(f\"./twotasks/pr_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397a4952-20cc-4e7d-81da-4513ac52a37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "figattractor, axsattractor = plt.subplots(1,1,figsize=(4,4))\n",
    "break_names = [\"same stimulus\", \"same response\", \"task 1 different stimulus\", \"task 2 different stimulus\"]\n",
    "for i in range(len(result_attractor_all[0])): \n",
    "    mean, std = [rs[i][0] for rs in result_attractor_all], [rs[i][1] for rs in result_attractor_all]\n",
    "    axsattractor.plot(counter_lst, mean, \"-o\", color=c_vals[i], label=f\"{break_names[i]}\")\n",
    "    axsattractor.fill_between(counter_lst, [mean[i]-std[i] for i in range(len(mean))],\\\n",
    "                              [mean[i]+std[i] for i in range(len(mean))], alpha=0.5, color=c_vals_l[i])\n",
    "axsattractor.set_xscale(\"log\")\n",
    "axsattractor.legend()\n",
    "figattractor.savefig(f\"./twotasks/attractor_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa1f3a6-af5b-445c-8cab-766721181c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# across different timestamp \n",
    "stimulus_end = None \n",
    "chosen_batch = 0\n",
    "while stimulus_end is None: \n",
    "    try: \n",
    "        input_part = test_input_attractor[chosen_batch,:,2:2+4].detach().cpu().numpy()\n",
    "        input_part_sum = np.sum(input_part, axis=1)\n",
    "        stimulus_end = np.where(input_part_sum > 0.5)[0][-1]\n",
    "        stimulus_start = np.where(input_part_sum > 0.5)[0][0] - 1\n",
    "    except IndexError: \n",
    "        chosen_batch += 1\n",
    "\n",
    "time_stamps[\"stimulus_start\"] = stimulus_start\n",
    "time_stamps[\"stimulus_end\"] = stimulus_end\n",
    "time_stamps[\"delay_start\"] = stimulus_end + 1\n",
    "time_stamps[\"trial_end\"] = len(input_part_sum) - 1\n",
    "\n",
    "_, M_attractor_end, h_attractor_end, _ = modulation_extraction(test_input_attractor, db_lst[1][-1], layer_index)\n",
    "result_attractor_end_all = {} \n",
    "for key in time_stamps.keys(): \n",
    "    result_attractor = analyze_similarity(M_attractor_end, h_attractor_end, net_params, label_task_comb_attractor, \\\n",
    "                                      checktime=time_stamps[key], compare=\"hidden\")\n",
    "    result_attractor_end_all[key] = result_attractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2c47a8-ba5a-4587-afe3-046fd2101578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# _, M_attractor_end, h_attractor_end, _ = modulation_extraction(test_input_attractor, \n",
    "#                                                                db_lst[1][-1], layer_index)\n",
    "\n",
    "# fig, axs = plt.subplots(2,2,figsize=(4*2,4*2))\n",
    "# for batch_iter in range(h_attractor_end.shape[0]): \n",
    "#     h_norms, m_norms = [], []\n",
    "#     h_corr, m_corr = [], [] \n",
    "#     for time_iter in range(1, h_attractor_end.shape[1]): \n",
    "#         h_norms.append(np.linalg.norm(h_attractor_end[batch_iter, time_iter, :]))\n",
    "#         m_norms.append(np.linalg.norm(M_attractor_end[batch_iter, time_iter, :, :]))\n",
    "#         h_corr.append(np.linalg.norm(h_attractor_end[batch_iter, time_iter-1, :] - h_attractor_end[batch_iter, time_iter, :]))\n",
    "#         m_corr.append(np.linalg.norm(M_attractor_end[batch_iter, time_iter-1, :, :].flatten() - \n",
    "#                                   M_attractor_end[batch_iter, time_iter, :, :].flatten()))\n",
    "        \n",
    "#     axs[0,0].plot(h_norms, color=c_vals[label_task_comb_attractor[batch_iter, 1]], alpha=0.1) \n",
    "#     axs[1,0].plot(m_norms, color=c_vals[label_task_comb_attractor[batch_iter, 1]], alpha=0.1)\n",
    "#     axs[0,1].plot(h_corr, color=c_vals[label_task_comb_attractor[batch_iter, 1]], alpha=0.1) \n",
    "#     axs[1,1].plot(m_corr, color=c_vals[label_task_comb_attractor[batch_iter, 1]], alpha=0.1)\n",
    "    \n",
    "#     for ax in axs.flatten(): \n",
    "#         ax.set_xlim([time_stamps[\"delay_start\"], time_stamps[\"delay_end\"]])\n",
    "#         ax.set_xlabel(\"Time Steps since Delay Start\", fontsize=15)\n",
    "\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(f\"./twotasks/attractor_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fdde58-10af-4ab3-b0ab-324be443f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "figattractorend, axsattractorend = plt.subplots(1,1,figsize=(6,6))\n",
    "for i in range(len(result_attractor_end_all[\"trial_end\"])): \n",
    "    mean, std = [rs[i][0] for rs in result_attractor_end_all.values()], [rs[i][1] for rs in result_attractor_end_all.values()]\n",
    "    stages_counter = [i for i in range(len(result_attractor_end_all))]\n",
    "    axsattractorend.plot(stages_counter, mean, \"-o\", color=c_vals[i], label=f\"{break_names[i]}\")\n",
    "    axsattractorend.fill_between(stages_counter, [mean[i]-std[i] for i in range(len(mean))],\\\n",
    "                              [mean[i]+std[i] for i in range(len(mean))], alpha=0.5, color=c_vals_l[i])\n",
    "axsattractorend.set_xticks(stages_counter)\n",
    "axsattractorend.set_xticklabels(list(result_attractor_end_all.keys()), rotation=45, ha=\"right\", fontsize=15)\n",
    "axsattractorend.legend(fontsize=15, frameon=True, loc=\"best\")\n",
    "axsattractorend.set_ylabel(\"Cosine Similarity of Modulation\", fontsize=15)\n",
    "figattractorend.tight_layout()\n",
    "figattractorend.savefig(f\"./twotasks/attractor_stage_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92674a0a-b92a-487d-8af0-d1a110de50fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def input_interpolation(test_input_attractor, test_output_attractor, label_task_comb_attractor, expand_stimulus=True):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    assert test_input_attractor.shape[0] == label_task_comb_attractor.shape[0] \n",
    "    pro_task, anti_task = {}, {} \n",
    "    pro_task_answer, anti_task_answer = {}, {} \n",
    "    for k in range(8): \n",
    "        ind1 = [i for i, lst in enumerate(label_task_comb_attractor) if np.array_equal(lst, [k, 0])]\n",
    "        ind1_sample = ind1[0]\n",
    "        pro_task[k] = test_input_attractor[ind1_sample,:,:]\n",
    "        pro_task_answer[k] = test_output_attractor[ind1_sample,:,:]\n",
    "\n",
    "        ind2 = [i for i, lst in enumerate(label_task_comb_attractor) if np.array_equal(lst, [k, 1])]\n",
    "        ind2_sample = ind2[0]\n",
    "        anti_task[k] = test_input_attractor[ind2_sample,:,:]\n",
    "        anti_task_answer[k] = test_output_attractor[ind2_sample,:,:]\n",
    "\n",
    "    # expand with some unseen stimulus\n",
    "    if expand_stimulus:\n",
    "        base_len = len(pro_task)          # original size (8)\n",
    "        for i in range(base_len):\n",
    "            i1, i2 = i % 8, (i + 1) % 8   # wrap-around indexing\n",
    "    \n",
    "            # input dictionaries\n",
    "            pro_task[base_len + i]  = (pro_task[i1]        + pro_task[i2])        / 2\n",
    "            anti_task[base_len + i] = (anti_task[i1]       + anti_task[i2])       / 2\n",
    "    \n",
    "            # answer dictionaries\n",
    "            pro_task_answer[base_len + i]  = (pro_task_answer[i1]        + pro_task_answer[i2])        / 2\n",
    "            anti_task_answer[base_len + i] = (anti_task_answer[i1]       + anti_task_answer[i2])       / 2\n",
    "\n",
    "        # re-sort the input and output in an interleaved way \n",
    "        interleaved_keys = [k for pair in zip(range(base_len), range(base_len, 2*base_len)) for k in pair]\n",
    "\n",
    "        pro_task = {k: pro_task[k] for k in interleaved_keys}\n",
    "        anti_task = {k: anti_task[k] for k in interleaved_keys}\n",
    "        pro_task_answer = {k: pro_task_answer[k] for k in interleaved_keys}\n",
    "        anti_task_answer = {k: anti_task_answer[k] for k in interleaved_keys}\n",
    "\n",
    "    \n",
    "    n = 10 \n",
    "    alpha_lst = [i/n for i in range(n+1)]\n",
    "\n",
    "    stacked_pro = torch.stack([pro_task[k] for k in sorted(pro_task)]) \n",
    "    stacked_anti = torch.stack([anti_task[k] for k in sorted(anti_task)])\n",
    "    stacked_pro_answer = torch.stack([pro_task_answer[k] for k in sorted(pro_task_answer)]) \n",
    "    stacked_anti_answer = torch.stack([anti_task_answer[k] for k in sorted(anti_task_answer)])\n",
    "    \n",
    "    stacked_interpolation = [alpha_lst[i] * stacked_pro + (1 - alpha_lst[i]) * stacked_anti for i in range(len(alpha_lst))]\n",
    "    stacked_interpolation_ans = [alpha_lst[i] * stacked_pro_answer + (1 - alpha_lst[i]) * stacked_anti_answer for i in range(len(alpha_lst))]               \n",
    "\n",
    "    return alpha_lst, stacked_interpolation, stacked_interpolation_ans\n",
    "\n",
    "alpha_lst, stacked_interpolation, stacked_interpolation_answer = input_interpolation(test_input_attractor, \\\n",
    "                                                                                     test_output_attractor, \\\n",
    "                                                                                     label_task_comb_attractor, \\\n",
    "                                                                                     expand_stimulus=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661cad69-fc25-4a29-9823-0a0712ea4352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 Red, 1 blue, 2 green, 3 purple, 4 orange, 5 teal, 6 gray, 7 pink, 8 yellow\n",
    "names = [\"hidden\", \"modulation\"]\n",
    "projected_data_all = [] \n",
    "\n",
    "for name in names:\n",
    "    fighs, axshs = plt.subplots(1,3,figsize=(5*3,5*1))\n",
    "    \n",
    "    PCA_downsample = 3\n",
    "        \n",
    "    Ms, Ms_orig, hs, bs = modulation_extraction(test_input_attractor, db_lst[1][-1], layer_index)\n",
    "    batch_num = Ms_orig.shape[0]\n",
    "    \n",
    "    if name == \"modulation\": \n",
    "        data = Ms\n",
    "    elif name == \"hidden\":\n",
    "        data = hs \n",
    "        \n",
    "    print(f\"data.shape: {data.shape}\")\n",
    "    \n",
    "    pca = PCA(n_components = PCA_downsample, random_state=42)\n",
    "    n_activity = data.shape[-1] \n",
    "    print(f\"n_activity: {n_activity}\")\n",
    "    activity_zero = np.zeros((1, n_activity))\n",
    "    \n",
    "    mask_task1 = label_task_comb_attractor[:,1] == 1\n",
    "    mask_task0 = label_task_comb_attractor[:,0] == 0\n",
    "        \n",
    "    as_flat_task1_delay = data[mask_task1][:,time_stamps[\"delay_start\"]:time_stamps[\"delay_end\"],:].reshape((-1, n_activity))\n",
    "    as_flat_delay = data[:,time_stamps[\"delay_start\"]:time_stamps[\"delay_end\"],:].reshape((-1, n_activity))\n",
    "    as_flat_stimulus = data[:,time_stamps[\"stimulus_start\"]:time_stamps[\"stimulus_end\"],:].reshape((-1, n_activity))\n",
    "    \n",
    "    as_flat = data.reshape((-1, n_activity))    \n",
    "    pca.fit(as_flat)\n",
    "    \n",
    "    total_ev_training = pca.explained_variance_ratio_.sum()  \n",
    "    print(total_ev_training)\n",
    "\n",
    "    if name == \"hidden\": \n",
    "        wout = net.W_output.detach().cpu().numpy() \n",
    "        wout_proj = pca.transform(wout) \n",
    "    \n",
    "    as_pca = pca.transform(as_flat)\n",
    "    projected_data = as_pca.reshape((data.shape[0], data.shape[1], -1))\n",
    "    print(projected_data.shape)\n",
    "    projected_data_all.append(projected_data)\n",
    "    zeros_pca = pca.transform(activity_zero)\n",
    "    \n",
    "    combination = [[0,1],[0,2],[1,2]]\n",
    "    \n",
    "    for i in range(batch_num):\n",
    "        data_batch = projected_data[i,:,:]\n",
    "        if label_task_comb_attractor[i,1] in (0,1,): \n",
    "            for index, comb in enumerate(combination):\n",
    "                axshs[index].plot(data_batch[time_stamps[\"stimulus_start\"]:time_stamps[\"trial_end\"],comb[0]], data_batch[time_stamps[\"stimulus_start\"]:time_stamps[\"trial_end\"],comb[1]], c=c_vals[label_task_comb_attractor[i,0]], \\\n",
    "                                         linestyle=linestyles[label_task_comb_attractor[i,1]], alpha=0.01)\n",
    "                axshs[index].scatter(data_batch[time_stamps[\"stimulus_start\"]:time_stamps[\"delay_start\"],comb[0]], data_batch[time_stamps[\"stimulus_start\"]:time_stamps[\"delay_start\"],comb[1]], c=c_vals[label_task_comb_attractor[i,0]], \\\n",
    "                                         marker=markers_vals[1], alpha=0.01)\n",
    "                axshs[index].scatter(data_batch[time_stamps[\"delay_start\"]:time_stamps[\"delay_end\"],comb[0]], data_batch[time_stamps[\"delay_start\"]:time_stamps[\"delay_end\"],comb[1]], c=c_vals[label_task_comb_attractor[i,0]], \\\n",
    "                                         marker=markers_vals[2], alpha=0.01)\n",
    "                axshs[index].scatter(data_batch[time_stamps[\"delay_end\"]:time_stamps[\"trial_end\"],comb[0]], data_batch[time_stamps[\"delay_end\"]:time_stamps[\"trial_end\"],comb[1]], c=c_vals[label_task_comb_attractor[i,0]], \\\n",
    "                                         marker=markers_vals[3], alpha=0.01)\n",
    "                axshs[index].set_xlabel(f\"PCA {comb[0]+1}\")\n",
    "                axshs[index].set_ylabel(f\"PCA {comb[1]+1}\")\n",
    "        \n",
    "    fighs.tight_layout()\n",
    "    fighs.savefig(f\"./twotasks/m_pca_{name}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b86079-6e62-4093-8c68-dc94c3afcf3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "for ind, projected_data in enumerate(projected_data_all): \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for i in range(batch_num):\n",
    "        data_batch = projected_data[i, :time_stamps[\"delay_end\"], :]\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=data_batch[:,0], y=data_batch[:,1], z=data_batch[:,2],\n",
    "                mode=\"lines\",\n",
    "                line=dict(width=2, color=c_vals[label_task_comb_attractor[i,0]]),\n",
    "                opacity=0.5,\n",
    "                showlegend=False           \n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # origin point\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=[zeros_pca[0, 0]], y=[zeros_pca[0, 1]], z=[zeros_pca[0, 2]],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=4, color=\"black\"),\n",
    "            showlegend=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    zero_pt = zeros_pca[0]\n",
    "    print(zero_pt.shape)\n",
    "    \n",
    "    # define the two spanning vesample_non_nanctors (from the origin)\n",
    "    v1 = wout_proj[0,:]\n",
    "    v2 = wout_proj[1,:]\n",
    "    \n",
    "    # pick a side-length that matches your datas overall scale\n",
    "    traj = projected_data[:, :time_stamps[\"delay_end\"], :].reshape(-1, 3)\n",
    "    plane_half = 0.5 * np.linalg.norm(traj - zero_pt, axis=1).max()           \n",
    "    \n",
    "    # build an (almost) orthonormal basis in the v1v2 plane\n",
    "    u_hat = v1 / np.linalg.norm(v1)\n",
    "    v2_proj = v2 - v2.dot(u_hat) * u_hat      \n",
    "    v_hat = v2_proj / np.linalg.norm(v2_proj)\n",
    "    \n",
    "    # four corners of a square patch centred at the origin\n",
    "    corners = np.array([\n",
    "        -plane_half*u_hat - plane_half*v_hat,\n",
    "         plane_half*u_hat - plane_half*v_hat,\n",
    "         plane_half*u_hat + plane_half*v_hat,\n",
    "        -plane_half*u_hat + plane_half*v_hat\n",
    "    ])\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Mesh3d(\n",
    "            x=corners[:, 0],\n",
    "            y=corners[:, 1],\n",
    "            z=corners[:, 2],\n",
    "            i=[0, 0],\n",
    "            j=[1, 2],\n",
    "            k=[2, 3],\n",
    "            opacity=0.25,\n",
    "            color=\"lightblue\",\n",
    "            name=\"spanning plane\",\n",
    "            showscale=False\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title=\"PCA 1\",\n",
    "            yaxis_title=\"PCA 2\",\n",
    "            zaxis_title=\"PCA 3\"\n",
    "        ),\n",
    "        width=800,          \n",
    "        height=800,\n",
    "        margin=dict(l=0, r=0, t=40, b=0),\n",
    "        showlegend=False     \n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "    if ind == 0: \n",
    "        endpoints = projected_data[:,time_stamps[\"delay_end\"]+1,:]\n",
    "        figproj, axproj = plt.subplots(1,1,figsize=(4,4))\n",
    "        for ei in range(endpoints.shape[0]):\n",
    "            endpoint = endpoints[ei,:] - zero_pt\n",
    "            u_coord = endpoint.dot(u_hat)\n",
    "            v_coord = endpoint.dot(v_hat) \n",
    "            endpoint_proj  = zero_pt + u_coord*u_hat + v_coord*v_hat\n",
    "            \n",
    "            if label_task_comb_attractor[ei,1] == 0: \n",
    "                color_index = label_task_comb_attractor[ei,0] \n",
    "            else: \n",
    "                color_index = (label_task_comb_attractor[ei,0] + 4) % 8 \n",
    "    \n",
    "            axproj.scatter(u_coord, v_coord, c=c_vals[color_index], alpha=0.1)\n",
    "            \n",
    "        figproj.show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cf0129-92db-45e7-82d6-4a3f53e164fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import SymLogNorm\n",
    "from scipy.spatial import ConvexHull   # only needed for 3-D volume\n",
    "\n",
    "def ring_length(pts: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    diffs = np.diff(pts, axis=0, append=pts[:1])   # close the loop\n",
    "    return np.linalg.norm(diffs, axis=1).sum()\n",
    "\n",
    "def ring_volume_3d(pts: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if pts.shape[1] != 3:\n",
    "        raise ValueError(\"ring_volume_3d expects a 3-D point set.\")\n",
    "\n",
    "    hull = ConvexHull(pts)                # triangulated convex surface\n",
    "    return hull.volume                    # signed; take abs if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93f208d-53fa-49c5-ba77-82ddefbb3812",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 0 Red, 1 blue, 2 green, 3 purple, 4 orange, 5 teal, 6 gray, 7 pink, 8 yellow\n",
    "names = [\"hidden\", \"modulation\"]\n",
    "\n",
    "raw_data_ring = [[], []] \n",
    "raw_data_ring_magnitude = [[], []]\n",
    "projected_data_ring = [[], []]\n",
    "\n",
    "for name in names:\n",
    "    fighs, axshs = plt.subplots(1,3,figsize=(5*3,5*1))\n",
    "    fighsadd, axshsadd = plt.subplots(1,3,figsize=(5*3,5*1))\n",
    "    \n",
    "    fig3dfix = go.Figure()\n",
    "    \n",
    "    PCA_downsample = 3\n",
    "    combination = [[0,1],[0,2],[1,2]]\n",
    "    \n",
    "    interpolation_label = [i for i in range(len(stacked_interpolation[0]))]\n",
    "    print(interpolation_label)\n",
    "    \n",
    "    def numbered_markers(n):\n",
    "        \"\"\"\n",
    "        Return a list ['\\$0\\$', '\\$1\\$', ... '\\$(n-1)\\$'] that Matplotlib accepts\n",
    "        as per-point marker styles.\n",
    "        \"\"\"\n",
    "        return [f'${i}$' for i in range(n)]\n",
    "    \n",
    "    marker_new = numbered_markers(len(stacked_interpolation))\n",
    "    \n",
    "    projected_data_fix_all = []\n",
    "    \n",
    "    for (int_index, int_input) in enumerate(stacked_interpolation): \n",
    "        stack_output, _, db_intp = net.iterate_sequence_batch(int_input, run_mode='track_states')\n",
    "\n",
    "        Ms, Ms_orig, hs, bs = modulation_extraction(int_input, db_intp, layer_index, cuda=True)\n",
    "        batch_num = Ms_orig.shape[0]\n",
    "    \n",
    "        if name == \"hidden\": \n",
    "            data = hs\n",
    "        elif name == \"modulation\": \n",
    "            data = Ms\n",
    "        print(f\"data.shape: {data.shape}\")\n",
    "        n_activity = data.shape[-1]\n",
    "\n",
    "        # extract the delay period information\n",
    "        as_flat_delay_ = data[:,time_stamps[\"delay_start\"]:time_stamps[\"delay_end\"],:]\n",
    "        as_flat_delay = as_flat_delay_.reshape((-1, n_activity))\n",
    "\n",
    "        # fixed point in original dimension\n",
    "        as_flat_fixedpoint_raw = data[:,time_stamps[\"delay_end\"],:]\n",
    "\n",
    "        raw_data_ring[names.index(name)].append(ring_length(as_flat_fixedpoint_raw))\n",
    "        fixpt_norm = np.linalg.norm(as_flat_fixedpoint_raw, axis=1)\n",
    "        raw_data_ring_magnitude[names.index(name)].append(fixpt_norm.mean())\n",
    "        \n",
    "        as_flat = data.reshape((-1, n_activity))\n",
    "    \n",
    "        if int_index == 0: \n",
    "            print(\"Generate New PCA axes\")\n",
    "            pca_delay = PCA(n_components = PCA_downsample, random_state=42)\n",
    "            activity_zero = np.zeros((1, n_activity))\n",
    "            pca_delay.fit(as_flat_delay) \n",
    "        \n",
    "        as_pca = pca_delay.transform(as_flat)\n",
    "        projected_data = as_pca.reshape((data.shape[0], data.shape[1], -1))\n",
    "    \n",
    "        projected_data_fix = projected_data[:,time_stamps[\"delay_end\"],:]\n",
    "\n",
    "        projected_data_ring[names.index(name)].append(ring_volume_3d(projected_data_fix))\n",
    "        \n",
    "        projected_data_fix_all.append(projected_data_fix)\n",
    "        \n",
    "        for i in range(batch_num):\n",
    "            data_batch = projected_data_fix[i,:]\n",
    "            for index, comb in enumerate(combination):\n",
    "                marker_value = marker_new[int_index] if int_index == 0 or int_index == len(stacked_interpolation)-1 else \"o\"\n",
    "                alpha_value = 0.1 if marker_value == \"o\" else 1.0\n",
    "                \n",
    "                axshs[index].scatter(data_batch[comb[0]], data_batch[comb[1]], c=c_vals[interpolation_label[i]], \\\n",
    "                                         marker=marker_value, alpha=alpha_value)\n",
    "                axshs[index].set_xlabel(f\"PCA {comb[0]+1}\")\n",
    "                axshs[index].set_ylabel(f\"PCA {comb[1]}\")\n",
    "    \n",
    "    for index, comb in enumerate(combination):\n",
    "        select1 = [pa[:,comb[0]] for pa in projected_data_fix_all] \n",
    "        min_select1 = min(arr.min() for arr in select1)\n",
    "        \n",
    "        select2 = [pa[:,comb[1]] for pa in projected_data_fix_all] \n",
    "        min_select2 = min(arr.min() for arr in select2) \n",
    "\n",
    "        epsilon = 1 if name == \"hidden\" else 10\n",
    "        min_select1 -= epsilon\n",
    "        min_select2 -= epsilon\n",
    "        \n",
    "        indices_lst = [0, 1, 2, -1] \n",
    "        for it_idx, it in enumerate(indices_lst):\n",
    "            xy = projected_data_fix_all[it][:,[comb[0],comb[1]]]  \n",
    "            num_xy = xy.shape[0] \n",
    "\n",
    "            for xy_index in range(num_xy): \n",
    "                axshsadd[index].plot([xy[xy_index%num_xy,0], xy[(xy_index+1)%num_xy,0]],\\\n",
    "                                   [xy[xy_index%num_xy,1], xy[(xy_index+1)%num_xy,1]],\\\n",
    "                                   linestyle=\"--\", linewidth=3, color=c_vals_l[it_idx])\n",
    "    \n",
    "        for i in range(len(interpolation_label)): \n",
    "            fixed_points = np.array([projected_data_fix[i,:] for projected_data_fix in projected_data_fix_all])\n",
    "            axshsadd[index].plot(fixed_points[:,comb[0]],\\\n",
    "                                 fixed_points[:,comb[1]],\\\n",
    "                                 \"-o\", c=c_vals[interpolation_label[i]]\n",
    "                                )\n",
    "            \n",
    "            # axshsadd[index].set_title(f\"Shift: {min_select1:2f};{min_select2:2f}\")\n",
    "\n",
    "            if index == 0:\n",
    "                fig3dfix.add_trace(\n",
    "                    go.Scatter3d(\n",
    "                        x=np.array(alpha_lst), y=fixed_points[:,0], z=fixed_points[:,1],\n",
    "                        mode=\"lines\",\n",
    "                        line=dict(width=2, color=c_vals[interpolation_label[i]]),\n",
    "                        opacity=0.5,\n",
    "                        name=f\"S{i}\",\n",
    "                        showlegend=True\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    # for ax in axshs: \n",
    "    #     ax.set_xscale('symlog')   # region |x| < 1 is kept linear\n",
    "    #     ax.set_yscale('symlog')\n",
    "\n",
    "    # for ax in axshsadd: \n",
    "    #     ax.set_xscale('log')\n",
    "    #     ax.set_yscale('log')\n",
    "\n",
    "\n",
    "    fighs.suptitle(name)\n",
    "    fighs.tight_layout()\n",
    "    fighs.savefig(f\"./twotasks/m_pca_attractor_{name}_seed{seed}_{hyp_dict['addon_name']}_{int_index}.png\", dpi=300)\n",
    "\n",
    "    fighsadd.suptitle(name) \n",
    "    fighsadd.tight_layout() \n",
    "    fighsadd.savefig(f\"./twotasks/m_pca_attractor_cycle_{name}_seed{seed}_{hyp_dict['addon_name']}_{int_index}.png\", dpi=300)\n",
    "\n",
    "    fig3dfix.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title=\"Alpha\",\n",
    "            yaxis_title=\"PCA 1\",\n",
    "            zaxis_title=\"PCA 2\"\n",
    "        ),\n",
    "        width=700,          \n",
    "        height=700,\n",
    "        margin=dict(l=0, r=0, t=40, b=0),\n",
    "        showlegend=True     \n",
    "    )\n",
    "    fig3dfix.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82da37f1-2688-4d70-bca0-1e126f224cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=(6*2,3))\n",
    "axs[0].plot(alpha_lst, raw_data_ring[0], \"-o\", color=c_vals[0], label=\"Hidden Ring Perimeter\")\n",
    "axs[0].plot(alpha_lst, raw_data_ring[1], \"-o\", color=c_vals[1], label=\"Modulation Ring Perimeter\")\n",
    "axs[1].plot(alpha_lst, projected_data_ring[0], \"-o\", color=c_vals[0], label=\"Hidden Ring Volume\")\n",
    "axs[1].plot(alpha_lst, projected_data_ring[1], \"-o\", color=c_vals[1], label=\"Modulation Ring Volume\")\n",
    "for ax in axs: \n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.legend(fontsize=15, frameon=True, loc=\"best\")\n",
    "    ax.set_xlabel(\"Interpolation Level\", fontsize=15)\n",
    "    ax.tick_params(axis=\"y\", labelsize=12)\n",
    "    ax.tick_params(axis=\"x\", labelsize=12)\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"./twotasks/m_pca_ring_{name}_seed{seed}_{hyp_dict['addon_name']}_{int_index}.png\", dpi=300)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(6,3))\n",
    "ax.plot(alpha_lst, raw_data_ring_magnitude[0], \"-o\", color=c_vals[0], label=\"Hidden Average Magnitude\")\n",
    "ax.plot(alpha_lst, raw_data_ring_magnitude[1], \"-o\", color=c_vals[1], label=\"Modulation Average Magnitude\")\n",
    "ax.legend(fontsize=15, frameon=True, loc=\"best\")\n",
    "ax.set_xlabel(\"Interpolation Level\")\n",
    "fig.savefig(f\"./twotasks/m_pca_ring_magnitude_{name}_seed{seed}_{hyp_dict['addon_name']}_{int_index}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329a9013-01bd-4a13-9d2b-2d1e507c7d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 Red, 1 blue, 2 green, 3 purple, 4 orange, 5 teal, 6 gray, 7 pink, 8 yellow\n",
    "fig3dresponse_cos = go.Figure()\n",
    "fig3dresponse_sin = go.Figure() \n",
    "fig3dresponse = [fig3dresponse_cos, fig3dresponse_sin]\n",
    "\n",
    "N = len(stacked_interpolation)\n",
    "\n",
    "wout = net.W_output.detach().cpu().numpy()\n",
    "print(wout.shape)\n",
    "\n",
    "name = \"hidden\" \n",
    "\n",
    "# anti, hybrid of anti and go with equal weight, and go \n",
    "anti_go = [stacked_interpolation[0], stacked_interpolation[int((N+1)/2)], stacked_interpolation[-1]]\n",
    "\n",
    "_, _, db_intp_anti = net.iterate_sequence_batch(anti_go[0], run_mode='track_states')\n",
    "_, _, db_intp_go = net.iterate_sequence_batch(anti_go[2], run_mode='track_states')\n",
    "\n",
    "Ms_anti, Ms_orig_anti, hs_anti, bs_anti = modulation_extraction(int_input, db_intp_anti, layer_index)\n",
    "Ms_go, Ms_orig_go, hs_go, bs_go = modulation_extraction(int_input, db_intp_go, layer_index)\n",
    "\n",
    "batch_num = Ms_orig_go.shape[0]\n",
    "    \n",
    "if name == \"hidden\": \n",
    "    data_anti, data_go = hs_anti, hs_go\n",
    "elif name == \"modulation\": \n",
    "    data_anti, data_go = Ms_anti, Ms_go\n",
    "\n",
    "data_all = np.concatenate((data_anti, data_go), axis=0)\n",
    "\n",
    "n_activity = data_anti.shape[-1]\n",
    "\n",
    "as_flat_stim = data_anti[:,time_stamps[\"stimulus_start\"]:time_stamps[\"stimulus_end\"],:].reshape((-1, n_activity))\n",
    "as_flat_stim_all = data_all[:,time_stamps[\"stimulus_start\"]:time_stamps[\"stimulus_end\"],:].reshape((-1, n_activity))\n",
    "\n",
    "as_flat_anti = data_anti.reshape((-1, n_activity))\n",
    "as_flat_go = data_go.reshape((-1, n_activity))\n",
    "\n",
    "pca_stim = PCA(n_components = PCA_downsample, random_state=42)\n",
    "pca_stim.fit(as_flat_stim)  \n",
    "# pca_stim.fit(as_flat_stim_all)\n",
    "# pca_stim.fit(wout)\n",
    "\n",
    "total_ev_training = pca_stim.explained_variance_ratio_.sum()  \n",
    "print(total_ev_training)\n",
    "print(pca_stim.components_.shape)\n",
    "\n",
    "as_pca_anti = pca_stim.transform(as_flat_anti)\n",
    "projected_data_anti = as_pca_anti.reshape((data_anti.shape[0], data_anti.shape[1], -1))\n",
    "as_pca_go = pca_stim.transform(as_flat_go)\n",
    "projected_data_go = as_pca_go.reshape((data_go.shape[0], data_go.shape[1], -1))\n",
    "\n",
    "projected_data_stim_anti = projected_data_anti[:,:time_stamps[\"trial_end\"],:]\n",
    "projected_data_stim_go = projected_data_go[:,:time_stamps[\"trial_end\"],:]\n",
    "\n",
    "response_anti = (hs_anti[:,:time_stamps[\"trial_end\"],:]) @ wout.T \n",
    "response_go = (hs_go[:,:time_stamps[\"trial_end\"],:]) @ wout.T \n",
    "\n",
    "for i in range(projected_data_stim_anti.shape[0]): \n",
    "    for resp in range(2): \n",
    "        # plot the trajectory for anti\n",
    "        fig3dresponse[resp].add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=np.array(projected_data_stim_anti[i,:,0]), y=projected_data_stim_anti[i,:,1], z=response_anti[i,:,resp+1],\n",
    "                mode=\"lines\",\n",
    "                line=dict(width=4, color=c_vals[i]),\n",
    "                opacity=1.0,\n",
    "                name=f\"S{i}\",\n",
    "                showlegend=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # plot the end point for anti \n",
    "        fig3dresponse[resp].add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=[projected_data_stim_anti[i, -1, 0]],\n",
    "                y=[projected_data_stim_anti[i, -1, 1]],\n",
    "                z=[response_anti[i, -1, resp+1]],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(size=6, color=c_vals[i], symbol=\"circle\"),\n",
    "                legendgroup=f\"S{i}\",        \n",
    "                showlegend=False           \n",
    "            )\n",
    "        )\n",
    "\n",
    "        # plot the trajectory for go \n",
    "        fig3dresponse[resp].add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=np.array(projected_data_stim_go[i,:,0]), y=projected_data_stim_go[i,:,1], z=response_go[i,:,resp+1],\n",
    "                mode=\"lines\",\n",
    "                line=dict(width=4, color=c_vals[(i+4)%8], dash=\"dash\"),\n",
    "                opacity=1.0,\n",
    "                name=f\"S{i}\",\n",
    "                showlegend=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # plot the end point for go \n",
    "        fig3dresponse[resp].add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=[projected_data_stim_go[i, -1, 0]],\n",
    "                y=[projected_data_stim_go[i, -1, 1]],\n",
    "                z=[response_go[i, -1, resp+1]],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(size=6, color=c_vals[(i+4) % 8], symbol=\"diamond\"),\n",
    "                legendgroup=f\"S{i}\",\n",
    "                showlegend=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "for resp in range(2): \n",
    "    zname = [\"cos\", \"sin\"]\n",
    "    fig3dresponse[resp].update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title=\"Memoryanti Stimulus PCA 1\",\n",
    "            yaxis_title=\"Memoryanti Stimulus PCA 2\",\n",
    "            zaxis_title=f\"{zname[resp]} theta\"\n",
    "        ),\n",
    "        width=700,          \n",
    "        height=700,\n",
    "        margin=dict(l=0, r=0, t=40, b=0),\n",
    "        showlegend=True     \n",
    "    )\n",
    "\n",
    "    fig3dresponse[resp].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2e7293-228b-4341-bb4b-5c06d1ecf576",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(stacked_interpolation)\n",
    "\n",
    "for name in names: \n",
    "    anti_go = [stacked_interpolation[0], stacked_interpolation[int((N+1)/2)], stacked_interpolation[-1]]\n",
    "    _, _, db_intp_anti = net.iterate_sequence_batch(anti_go[0], run_mode='track_states')\n",
    "    _, _, db_inp_middle = net.iterate_sequence_batch(anti_go[1], run_mode='track_states')\n",
    "    _, _, db_intp_go = net.iterate_sequence_batch(anti_go[2], run_mode='track_states')\n",
    "    \n",
    "    Ms_anti, Ms_orig_anti, hs_anti, bs_anti = modulation_extraction(int_input, db_intp_anti, layer_index)\n",
    "    Ms_middle, Ms_orig_middle, hs_middle, bs_middle = modulation_extraction(int_input, db_inp_middle, layer_index)\n",
    "    Ms_go, Ms_orig_go, hs_go, bs_go = modulation_extraction(int_input, db_intp_go, layer_index)\n",
    "\n",
    "    batch_num = Ms_orig_go.shape[0]\n",
    "        \n",
    "    if name == \"hidden\": \n",
    "        data_anti, data_middle, data_go = hs_anti, hs_middle, hs_go\n",
    "    elif name == \"modulation\": \n",
    "        data_anti, data_middle, data_go = Ms_anti, Ms_middle, Ms_go\n",
    "        \n",
    "    n_activity = data_anti.shape[-1]\n",
    "\n",
    "    as_flat_stim = data_anti[:,time_stamps[\"stimulus_start\"]:time_stamps[\"stimulus_end\"],:].reshape((-1, n_activity))\n",
    "\n",
    "    as_flat_anti = data_anti.reshape((-1, n_activity))\n",
    "    as_flat_middle = data_middle.reshape((-1, n_activity))\n",
    "    as_flat_go = data_go.reshape((-1, n_activity))\n",
    "\n",
    "    pca_stim = PCA(n_components = PCA_downsample, random_state=42)\n",
    "    pca_stim.fit(as_flat_stim) \n",
    "\n",
    "    as_pca_anti = pca_stim.transform(as_flat_anti)\n",
    "    projected_data_anti = as_pca_anti.reshape((data_anti.shape[0], data_anti.shape[1], -1))\n",
    "    as_pca_middle = pca_stim.transform(as_flat_middle)\n",
    "    projected_data_middle = as_pca_middle.reshape((data_middle.shape[0], data_middle.shape[1], -1))\n",
    "    as_pca_go = pca_stim.transform(as_flat_go)\n",
    "    projected_data_go = as_pca_go.reshape((data_go.shape[0], data_go.shape[1], -1))\n",
    "\n",
    "    projected_data_stim_anti = projected_data_anti[:,time_stamps[\"stimulus_start\"]:time_stamps[\"stimulus_end\"],:]\n",
    "    projected_data_stim_middle = projected_data_middle[:,time_stamps[\"stimulus_start\"]:time_stamps[\"stimulus_end\"],:]\n",
    "    projected_data_stim_go = projected_data_go[:,time_stamps[\"stimulus_start\"]:time_stamps[\"stimulus_end\"],:]\n",
    "\n",
    "    fig, axs = plt.subplots(1,3,figsize=(4*3,4))\n",
    "    combination = [[0,1],[0,2],[1,2]]\n",
    "    for comb_index, comb in enumerate(combination): \n",
    "        for i in range(projected_data_stim_anti.shape[0]): \n",
    "            axs[comb_index].plot(projected_data_stim_anti[i,:,comb[0]], projected_data_stim_anti[i,:,comb[1]], \\\n",
    "                                color=c_vals[i], linestyle=linestyles[0])\n",
    "            axs[comb_index].plot(projected_data_stim_middle[i,:,comb[0]], projected_data_stim_middle[i,:,comb[1]], \\\n",
    "                                color=c_vals[i], linestyle=linestyles[1])\n",
    "            axs[comb_index].plot(projected_data_stim_go[i,:,comb[0]], projected_data_stim_go[i,:,comb[1]], \\\n",
    "                                color=c_vals[i], linestyle=linestyles[2])\n",
    "    for ax in axs: \n",
    "        ax.set_title(name)\n",
    "\n",
    "    fig.savefig(f\"./twotasks/m_pca_stimulus_{name}_seed{seed}_{hyp_dict['addon_name']}_{int_index}.png\", dpi=300)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4e67c5-e152-4586-ba9c-42875456bd5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mpn)",
   "language": "python",
   "name": "mpn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
