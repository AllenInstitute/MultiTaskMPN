{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd1556c8-a4b6-434b-a60f-37035980bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "import copy \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# PyTorch Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Data Handling and Image Processing\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Style for Matplotlib\n",
    "import scienceplots\n",
    "plt.style.use('science')\n",
    "plt.style.use(['no-latex'])\n",
    "\n",
    "# Scientific Computing and Machine Learning\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.linalg import subspace_angles\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Custom Modules and Extensions\n",
    "sys.path.append(\"../netrep/\")\n",
    "sys.path.append(\"../svcca/\")\n",
    "\n",
    "import networks as nets  # Contains RNNs\n",
    "import net_helpers\n",
    "import mpn_tasks\n",
    "import helper\n",
    "import mpn\n",
    "\n",
    "import scienceplots\n",
    "plt.style.use('science')\n",
    "plt.style.use(['no-latex'])\n",
    "\n",
    "# Memory Optimization\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f19a5112-da33-4c1a-b6ad-1679afd19ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59c418b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 Red, 1 blue, 2 green, 3 purple, 4 orange, 5 teal, 6 gray, 7 pink, 8 yellow\n",
    "c_vals = ['#e53e3e', '#3182ce', '#38a169', '#805ad5','#dd6b20', '#319795', '#718096', '#d53f8c', '#d69e2e',] * 10\n",
    "c_vals_l = ['#feb2b2', '#90cdf4', '#9ae6b4', '#d6bcfa', '#fbd38d', '#81e6d9', '#e2e8f0', '#fbb6ce', '#faf089',] * 10\n",
    "c_vals_d = ['#9b2c2c', '#2c5282', '#276749', '#553c9a', '#9c4221', '#285e61', '#2d3748', '#97266d', '#975a16',] * 10 \n",
    "l_vals = ['solid', 'dashed', 'dotted', 'dashdot', '-', '--', '-.', ':', (0, (3, 1, 1, 1)), (0, (5, 10))]\n",
    "markers_vals = ['o', 'v', '*', 'x', '>', '1', '2', '3', '4', 's', 'p', '*', 'h', 'H', '+', 'x', 'D', 'd', '|', '_']\n",
    "linestyles = [\"-\", \"--\", \"-.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34978bf6-67b8-41bd-a022-a7b46a320686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set seed 992\n",
      "Fixation_off: True; Task_info: True\n",
      "Rules: ['delaygo', 'delayanti']\n",
      "  Input size 8, Output size 3\n",
      "Using CUDA...\n"
     ]
    }
   ],
   "source": [
    "# Reload modules if changes have been made to them\n",
    "from importlib import reload\n",
    "\n",
    "reload(nets)\n",
    "reload(net_helpers)\n",
    "\n",
    "fixseed = False # randomize setting the seed may lead to not perfectly solved results\n",
    "seed = random.randint(1,1000) if not fixseed else 8 # random set the seed to test robustness by default\n",
    "print(f\"Set seed {seed}\")\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "hyp_dict = {}\n",
    "\n",
    "hyp_dict['task_type'] = 'multitask' # int, NeuroGym, multitask\n",
    "hyp_dict['mode_for_all'] = \"random_batch\"\n",
    "hyp_dict['ruleset'] = 'delaygofamily' # low_dim, all, test\n",
    "\n",
    "accept_rules = ('fdgo', 'fdanti', 'delaygo', 'delayanti', 'reactgo', 'reactanti', \n",
    "                'delaydm1', 'delaydm2', 'dmsgo', 'dmcgo', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm')\n",
    "\n",
    "\n",
    "rules_dict = \\\n",
    "    {'all' : ['fdgo', 'reactgo', 'delaygo', 'fdanti', 'reactanti', 'delayanti',\n",
    "              'dm1', 'dm2', 'contextdm1', 'contextdm2', 'multidm',\n",
    "              'delaydm1', 'delaydm2', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm',\n",
    "              'dmsgo', 'dmsnogo', 'dmcgo', 'dmcnogo'],\n",
    "     'low_dim' : ['fdgo', 'reactgo', 'delaygo', 'fdanti', 'reactanti', 'delayanti',\n",
    "                 'delaydm1', 'delaydm2', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm',\n",
    "                 'dmsgo', 'dmsnogo', 'dmcgo', 'dmcnogo'],\n",
    "     'gofamily': ['fdgo', 'fdanti', 'reactgo', 'reactanti', 'delaygo', 'delayanti'],\n",
    "     'delaygo': ['delaygo'],\n",
    "     'delaygofamily': ['delaygo', 'delayanti'],\n",
    "     'fdgo': ['fdgo'],\n",
    "     'fdfamily': ['fdgo', 'fdanti'],\n",
    "     'reactgo': ['reactgo'],\n",
    "     'reactfamily': ['reactgo', 'reactanti'],\n",
    "     'delaydm1': ['delaydm1'],\n",
    "     'delaydmfamily': ['delaydm1', 'delaydm2'],\n",
    "     'dmsgofamily': ['dmsgo', 'dmsnogo'],\n",
    "     'dmsgo': ['dmsgo'],\n",
    "     'dmcgo': ['dmcgo'],\n",
    "     'contextdelayfamily': ['contextdelaydm1', 'contextdelaydm2'],\n",
    "    }\n",
    "    \n",
    "\n",
    "# This can either be used to set parameters OR set parameters and train\n",
    "train = True # whether or not to train the network\n",
    "verbose = True\n",
    "hyp_dict['run_mode'] = 'minimal' # minimal, debug\n",
    "hyp_dict['chosen_network'] = \"dmpn\"\n",
    "\n",
    "# suffix for saving images\n",
    "# inputadd, Wfix, WL2, hL2\n",
    "# inputrandom, Wtrain\n",
    "# noise001\n",
    "# largeregularization\n",
    "# trainetalambda\n",
    "\n",
    "mpn_depth = 1\n",
    "n_hidden = 200\n",
    "\n",
    "hyp_dict['addon_name'] = \"Wtrain+WL2+hL2+reg1e3\"\n",
    "hyp_dict['addon_name'] += f\"+hidden{n_hidden}\"\n",
    "\n",
    "# for coding \n",
    "if hyp_dict['chosen_network'] in (\"gru\", \"vanilla\"):\n",
    "    mpn_depth = 1\n",
    "\n",
    "def current_basic_params():\n",
    "    task_params = {\n",
    "        'task_type': hyp_dict['task_type'],\n",
    "        'rules': rules_dict[hyp_dict['ruleset']],\n",
    "        'dt': 40, # ms, directly influence sequence lengths,\n",
    "        'ruleset': hyp_dict['ruleset'],\n",
    "        'n_eachring': 8, # Number of distinct possible inputs on each ring\n",
    "        'in_out_mode': 'low_dim',  # high_dim or low_dim or low_dim_pos (Robert vs. Laura's paper, resp)\n",
    "        'sigma_x': 0.00, # Laura raised to 0.1 to prevent overfitting (Robert uses 0.01)\n",
    "        'mask_type': 'cost', # 'cost', None\n",
    "        'fixate_off': True, # Second fixation signal goes on when first is off\n",
    "        'task_info': True, \n",
    "        'randomize_inputs': False,\n",
    "        'n_input': 20, # Only used if inputs are randomized,\n",
    "        'modality_diff': False,\n",
    "        'label_strength': False, \n",
    "        'long_delay': 'normal',\n",
    "        'long_response': 'normal',\n",
    "        'long_stimulus': 'normal', \n",
    "        'long_fixation': 'normal', \n",
    "        'adjust_task_prop': True,\n",
    "        'adjust_task_decay': 0.9, \n",
    "    }\n",
    "\n",
    "    print(f\"Fixation_off: {task_params['fixate_off']}; Task_info: {task_params['task_info']}\")\n",
    "\n",
    "    train_params = {\n",
    "        'lr': 1e-3,\n",
    "        'n_batches': 128,\n",
    "        'batch_size': 128,\n",
    "        'gradient_clip': 10,\n",
    "        'valid_n_batch': 50,\n",
    "        'n_datasets': 10000, # Number of distinct batches\n",
    "        'valid_check': None, \n",
    "        'n_epochs_per_set': 1, # longer/shorter training\n",
    "        'weight_reg': 'L2',\n",
    "        'activity_reg': 'L2', \n",
    "        'reg_lambda': 1e-3,\n",
    "\n",
    "        'scheduler': {\n",
    "            'type': 'ReduceLROnPlateau',  # or 'StepLR'\n",
    "            'mode': 'min',                # for ReduceLROnPlateau\n",
    "            'factor': 0.5,                # factor to reduce LR\n",
    "            'patience': 10,                # epochs to wait before reducing LR\n",
    "            'min_lr': 1e-8,\n",
    "            'step_size': 30,              # for StepLR (step every 30 datasets)\n",
    "            'gamma': 0.1                  # for StepLR (multiply LR by 0.1)\n",
    "        },\n",
    "    }\n",
    "\n",
    "    if not train: # some \n",
    "        assert train_params['n_epochs_per_set'] == 0\n",
    "\n",
    "    net_params = {\n",
    "        'net_type': hyp_dict['chosen_network'], # mpn1, dmpn, vanilla\n",
    "        'n_neurons': [1] + [n_hidden] * mpn_depth + [1],\n",
    "        'linear_embed': 200, \n",
    "        'output_bias': False, # Turn off biases for easier interpretation\n",
    "        'loss_type': 'MSE', # XE, MSE\n",
    "        'activation': 'tanh', # linear, ReLU, sigmoid, tanh, tanh_re, tukey, heaviside\n",
    "        'cuda': True,\n",
    "        'monitor_freq': train_params[\"n_epochs_per_set\"],\n",
    "        'monitor_valid_out': True, # Whether or not to save validation output throughout training\n",
    "        'output_matrix': '',# \"\" (default); \"untrained\", or \"orthogonal\"\n",
    "        'input_layer_add': True, \n",
    "        'input_layer_add_trainable': True, # revise this is effectively to [randomize_inputs], tune this\n",
    "        'input_layer_bias': False, \n",
    "        'input_layer': \"trainable\", # for RNN only\n",
    "        'acc_measure': 'stimulus', \n",
    "        \n",
    "        # for one-layer MPN, GRU or Vanilla\n",
    "        'ml_params': {\n",
    "            'bias': True, # Bias of layer\n",
    "            'mp_type': 'mult',\n",
    "            'm_update_type': 'hebb_assoc', # hebb_assoc, hebb_pre\n",
    "            'eta_type': 'scalar', # scalar, pre_vector, post_vector, matrix\n",
    "            'eta_train': False,\n",
    "            # 'eta_init': 'mirror_gaussian', #0.0,\n",
    "            'lam_type': 'scalar', # scalar, pre_vector, post_vector, matrix\n",
    "            'm_time_scale': 4000, # ms, sets lambda\n",
    "            'lam_train': False,\n",
    "            'W_freeze': False, # different combination with [input_layer_add_trainable]\n",
    "        },\n",
    "\n",
    "        # Vanilla RNN params\n",
    "        'leaky': True,\n",
    "        'alpha': 0.2,\n",
    "    }\n",
    "\n",
    "    # Ensure the two options are *not* activated at the same time\n",
    "    assert not (task_params[\"randomize_inputs\"] and net_params[\"input_layer_add\"]), (\n",
    "        \"task_params['randomize_inputs'] and net_params['input_layer_add'] cannot both be True.\"\n",
    "    )\n",
    "\n",
    "    # for multiple MPN layers, assert \n",
    "    if mpn_depth > 1:\n",
    "        for mpl_idx in range(mpn_depth - 1):\n",
    "            assert f'ml_params{mpl_idx}' in net_params.keys()\n",
    "\n",
    "    # actually I don't think it is needed\n",
    "    # putting here to warn the parameter checking every time \n",
    "    # when switching network\n",
    "    if hyp_dict['chosen_network'] in (\"gru\", \"vanilla\"):\n",
    "        assert f'ml_params' in net_params.keys()\n",
    "\n",
    "    return task_params, train_params, net_params\n",
    "\n",
    "task_params, train_params, net_params = current_basic_params()\n",
    "\n",
    "shift_index = 1 if not task_params['fixate_off'] else 0\n",
    "\n",
    "if hyp_dict['task_type'] in ('multitask',):\n",
    "    task_params, train_params, net_params = mpn_tasks.convert_and_init_multitask_params(\n",
    "        (task_params, train_params, net_params)\n",
    "    )\n",
    "\n",
    "    net_params['prefs'] = mpn_tasks.get_prefs(task_params['hp'])\n",
    "\n",
    "    print('Rules: {}'.format(task_params['rules']))\n",
    "    print('  Input size {}, Output size {}'.format(\n",
    "        task_params['n_input'], task_params['n_output'],\n",
    "    ))\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "if net_params['cuda']:\n",
    "    print('Using CUDA...')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('Using CPU...')\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# how many epoch each dataset will be trained on\n",
    "epoch_multiply = train_params[\"n_epochs_per_set\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a341b36-dc7a-42b0-bffb-cb17d380041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_dict[\"mess_with_training\"] = False\n",
    "\n",
    "if hyp_dict['mess_with_training']:\n",
    "    hyp_dict['addon_name'] += \"messwithtraining\"\n",
    "\n",
    "params = task_params, train_params, net_params\n",
    "\n",
    "if net_params['net_type'] == 'mpn1':\n",
    "    netFunction = mpn.MultiPlasticNet\n",
    "elif net_params['net_type'] == 'dmpn':\n",
    "    netFunction = mpn.DeepMultiPlasticNet\n",
    "elif net_params['net_type'] == 'vanilla':\n",
    "    netFunction = nets.VanillaRNN\n",
    "elif net_params['net_type'] == 'gru':\n",
    "    netFunction = nets.GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07e4fe48-2af6-4741-8d42-01803ba0abf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Align ['delaygo', 'delayanti'] With Same Time\n",
      "rng reset with seed 6277\n",
      "rng reset with seed 6277\n",
      "rng reset with seed 6277\n",
      "rng reset with seed 6277\n",
      "rng reset with seed 6277\n",
      "rng reset with seed 6277\n",
      "rng reset with seed 6277\n",
      "rng reset with seed 6277\n",
      "rng reset with seed 6277\n",
      "rng reset with seed 6277\n",
      "delaygo\n",
      "delayanti\n",
      "test_input_longdelay.shape: torch.Size([100, 288, 8])\n",
      "test_output_longdelay.shape: torch.Size([100, 288, 3])\n",
      "test_input_longresponse.shape: torch.Size([100, 307, 8])\n",
      "test_output_longresponse.shape: torch.Size([100, 307, 3])\n",
      "test_input_longstimulus.shape: torch.Size([100, 288, 8])\n",
      "test_output_longstimulus.shape: torch.Size([100, 288, 3])\n"
     ]
    }
   ],
   "source": [
    "test_n_batch = train_params[\"valid_n_batch\"]\n",
    "color_by = \"stim\" # or \"resp\" \n",
    "\n",
    "task_random_fix = True\n",
    "if task_random_fix:\n",
    "    print(f\"Align {task_params['rules']} With Same Time\")\n",
    "\n",
    "if task_params['task_type'] in ('multitask',): # Test batch consists of all the rules\n",
    "    task_params['hp']['batch_size_train'] = test_n_batch\n",
    "    # using homogeneous cutting off\n",
    "    test_mode_for_all = \"random\"\n",
    "    # ZIHAN\n",
    "    # generate test data using \"random\"\n",
    "    test_data, test_trials_extra = mpn_tasks.generate_trials_wrap(task_params, test_n_batch, \\\n",
    "                rules=task_params['rules'], mode_input=test_mode_for_all, fix=task_random_fix\n",
    "    )\n",
    "    _, test_trials, test_rule_idxs = test_trials_extra\n",
    "\n",
    "    # generate test input data with separate period extension to obtain fixed points information\n",
    "    task_params_longdelay = copy.deepcopy(task_params)\n",
    "    task_params_longdelay[\"long_delay\"] = \"long\"\n",
    "    test_data_longdelay, test_trials_extra_longdelay = mpn_tasks.generate_trials_wrap(task_params_longdelay, test_n_batch, \\\n",
    "                                                                                      rules=task_params_longdelay['rules'], \\\n",
    "                                                                                      mode_input=test_mode_for_all, fix=task_random_fix)\n",
    "    \n",
    "    _, test_trials_longdelay, test_rule_idxs_longdelay = test_trials_extra_longdelay\n",
    "\n",
    "    task_params_longresponse = copy.deepcopy(task_params)\n",
    "    task_params_longresponse[\"long_response\"] = \"long\"\n",
    "    test_data_longresponse, test_trials_extra_longresponse = mpn_tasks.generate_trials_wrap(task_params_longresponse, test_n_batch, \\\n",
    "                                                                                      rules=task_params_longresponse['rules'], \\\n",
    "                                                                                      mode_input=test_mode_for_all, fix=task_random_fix)\n",
    "    \n",
    "    _, test_trials_longresponse, test_rule_idxs_longresponse = test_trials_extra_longresponse\n",
    "\n",
    "    task_params_longstimulus = copy.deepcopy(task_params)\n",
    "    task_params_longstimulus[\"long_stimulus\"] = \"long\"\n",
    "    test_data_longstimulus, test_trials_extra_longstimulus = mpn_tasks.generate_trials_wrap(task_params_longstimulus, test_n_batch, \\\n",
    "                                                                                      rules=task_params_longstimulus['rules'], \\\n",
    "                                                                                      mode_input=test_mode_for_all, fix=task_random_fix)\n",
    "    \n",
    "    _, test_trials_longstimulus, test_rule_idxs_longstimulus = test_trials_extra_longstimulus\n",
    "\n",
    "    task_params_longfixation = copy.deepcopy(task_params)\n",
    "    task_params_longfixation[\"long_fixation\"] = \"long\"\n",
    "    test_data_longfixation, test_trials_extra_longfixation = mpn_tasks.generate_trials_wrap(task_params_longfixation, test_n_batch, \\\n",
    "                                                                                      rules=task_params_longfixation['rules'], \\\n",
    "                                                                                      mode_input=test_mode_for_all, fix=task_random_fix)\n",
    "    \n",
    "    _, test_trials_longfixation, test_rule_idxs_longfixation = test_trials_extra_longfixation\n",
    "    \n",
    "\n",
    "    task_params['dataset_name'] = 'multitask'\n",
    "\n",
    "    if task_params['in_out_mode'] in ('low_dim_pos',):\n",
    "        output_dim_labels = ('Fixate', 'Cos', '-Cos', 'Sin', '-Sin')\n",
    "    elif task_params['in_out_mode'] in ('low_dim',):\n",
    "        output_dim_labels = ('Fixate', 'Cos', 'Sin')\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def generate_response_stimulus(task_params, test_trials): \n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        labels_resp, labels_stim = [], []\n",
    "        for rule_idx, rule in enumerate(task_params['rules']):\n",
    "            print(rule)\n",
    "            if rule in accept_rules:\n",
    "                if hyp_dict['ruleset'] in ('dmsgo', 'dmcgo'):\n",
    "                    labels.append(test_trials[rule_idx].meta['matches'])\n",
    "                else:\n",
    "                    labels_resp.append(test_trials[rule_idx].meta['resp1'])\n",
    "                    labels_stim.append(test_trials[rule_idx].meta['stim1']) \n",
    "    \n",
    "            else:\n",
    "                raise NotImplementedError()\n",
    "                \n",
    "        labels_resp = np.concatenate(labels_resp, axis=0).reshape(-1,1)\n",
    "        labels_stim = np.concatenate(labels_stim, axis=0).reshape(-1,1)\n",
    "\n",
    "        return labels_resp, labels_stim\n",
    "\n",
    "    labels_resp, labels_stim = generate_response_stimulus(task_params, test_trials)\n",
    "\n",
    "labels = labels_stim if color_by == \"stim\" else labels_resp\n",
    "    \n",
    "test_input, test_output, test_mask = test_data\n",
    "test_input_longfixation, test_output_longfixation, test_mask_longfixation = test_data_longfixation\n",
    "test_input_longstimulus, test_output_longstimulus, test_mask_longstimulus = test_data_longstimulus\n",
    "test_input_longdelay, test_output_longdelay, test_mask_longdelay = test_data_longdelay\n",
    "test_input_longresponse, test_output_longresponse, test_mask_longresponse = test_data_longresponse\n",
    "\n",
    "print(f\"test_input_longdelay.shape: {test_input_longdelay.shape}\")\n",
    "print(f\"test_output_longdelay.shape: {test_output_longdelay.shape}\")\n",
    "print(f\"test_input_longresponse.shape: {test_input_longresponse.shape}\")\n",
    "print(f\"test_output_longresponse.shape: {test_output_longresponse.shape}\")\n",
    "print(f\"test_input_longstimulus.shape: {test_input_longstimulus.shape}\")\n",
    "print(f\"test_output_longstimulus.shape: {test_output_longstimulus.shape}\")\n",
    "\n",
    "# permutation = np.random.permutation(test_input.shape[0])\n",
    "# test_input = test_input[permutation]\n",
    "# test_output = test_output[permutation]\n",
    "# test_mask = test_mask[permutation]\n",
    "# labels = labels[permutation]\n",
    "\n",
    "test_input_np = test_input.detach().cpu().numpy()\n",
    "test_output_np = test_output.detach().cpu().numpy()\n",
    "\n",
    "# Total number of batches, might be different than test_n_batch\n",
    "# this should be the same regardless of variety of test_input\n",
    "n_batch_all = test_input_np.shape[0] \n",
    "\n",
    "def find_task(task_params, test_input_np, shift_index):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    test_task = [] # which task\n",
    "    for batch_idx in range(test_input_np.shape[0]):\n",
    "        \n",
    "        if task_params[\"randomize_inputs\"]: \n",
    "            test_input_np_ = test_input_np @ np.linalg.pinv(task_params[\"randomize_matrix\"])\n",
    "        else: \n",
    "            test_input_np_ = test_input_np\n",
    "            \n",
    "        task_label = test_input_np_[batch_idx, 0, 6-shift_index:]\n",
    "        # task_label_index = np.where(task_label == 1)[0][0]\n",
    "        \n",
    "        # tol = 1e-3      \n",
    "        # mask = np.isclose(task_label, 1, atol=tol)\n",
    "        task_label = np.asarray(task_label)       \n",
    "        dist = np.abs(task_label - 1)     \n",
    "        mask = dist == dist.min() \n",
    "        \n",
    "        indices = np.where(mask)[0]\n",
    "        \n",
    "        if indices.size:                \n",
    "            task_label_index = indices[0]   \n",
    "        else:\n",
    "            raise ValueError(\"No entry close enough to 1 found\")\n",
    "            \n",
    "        test_task.append(task_label_index)\n",
    "\n",
    "    return test_task  \n",
    "\n",
    "test_task = find_task(task_params, test_input_np, shift_index)\n",
    "\n",
    "test_task_longfixation = find_task(task_params_longfixation, test_input_longfixation.detach().cpu().numpy(), shift_index)\n",
    "test_task_longstimulus = find_task(task_params_longstimulus, test_input_longstimulus.detach().cpu().numpy(), shift_index)\n",
    "test_task_longdelay = find_task(task_params_longdelay, test_input_longdelay.detach().cpu().numpy(), shift_index)\n",
    "test_task_longresponse = find_task(task_params_longresponse, test_input_longresponse.detach().cpu().numpy(), shift_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78b44e9-b6e3-4c1c-a5c7-5a608a6d090b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiPlastic Net:\n",
      "  output neurons: 3\n",
      "  Act: tanh\n",
      "\n",
      "40\n",
      "  MP Layer1 parameters:\n",
      "    n_neurons - input: 200, output: 200\n",
      "    M matrix parameters:    update bounds - Max mult: 1.0, Min mult: -1.0\n",
      "      type: mult // Update - type: hebb_assoc // Act fn: linear\n",
      "      Eta: scalar (fixed) // Lambda: scalar (fixed) // Lambda_max: 0.99 (tau: 4.0e+03)\n",
      "  No Hidden Recurrency.\n",
      "Trainable parameters: 42,400\n",
      "W_output: (3, 200)\n",
      "W_initial_linear.weight: (200, 8)\n",
      "mp_layer1.W: (200, 200)\n",
      "mp_layer1.b: (200,)\n",
      "task_params['rules_probs']: [0.5 0.5]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 108, max_seq_len 108\n",
      "inputs_all paddled: (128, 108, 8)\n",
      "inputs_all: torch.Size([128, 108, 8])\n",
      "========== Setup Parameters ==========\n",
      "Train parameters:\n",
      "  Loss: MSE // LR: 1.00e-03 // Optim: adam\n",
      "  Grad type: backprop // Gradient clip: 1.0e+01\n",
      "Weight reg: L2, coef: 1.0e-03\n",
      "Activity reg: L2, coef: 1.0e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zihan.zhang/.conda/envs/mpn/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 100, LR: 1.000e-03 - train_loss:1.068e-01, rounded train_acc:0.783, valid_loss:1.134e-01, rounded valid_acc:0.681\n",
      "task_params['rules_probs']: [0.53828454 0.46171546]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 103, max_seq_len 103\n",
      "inputs_all paddled: (128, 103, 8)\n",
      "inputs_all: torch.Size([128, 103, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 200, LR: 1.000e-03 - train_loss:8.043e-02, rounded train_acc:0.938, valid_loss:7.913e-02, rounded valid_acc:0.980\n",
      "task_params['rules_probs']: [0.39017033 0.60982967]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 103, max_seq_len 103\n",
      "inputs_all paddled: (128, 103, 8)\n",
      "inputs_all: torch.Size([128, 103, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 300, LR: 1.000e-03 - train_loss:6.472e-02, rounded train_acc:0.982, valid_loss:6.492e-02, rounded valid_acc:0.987\n",
      "task_params['rules_probs']: [0.25629111 0.74370889]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 98, max_seq_len 98\n",
      "inputs_all paddled: (128, 98, 8)\n",
      "inputs_all: torch.Size([128, 98, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 400, LR: 1.000e-03 - train_loss:5.429e-02, rounded train_acc:0.969, valid_loss:5.463e-02, rounded valid_acc:0.986\n",
      "task_params['rules_probs']: [0.47010755 0.52989245]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 103, max_seq_len 103\n",
      "inputs_all paddled: (128, 103, 8)\n",
      "inputs_all: torch.Size([128, 103, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 500, LR: 1.000e-03 - train_loss:4.593e-02, rounded train_acc:0.966, valid_loss:4.553e-02, rounded valid_acc:0.993\n",
      "task_params['rules_probs']: [0.15998111 0.84001889]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 95, max_seq_len 95\n",
      "inputs_all paddled: (128, 95, 8)\n",
      "inputs_all: torch.Size([128, 95, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 600, LR: 1.000e-03 - train_loss:3.909e-02, rounded train_acc:0.990, valid_loss:3.966e-02, rounded valid_acc:0.984\n",
      "task_params['rules_probs']: [0.41998176 0.58001824]\n",
      "Rule: delaygo\n",
      "Rule delaygo seq_len 102, max_seq_len 102\n",
      "inputs_all paddled: (128, 102, 8)\n",
      "inputs_all: torch.Size([128, 102, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 700, LR: 1.000e-03 - train_loss:3.977e-02, rounded train_acc:0.915, valid_loss:4.611e-02, rounded valid_acc:0.866\n",
      "task_params['rules_probs']: [0.53058153 0.46941847]\n",
      "Rule: delaygo\n",
      "Rule delaygo seq_len 105, max_seq_len 105\n",
      "inputs_all paddled: (128, 105, 8)\n",
      "inputs_all: torch.Size([128, 105, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 800, LR: 1.000e-03 - train_loss:3.139e-02, rounded train_acc:0.977, valid_loss:3.129e-02, rounded valid_acc:0.991\n",
      "task_params['rules_probs']: [0.11862489 0.88137511]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 102, max_seq_len 102\n",
      "inputs_all paddled: (128, 102, 8)\n",
      "inputs_all: torch.Size([128, 102, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 900, LR: 1.000e-03 - train_loss:2.816e-02, rounded train_acc:0.986, valid_loss:2.741e-02, rounded valid_acc:0.997\n",
      "task_params['rules_probs']: [0.02157725 0.97842275]\n",
      "Rule: delaygo\n",
      "Rule delaygo seq_len 104, max_seq_len 104\n",
      "inputs_all paddled: (128, 104, 8)\n",
      "inputs_all: torch.Size([128, 104, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 1000, LR: 1.000e-03 - train_loss:2.446e-02, rounded train_acc:0.991, valid_loss:2.413e-02, rounded valid_acc:0.995\n",
      "task_params['rules_probs']: [0.28892812 0.71107188]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 99, max_seq_len 99\n",
      "inputs_all paddled: (128, 99, 8)\n",
      "inputs_all: torch.Size([128, 99, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 1100, LR: 1.000e-03 - train_loss:3.023e-02, rounded train_acc:0.922, valid_loss:2.395e-02, rounded valid_acc:0.988\n",
      "task_params['rules_probs']: [0.59279828 0.40720172]\n",
      "Rule: delaygo\n",
      "Rule delaygo seq_len 97, max_seq_len 97\n",
      "inputs_all paddled: (128, 97, 8)\n",
      "inputs_all: torch.Size([128, 97, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 1200, LR: 1.000e-03 - train_loss:2.040e-02, rounded train_acc:0.992, valid_loss:2.175e-02, rounded valid_acc:0.996\n",
      "task_params['rules_probs']: [0.14623433 0.85376567]\n",
      "Rule: delaygo\n",
      "Rule delaygo seq_len 96, max_seq_len 96\n",
      "inputs_all paddled: (128, 96, 8)\n",
      "inputs_all: torch.Size([128, 96, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 1300, LR: 1.000e-03 - train_loss:2.421e-02, rounded train_acc:0.977, valid_loss:2.008e-02, rounded valid_acc:0.987\n",
      "task_params['rules_probs']: [0.36050292 0.63949708]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 103, max_seq_len 103\n",
      "inputs_all paddled: (128, 103, 8)\n",
      "inputs_all: torch.Size([128, 103, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 1400, LR: 1.000e-03 - train_loss:2.027e-02, rounded train_acc:0.968, valid_loss:2.000e-02, rounded valid_acc:0.990\n",
      "task_params['rules_probs']: [0.1200983 0.8799017]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 105, max_seq_len 105\n",
      "inputs_all paddled: (128, 105, 8)\n",
      "inputs_all: torch.Size([128, 105, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 1500, LR: 1.000e-03 - train_loss:1.814e-02, rounded train_acc:0.987, valid_loss:1.755e-02, rounded valid_acc:0.996\n",
      "task_params['rules_probs']: [0.27947393 0.72052607]\n",
      "Rule: delaygo\n",
      "Rule delaygo seq_len 105, max_seq_len 105\n",
      "inputs_all paddled: (128, 105, 8)\n",
      "inputs_all: torch.Size([128, 105, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 1600, LR: 1.000e-03 - train_loss:1.645e-02, rounded train_acc:0.991, valid_loss:1.595e-02, rounded valid_acc:0.996\n",
      "task_params['rules_probs']: [0.32171793 0.67828207]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 104, max_seq_len 104\n",
      "inputs_all paddled: (128, 104, 8)\n",
      "inputs_all: torch.Size([128, 104, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 1700, LR: 1.000e-03 - train_loss:1.644e-02, rounded train_acc:0.891, valid_loss:1.531e-02, rounded valid_acc:0.979\n",
      "task_params['rules_probs']: [0.12199906 0.87800094]\n",
      "Rule: delaygo\n",
      "Rule delaygo seq_len 108, max_seq_len 108\n",
      "inputs_all paddled: (128, 108, 8)\n",
      "inputs_all: torch.Size([128, 108, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 1800, LR: 1.000e-03 - train_loss:1.460e-02, rounded train_acc:0.973, valid_loss:1.368e-02, rounded valid_acc:0.996\n",
      "task_params['rules_probs']: [0.28316835 0.71683165]\n",
      "Rule: delaygo\n",
      "Rule delaygo seq_len 103, max_seq_len 103\n",
      "inputs_all paddled: (128, 103, 8)\n",
      "inputs_all: torch.Size([128, 103, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 1900, LR: 1.000e-03 - train_loss:1.257e-02, rounded train_acc:0.993, valid_loss:1.279e-02, rounded valid_acc:0.996\n",
      "task_params['rules_probs']: [0.16464284 0.83535716]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 100, max_seq_len 100\n",
      "inputs_all paddled: (128, 100, 8)\n",
      "inputs_all: torch.Size([128, 100, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 2000, LR: 1.000e-03 - train_loss:2.348e-02, rounded train_acc:0.833, valid_loss:1.310e-02, rounded valid_acc:0.996\n",
      "task_params['rules_probs']: [0.50789574 0.49210426]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 107, max_seq_len 107\n",
      "inputs_all paddled: (128, 107, 8)\n",
      "inputs_all: torch.Size([128, 107, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 2100, LR: 1.000e-03 - train_loss:1.255e-02, rounded train_acc:0.995, valid_loss:1.311e-02, rounded valid_acc:0.997\n",
      "task_params['rules_probs']: [0.00557455 0.99442545]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 104, max_seq_len 104\n",
      "inputs_all paddled: (128, 104, 8)\n",
      "inputs_all: torch.Size([128, 104, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 2200, LR: 1.000e-03 - train_loss:1.149e-02, rounded train_acc:0.989, valid_loss:1.188e-02, rounded valid_acc:0.997\n",
      "task_params['rules_probs']: [0.04541128 0.95458872]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 95, max_seq_len 95\n",
      "inputs_all paddled: (128, 95, 8)\n",
      "inputs_all: torch.Size([128, 95, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 2300, LR: 1.000e-03 - train_loss:1.155e-02, rounded train_acc:0.974, valid_loss:1.191e-02, rounded valid_acc:0.987\n",
      "task_params['rules_probs']: [0.43406661 0.56593339]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 100, max_seq_len 100\n",
      "inputs_all paddled: (128, 100, 8)\n",
      "inputs_all: torch.Size([128, 100, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 2400, LR: 1.000e-03 - train_loss:2.614e-02, rounded train_acc:0.716, valid_loss:1.578e-02, rounded valid_acc:0.863\n",
      "task_params['rules_probs']: [0.8083143 0.1916857]\n",
      "Rule: delaygo\n",
      "Rule delaygo seq_len 101, max_seq_len 101\n",
      "inputs_all paddled: (128, 101, 8)\n",
      "inputs_all: torch.Size([128, 101, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 2500, LR: 1.000e-03 - train_loss:1.022e-02, rounded train_acc:0.998, valid_loss:1.131e-02, rounded valid_acc:0.999\n",
      "task_params['rules_probs']: [0.0784654 0.9215346]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 101, max_seq_len 101\n",
      "inputs_all paddled: (128, 101, 8)\n",
      "inputs_all: torch.Size([128, 101, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 2600, LR: 1.000e-03 - train_loss:9.387e-03, rounded train_acc:0.994, valid_loss:1.071e-02, rounded valid_acc:0.997\n",
      "task_params['rules_probs']: [2.08415084e-06 9.99997916e-01]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 103, max_seq_len 103\n",
      "inputs_all paddled: (128, 103, 8)\n",
      "inputs_all: torch.Size([128, 103, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 2700, LR: 1.000e-03 - train_loss:9.059e-03, rounded train_acc:0.989, valid_loss:9.410e-03, rounded valid_acc:0.997\n",
      "task_params['rules_probs']: [0.0085256 0.9914744]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 98, max_seq_len 98\n",
      "inputs_all paddled: (128, 98, 8)\n",
      "inputs_all: torch.Size([128, 98, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 2800, LR: 1.000e-03 - train_loss:9.658e-03, rounded train_acc:0.987, valid_loss:8.940e-03, rounded valid_acc:0.999\n",
      "task_params['rules_probs']: [0.17057474 0.82942526]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 103, max_seq_len 103\n",
      "inputs_all paddled: (128, 103, 8)\n",
      "inputs_all: torch.Size([128, 103, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 2900, LR: 1.000e-03 - train_loss:8.073e-03, rounded train_acc:0.993, valid_loss:8.928e-03, rounded valid_acc:0.996\n",
      "task_params['rules_probs']: [0.0017151 0.9982849]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 104, max_seq_len 104\n",
      "inputs_all paddled: (128, 104, 8)\n",
      "inputs_all: torch.Size([128, 104, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 3000, LR: 1.000e-03 - train_loss:8.143e-03, rounded train_acc:0.990, valid_loss:8.480e-03, rounded valid_acc:0.999\n",
      "task_params['rules_probs']: [0.00241669 0.99758331]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 102, max_seq_len 102\n",
      "inputs_all paddled: (128, 102, 8)\n",
      "inputs_all: torch.Size([128, 102, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 3100, LR: 1.000e-03 - train_loss:3.381e-02, rounded train_acc:0.699, valid_loss:1.967e-02, rounded valid_acc:0.838\n",
      "task_params['rules_probs']: [0.47223002 0.52776998]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 100, max_seq_len 100\n",
      "inputs_all paddled: (128, 100, 8)\n",
      "inputs_all: torch.Size([128, 100, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 3200, LR: 1.000e-03 - train_loss:9.404e-03, rounded train_acc:0.999, valid_loss:9.340e-03, rounded valid_acc:1.000\n",
      "task_params['rules_probs']: [0.50355549 0.49644451]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 101, max_seq_len 101\n",
      "inputs_all paddled: (128, 101, 8)\n",
      "inputs_all: torch.Size([128, 101, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 3300, LR: 1.000e-03 - train_loss:8.318e-03, rounded train_acc:0.999, valid_loss:8.307e-03, rounded valid_acc:1.000\n",
      "task_params['rules_probs']: [0.50000009 0.49999991]\n",
      "Rule: delaygo\n",
      "Rule delaygo seq_len 94, max_seq_len 94\n",
      "inputs_all paddled: (128, 94, 8)\n",
      "inputs_all: torch.Size([128, 94, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 3400, LR: 1.000e-03 - train_loss:7.603e-03, rounded train_acc:0.998, valid_loss:8.111e-03, rounded valid_acc:0.999\n",
      "task_params['rules_probs']: [2.53764396e-04 9.99746236e-01]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 105, max_seq_len 105\n",
      "inputs_all paddled: (128, 105, 8)\n",
      "inputs_all: torch.Size([128, 105, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 3500, LR: 1.000e-03 - train_loss:7.849e-03, rounded train_acc:0.987, valid_loss:8.845e-03, rounded valid_acc:0.999\n",
      "task_params['rules_probs']: [0.08890156 0.91109844]\n",
      "Rule: delaygo\n",
      "Rule delaygo seq_len 102, max_seq_len 102\n",
      "inputs_all paddled: (128, 102, 8)\n",
      "inputs_all: torch.Size([128, 102, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 3600, LR: 1.000e-03 - train_loss:7.266e-03, rounded train_acc:0.987, valid_loss:7.854e-03, rounded valid_acc:0.999\n",
      "task_params['rules_probs']: [3.00729497e-04 9.99699271e-01]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 107, max_seq_len 107\n",
      "inputs_all paddled: (128, 107, 8)\n",
      "inputs_all: torch.Size([128, 107, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 3700, LR: 1.000e-03 - train_loss:6.903e-03, rounded train_acc:0.990, valid_loss:7.233e-03, rounded valid_acc:0.999\n",
      "task_params['rules_probs']: [0.04544181 0.95455819]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 105, max_seq_len 105\n",
      "inputs_all paddled: (128, 105, 8)\n",
      "inputs_all: torch.Size([128, 105, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 3800, LR: 1.000e-03 - train_loss:6.912e-03, rounded train_acc:0.979, valid_loss:7.538e-03, rounded valid_acc:0.999\n",
      "task_params['rules_probs']: [2.31821327e-04 9.99768179e-01]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 102, max_seq_len 102\n",
      "inputs_all paddled: (128, 102, 8)\n",
      "inputs_all: torch.Size([128, 102, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 3900, LR: 1.000e-03 - train_loss:7.383e-03, rounded train_acc:0.992, valid_loss:6.919e-03, rounded valid_acc:0.999\n",
      "task_params['rules_probs']: [0.0387443 0.9612557]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 99, max_seq_len 99\n",
      "inputs_all paddled: (128, 99, 8)\n",
      "inputs_all: torch.Size([128, 99, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 4000, LR: 1.000e-03 - train_loss:6.965e-03, rounded train_acc:0.994, valid_loss:6.885e-03, rounded valid_acc:1.000\n",
      "task_params['rules_probs']: [0.23315522 0.76684478]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 104, max_seq_len 104\n",
      "inputs_all paddled: (128, 104, 8)\n",
      "inputs_all: torch.Size([128, 104, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 4100, LR: 1.000e-03 - train_loss:6.633e-03, rounded train_acc:0.990, valid_loss:6.902e-03, rounded valid_acc:0.997\n",
      "task_params['rules_probs']: [0.43567839 0.56432161]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 98, max_seq_len 98\n",
      "inputs_all paddled: (128, 98, 8)\n",
      "inputs_all: torch.Size([128, 98, 8])\n",
      "valid_acc_history: [None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "# we use net at different training stage on the same test_input\n",
    "start_time = time.time()\n",
    "net, _, (counter_lst, netout_lst, db_lst, Winput_lst, Winputbias_lst,\\\n",
    "         Woutput_lst, Wall_lst, marker_lst, loss_lst, acc_lst), _ = net_helpers.train_network(params, device=device, verbose=verbose,\n",
    "                                                                                              train=train, hyp_dict=hyp_dict,\n",
    "                                                                                              netFunction=netFunction,\n",
    "                                                                                              test_input=[test_input, \n",
    "                                                                                                          test_input_longfixation,\n",
    "                                                                                                          test_input_longstimulus, \n",
    "                                                                                                          test_input_longdelay,\n",
    "                                                                                                          test_input_longresponse],\n",
    "                                                                                              print_frequency=100)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Running Time: {end_time - start_time}\")\n",
    "counter_lst = [x * epoch_multiply + 1 for x in counter_lst] # avoid log plot issue    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07eee03-d879-44bc-89ef-60f11bdf721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyp_dict['chosen_network'] == \"dmpn\":\n",
    "    if net_params[\"input_layer_add\"]:\n",
    "        fignorm, axsnorm = plt.subplots(1,1,figsize=(4,4))\n",
    "        axsnorm.plot(counter_lst, [np.linalg.norm(Winput_matrix) for Winput_matrix in Winput_lst], \"-o\")\n",
    "        axsnorm.set_xscale(\"log\")\n",
    "        axsnorm.set_ylabel(\"Frobenius Norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f0805a-0938-48e5-976d-3682d45b4448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check, if W_freeze, then the recorded W matrix for the modulation layer should not be changed\n",
    "if net_params[\"ml_params\"][\"W_freeze\"]: \n",
    "    assert np.allclose(Wall_lst[-1][0], Wall_lst[0][0])\n",
    "\n",
    "if net_params[\"input_layer_bias\"]: \n",
    "    assert net_params[\"input_layer_add\"] is True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47256480-5b47-496e-887e-b8de35dcc8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    fig, ax = plt.subplots(1,1,figsize=(3,3))\n",
    "    ax.plot(net.hist['iters_monitor'][1:], net.hist['train_acc'][1:], color=c_vals[0], label='Full train accuracy')\n",
    "    ax.plot(net.hist['iters_monitor'][1:], net.hist['valid_acc'][1:], color=c_vals[1], label='Full valid accuracy')\n",
    "    if net.weight_reg is not None:\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['train_loss_output_label'], color=c_vals_l[0], zorder=-1, label='Output label')\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['train_loss_reg_term'], color=c_vals_l[0], zorder=-1, label='Reg term', linestyle='dashed')\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['valid_loss_output_label'], color=c_vals_l[1], zorder=-1, label='Output valid label')\n",
    "        ax.plot(net.hist['iters_monitor'], net.hist['valid_loss_reg_term'], color=c_vals_l[1], zorder=-1, label='Reg valid term', linestyle='dashed')\n",
    "    \n",
    "    # ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.set_ylim([0.90, 1.05])\n",
    "    # ax.set_ylabel('Loss ({})'.format(net.loss_type))\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_xlabel('# Batches')\n",
    "    plt.savefig(f\"./twotasks/loss_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "    \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e544aca8-271f-49d3-9ed3-ab7023f23600",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    net_helpers.net_eta_lambda_analysis(net, net_params, hyp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c10ab17-bab6-4228-9096-af042e9ac385",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_finalstage = False\n",
    "if use_finalstage:\n",
    "    # plotting output in the validation set\n",
    "    net_out, db = net.iterate_sequence_batch(test_input, run_mode='track_states')\n",
    "    W_output = net.W_output.detach().cpu().numpy()\n",
    "\n",
    "    W_all_ = []\n",
    "    for i in range(len(net.mp_layers)):\n",
    "        W_all_.append(net.mp_layers[i].W.detach().cpu().numpy())\n",
    "    W_ = W_all_[0]\n",
    "    \n",
    "else:\n",
    "    # ind = len(marker_lst)-1 \n",
    "    ind = -1\n",
    "    # network_at_percent = (marker_lst[ind]+1)/train_params['n_datasets']*100\n",
    "    # print(f\"Using network at {network_at_percent}%\")\n",
    "    # by default using the first test_input \n",
    "    net_out = netout_lst[0][ind]\n",
    "    db = db_lst[0][ind]\n",
    "    W_output = Woutput_lst[ind]\n",
    "    W_ = Wall_lst[ind][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8feb8c0-4029-49b1-b6e9-a5fd01ab5efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_input_output(test_input_np, net_out, test_output_np, test_task=None, tag=\"\", batch_num=5, label=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    test_input_np = helper.to_ndarray(test_input_np)\n",
    "    net_out = helper.to_ndarray(net_out)\n",
    "    test_output_np = helper.to_ndarray(test_output_np)\n",
    "    \n",
    "    fig_all, axs_all = plt.subplots(batch_num,2,figsize=(4*2,batch_num*2))\n",
    "    \n",
    "    if test_output_np.shape[-1] == 1:\n",
    "        for batch_idx, ax in enumerate(axs):\n",
    "            ax.plot(net_out[batch_idx, :, 0], color=c_vals[batch_idx])\n",
    "            ax.plot(test_output_np[batch_idx, :, 0], color=c_vals_l[batch_idx])\n",
    "    \n",
    "    else:\n",
    "        for batch_idx in range(batch_num):\n",
    "            for out_idx in range(test_output_np.shape[-1]):\n",
    "                axs_all[batch_idx,0].plot(net_out[batch_idx, :, out_idx], color=c_vals[out_idx], label=out_idx)\n",
    "                axs_all[batch_idx,0].plot(test_output_np[batch_idx, :, out_idx], color=c_vals_l[out_idx], linewidth=5, alpha=0.5)\n",
    "                if test_task is not None: \n",
    "                    outname = f\"{task_params['rules'][test_task[batch_idx]]}; {tag}\"\n",
    "                    axs_all[batch_idx,0].set_title(outname)\n",
    "                axs_all[batch_idx,0].legend()\n",
    "    \n",
    "            input_batch = test_input_np[batch_idx,:,:]\n",
    "            if task_params[\"randomize_inputs\"]: \n",
    "                input_batch = input_batch @ np.linalg.pinv(task_params[\"randomize_matrix\"])\n",
    "            for inp_idx in range(input_batch.shape[-1]):\n",
    "                axs_all[batch_idx,1].plot(input_batch[:,inp_idx], color=c_vals[inp_idx], label=inp_idx)\n",
    "                if test_task is not None: \n",
    "                    axs_all[batch_idx,1].set_title(f\"{task_params['rules'][test_task[batch_idx]]}; {tag}\")\n",
    "                axs_all[batch_idx,1].legend()\n",
    "\n",
    "    for ax in axs_all.flatten(): \n",
    "        ax.set_ylim([-2, 2])\n",
    "    fig_all.tight_layout()\n",
    "    fig_all.savefig(f\"./twotasks/lowD_{hyp_dict['ruleset']}_{hyp_dict['chosen_network']}_seed{seed}_{hyp_dict['addon_name']}_{tag}.png\", dpi=300)\n",
    "\n",
    "    return fig_all, axs_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d2228b-2da9-40aa-8cdc-e624560c4b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_input_output(test_input_np, net_out, test_output_np, test_task, tag=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a24480-1cee-46ca-9210-6fb310457468",
   "metadata": {},
   "outputs": [],
   "source": [
    "figld, axsld = plot_input_output(test_input_longdelay, netout_lst[3][ind], test_output_longdelay, \n",
    "                                 test_task_longdelay, tag=\"longdelay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b2fa7-d1c1-44a9-829d-374a519519ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "figlr, axslr = plot_input_output(test_input_longresponse, netout_lst[4][ind], test_output_longresponse, \n",
    "                                 test_task_longresponse, tag=\"longresponse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e010f86-ae8e-4178-a1e5-92ab41e586c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "figls, axsls = plot_input_output(test_input_longstimulus, netout_lst[2][ind], test_output_longstimulus, \n",
    "                                 test_task_longstimulus, tag=\"longstimulus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6077aaf-583d-4b78-8639-a8e8abb4aa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "figlf, axslf = plot_input_output(test_input_longfixation, netout_lst[1][ind], test_output_longfixation, \n",
    "                                 test_task_longfixation, tag=\"longfixation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beee7f6-5c64-48fb-9f21-697e5beeea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimensionality_measure(W):\n",
    "    \"\"\"\n",
    "    Dimensionality in recurrent spiking networks: Global trends in activity and local origins in\n",
    "    connectivity (Equation 3)\n",
    "    \"The dimensionality is a weighted measure of the number of axes explored by that cloud\"\n",
    "    Recanatesi, et al., 2019\n",
    "    return value in range of (0, 1]\n",
    "    \"\"\"\n",
    "    covW = np.cov(W)\n",
    "    assert covW.shape[0] == n_hidden\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(covW)\n",
    "    numerator = np.sum(eigenvalues) ** 2\n",
    "    denominator = np.sum(eigenvalues ** 2)\n",
    "    return (numerator / denominator) / W.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b78d25-9681-4c92-9fff-0cf68f6d1b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_label_task_comb(task_params, test_trials, test_task, color_by=\"stim\"):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    labels_resp, labels_stim = generate_response_stimulus(task_params, test_trials)\n",
    "    labels_ = labels_stim if color_by == \"stim\" else labels_resp\n",
    "    # pair: [label_first_dim, task_id] for each trial\n",
    "    return np.column_stack((labels_[:, 0], test_task))\n",
    "\n",
    "label_task_comb_longdelay = make_label_task_comb(\n",
    "    task_params_longdelay, test_trials_longdelay, test_task_longdelay, color_by=color_by\n",
    ")\n",
    "\n",
    "label_task_comb_longresponse = make_label_task_comb(\n",
    "    task_params_longresponse, test_trials_longresponse, test_task_longresponse, color_by=color_by\n",
    ")\n",
    "\n",
    "label_task_comb_longstimulus = make_label_task_comb(\n",
    "    task_params_longstimulus, test_trials_longstimulus, test_task_longstimulus, color_by=color_by\n",
    ")\n",
    "\n",
    "label_task_comb_longfixation = make_label_task_comb(\n",
    "    task_params_longfixation, test_trials_longfixation, test_task_longfixation, color_by=color_by\n",
    ")\n",
    "\n",
    "label_task_comb = make_label_task_comb(\n",
    "    task_params, test_trials, test_task, color_by=color_by\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2857b0d8-4c68-438f-a31f-ea911792ef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here db is selected based on learning stage selection \n",
    "\n",
    "layer_index = 0 # 1 layer MPN \n",
    "if net_params[\"input_layer_add\"]:\n",
    "    layer_index += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcc8798-2125-4915-b01d-d8befabc57dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [i for i in range(len(label_task_comb)) if (label_task_comb[i] == [5, 1]).all()]\n",
    "[db[f\"hidden{layer_index}\"][idx_].sum() for idx_ in idx]\n",
    "# print(db.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b513939c-1e16-45f1-97ea-8f71cf5dc4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_non_nan(arr, k):\n",
    "    \"\"\"\n",
    "    Pick `k` distinct (non-NaN) numbers from a 2-D NumPy array.\n",
    "    \"\"\"\n",
    "    pool = arr[~np.isnan(arr)]            # flatten & keep only real numbers\n",
    "    if k > pool.size:                     # ensure enough unique values\n",
    "        raise ValueError(\"k exceeds number of non-NaN entries.\")\n",
    "    return np.random.choice(pool, k, replace=False).tolist()\n",
    "\n",
    "\n",
    "def assert_sums_close(arr_list, rtol=1e-5, atol=1e-8):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    assert len(arr_list) > 0, \"Empty list.\"\n",
    "    sums = np.array([np.sum(a) for a in arr_list], dtype=float)\n",
    "    ref = sums[0]\n",
    "    ok = np.isclose(sums, ref, rtol=rtol, atol=atol)\n",
    "\n",
    "    if not np.all(ok):\n",
    "        bad = np.where(~ok)[0]\n",
    "        raise AssertionError(\n",
    "            f\"Sum mismatch at indices {bad.tolist()}.\\n\"\n",
    "            f\"ref_sum={ref}, bad_sums={sums[bad].tolist()}, all_sums={sums.tolist()}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c23ea03-ed43-4b9c-a4ca-b922020fa4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_similarity(Ms_orig, hs, net, net_params, label_task_comb, checktime, compare=\"modulation\", moddim=0): \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # print(f\"compare: {compare}; moddim: {moddim}\")\n",
    "    inverse_modulation_ss_dt = []\n",
    "    inverse_modulation_sr_dt = []\n",
    "    inverse_modulation_st_ds = [[], []]\n",
    "    modulation_save = [[],[]]\n",
    "    modulation_save_time = [[],[]]\n",
    "    hidden_save_time = [[],[]]\n",
    "    W = net.mp_layer1.W.data.detach().cpu().numpy()\n",
    "\n",
    "    if compare == \"w_modulation\":\n",
    "        Ms_orig = Ms_orig * W[None,None,:,:]\n",
    "\n",
    "    # same stimulus (effectively anti-response), different task\n",
    "    for k in range(8):\n",
    "        ind1 = [i for i, lst in enumerate(label_task_comb) if np.array_equal(lst, [k, 0])]\n",
    "        ind2 = [i for i, lst in enumerate(label_task_comb) if np.array_equal(lst, [k, 1])]\n",
    "        ll = min(len(ind1), len(ind2))\n",
    "\n",
    "        win = net.W_initial_linear.weight.data.detach().cpu().numpy()\n",
    "\n",
    "        # M @ win: effective plastic sensitivity\n",
    "        # How does the current memory (M) distort or amplify specific input channels?\n",
    "        if compare in (\"modulation\", \"w_modulation\"):                 \n",
    "            winadd = False if moddim == None else True\n",
    " \n",
    "            if winadd: \n",
    "                if moddim == \"Win\": \n",
    "                    Ms1_change_stimulus = [((Ms_orig[ind1[i],checktime,:,:]) @ win).flatten() for i in range(ll)]\n",
    "                    Ms2_change_stimulus = [((Ms_orig[ind2[i],checktime,:,:]) @ win).flatten() for i in range(ll)]\n",
    "                else:\n",
    "                    Ms1_change_stimulus = [((Ms_orig[ind1[i],checktime,:,:]) @ win)[:,moddim].flatten() for i in range(ll)]\n",
    "                    Ms2_change_stimulus = [((Ms_orig[ind2[i],checktime,:,:]) @ win)[:,moddim].flatten() for i in range(ll)]\n",
    "            else:\n",
    "                Ms1_change_stimulus = [(Ms_orig[ind1[i],checktime,:,:]).flatten() for i in range(ll)]\n",
    "                Ms2_change_stimulus = [(Ms_orig[ind2[i],checktime,:,:]).flatten() for i in range(ll)]\n",
    "                \n",
    "        elif compare == \"hidden\": \n",
    "            Ms1_change_stimulus = [hs[ind1[i],checktime,:].flatten() for i in range(ll)]\n",
    "            Ms2_change_stimulus = [hs[ind2[i],checktime,:].flatten() for i in range(ll)]\n",
    "\n",
    "        # since we group by the same stimulus at the same task\n",
    "        # the period is aligned in all trials of testing data\n",
    "        # therefore the resulting modulation & hidden should be identical\n",
    "        assert_sums_close(Ms1_change_stimulus, rtol=1e-3, atol=1e-3)\n",
    "        assert_sums_close(Ms2_change_stimulus, rtol=1e-3, atol=1e-3)\n",
    "        \n",
    "        inverse_modulation_ss_dt.append(1 - cosine(Ms1_change_stimulus[0], Ms2_change_stimulus[0]))\n",
    "\n",
    "        modulation_save[0].append(Ms1_change_stimulus[0])\n",
    "        modulation_save[1].append(Ms2_change_stimulus[0])\n",
    "\n",
    "        # modulation for one type of stimulus for two tasks\n",
    "        Ms1_all = Ms_orig[ind1[0],:,:,:]\n",
    "        Ms2_all = Ms_orig[ind2[0],:,:,:]\n",
    "        h1_all = hs[ind1[0],:,:]\n",
    "        h2_all = hs[ind2[0],:,:]\n",
    "        # save the whole modulation trajectory for this stimulus (go and anti)\n",
    "        modulation_save_time[0].append(Ms1_all)\n",
    "        modulation_save_time[1].append(Ms2_all)\n",
    "        hidden_save_time[0].append(h1_all)\n",
    "        hidden_save_time[1].append(h2_all)\n",
    "\n",
    "    # same response, different task \n",
    "    for k in range(8):\n",
    "        ind1 = [i for i, lst in enumerate(label_task_comb) if np.array_equal(lst, [k, 0])]\n",
    "        ind2 = [i for i, lst in enumerate(label_task_comb) if np.array_equal(lst, [(k + 4) % 8, 1])]\n",
    "        ll = min(len(ind1), len(ind2))\n",
    "\n",
    "        if compare in (\"modulation\", \"w_modulation\"): \n",
    "            winadd = False if moddim == None else True\n",
    " \n",
    "            if winadd: \n",
    "                if moddim == \"Win\": \n",
    "                    Ms1_change_stimulus = [((Ms_orig[ind1[i],checktime,:,:]) @ win).flatten() for i in range(ll)]\n",
    "                    Ms2_change_stimulus = [((Ms_orig[ind2[i],checktime,:,:]) @ win).flatten() for i in range(ll)]\n",
    "                else:\n",
    "                    Ms1_change_stimulus = [((Ms_orig[ind1[i],checktime,:,:]) @ win)[:,moddim].flatten() for i in range(ll)]\n",
    "                    Ms2_change_stimulus = [((Ms_orig[ind2[i],checktime,:,:]) @ win)[:,moddim].flatten() for i in range(ll)]\n",
    "            else:\n",
    "                Ms1_change_stimulus = [(Ms_orig[ind1[i],checktime,:,:]).flatten() for i in range(ll)]\n",
    "                Ms2_change_stimulus = [(Ms_orig[ind2[i],checktime,:,:]).flatten() for i in range(ll)]\n",
    "                \n",
    "        elif compare == \"hidden\": \n",
    "            Ms1_change_stimulus = [hs[ind1[i],checktime,:].flatten() for i in range(ll)]\n",
    "            Ms2_change_stimulus = [hs[ind2[i],checktime,:].flatten() for i in range(ll)]\n",
    "\n",
    "        assert_sums_close(Ms1_change_stimulus, rtol=1e-3, atol=1e-3)\n",
    "        assert_sums_close(Ms2_change_stimulus, rtol=1e-3, atol=1e-3)\n",
    "        \n",
    "        inverse_modulation_sr_dt.append(1 - cosine(Ms1_change_stimulus[0], Ms2_change_stimulus[0]))\n",
    "\n",
    "    # same task, different stimulus \n",
    "    repeat = 100\n",
    "    modulation_matrices_all = [] \n",
    "    for _ in range(repeat): \n",
    "        modulation_matrices = [\n",
    "            np.full((len(modulation_save[0]), len(modulation_save[0])), np.nan), \n",
    "            np.full((len(modulation_save[0]), len(modulation_save[0])), np.nan)\n",
    "        ]\n",
    "        \n",
    "        for i in range(len(modulation_save[0])):\n",
    "            for j in range(i+1, len(modulation_save[0])):\n",
    "                modulation_matrices[0][i,j] = 1 - cosine(modulation_save[0][i], modulation_save[0][j])\n",
    "                modulation_matrices[1][i,j] = 1 - cosine(modulation_save[1][i], modulation_save[1][j])\n",
    "\n",
    "        try: \n",
    "            modulation_matrices_all.append([np.nanmean(sample_non_nan(modulation_matrices[0], 8)),\n",
    "                                            np.nanmean(sample_non_nan(modulation_matrices[1], 8))])\n",
    "        except Exception as e:\n",
    "            modulation_matrices_all.append([np.nan, np.nan])\n",
    "\n",
    "    modulation_matrices_all = np.array(modulation_matrices_all)\n",
    "\n",
    "    result = [[np.mean(inverse_modulation_ss_dt), np.std(inverse_modulation_ss_dt)], \n",
    "              [np.mean(inverse_modulation_sr_dt), np.std(inverse_modulation_sr_dt)], \n",
    "              [np.mean(modulation_matrices_all[:,0]), np.std(modulation_matrices_all[:,0])], \n",
    "              [np.mean(modulation_matrices_all[:,1]), np.std(modulation_matrices_all[:,1])]]\n",
    "\n",
    "    return result, modulation_save_time, hidden_save_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1f92a0-c04f-4a7a-8b08-e753efb1b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modulation_extraction(test_input, db, layer_index, cuda=False):\n",
    "    \"\"\"\n",
    "    Extracts modulation tensors from `db` and returns:\n",
    "        Ms:      (batch, seq, features) reshaped version of M\n",
    "        Ms_orig: original M (no reshape)\n",
    "        hs:      (batch, seq, features) reshaped version of hidden\n",
    "        bs:      bias vector/matrix as-is (or concatenated if list)\n",
    "    \"\"\"\n",
    "\n",
    "    def _to_numpy(x):\n",
    "        # Convert torch.Tensor -> numpy, otherwise np.asarray\n",
    "        try:\n",
    "            import torch\n",
    "            if isinstance(x, torch.Tensor):\n",
    "                return x.detach().cpu().numpy()\n",
    "        except Exception:\n",
    "            pass\n",
    "        return np.asarray(x)\n",
    "\n",
    "    def _concat_last(x):\n",
    "        # If list/tuple of arrays: concatenate on last axis; else return as-is\n",
    "        return np.concatenate(x, axis=-1) if isinstance(x, (list, tuple)) else x\n",
    "\n",
    "    n_batch, max_seq_len = test_input.shape[0], test_input.shape[1]\n",
    "\n",
    "    # M\n",
    "    M_raw = _concat_last(_to_numpy(db[f\"M{layer_index}\"]))\n",
    "    Ms = M_raw.reshape(n_batch, max_seq_len, -1)\n",
    "    Ms_orig = M_raw  # unreshaped\n",
    "\n",
    "    # b\n",
    "    bs = _concat_last(_to_numpy(db[f\"b{layer_index}\"]))\n",
    "\n",
    "    # hidden\n",
    "    H_raw = _concat_last(_to_numpy(db[f\"hidden{layer_index}\"]))\n",
    "    hs = H_raw.reshape(n_batch, max_seq_len, -1)\n",
    "\n",
    "    return Ms, Ms_orig, hs, bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a350fae9-ae28-4341-a5e9-a6d98bd227f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# across training stage\n",
    "result_attractor_all_h, result_attractor_all_m, result_attractor_all_wmwin = [], [], []\n",
    "modulation_save_time = []\n",
    "pr_all = [] \n",
    "test_input_long_all = [test_input, test_input_longfixation, test_input_longstimulus, \n",
    "                       test_input_longdelay, test_input_longresponse]\n",
    "label_task_comb_long_all = [label_task_comb, label_task_comb_longfixation, label_task_comb_longstimulus, \n",
    "                            label_task_comb_longdelay, label_task_comb_longresponse]\n",
    "# register the timestamps information for different long periods\n",
    "time_stamps_usual, time_stamps_longfixation, time_stamps_longstimulus, time_stamps, time_stamps_longresponse = {}, {}, {}, {}, {}\n",
    "\n",
    "for i in range(5): \n",
    "    for db_attractor in db_lst[i]:\n",
    "        _, M_long, h_long, _, = modulation_extraction(test_input_long_all[i], db_attractor, layer_index)\n",
    "    \n",
    "        prs = [dimensionality_measure(h_long[i,:,:].T) for i in range(h_long.shape[0])]\n",
    "        \n",
    "        if i == 0: \n",
    "            pr_all.append([np.mean(prs), np.std(prs)])\n",
    "        \n",
    "        # to handle noise, find the time when fixation is off\n",
    "        checktime_sample = test_input_long_all[i][0,:,0].detach().cpu()\n",
    "        mask = checktime_sample < 0.5                          \n",
    "        idx = torch.nonzero(mask, as_tuple=False) \n",
    "        checktime_attractor = idx[0].item()  \n",
    "\n",
    "        if i == 3: \n",
    "            time_stamps[\"delay_end\"] = checktime_attractor - 1 # a little bit before the fixation off\n",
    "        elif i == 4:\n",
    "            time_stamps_longresponse[\"delay_end\"] = checktime_attractor - 1\n",
    "        elif i == 0: \n",
    "            time_stamps_usual[\"delay_end\"] = checktime_attractor - 1\n",
    "            cc = time_stamps_usual[\"delay_end\"] \n",
    "        elif i == 2: \n",
    "            time_stamps_longstimulus[\"delay_end\"] = checktime_attractor - 1\n",
    "        elif i == 1:\n",
    "            time_stamps_longfixation[\"delay_end\"] = checktime_attractor - 1\n",
    "\n",
    "        if i == 0: \n",
    "            result_attractor_h, _, _ = analyze_similarity(M_long, h_long, net, net_params, label_task_comb_long_all[i], \n",
    "                                                            checktime=cc, compare=\"hidden\")\n",
    "            result_attractor_m, m_save, _ = analyze_similarity(M_long, h_long, net, net_params, label_task_comb_long_all[i], \n",
    "                                                            checktime=cc, compare=\"modulation\")\n",
    "            result_attractor_wmwin, _, _ = analyze_similarity(M_long, h_long, net, net_params, label_task_comb_long_all[i], \n",
    "                                                            checktime=cc, compare=\"w_modulation\", moddim=\"Win\")\n",
    "            \n",
    "            result_attractor_all_h.append(result_attractor_h)\n",
    "            result_attractor_all_m.append(result_attractor_m)\n",
    "            result_attractor_all_wmwin.append(result_attractor_wmwin)\n",
    "            modulation_save_time.append(m_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e14f96-fac6-4793-a721-4740b9c8d9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_out = net.W_output.data.detach().cpu().numpy()\n",
    "db_lst[0][-1][f\"b{layer_index}\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397a4952-20cc-4e7d-81da-4513ac52a37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "figattractor, axsattractor = plt.subplots(1,3,figsize=(4*3,4))\n",
    "break_names = [\n",
    "    \"Same Stim\",\n",
    "    \"Same Resp\",\n",
    "    \"MemoryPro Diff Stim\",\n",
    "    \"MemoryAnti Diff Stim\",\n",
    "]\n",
    "\n",
    "def plot_mean_std(ax, x, mean, std, color, fill_color, label):\n",
    "    ax.plot(x, mean, \"-o\", color=color, label=label)\n",
    "    ax.fill_between(x, np.asarray(mean) - np.asarray(std), np.asarray(mean) + np.asarray(std),\n",
    "                    alpha=0.5, color=fill_color)\n",
    "\n",
    "# each entry: (results, axis_index)\n",
    "panels = [\n",
    "    (result_attractor_all_h, 0),\n",
    "    (result_attractor_all_m, 1),\n",
    "    (result_attractor_all_wmwin, 2)\n",
    "]\n",
    "\n",
    "n_groups = len(result_attractor_all_h[0])  # same as your original len(result_attractor_all[0])\n",
    "\n",
    "for i in range(n_groups):\n",
    "    for results, ax_idx in panels:\n",
    "        mean = [rs[i][0] for rs in results]\n",
    "        std  = [rs[i][1] for rs in results]\n",
    "        plot_mean_std(\n",
    "            axsattractor[ax_idx],\n",
    "            counter_lst,\n",
    "            mean,\n",
    "            std,\n",
    "            color=c_vals[i],\n",
    "            fill_color=c_vals_l[i],\n",
    "            label=break_names[i],\n",
    "        )\n",
    "\n",
    "for ax in axsattractor:\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.legend()\n",
    "    ax.set_ylabel(\"Cosine Similarity\", fontsize=12)\n",
    "    ax.set_xlabel(\"Iteration\", fontsize=12)\n",
    "    \n",
    "axsattractor[0].set_ylim([0,1.05])\n",
    "axsattractor[1].set_ylim([-1.05,1.05])\n",
    "    \n",
    "axsattractor[0].set_title(\"Hidden\", fontsize=12)\n",
    "axsattractor[1].set_title(\"Modulation\", fontsize=12)\n",
    "\n",
    "figattractor.tight_layout()\n",
    "figattractor.savefig(\n",
    "    f\"./twotasks/attractor_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\",\n",
    "    dpi=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629d602b-e46b-465c-bccb-f55c6b3a8634",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_hm_similarity = {\n",
    "    \"break_names\": break_names, \n",
    "    \"counter_lst\": counter_lst, \n",
    "    \"result_attractor_all_h\": result_attractor_all_h, \n",
    "    \"result_attractor_all_m\": result_attractor_all_m,\n",
    "    \"result_attractor_all_wmwin\": result_attractor_all_wmwin\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa1f3a6-af5b-445c-8cab-766721181c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# across different timestamp \n",
    "def time_stamp_extract(test_input_long, time_stamps):\n",
    "    stimulus_end = None \n",
    "    chosen_batch = 0\n",
    "    while stimulus_end is None: \n",
    "        try: \n",
    "            input_part = test_input_long[chosen_batch,:,2:2+4].detach().cpu().numpy()\n",
    "            input_part_sum = np.sum(input_part, axis=1)\n",
    "            stimulus_end = np.where(input_part_sum > 0.5)[0][-1]\n",
    "            stimulus_start = np.where(input_part_sum > 0.5)[0][0] - 1\n",
    "        except IndexError: \n",
    "            chosen_batch += 1\n",
    "    \n",
    "    time_stamps[\"stimulus_start\"] = stimulus_start + 1\n",
    "    time_stamps[\"stimulus_end\"] = stimulus_end + 1\n",
    "    time_stamps[\"delay_start\"] = stimulus_end + 1\n",
    "    time_stamps[\"trial_end\"] = len(input_part_sum) - 1\n",
    "    time_stamps[\"fixation_end\"] = stimulus_start\n",
    "    time_stamps[\"fixation_start\"] = 0 \n",
    "\n",
    "    return time_stamps\n",
    "\n",
    "time_stamps = time_stamp_extract(test_input_longdelay, time_stamps)\n",
    "time_stamps_longresponse = time_stamp_extract(test_input_longresponse, time_stamps_longresponse)\n",
    "time_stamps_usual = time_stamp_extract(test_input, time_stamps_usual)\n",
    "time_stamps_longstimulus = time_stamp_extract(test_input_longstimulus, time_stamps_longstimulus)\n",
    "time_stamps_longfixation = time_stamp_extract(test_input_longfixation, time_stamps_longfixation)\n",
    "\n",
    "print(f\"time_stamps: {time_stamps}\")\n",
    "print(f\"time_stamps_longresponse: {time_stamps_longresponse}\")\n",
    "print(f\"time_stamps_usual: {time_stamps_usual}\")\n",
    "print(f\"time_stamps_longstimulus: {time_stamps_longstimulus}\")\n",
    "print(f\"time_stamps_longfixation: {time_stamps_longfixation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f421b73-06bf-42fe-87b4-e4449d3bbd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_all = db_lst[0][-1][f\"M{layer_index}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368d5da3-0b9d-4e35-a71b-56d661d5786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_change(U, X):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    eps = 1e-12\n",
    "    \n",
    "    # U: (B, T, 3), X: (B, T, 200) as numpy arrays\n",
    "    u2 = np.sum(U**2, axis=-1)   # (B, T)\n",
    "    x2 = np.sum(X**2, axis=-1)   # (B, T)\n",
    "    \n",
    "    # A) Global RMS gain (stable)\n",
    "    g_rms = np.sqrt(x2.mean()) / (np.sqrt(u2.mean()) + eps)\n",
    "    \n",
    "    # Per-(b,t) gain distribution\n",
    "    g_per = np.sqrt(x2) / (np.sqrt(u2) + eps)  # (B, T)\n",
    "    \n",
    "    g_median = np.median(g_per)\n",
    "    g_p10, g_p90 = np.quantile(g_per, [0.10, 0.90])\n",
    "\n",
    "    return g_rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0badfed-5f67-4bfa-a6e1-c272395cf0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainable_parameters(model):\n",
    "    \"\"\"Return a list of (name, parameter) for trainable parameters.\"\"\"\n",
    "    return [(name) for name, p in model.named_parameters() if p.requires_grad]\n",
    "\n",
    "print(trainable_parameters(net))\n",
    "m_save = modulation_save_time[-1]\n",
    "\n",
    "projs_all = [[], [], []]\n",
    "\n",
    "fig, axs = plt.subplots(8,2,figsize=(4*2,8*2))\n",
    "for i in range(8): \n",
    "    mod1_stim1 = m_save[0][i]\n",
    "    mod2_stim1 = m_save[1][i]\n",
    "    \n",
    "    M_all = db_lst[0][-1][f\"M{layer_index}\"]\n",
    "    \n",
    "    for j in range(M_all.shape[0]):\n",
    "        if np.sum(np.abs(M_all[j,:,:,:]-mod1_stim1)) <= 1e-3:\n",
    "            mod1_j = j\n",
    "        if np.sum(np.abs(M_all[j,:,:,:]-mod2_stim1)) <= 1e-3:\n",
    "            mod2_j = j\n",
    "\n",
    "    all_input = db_lst[0][-1][f\"input{layer_index}\"]\n",
    "    input_mod1 = all_input[mod1_j]\n",
    "    input_orig = test_input_np\n",
    "    shrink = input_change(input_orig, all_input)\n",
    "    \n",
    "    def keep_k_dims_zero_rest(X, keep_idx):\n",
    "        \"\"\"\n",
    "        X: (T, N)\n",
    "        keep_idx: iterable of column indices to keep (length K)\n",
    "        returns: (T, N) with only those columns kept; others set to 0\n",
    "        \"\"\"\n",
    "        Y = np.zeros_like(X)\n",
    "        Y[:, keep_idx] = X[:, keep_idx]\n",
    "        return Y\n",
    "\n",
    "    if net_params[\"input_layer_add\"]:\n",
    "        W = net.mp_layer1.W.data.detach().cpu().numpy()\n",
    "        W_out = net.W_output.data.detach().cpu().numpy()\n",
    "        W_in = Winput_lst[-1]\n",
    "        bias = net.mp_layer1.b.data.detach().cpu().numpy()\n",
    "\n",
    "        x_fix_on = np.array([1,0,0,0,0,0,0,0], dtype=float)\n",
    "        x_fix_off = np.array([0,1,0,0,0,0,0,0])\n",
    "        null = np.array([0,0,0,0,0,0,0,0], dtype=float)\n",
    "\n",
    "        x_task1 = np.array([0,0,0,0,0,0,1,0], dtype=float)\n",
    "        x_task2 = np.array([0,0,0,0,0,0,0,1], dtype=float)\n",
    "        x_fixoff = np.array([0,1,0,0,0,0,0,0], dtype=float)\n",
    "\n",
    "        x_task1_all, x_task2_all = [], []\n",
    "        x_fix_on_all = []\n",
    "        x_fix_off_all = []\n",
    "        for T in range(mod1_stim1.shape[0]):\n",
    "            if T <= time_stamps_usual[\"delay_end\"]:\n",
    "                x_task1_all.append(x_task1)\n",
    "                x_task2_all.append(x_task2)\n",
    "                x_fix_on_all.append(x_fix_on)\n",
    "                x_fix_off_all.append(null)\n",
    "            else:\n",
    "                x_task1_all.append(x_task1 + x_fixoff)\n",
    "                x_task2_all.append(x_task2 + x_fixoff)\n",
    "                x_fix_on_all.append(null)\n",
    "                x_fix_off_all.append(x_fix_off)\n",
    "        \n",
    "        Y_resp_cos, Y_resp_sin = W_out[1,:].reshape(1,-1), W_out[2,:].reshape(1,-1)\n",
    "\n",
    "        fixon_proj1 = np.stack([Y_resp_cos @ (W + W * mod1_stim1[T]) @ (W_in @ x_fix_on_all[T]) \n",
    "                                for T in range(mod1_stim1.shape[0])],axis=0)\n",
    "        fixon_proj2 = np.stack([Y_resp_cos @ (W + W * mod2_stim1[T]) @ (W_in @ x_fix_on_all[T]) \n",
    "                                for T in range(mod1_stim1.shape[0])],axis=0)\n",
    "        x_task1_proj = np.stack([Y_resp_cos @ (W + W * mod1_stim1[T]) @ (W_in @ x_task1_all[T]) \n",
    "                                 for T in range(mod1_stim1.shape[0])],axis=0)\n",
    "        x_task2_proj = np.stack([Y_resp_cos @ (W + W * mod2_stim1[T]) @ (W_in @ x_task2_all[T]) \n",
    "                                 for T in range(mod1_stim1.shape[0])],axis=0)\n",
    "        fixoff_proj1 = np.stack([Y_resp_cos @ (W + W * mod1_stim1[T]) @ (W_in @ x_fix_off_all[T]) \n",
    "                                 for T in range(mod1_stim1.shape[0])],axis=0)\n",
    "        fixoff_proj2 = np.stack([Y_resp_cos @ (W + W * mod2_stim1[T]) @ (W_in @ x_fix_off_all[T]) \n",
    "                                 for T in range(mod1_stim1.shape[0])],axis=0)\n",
    "\n",
    "        bias_proj = Y_resp_cos @ bias\n",
    "                \n",
    "    else:\n",
    "        x_fix_on = keep_k_dims_zero_rest(input_mod1,0)\n",
    "        x_task1 = keep_k_dims_zero_rest(input_mod1,6)\n",
    "        x_task2 = keep_k_dims_zero_rest(input_mod1,7)\n",
    "    \n",
    "        W = net.mp_layer0.W.data.detach().cpu().numpy()\n",
    "        W_out = net.W_output.data.detach().cpu().numpy()\n",
    "        bias = net.mp_layer0.b.data.detach().cpu().numpy()\n",
    "        \n",
    "        Y_resp_cos, Y_resp_sin = W_out[1,:].reshape(1,-1), W_out[2,:].reshape(1,-1)\n",
    "        fixon_proj1 = np.stack([Y_resp_cos @ (W + W * mod1_stim1[T]) @ (x_fix_on[T]) \n",
    "                                for T in range(mod1_stim1.shape[0])],axis=0)\n",
    "        fixon_proj2 = np.stack([Y_resp_cos @ (W + W * mod2_stim1[T]) @ (x_fix_on[T]) \n",
    "                                for T in range(mod1_stim1.shape[0])],axis=0)\n",
    "        x_task1_proj = np.stack([Y_resp_cos @ (W + W * mod1_stim1[T]) @ (x_task1[T]) \n",
    "                                 for T in range(mod1_stim1.shape[0])],axis=0)\n",
    "        x_task2_proj = np.stack([Y_resp_cos @ (W + W * mod2_stim1[T]) @ (x_task2[T]) \n",
    "                                 for T in range(mod1_stim1.shape[0])],axis=0)\n",
    "        bias_proj = Y_resp_cos @ bias\n",
    "    \n",
    "    axs[i,0].plot(fixon_proj1 + x_task1_proj + bias_proj, \n",
    "                  color=c_vals[0], label=\"Combine\", linewidth=3, alpha=0.5)\n",
    "    axs[i,0].plot(fixon_proj1, color=c_vals[1], label=\"Fix On\")\n",
    "    axs[i,0].plot(fixoff_proj1, color=c_vals[3], label=\"Fixoff\")\n",
    "    axs[i,0].plot(x_task1_proj + bias_proj, color=c_vals[2], label=\"Task + Bias\")\n",
    "    axs[i,1].plot(fixon_proj2 + x_task2_proj + bias_proj, \n",
    "                  color=c_vals[0], label=\"Combine\", linewidth=3, alpha=0.5)\n",
    "    axs[i,1].plot(fixon_proj2, color=c_vals[1], label=\"Fix On\")\n",
    "    axs[i,1].plot(fixoff_proj2, color=c_vals[3], label=\"Fixoff\")\n",
    "    axs[i,1].plot(x_task2_proj + bias_proj, color=c_vals[2], label=\"Task + Bias\")\n",
    "\n",
    "    # projection of fixon \n",
    "    T_delayend = time_stamps_usual[\"delay_end\"] - 1\n",
    "\n",
    "    h1 = (W + W * mod1_stim1[T_delayend]) @ (W_in @ x_fix_on_all[T_delayend])  # (N,)\n",
    "    h2 = (W + W * mod2_stim1[T_delayend]) @ (W_in @ x_fix_on_all[T_delayend])  # (N,)\n",
    "    h_sum = (h1 + h2).reshape(-1)  # (N,)\n",
    "\n",
    "    y = Y_resp_cos.reshape(-1)\n",
    "    y = y / (np.linalg.norm(y) + 1e-12)  # unit (N,)\n",
    "    \n",
    "    fixon_proj1 = float(y @ h1.reshape(-1))\n",
    "    fixon_proj2 = float(y @ h2.reshape(-1))\n",
    "    projs_all[0].append([np.abs(fixon_proj1 + fixon_proj2), np.abs(fixon_proj1), np.abs(fixon_proj2)])\n",
    "\n",
    "    # orthogonal complement of readout of fixon\n",
    "    P_perp = np.eye(y.size) - np.outer(y, y)\n",
    "    \n",
    "    def proj_perp_norm(h_vec):\n",
    "        h_vec = h_vec.reshape(-1)\n",
    "        return np.linalg.norm(P_perp @ h_vec)\n",
    "    \n",
    "    perp_ctrl = proj_perp_norm(h1) + proj_perp_norm(h2)\n",
    "    projs_all[1].append([np.abs(perp_ctrl), np.abs(proj_perp_norm(h1)), np.abs(proj_perp_norm(h2))])\n",
    "\n",
    "    # --- random orthogonal 1D controls (many random directions in y^) ---\n",
    "    def sample_random_orthogonal_matrix(y_unit, n_dirs=64, seed=0, max_tries=50):\n",
    "        \"\"\"\n",
    "        Returns U of shape (K, N) with rows ~ uniform random directions in y^.\n",
    "        Each row is unit norm and orthogonal to y_unit.\n",
    "        \"\"\"\n",
    "        rng = np.random.default_rng(seed)\n",
    "        N = y_unit.size\n",
    "        U = np.empty((n_dirs, N), dtype=float)\n",
    "        k = 0\n",
    "        tries = 0\n",
    "        while k < n_dirs:\n",
    "            tries += 1\n",
    "            if tries > max_tries * n_dirs:\n",
    "                raise RuntimeError(\"Failed to sample enough orthogonal directions.\")\n",
    "            u = rng.standard_normal(N)\n",
    "            u = u - (u @ y_unit) * y_unit          # remove component along y\n",
    "            nu = np.linalg.norm(u)\n",
    "            if nu < 1e-10:\n",
    "                continue\n",
    "            U[k] = u / nu\n",
    "            k += 1\n",
    "        return U\n",
    "    \n",
    "    U = sample_random_orthogonal_matrix(y, n_dirs=64, seed=1)  # (K, N)\n",
    "    \n",
    "    # projections onto those random orthogonal directions: (K,)\n",
    "    proj_K = U @ h_sum\n",
    "    \n",
    "    # good scalar summary: RMS magnitude in random orthogonal 1D directions\n",
    "    rand_ctrl_rms = float(np.sqrt(np.mean(proj_K**2)))   # scalar >= 0\n",
    "    \n",
    "    # (optional) another summary: typical magnitude (mean abs)\n",
    "    rand_ctrl_meanabs = float(np.mean(np.abs(proj_K)))   # scalar >= 0\n",
    "    \n",
    "    projs_all[2].append([np.abs(rand_ctrl_rms)])   # or append rand_ctrl_meanabs\n",
    "    \n",
    "for i in range(8): \n",
    "    for j in range(2): \n",
    "        axs[i,j].axvline(time_stamps_usual[\"fixation_end\"])\n",
    "        axs[i,j].axvline(time_stamps_usual[\"stimulus_end\"])\n",
    "        axs[i,j].axvline(time_stamps_usual[\"delay_end\"])\n",
    "        axs[i,j].set_ylim([-1.5,1.5])\n",
    "        axs[i,j].set_xlabel(\"Timestep\", fontsize=12)\n",
    "        axs[i,j].set_ylabel(\"Proj Cos Mag\", fontsize=12)\n",
    "        axs[i,j].set_title(f\"Stimulus {i}\", fontsize=12)\n",
    "        if i == 2 and j == 0: \n",
    "            axs[i,j].legend(loc=\"upper left\", frameon=True)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\n",
    "    f\"./twotasks/cancel_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\",\n",
    "    dpi=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d743e71-3ce7-4846-b33d-55f1101788c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(4,4))\n",
    "for i in range(len(projs_all)):\n",
    "    for k in range(8):\n",
    "        ax.scatter(i, projs_all[i][k][0], color=c_vals[i])\n",
    "\n",
    "ax.set_xticks([i for i in range(len(projs_all))])\n",
    "ax.set_xticklabels([\"Projection to Cosine Output\", \n",
    "                    \"Orthogonal Complement\", \n",
    "                    \"Random Vector\"], rotation=10, fontsize=12)\n",
    "ax.set_ylabel(\"Cancelation between Same Stimulus\", fontsize=12)\n",
    "ax.tick_params(axis=\"both\", which=\"both\", labelsize=12)\n",
    "ax.set_yscale(\"log\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(\n",
    "    f\"./twotasks/outputsubspace_cancel_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\",\n",
    "    dpi=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf40575-09a9-4680-b17a-80a6b77dd9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(4,4))\n",
    "for i in range(2):\n",
    "    for k in range(8):\n",
    "        ax.scatter(i, projs_all[i][k][1], color=c_vals[i])\n",
    "\n",
    "ax.set_xticks([i for i in range(2)])\n",
    "ax.set_xticklabels([\"Projection to Cosine Output\", \n",
    "                    \"Orthogonal Complement\"], rotation=10, fontsize=12)\n",
    "ax.set_ylabel(\"Projection Magnitude\", fontsize=12)\n",
    "ax.tick_params(axis=\"both\", which=\"both\", labelsize=12)\n",
    "ax.set_yscale(\"log\")\n",
    "fig.tight_layout\n",
    "fig.savefig(\n",
    "    f\"./twotasks/outputsubspace_mag_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\",\n",
    "    dpi=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58a22e4-08ae-490a-85c6-4f3ec4fae599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure2A_pca_fve(\n",
    "    H,\n",
    "    task_id,\n",
    "    periods,\n",
    "    k=2,\n",
    "    max_pcs=10,\n",
    "    center=\"global\",          # {\"global\", \"none\"}\n",
    "    flatten=\"trial_time\",     # currently only \"trial_time\"\n",
    "    dtype=np.float64,\n",
    "    return_cross_task=True,   # NEW\n",
    "):\n",
    "    \"\"\"\n",
    "    Figure 2A-style cross-period PCA explained-variance analysis.\n",
    "\n",
    "    NEW: optionally compute a cross-task, cross-period FVE matrix across all (task, period)\n",
    "         pairs and return it as `fve_k_all` (Q x Q). If you have 2 tasks each with P periods,\n",
    "         Q=2P and fve_k_all is 2P x 2P.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results : dict\n",
    "        results[task] = {\n",
    "            \"period_names\": list[str] length P_task,\n",
    "            \"fve_k\": np.ndarray (P_task, P_task), rows=target period X, cols=basis period Y,\n",
    "            \"evr_curves\": np.ndarray (P_task, max_pcs),\n",
    "            \"pca\": {period_name: {\"components\": (N, r), \"singular_values\": (r,), \"mean\": (N,), \"evr\": (r,)}}\n",
    "        }\n",
    "        plus, if return_cross_task:\n",
    "        results[\"__cross_task__\"] = {\n",
    "            \"labels\": list[tuple(task, period_name)] length Q,\n",
    "            \"fve_k_all\": np.ndarray (Q, Q), rows=target (task,period), cols=basis (task,period)\n",
    "        }\n",
    "    \"\"\"\n",
    "    # --- helper: convert torch -> numpy without requiring torch import\n",
    "    if hasattr(H, \"detach\"):\n",
    "        H_np = H.detach().cpu().numpy()\n",
    "    else:\n",
    "        H_np = np.asarray(H)\n",
    "    H_np = H_np.astype(dtype, copy=False)\n",
    "\n",
    "    task_id = np.asarray(task_id)\n",
    "    B, T, N = H_np.shape\n",
    "\n",
    "    def _get_period_matrix(H_task, t0, t1):\n",
    "        X = H_task[:, t0:t1, :]  # (B_task, L, N)\n",
    "        if flatten == \"trial_time\":\n",
    "            X = X.reshape(-1, N)  # (B_task*L, N)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported flatten mode: {flatten}\")\n",
    "        return X\n",
    "\n",
    "    def _center(X, mean=None):\n",
    "        if center == \"none\":\n",
    "            mu = np.zeros((X.shape[1],), dtype=X.dtype) if mean is None else mean\n",
    "            return X, mu\n",
    "        if mean is None:\n",
    "            mu = X.mean(axis=0)\n",
    "        return X - mu, mu\n",
    "\n",
    "    def _pca_svd(X, r):\n",
    "        \"\"\"\n",
    "        PCA via SVD on centered data X (M,N).\n",
    "        Returns components V (N,r_eff), EVR (r_eff,), singular values (r_eff,).\n",
    "        \"\"\"\n",
    "        U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "        r_eff = min(r, Vt.shape[0])\n",
    "        V = Vt[:r_eff, :].T  # (N, r_eff)\n",
    "        S = S[:r_eff]\n",
    "\n",
    "        M = X.shape[0]\n",
    "        denom = (np.sum(X * X) / max(M - 1, 1))\n",
    "        evals = (S * S) / max(M - 1, 1)\n",
    "        evr = evals / denom if denom > 0 else np.zeros_like(evals)\n",
    "        return V, S, evr\n",
    "\n",
    "    def _fve_project(X, V):\n",
    "        \"\"\"\n",
    "        Fraction of variance explained by projecting X onto span(V).\n",
    "        X should be centered consistently with the PCA basis.\n",
    "        V: (N, r)\n",
    "        \"\"\"\n",
    "        tot = np.sum(X * X)\n",
    "        if tot <= 0:\n",
    "            return 0.0\n",
    "        XV = X @ V\n",
    "        Xhat = XV @ V.T\n",
    "        num = np.sum(Xhat * Xhat)\n",
    "        return float(num / tot)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Per-task results (unchanged)\n",
    "    # -----------------------------\n",
    "    results = {}\n",
    "\n",
    "    # For cross-task aggregation (NEW)\n",
    "    all_labels = []          # list of (task, period_name)\n",
    "    all_Xc = {}              # (task, period) -> centered matrix\n",
    "    all_Vk = {}              # (task, period) -> top-k basis (N, k_eff)\n",
    "\n",
    "    for task, per_dict in periods.items():\n",
    "        idx = np.where(task_id == task)[0]\n",
    "        if idx.size == 0:\n",
    "            continue\n",
    "\n",
    "        H_task = H_np[idx, :, :]  # (B_task, T, N)\n",
    "\n",
    "        period_names = list(per_dict.keys())\n",
    "        P = len(period_names)\n",
    "\n",
    "        pca_info = {}\n",
    "        X_period_centered = {}\n",
    "\n",
    "        for pname in period_names:\n",
    "            t0, t1 = per_dict[pname]\n",
    "            if not (0 <= t0 < t1 <= T):\n",
    "                raise ValueError(f\"[{task}:{pname}] invalid period bounds {(t0,t1)} for T={T}\")\n",
    "            X = _get_period_matrix(H_task, t0, t1)\n",
    "            Xc, mu = _center(X)\n",
    "            X_period_centered[pname] = Xc\n",
    "\n",
    "            V, S, evr = _pca_svd(Xc, r=max(max_pcs, k))\n",
    "            pca_info[pname] = {\n",
    "                \"components\": V,\n",
    "                \"singular_values\": S,\n",
    "                \"mean\": mu,\n",
    "                \"evr\": evr,\n",
    "            }\n",
    "\n",
    "            # NEW: cache (task, period) for cross-task matrix\n",
    "            if return_cross_task:\n",
    "                key = (task, pname)\n",
    "                all_labels.append(key)\n",
    "                all_Xc[key] = Xc\n",
    "                k_eff = min(k, V.shape[1])\n",
    "                all_Vk[key] = V[:, :k_eff]  # top-k PCs for this (task, period)\n",
    "\n",
    "        # intra-task cross-period FVE matrix (same as before)\n",
    "        fve_k = np.zeros((P, P), dtype=dtype)\n",
    "        for i, px in enumerate(period_names):\n",
    "            Xc = X_period_centered[px]\n",
    "            for j, py in enumerate(period_names):\n",
    "                V = pca_info[py][\"components\"]\n",
    "                r_eff = min(k, V.shape[1])\n",
    "                fve_k[i, j] = _fve_project(Xc, V[:, :r_eff])\n",
    "\n",
    "        # per-period EVR curves (same as before)\n",
    "        evr_curves = np.zeros((P, max_pcs), dtype=dtype)\n",
    "        for i, pname in enumerate(period_names):\n",
    "            evr = pca_info[pname][\"evr\"]\n",
    "            evr_curves[i, :min(max_pcs, evr.shape[0])] = evr[:min(max_pcs, evr.shape[0])]\n",
    "\n",
    "        results[task] = {\n",
    "            \"period_names\": period_names,\n",
    "            \"fve_k\": fve_k,\n",
    "            \"evr_curves\": evr_curves,\n",
    "            \"pca\": pca_info,\n",
    "        }\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Cross-task, cross-period FVE matrix (NEW)\n",
    "    # --------------------------------------------\n",
    "    if return_cross_task:\n",
    "        # Keep a stable order (task insertion order from `periods`, then period order in each task)\n",
    "        # all_labels already appended in that order.\n",
    "\n",
    "        Q = len(all_labels)\n",
    "        fve_k_all = np.zeros((Q, Q), dtype=dtype)\n",
    "\n",
    "        for i, key_x in enumerate(all_labels):       # target\n",
    "            Xc = all_Xc[key_x]\n",
    "            for j, key_y in enumerate(all_labels):   # basis\n",
    "                Vk = all_Vk[key_y]\n",
    "                fve_k_all[i, j] = _fve_project(Xc, Vk)\n",
    "\n",
    "        results[\"__cross_task__\"] = {\n",
    "            \"labels\": all_labels,      # rows/cols correspond to this list\n",
    "            \"fve_k_all\": fve_k_all,    # (Q, Q); for 2 tasks with P periods each => 2P x 2P\n",
    "        }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63345b41-79e7-4690-a75d-349370a96444",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = db[f\"hidden{layer_index}\"]\n",
    "M = db[f\"M{layer_index}\"].reshape(db[f\"M{layer_index}\"].shape[0], db[f\"M{layer_index}\"].shape[1], -1)\n",
    "task_id = test_task\n",
    "periods = time_stamp_extract(test_input, time_stamps_usual)\n",
    "print(f\"periods: {periods}\")\n",
    "periods_ = {\n",
    "    0: {\"context\": (0, periods[\"stimulus_start\"]-1), \n",
    "          \"stim\": (periods[\"stimulus_start\"], periods[\"stimulus_end\"]), \n",
    "          \"delay\": (periods[\"delay_start\"], periods[\"delay_end\"]-1), \n",
    "          \"resp\": (periods[\"delay_end\"], periods[\"trial_end\"])}, \n",
    "    1: {\"context\": (0, periods[\"stimulus_start\"]-1), \n",
    "          \"stim\": (periods[\"stimulus_start\"], periods[\"stimulus_end\"]), \n",
    "          \"delay\": (periods[\"delay_start\"], periods[\"delay_end\"]-1), \n",
    "          \"resp\": (periods[\"delay_end\"], periods[\"trial_end\"])}, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c11822d-ab84-4916-bd81-22bb9e73c901",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 4\n",
    "print(f\"H.shape: {H.shape}\")\n",
    "print(f\"M.shape: {M.shape}\")\n",
    "res_H = figure2A_pca_fve(H, task_id, periods_, k=top_k, max_pcs=10, center=\"None\")\n",
    "res_M = figure2A_pca_fve(M, task_id, periods_, k=top_k, max_pcs=10, center=\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43824b8-2eb7-4d24-890e-622f361628be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = [[\"hidden\", res_H], [\"modulation\", res_M]]\n",
    "pcs = {}\n",
    "for name, res in data_all: \n",
    "    fig, axs = plt.subplots(1,2,figsize=(4*2,4))\n",
    "    for task_index in range(2):\n",
    "        for period_index in range(4):\n",
    "            evr_curve = res[task_index][\"evr_curves\"][period_index]\n",
    "            period_name = res[task_index][\"period_names\"][period_index]\n",
    "            cev = np.cumsum(evr_curve, axis=0)\n",
    "            axs[task_index].plot([i+1 for i in range(len(cev))], cev, \"-o\", color=c_vals[period_index], label=period_name)\n",
    "            pcs[f\"{name}_task{task_index}_{period_name}\"] = cev\n",
    "            \n",
    "    axs[0].set_ylabel(\"Go Task; Var. expl.\", fontsize=15)\n",
    "    axs[1].set_ylabel(\"Anti Task; Var. expl.\", fontsize=15)\n",
    "    for ax in axs:\n",
    "        ax.set_xlabel(\"No. of PCs\", fontsize=15)\n",
    "        ax.legend(fontsize=12, frameon=True, loc=\"best\")\n",
    "        ax.set_title(name, fontsize=12)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\n",
    "        f\"./twotasks/dimension_{hyp_dict['ruleset']}_seed{seed}_{name}_{hyp_dict['addon_name']}.png\",\n",
    "        dpi=300,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c72236-c959-435f-93c0-7df807d8b584",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, res in data_all: \n",
    "    fig, axs = plt.subplots(1,2,figsize=(4*2,4))\n",
    "    for task_index in range(2):\n",
    "        fve_k = res[task_index][\"fve_k\"]\n",
    "        sns.heatmap(fve_k, ax=axs[task_index], \n",
    "                    xticklabels=res_H[task_index][\"period_names\"], \n",
    "                    yticklabels=res_H[task_index][\"period_names\"],\n",
    "                    annot=True, fmt=\".2f\")\n",
    "    axs[0].set_title(f\"Go Task, k={top_k}\", fontsize=15)\n",
    "    axs[1].set_title(f\"Anti Task, k={top_k}\", fontsize=15)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\n",
    "        f\"./twotasks/d_separate_{hyp_dict['ruleset']}_seed{seed}_{name}_{hyp_dict['addon_name']}.png\",\n",
    "        dpi=300,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc89edc-b439-4bce-867d-26d91b82af0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fve_k_alls = []\n",
    "\n",
    "for name, res in data_all: \n",
    "    fig, axs = plt.subplots(1,1,figsize=(4*1,4))\n",
    "    fve_k_all = res[\"__cross_task__\"][\"fve_k_all\"]\n",
    "    labels_all = res[\"__cross_task__\"][\"labels\"]\n",
    "    \n",
    "    permute = [0,4,1,5,2,6,3,7]\n",
    "    fve_k_all = fve_k_all[np.ix_(permute, permute)]\n",
    "    labels_all = [labels_all[i] for i in permute]\n",
    "    \n",
    "    labels_alt = [\"Pro Context\", \"Anti Context\", \"Pro Stim\", \"Anti Stim\",\n",
    "                 \"Pro Memory\", \"Anti Memory\", \"Pro Response\", \"Anti Response\"]\n",
    "    \n",
    "    sns.heatmap(fve_k_all, ax=axs, \n",
    "                xticklabels=labels_alt,\n",
    "                yticklabels=labels_alt,\n",
    "                annot=True, fmt=\".2f\", vmin=0.0, vmax=1.0)\n",
    "\n",
    "    fve_k_alls.append(fve_k_all)\n",
    "    \n",
    "    # axs.set_title(f\"Go/Anti Task\", fontsize=15)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\n",
    "        f\"./twotasks/d_combine_{hyp_dict['ruleset']}_seed{seed}_{name}_{hyp_dict['addon_name']}.png\",\n",
    "        dpi=300,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce94b6f4-ca0a-4bf2-a12f-eef143a8e1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "save_all = {\n",
    "    \"learning_hm_similarity\": learning_hm_similarity, \n",
    "    \"pcs\": pcs,\n",
    "    \"fve_k_alls\": fve_k_alls\n",
    "}\n",
    "\n",
    "filename = f\"twotasks_data/seed{seed}_{hyp_dict['addon_name']}.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(save_all, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce6c12e-426a-4ffb-b9c7-2d863d2008e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_in = Winput_lst[-1]\n",
    "W = net.mp_layer1.W.data.detach().cpu().numpy()\n",
    "input_label = [\"Fix On\", \"Fix Off\", \"Stim 1 Cos\", \"Stim 1 Sin\", \"Stim 2 Cos\", \"Stim 2 Sin\", \"Task 1\", \"Task 2\"]\n",
    "\n",
    "Wcombs = [W_in, W @ W_in]\n",
    "fig, axs = plt.subplots(1,2,figsize=(4*2,4))\n",
    "for idx, Wcomb in enumerate(Wcombs): \n",
    "    C = np.corrcoef(Wcomb, rowvar=False) \n",
    "    C_upper = C.copy()\n",
    "    C_upper[np.tril_indices_from(C_upper, k=-1)] = np.nan \n",
    "    \n",
    "    sns.heatmap(C_upper, xticklabels=input_label, cmap=\"coolwarm\", ax=axs[idx],\n",
    "                yticklabels=input_label, annot=True, fmt=\".2f\", vmin=-1.0, vmax=1.0)\n",
    "axs[0].set_title(\"Win\",fontsize=12)\n",
    "axs[1].set_title(\"Win @ W\", fontsize=12)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\n",
    "    f\"./twotasks/w_stim_corr_{hyp_dict['ruleset']}_seed{seed}_{name}_{hyp_dict['addon_name']}.png\",\n",
    "    dpi=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8f73b7-5440-4d37-a310-a9966c5935d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ax in axsld.flatten():\n",
    "#     for pindex, period in enumerate(list(time_stamps.values())):\n",
    "#         ax.axvline(period, alpha=0.5, color=c_vals_d[pindex])\n",
    "        \n",
    "# figld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9c9132-75b3-4bb0-b93c-0305bc00117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ax in axslr.flatten():\n",
    "#     for pindex, period in enumerate(list(time_stamps_longresponse.values())):\n",
    "#         ax.axvline(period, alpha=0.5, color=c_vals_d[pindex])\n",
    "        \n",
    "# figlr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b805c4b3-4d47-41a9-964c-c2a6a7d8e745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ax in axsls.flatten():\n",
    "#     for pindex, period in enumerate(list(time_stamps_longstimulus.values())):\n",
    "#         ax.axvline(period, alpha=0.5, color=c_vals_d[pindex])\n",
    "        \n",
    "# figls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4532626d-32b0-4bbe-b40b-d3ed90e89cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ax in axslf.flatten():\n",
    "#     for pindex, period in enumerate(list(time_stamps_longfixation.values())):\n",
    "#         ax.axvline(period, alpha=0.5, color=c_vals_d[pindex])\n",
    "        \n",
    "# figlf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aaae28-f7d4-468c-85fe-847d2d6b6632",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_stamps_usual_copy = copy.deepcopy(time_stamps_usual)\n",
    "time_stamps_usual_copy[\"fixation_half\"] = int(time_stamps_usual_copy[\"fixation_end\"] / 2)\n",
    "time_stamps_usual_copy[\"stimulus_half\"] = int((time_stamps_usual_copy[\"stimulus_end\"] - time_stamps_usual_copy[\"stimulus_start\"]) / 2) + time_stamps_usual_copy[\"stimulus_start\"]\n",
    "time_stamps_usual_copy[\"delay_half\"] = int((time_stamps_usual_copy[\"delay_end\"] - time_stamps_usual_copy[\"delay_start\"]) / 2) + time_stamps_usual_copy[\"delay_start\"]\n",
    "time_stamps_usual_copy[\"response_half\"] = int((time_stamps_usual_copy[\"trial_end\"] - time_stamps_usual_copy[\"delay_end\"]) / 2) + time_stamps_usual_copy[\"delay_end\"]\n",
    "time_stamps_usual_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd5e7e5-9577-4362-b4d6-4e5c73273318",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_values = [[\"hidden\",None,\"hidden\"], \n",
    "                  [\"modulation\",0,\"modulation fixon\"], \n",
    "                  [\"modulation\",1,\"modulation fixoff\"], \n",
    "                  [\"modulation\",None,\"modulation all\"],\n",
    "                  [\"modulation\",\"Win\",\"modulation all @ Win\"],\n",
    "                  [\"w_modulation\",None,\"w_modulation all\"],\n",
    "                  [\"w_modulation\",\"Win\",\"w_modulation all @ Win\"]]\n",
    "\n",
    "cl = len(compare_values)\n",
    "hidden_over_time_save = None\n",
    "\n",
    "stages = [[0, \"Before Training\", \"beforetraining\"], [-1, \"Post Training\", \"posttraining\"]]\n",
    "\n",
    "for stage_idx, stage_name, save_name in stages: \n",
    "    figattractorend, axsattractorend = plt.subplots(1,cl,figsize=(4*cl,4))\n",
    "    for idx, (compare_value, moddim, compare_name) in enumerate(compare_values): \n",
    "        # print(compare_name)\n",
    "        _, M_end, h_end, _ = modulation_extraction(test_input, db_lst[0][stage_idx], layer_index)\n",
    "        result_attractor_end_all = {} \n",
    "        all_keys = [\"fixation_half\", \"fixation_end\", \n",
    "                    \"stimulus_half\", \"stimulus_end\", \n",
    "                    \"delay_half\", \"delay_end\", \n",
    "                    \"response_half\", \"trial_end\"]\n",
    "        \n",
    "        for key_idx, key in enumerate(all_keys): \n",
    "            result_attractor, _, hidden_over_time = analyze_similarity(M_end, h_end, net, net_params, label_task_comb, \n",
    "                                                     checktime=time_stamps_usual_copy[key], compare=compare_value, \n",
    "                                                     moddim=moddim)\n",
    "            result_attractor_end_all[key] = result_attractor\n",
    "    \n",
    "            if key_idx == 0 and idx == 0:\n",
    "                hidden_over_time_save = hidden_over_time\n",
    "                    \n",
    "        for i in range(len(result_attractor_end_all[\"trial_end\"])): \n",
    "            mean, std = [rs[i][0] for rs in result_attractor_end_all.values()], [rs[i][1] for rs in result_attractor_end_all.values()]\n",
    "            stages_counter = [i for i in range(len(result_attractor_end_all))]\n",
    "            # print(mean)\n",
    "            axsattractorend[idx].plot(stages_counter, mean, \"-o\", color=c_vals[i], label=f\"{break_names[i]}\")\n",
    "            axsattractorend[idx].fill_between(stages_counter, [mean[i]-std[i] for i in range(len(mean))],\\\n",
    "                                      [mean[i]+std[i] for i in range(len(mean))], alpha=0.5, color=c_vals_l[i])\n",
    "    \n",
    "        axsattractorend[idx].set_xticks(stages_counter)\n",
    "        axsattractorend[idx].set_xticklabels(list(result_attractor_end_all.keys()), rotation=45, ha=\"right\", fontsize=12)\n",
    "        axsattractorend[idx].legend(fontsize=12, frameon=True, loc=\"best\")\n",
    "        axsattractorend[idx].set_ylabel(f\"Cosine Sim of {compare_name}\", fontsize=12)\n",
    "        axsattractorend[idx].set_ylim([-1.1,1.1])\n",
    "\n",
    "    figattractorend.suptitle(stage_name, fontsize=15)\n",
    "    figattractorend.tight_layout()\n",
    "    \n",
    "    figattractorend.savefig(f\"./twotasks/attractor_stage{save_name}_{compare_value}_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b654cf4d-c8f9-4250-a796-61f4efbb9d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_save = modulation_save_time[-1]\n",
    "delta_M_tnorm_all, delta_MW_tnorm_all = [], []\n",
    "\n",
    "for i in range(8): \n",
    "    mod1_stim1 = m_save[0][i]\n",
    "    mod2_stim1 = m_save[1][i]\n",
    "    delta_M = mod1_stim1 - mod2_stim1\n",
    "    delta_MW = (mod1_stim1 * W[None,:,:] - mod2_stim1 * W[None,:,:])\n",
    "    delta_M_tnorm_all.append([np.linalg.norm(delta_M[t]) for t in range(delta_M.shape[0])])\n",
    "    delta_MW_tnorm_all.append([np.linalg.norm(delta_MW[t]) for t in range(delta_MW.shape[0])])\n",
    "\n",
    "delta_M_tnorm_all = np.array(delta_M_tnorm_all)\n",
    "delta_MW_tnorm_all = np.array(delta_MW_tnorm_all)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 2))\n",
    "m1 = np.mean(delta_M_tnorm_all, axis=0)\n",
    "s1 = np.std(delta_M_tnorm_all, axis=0)\n",
    "m2 = np.mean(delta_MW_tnorm_all, axis=0)\n",
    "s2 = np.std(delta_MW_tnorm_all, axis=0)\n",
    "\n",
    "x = np.arange(m1.shape[0])\n",
    "ax.plot(x, m1, color=c_vals[0], label=\"delta_M\")\n",
    "ax.fill_between(x, m1 - s1, m1 + s1, color=c_vals_l[0], alpha=0.2)\n",
    "ax.plot(x, m2, color=c_vals[1], label=\"delta_MW\")\n",
    "ax.fill_between(x, m2 - s2, m2 + s2, color=c_vals_l[1], alpha=0.2)\n",
    "\n",
    "ax.set_yscale(\"log\")\n",
    "ax.axvline(time_stamps_usual[\"fixation_end\"], linestyle=\"--\", color=c_vals[2])\n",
    "ax.axvline(time_stamps_usual[\"stimulus_end\"], linestyle=\"--\", color=c_vals[2])\n",
    "ax.axvline(time_stamps_usual[\"delay_end\"], linestyle=\"--\", color=c_vals[2])\n",
    "ax.axvline(time_stamps_usual[\"trial_end\"], linestyle=\"--\", color=c_vals[2])\n",
    "ax.set_xlabel(\"Timestep\", fontsize=12)\n",
    "ax.set_ylabel(\"Magnitude\", fontsize=12)\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"./twotasks/m_magnitude_{compare_value}_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a1745f-d479-4510-bc25-fa71c3a812bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_by_sorted_x(x, y, nbins=100, drop_nonfinite=True, return_counts=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    x = np.asarray(x).ravel()\n",
    "    y = np.asarray(y).ravel()\n",
    "    if x.shape != y.shape:\n",
    "        raise ValueError(f\"x and y must have the same number of elements; got {x.size} vs {y.size}\")\n",
    "\n",
    "    if drop_nonfinite:\n",
    "        mask = np.isfinite(x) & np.isfinite(y)\n",
    "        x = x[mask]\n",
    "        y = y[mask]\n",
    "\n",
    "    n = x.size\n",
    "    if n == 0:\n",
    "        raise ValueError(\"No valid data points after filtering.\")\n",
    "    if nbins < 1:\n",
    "        raise ValueError(\"nbins must be >= 1.\")\n",
    "\n",
    "    nb = min(nbins, n)  # avoid empty bins if nbins > n\n",
    "\n",
    "    idx = np.argsort(x)\n",
    "    x_s = x[idx]\n",
    "    y_s = y[idx]\n",
    "\n",
    "    x_chunks = np.array_split(x_s, nb)\n",
    "    y_chunks = np.array_split(y_s, nb)\n",
    "\n",
    "    x_mean = np.array([c.mean() for c in x_chunks])\n",
    "    y_mean = np.array([c.mean() for c in y_chunks])\n",
    "\n",
    "    if return_counts:\n",
    "        counts = np.array([c.size for c in x_chunks], dtype=int)\n",
    "        return x_mean, y_mean, counts\n",
    "\n",
    "    return x_mean, y_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fdefcc-8785-4ffa-8b7e-da9758b692a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modulation magnitude to W magnitude\n",
    "time_cut = [\"fixation_end\", \"stimulus_end\", \"delay_end\", \"trial_end\"]\n",
    "fig, axs = plt.subplots(2,4,figsize=(4*4,4*2))\n",
    "W = net.mp_layer1.W.data.detach().cpu().numpy()\n",
    "\n",
    "for t, time in enumerate(time_cut): \n",
    "    for i in range(1): \n",
    "        mod1_stim1 = m_save[0][i]\n",
    "        mod2_stim1 = m_save[1][i]\n",
    "        Mmod1 = mod1_stim1[time_stamps_usual[time]]\n",
    "        Mmod2 = mod2_stim1[time_stamps_usual[time]]\n",
    "        axs[0,t].scatter(W.flatten(), np.abs(Mmod1).flatten(), alpha=0.1, c=c_vals[0])\n",
    "        x1, y1 = bin_by_sorted_x(W.flatten(), np.abs(Mmod1).flatten())\n",
    "        axs[0,t].plot(x1, y1, \"-o\", c=c_vals[1])\n",
    "        axs[1,t].scatter(W.flatten(), np.abs(Mmod2).flatten(), alpha=0.1, c=c_vals[0])\n",
    "        x2, y2 = bin_by_sorted_x(W.flatten(), np.abs(Mmod2).flatten())\n",
    "        axs[1,t].plot(x2, y2, \"-o\", c=c_vals[1])\n",
    "\n",
    "for ax in axs.flatten():\n",
    "    ax.set_xlabel(\"W Entry\", fontsize=15)\n",
    "    ax.set_ylabel(f\"Abs(M) Entry at {time}\", fontsize=15)\n",
    "    \n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"./twotasks/m_w_{compare_value}_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab33e137-fd32-4e55-b968-daee0357af2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = net.mp_layer1.W.data.detach().cpu().numpy()\n",
    "W_in = Winput_lst[-1]\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(4*2,4))\n",
    "sns.heatmap(W, ax=axs[0], cmap=\"coolwarm\", square=True)\n",
    "sns.heatmap(W @ W_in, ax=axs[1], cmap=\"coolwarm\")\n",
    "axs[0].set_ylabel(\"MPN Postsynaptic Neuron\", fontsize=12)\n",
    "axs[0].set_xlabel(\"MPN Presynaptic Neuron\", fontsize=12)\n",
    "axs[1].set_ylabel(\"MPN Postsynaptic Neuron\", fontsize=12)\n",
    "axs[1].set_xlabel(\"Input Neuron\", fontsize=12)\n",
    "axs[0].set_title(\"W\", fontsize=12)\n",
    "axs[1].set_title(r\"$W \\, @ \\, W_{\\mathrm{proj}}$\", fontsize=12)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"./twotasks/w_wwin{compare_value}_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40cede6-b3c1-415a-9e54-13286a3c98f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "delay1 = np.array([1,0,0,0,0,0,1,0]).reshape(-1,1)\n",
    "delay2 = np.array([1,0,0,0,0,0,0,1]).reshape(-1,1)\n",
    "fixon = np.array([1,0,0,0,0,0,0,0]).reshape(-1,1)\n",
    "fixoff = np.array([0,1,0,0,0,0,0,0]).reshape(-1,1)\n",
    "task1 = np.array([0,0,0,0,0,0,1,0]).reshape(-1,1)\n",
    "task2 = np.array([0,0,0,0,0,0,0,1]).reshape(-1,1)\n",
    "\n",
    "fixon_act = np.abs(W_in @ fixon)\n",
    "fixoff_act = np.abs(W_in @ fixoff)\n",
    "task1_act = np.abs(W_in @ task1)\n",
    "task2_act = np.abs(W_in @ task2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ee4f6d-2d24-4d3d-8922-836d10b7e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,1,figsize=(6,3*2))\n",
    "sumWpost = np.sum(np.abs(W), axis=1)\n",
    "sumWpre = np.sum(np.abs(W), axis=0)\n",
    "\n",
    "axs[0].plot(sumWpost / np.mean(sumWpost), color=c_vals[0], label=\"W_post\")\n",
    "axs[1].plot(sumWpre / np.mean(sumWpre), color=c_vals[0], label=\"W_pre\")\n",
    "\n",
    "axs[2].plot(fixon_act, color=c_vals[0], label=\"fixon\")\n",
    "axs[2].plot(fixoff_act, color=c_vals[1], label=\"fixoff\")\n",
    "axs[2].plot(task1_act, color=c_vals[2], label=\"task1\")\n",
    "axs[2].plot(task2_act, color=c_vals[3], label=\"task2\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlabel(\"MPN Postsynaptic Neuron\", fontsize=12)\n",
    "    ax.set_ylabel(\"Normalized Total Weight\", fontsize=10)\n",
    "    ax.legend(fontsize=12, frameon=True)\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"./twotasks/wreceive_{compare_value}_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c830137-9783-42e7-bb3b-8838fc5c7577",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = net.mp_layer1.W.data.detach().cpu().numpy()\n",
    "W = np.asarray(W)\n",
    "mags = np.abs(W).ravel()\n",
    "mags = mags[np.isfinite(mags)] \n",
    "N = mags.size\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(4,2))\n",
    "weights = np.ones_like(mags) / N * 100.0\n",
    "n, bins, patches = ax.hist(mags, weights=weights, bins=50)\n",
    "ax.set_yscale(\"log\") \n",
    "ax.set_xlabel(\"W Magnitude\", fontsize=12)\n",
    "ax.set_ylabel(\"Prop of Entries\", fontsize=12)\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"./twotasks/w_magnitude_{compare_value}_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6f64bc-3780-456d-92d8-33c415846447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_cuda_parameters(model):\n",
    "    cuda_params = []\n",
    "    for name, p in model.named_parameters(recurse=True):\n",
    "        if p is None:\n",
    "            continue\n",
    "        if p.is_cuda:  # same as (p.device.type == \"cuda\")\n",
    "            cuda_params.append((name, tuple(p.shape), str(p.dtype), str(p.device)))\n",
    "    return cuda_params\n",
    "\n",
    "for name, p in net.named_parameters():\n",
    "    print(f\"{name:50s}  {p.device}  {tuple(p.shape)}  {p.dtype}\")\n",
    "\n",
    "for name, b in net.named_buffers():\n",
    "    print(f\"[buffer] {name:43s}  {b.device}  {tuple(b.shape)}  {b.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c4c3d8-4343-4434-8758-73f9293d6dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ba2272-2503-42a7-a258-e8b271e042e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_orig = net.mp_layer1.W.detach().cpu().numpy().copy()\n",
    "\n",
    "K_lst = [0.0, 10.0, 50.0, 90.0, 95.0, 98.0, 99.0, 99.90]\n",
    "acc_K_lst = []\n",
    "\n",
    "for K in K_lst: \n",
    "    with torch.no_grad():\n",
    "        W = net.mp_layer1.W\n",
    "        w_np = W.detach().cpu().numpy()\n",
    "    \n",
    "        n = w_np.size\n",
    "        k = int(round(K / 100.0 * n))\n",
    "        if k > 0:\n",
    "            idx = np.argpartition(np.abs(w_np).ravel(), k-1)[:k]\n",
    "            w_flat = w_np.ravel()\n",
    "            w_flat[idx] = 0.0\n",
    "    \n",
    "        W.copy_(torch.from_numpy(w_np).to(W.device))\n",
    "        \n",
    "        net_out_redo, _, db = net.iterate_sequence_batch(test_input, run_mode='track_states')\n",
    "        # plot_input_output(test_input_np, net_out, test_output_np, test_task, tag=\"redo\")\n",
    "        acc_K_lst.append(net.compute_acc(net_out_redo, test_output, test_mask, test_input, isvalid=False, mode=\"stimulus\")[0].item())\n",
    "        \n",
    "        # plot_input_output(test_input_np, net_out_redo, test_output_np, test_task, tag=\"\")\n",
    "    \n",
    "        W.copy_(torch.as_tensor(W_orig, device=W.device, dtype=W.dtype))\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(4,2))\n",
    "ax.plot([i for i in range(len(K_lst))], np.array(acc_K_lst) * 100, \"-o\", color=c_vals[0])\n",
    "ax.set_xticks([i for i in range(len(K_lst))])\n",
    "ax.set_xticklabels(K_lst)\n",
    "ax.set_xlabel(\"Sparsity of W (%)\", fontsize=12)\n",
    "ax.set_ylabel(\"Accuracy\", fontsize=12)\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"./twotasks/w_hurt_{compare_value}_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477b4a57-2133-498d-b1cb-493f96d21d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    return 1.0 - cosine(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ca327b-dfa5-4cd8-a0be-361fbe9cb5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = db_lst[0][-1][\"input1\"]\n",
    "hidden1 = db_lst[0][-1][\"hidden1\"]\n",
    "W = net.mp_layer1.W.data.detach().cpu().numpy()\n",
    "W_in = Winput_lst[-1]\n",
    "_, M_all, h_end, _ = modulation_extraction(test_input, db_lst[0][-1], layer_index)\n",
    "\n",
    "fig, axs = plt.subplots(11,1,figsize=(6,11*2))\n",
    "fignorm, axsnorm = plt.subplots(1,1,figsize=(10,3))\n",
    "figMdiff, axsMdiff = plt.subplots(1,1,figsize=(10,3))\n",
    "maps = [lambda i: i, lambda i: (i + 4) % 8]\n",
    "map_names = [\"Same Stim\", \"Same Resp\"]\n",
    "figcc, axscc = plt.subplots(8,2,figsize=(10*2,4*8))\n",
    "cc_all = [[], []]\n",
    "\n",
    "cosines_fake = [[], []]\n",
    "for idx, map_ in enumerate(maps): \n",
    "    for k in range(8): \n",
    "        ind1 = [i for i, lst in enumerate(label_task_comb) if np.array_equal(lst, [k, 0])][0]\n",
    "        ind2 = [i for i, lst in enumerate(label_task_comb) if np.array_equal(lst, [map_(k), 1])][0]\n",
    "        input1_, input2_ = input1[ind1], input1[ind2]\n",
    "        hidden1_, hidden2_ = hidden1[ind1], hidden1[ind2]\n",
    "        cosines = [[],[],[],[],[],[],[],[],[],[],[]]\n",
    "        norm_ratios = [[],[]]\n",
    "        M_diff = []\n",
    "        component_compare_all = []\n",
    "        for t in range(1, input1_.shape[0]):\n",
    "            # h_t\n",
    "            h1, i1 = hidden1_[t].reshape(-1,1), input1_[t].reshape(-1,1)\n",
    "            h2, i2 = hidden2_[t].reshape(-1,1), input2_[t].reshape(-1,1)\n",
    "            # M_{t-1}\n",
    "            M_tp1, M_tp2 = M_all[ind1,t-1,:,:], M_all[ind2,t-1,:,:]\n",
    "            assert W.shape == M_tp1.shape == M_tp2.shape\n",
    "            proj1, proj2 = ((h1 @ i1.T)).flatten(), ((h2 @ i2.T)).flatten()\n",
    "            proj1W, proj2W = ((h1 @ i1.T) * W).flatten(), ((h2 @ i2.T) * W).flatten()\n",
    "            proj1Wx, proj2Wx = ((W @ i1)).flatten(), ((W @ i2)).flatten()\n",
    "            proj1x, proj2x = i1.flatten(), i2.flatten()\n",
    "            proj1MW, proj2MW = (M_tp1 * W).flatten(), (M_tp2 * W).flatten()\n",
    "            \n",
    "            proj1MWx, proj2MWx = (((M_tp1 * W) @ i1)).flatten(), (((M_tp2 * W) @ i2)).flatten()\n",
    "\n",
    "            proj1MWx_fake, proj2MWx_fake = (((M_tp1 * W) @ i2)).flatten(), (((M_tp2 * W) @ i1)).flatten()\n",
    "\n",
    "            A = M_tp1 * W\n",
    "            B = M_tp2 * W\n",
    "            # fro_inner = np.trace(A.T @ B)\n",
    "            # dirc = ((i1).T @ A.T @ B @ i2)[0,0]\n",
    "\n",
    "            def _fro_norm(X, eps=1e-12):\n",
    "                return np.linalg.norm(X, ord=\"fro\") + eps\n",
    "            \n",
    "            def _l2_norm(v, eps=1e-12):\n",
    "                v = np.asarray(v).reshape(-1, 1)\n",
    "                return float(np.linalg.norm(v, ord=2) + eps)\n",
    "\n",
    "            fro_inner = float(np.sum(A * B))  # == trace(A.T @ B)\n",
    "            fro_cos   = fro_inner / (_fro_norm(A) * _fro_norm(B))\n",
    "\n",
    "            i1 = np.asarray(i1).reshape(-1, 1)\n",
    "            i2 = np.asarray(i2).reshape(-1, 1)\n",
    "            \n",
    "            dirc_inner = float((i1.T @ A.T @ B @ i2)[0, 0])\n",
    "            \n",
    "            den1 = float((i1.T @ A.T @ A @ i1)[0, 0])\n",
    "            den2 = float((i2.T @ B.T @ B @ i2)[0, 0])\n",
    "            dirc_cos = dirc_inner / (np.sqrt(den1 + 1e-12) * np.sqrt(den2 + 1e-12))\n",
    "\n",
    "            # \n",
    "            x1, _ ,_ , _ = np.linalg.lstsq(W_in, i1, rcond=None)  \n",
    "            x2, _ ,_ , _ = np.linalg.lstsq(W_in, i2, rcond=None)  \n",
    "\n",
    "            component_compare = []\n",
    "            for nidx in range(W.shape[1]):\n",
    "                proj1MWxd = ((M_tp1[:,nidx] * W[:,nidx]) * i1[nidx])\n",
    "                component_compare.append(cosine_sim(proj1MWxd.flatten(), proj2MWx.flatten()))\n",
    "            component_compare_all.append(component_compare)\n",
    "\n",
    "            proj1WMWx, proj2WMWx = (((W + M_tp1 * W) @ i1)).flatten(), (((W + M_tp2 * W) @ i2)).flatten()\n",
    "            proj1M, proj2M = (M_tp1.flatten(), M_tp2.flatten())\n",
    "            \n",
    "            cosines[0].append(cosine_sim(proj1, proj2))\n",
    "            cosines[1].append(cosine_sim(proj1W, proj2W))\n",
    "            cosines[2].append(cosine_sim(proj1Wx, proj2Wx))\n",
    "            cosines[3].append(cosine_sim(proj1x, proj2x))\n",
    "            cosines[4].append(cosine_sim(proj1M, proj2M))\n",
    "            cosines[5].append(cosine_sim(proj1MWx, proj2MWx))\n",
    "            cosines[6].append(cosine_sim(proj1MW, proj2MW))\n",
    "            cosines[7].append(fro_cos)\n",
    "            cosines[8].append(dirc_cos)\n",
    "            \n",
    "            cosines[9].append(cosine_sim(proj1MWx_fake, proj1MWx))\n",
    "            cosines[10].append(cosine_sim(proj2MWx_fake, proj1MWx))\n",
    "            \n",
    "            norm_ratios[0].append(np.linalg.norm(proj1MWx)/np.linalg.norm(proj1WMWx))\n",
    "            norm_ratios[1].append(np.linalg.norm(proj2MWx)/np.linalg.norm(proj2WMWx))\n",
    "            M_diff.append(np.linalg.norm(proj1M-proj2M))\n",
    "\n",
    "        for u in range(len(cosines)):\n",
    "            label = map_names[idx] if k == 0 else None\n",
    "            axs[u].plot(cosines[u], color=c_vals[idx], linestyle=\"-\", label=label)\n",
    "\n",
    "        axsMdiff.plot(M_diff, color=c_vals[idx], linestyle=\"-\")\n",
    "\n",
    "        if idx == 0: \n",
    "            for u in range(len(norm_ratios)):\n",
    "                axsnorm.plot(norm_ratios[u], color=c_vals[idx], linestyle=linestyles[u])\n",
    "\n",
    "        component_compare_all = np.array(component_compare_all)\n",
    "        cc_all[idx].append(component_compare_all)\n",
    "        sns.heatmap(component_compare_all, ax=axscc[k,idx], \n",
    "                    # vmin=-1, vmax=1, \n",
    "                    cmap=\"coolwarm\")\n",
    "        axscc[k,idx].axhline(y=time_stamps_usual[\"stimulus_end\"], linewidth=2) \n",
    "        axscc[k,idx].axhline(y=time_stamps_usual[\"delay_end\"], linewidth=2) \n",
    "        \n",
    "\n",
    "for idx, ax in enumerate(axs):\n",
    "    ax.set_xlabel(\"Time Steps\", fontsize=15)\n",
    "    ax.axvline(time_stamps_usual[\"delay_end\"], color=c_vals[2])\n",
    "    if idx not in (7,8):\n",
    "        ax.set_ylim([-1.1,1.1])\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=14, length=6, width=1.2)\n",
    "    ax.tick_params(axis=\"both\", which=\"minor\", labelsize=14, length=3, width=1.0)\n",
    "    ax.axhline(0.0, color=c_vals[3], linestyle=\"--\")\n",
    "    ax.legend(fontsize=12, frameon=True, loc=\"upper left\")\n",
    "\n",
    "axsnorm.set_xlabel(\"Time Steps\", fontsize=15)\n",
    "axsnorm.axvline(time_stamps_usual[\"delay_end\"], color=c_vals[2])\n",
    "axsnorm.set_ylim([-1.1,1.1])\n",
    "\n",
    "axsMdiff.set_xlabel(\"Time Steps\", fontsize=15)\n",
    "axsMdiff.axvline(time_stamps_usual[\"delay_end\"], color=c_vals[2])\n",
    "    \n",
    "axs[0].set_ylabel(r\"$(h_t x_t^{\\top})$\", fontsize=15)\n",
    "axs[1].set_ylabel(r\"$(h_t x_t^{\\top}) \\odot W$\", fontsize=15)\n",
    "axs[2].set_ylabel(r\"$Wx_t$\", fontsize=15)\n",
    "axs[3].set_ylabel(r\"$x_t$\", fontsize=15)\n",
    "axs[4].set_ylabel(r\"$M_{t-1}$\", fontsize=15)\n",
    "axs[5].set_ylabel(r\"$(M_{t-1} \\odot W)x_t$\", fontsize=15)\n",
    "axs[6].set_ylabel(r\"$M_{t-1} \\odot W$\", fontsize=15)\n",
    "axs[9].set_ylabel(r\"Fake Input\", fontsize=15)\n",
    "axs[10].set_ylabel(r\"Fake Modulation\", fontsize=15)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"./twotasks/hxw_{compare_value}_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "\n",
    "axsnorm.set_xlabel(\"Time Steps\", fontsize=15)\n",
    "axsnorm.set_ylabel(r\"$||z^{\\text{mod}}||/||z||$\", fontsize=15)\n",
    "fignorm.tight_layout()\n",
    "fignorm.savefig(f\"./twotasks/zmodz_{compare_value}_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "\n",
    "axsMdiff.set_xlabel(\"Time Steps\", fontsize=15)\n",
    "axsMdiff.set_ylabel(r\"$||M_{t-1}^A-M_{t-1}^B||$\", fontsize=15)\n",
    "figMdiff.tight_layout()\n",
    "figMdiff.savefig(f\"./twotasks/mdiff_{compare_value}_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "\n",
    "figcc.tight_layout()\n",
    "figcc.savefig(f\"./twotasks/MWxcomponent_{compare_value}_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2224df09-aa65-49de-b7a2-f47d31f46abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "timemarkers = [[\"delay_end\", \"trial_end\"]]\n",
    "\n",
    "for ts, te in timemarkers: \n",
    "    allcomps = [[],[]]\n",
    "    for i in range(8):\n",
    "        samestim, sameresp = cc_all[0][i], cc_all[1][i]\n",
    "        samestim = samestim[time_stamps_usual[ts]:time_stamps_usual[te],:]\n",
    "        sameresp = sameresp[time_stamps_usual[ts]:time_stamps_usual[te],:]\n",
    "        allcomps[0].append(np.mean(samestim, axis=0))\n",
    "        allcomps[1].append(np.mean(sameresp, axis=0))\n",
    "            \n",
    "    fig, axs = plt.subplots(2,1,figsize=(6,2*2))\n",
    "    sns.heatmap(np.array(allcomps[0]), ax=axs[0], cmap=\"coolwarm\")\n",
    "    sns.heatmap(np.array(allcomps[1]), ax=axs[1], cmap=\"coolwarm\")\n",
    "    for ax in axs: \n",
    "        ax.set_xlabel(\"MPN Input Neuron Index\", fontsize=15)\n",
    "        ax.set_ylabel(\"Stimulus Type\", fontsize=15)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"./twotasks/mpninput_resp_{compare_value}_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\", dpi=300)\n",
    "\n",
    "    # same stim (average across stim)\n",
    "    samestimavg = np.mean(np.array(allcomps[0]),axis=0)\n",
    "    samerespavg = np.mean(np.array(allcomps[1]),axis=0)\n",
    "    fig, ax = plt.subplots(1,1,figsize=(6,2))\n",
    "    ax.plot(samestimavg, c=c_vals[0], label=\"Same Stim\")\n",
    "    ax.plot(samerespavg, c=c_vals[1], label=\"Same Resp\")\n",
    "    ax.legend()\n",
    "    print(cosine_sim(samestimavg, samerespavg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab96d6b-9a7e-4147-9e30-0d817c6bf5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "delay1 = np.array([1,0,0,0,0,0,1,0]).reshape(-1,1)\n",
    "delay2 = np.array([1,0,0,0,0,0,0,1]).reshape(-1,1)\n",
    "resp1 = np.array([0,1,0,0,0,0,1,0]).reshape(-1,1)\n",
    "resp2 = np.array([0,1,0,0,0,0,0,1]).reshape(-1,1)\n",
    "W_in = Winput_lst[-1]\n",
    "W = net.mp_layer1.W.data.detach().cpu().numpy()\n",
    "print(cosine_sim((W_in @ resp1).ravel(), (W_in @ resp2).ravel()))\n",
    "print(cosine_sim((W @ (W_in @ resp1)).ravel(), (W @ (W_in @ resp2)).ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961eecc9-8895-42ed-878f-03ff2b535488",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_stamps_usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5c0f88-1a35-45d1-aeb1-10369238264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92674a0a-b92a-487d-8af0-d1a110de50fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def input_interpolation(test_input_long, test_output_long, label_task_comb_long, expand_stimulus=True):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    assert test_input_long.shape[0] == label_task_comb_long.shape[0] \n",
    "    pro_task, anti_task = {}, {} \n",
    "    pro_task_answer, anti_task_answer = {}, {} \n",
    "    for k in range(8): \n",
    "        ind1 = [i for i, lst in enumerate(label_task_comb_long) if np.array_equal(lst, [k, 0])]\n",
    "        ind1_sample = ind1[0]\n",
    "        pro_task[k] = test_input_long[ind1_sample,:,:]\n",
    "        pro_task_answer[k] = test_output_long[ind1_sample,:,:]\n",
    "\n",
    "        ind2 = [i for i, lst in enumerate(label_task_comb_long) if np.array_equal(lst, [k, 1])]\n",
    "        ind2_sample = ind2[0]\n",
    "        anti_task[k] = test_input_long[ind2_sample,:,:]\n",
    "        anti_task_answer[k] = test_output_long[ind2_sample,:,:]\n",
    "\n",
    "    # expand with some unseen stimulus\n",
    "    if expand_stimulus:\n",
    "        base_len = len(pro_task)          # original size (8)\n",
    "        for i in range(base_len):\n",
    "            i1, i2 = i % 8, (i + 1) % 8   # wrap-around indexing\n",
    "    \n",
    "            # input dictionaries\n",
    "            pro_task[base_len + i]  = (pro_task[i1]        + pro_task[i2])        / 2\n",
    "            anti_task[base_len + i] = (anti_task[i1]       + anti_task[i2])       / 2\n",
    "    \n",
    "            # answer dictionaries\n",
    "            pro_task_answer[base_len + i]  = (pro_task_answer[i1]        + pro_task_answer[i2])        / 2\n",
    "            anti_task_answer[base_len + i] = (anti_task_answer[i1]       + anti_task_answer[i2])       / 2\n",
    "\n",
    "        # re-sort the input and output in an interleaved way \n",
    "        interleaved_keys = [k for pair in zip(range(base_len), range(base_len, 2*base_len)) for k in pair]\n",
    "\n",
    "        pro_task = {k: pro_task[k] for k in interleaved_keys}\n",
    "        anti_task = {k: anti_task[k] for k in interleaved_keys}\n",
    "        pro_task_answer = {k: pro_task_answer[k] for k in interleaved_keys}\n",
    "        anti_task_answer = {k: anti_task_answer[k] for k in interleaved_keys}\n",
    "    \n",
    "    n = 20 \n",
    "    alpha_lst = [i/n for i in range(n+1)]\n",
    "\n",
    "    stacked_pro = torch.stack([pro_task[k] for k in sorted(pro_task)]) \n",
    "    stacked_anti = torch.stack([anti_task[k] for k in sorted(anti_task)])\n",
    "    stacked_pro_answer = torch.stack([pro_task_answer[k] for k in sorted(pro_task_answer)]) \n",
    "    stacked_anti_answer = torch.stack([anti_task_answer[k] for k in sorted(anti_task_answer)])\n",
    "    \n",
    "    stacked_interpolation = [alpha_lst[i] * stacked_pro + (1 - alpha_lst[i]) * stacked_anti \n",
    "                             for i in range(len(alpha_lst))]\n",
    "    stacked_interpolation_ans = [alpha_lst[i] * stacked_pro_answer + (1 - alpha_lst[i]) * stacked_anti_answer for i in range(len(alpha_lst))]               \n",
    "\n",
    "    return alpha_lst, stacked_interpolation, stacked_interpolation_ans\n",
    "\n",
    "alpha_lst, stacked_interpolation_ld, stacked_interpolation_answer_ld = input_interpolation(test_input_longdelay, \n",
    "                                                                                           test_output_longdelay, \n",
    "                                                                                           label_task_comb_longdelay, \n",
    "                                                                                           expand_stimulus=False)\n",
    "\n",
    "_, stacked_interpolation_lr, stacked_interpolation_answer_lr = input_interpolation(test_input_longresponse, \n",
    "                                                                                   test_output_longresponse, \n",
    "                                                                                   label_task_comb_longresponse, \n",
    "                                                                                   expand_stimulus=False)\n",
    "\n",
    "_, stacked_interpolation_ls, stacked_interpolation_answer_ls = input_interpolation(test_input_longstimulus, \n",
    "                                                                                   test_output_longstimulus, \n",
    "                                                                                   label_task_comb_longstimulus, \n",
    "                                                                                   expand_stimulus=False)\n",
    "\n",
    "_, stacked_interpolation_lf, stacked_interpolation_answer_lf = input_interpolation(test_input_longfixation, \n",
    "                                                                                   test_output_longfixation, \n",
    "                                                                                   label_task_comb_longfixation, \n",
    "                                                                                   expand_stimulus=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2431ca-eb26-434a-bf42-6cd73b98d91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_stamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18827650-ee34-46ba-964f-c3378fb6fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_stamp_input_map = [\n",
    "    [time_stamps_usual, test_input, \"normal\", 0, \"delay_end\", label_task_comb], \n",
    "    [time_stamps, test_input_longdelay, \"longdelay\", 3, \"delay_end\", label_task_comb_longdelay], \n",
    "    [time_stamps_longstimulus, test_input_longstimulus, \"longstimulus\", 2, \"stimulus_end\", label_task_comb_longstimulus], \n",
    "    [time_stamps_longresponse, test_input_longresponse, \"longresponse\", 4, \"trial_end\", label_task_comb_longresponse],\n",
    "    [time_stamps_longfixation, test_input_longfixation, \"longfixation\", 1, \"fixation_end\", label_task_comb_longfixation]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4d637d-ba65-40b7-a9f2-cf8e7265e10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_consecutive_diff(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    X: (T, N)\n",
    "    returns: (T-1,) array, per-time-step L2 difference\n",
    "    \"\"\"\n",
    "    d = np.diff(X, axis=0)          # (T-1, N)\n",
    "    return np.linalg.norm(d, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be64941-b474-4363-8074-2aa67a5817c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for time_stamp_long, test_input_long, sname, db_index, end_time_name, _ in time_stamp_input_map: \n",
    "    end_time = time_stamp_long[end_time_name]\n",
    "    _, M, h, _ = modulation_extraction(test_input_long, db_lst[db_index][-1], layer_index)\n",
    "    M = M.reshape(M.shape[0], M.shape[1], -1)\n",
    "    diff_h_all = np.array([l2_consecutive_diff(h[batch_size]) for batch_size in range(h.shape[0])])\n",
    "    diff_M_all = np.array([l2_consecutive_diff(M[batch_size]) for batch_size in range(M.shape[0])])\n",
    "\n",
    "    fig, axs = plt.subplots(1,2,figsize=(4*2,4))\n",
    "    axs[0].plot(np.mean(diff_h_all, axis=0), color=c_vals[0])\n",
    "    axs[1].plot(np.mean(diff_M_all, axis=0), color=c_vals[0])\n",
    "    for ax in axs:\n",
    "        ax.set_xlabel(\"Time Steps\", fontsize=12)\n",
    "        ax.set_yscale(\"log\")\n",
    "        ax.axvline(end_time, color=c_vals[1], linewidth=2)\n",
    "        ax.set_title(sname, fontsize=15)\n",
    "    axs[0].set_ylabel(\"Hidden Per-Time-Step L2 Diff\", fontsize=12)\n",
    "    axs[1].set_ylabel(\"Modulation Per-Time-Step L2 Diff\", fontsize=12)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"./twotasks/l2_timee_diff_seed{seed}_{hyp_dict['addon_name']}_{sname}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661cad69-fc25-4a29-9823-0a0712ea4352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "\n",
    "projected_data_all = [] \n",
    "\n",
    "for time_stamp_long, test_input_long, sname, db_index, _, label_task_comb_long in time_stamp_input_map: \n",
    "    print(f\"sname: {sname}; db_index: {db_index}\")\n",
    "    # 0 Red, 1 blue, 2 green, 3 purple, 4 orange, 5 teal, 6 gray, 7 pink, 8 yellow\n",
    "    names = [\"hidden\", \"modulation\"]\n",
    "        \n",
    "    for name in names:\n",
    "        fighs, axshs = plt.subplots(1,3,figsize=(5*3,5*1))\n",
    "        \n",
    "        PCA_downsample = 3\n",
    "            \n",
    "        Ms, Ms_orig, hs, bs = modulation_extraction(test_input_long, db_lst[db_index][-1], layer_index)\n",
    "        batch_num = Ms_orig.shape[0]\n",
    "        \n",
    "        if name == \"modulation\": \n",
    "            data = Ms\n",
    "        elif name == \"hidden\":\n",
    "            data = hs \n",
    "            \n",
    "        print(f\"data.shape: {data.shape}\")\n",
    "        \n",
    "        pca = PCA(n_components = PCA_downsample, random_state=42)\n",
    "        n_activity = data.shape[-1] \n",
    "    \n",
    "        activity_zero = np.zeros((1, n_activity))\n",
    "        \n",
    "        mask_task1 = label_task_comb_long[:,1] == 1\n",
    "        mask_task0 = label_task_comb_long[:,0] == 0\n",
    "                \n",
    "        as_flat = data.reshape((-1, n_activity))    \n",
    "        pca.fit(as_flat)\n",
    "        \n",
    "        total_ev_training = pca.explained_variance_ratio_.sum()  \n",
    "    \n",
    "        if name == \"hidden\": \n",
    "            wout = net.W_output.detach().cpu().numpy() \n",
    "            wout_proj = pca.transform(wout) \n",
    "        \n",
    "        as_pca = pca.transform(as_flat)\n",
    "        projected_data = as_pca.reshape((data.shape[0], data.shape[1], -1))\n",
    "        print(projected_data.shape)\n",
    "\n",
    "        # save for the delay fixed point analysis\n",
    "        if db_index == 0: \n",
    "            projected_data_all.append(projected_data)\n",
    "            \n",
    "        zeros_pca = pca.transform(activity_zero)\n",
    "        \n",
    "        combination = [(0, 1), (0, 2), (1, 2)]\n",
    "\n",
    "        phases = [\n",
    "            (\"fix\",   \"fixation_start\", \"fixation_end\", 1),\n",
    "            (\"stim\",  \"stimulus_start\", \"delay_start\",  2),\n",
    "            (\"delay\", \"delay_start\",    \"delay_end\",    3),\n",
    "            (\"resp\",  \"delay_end\",      \"trial_end\",    0),\n",
    "        ]\n",
    "        \n",
    "        # transition endpoints to highlight (end of each phase)\n",
    "        # (key, marker index)\n",
    "        transitions = [\n",
    "            (\"fixation_end\", 1), # end of fixation\n",
    "            (\"delay_start\", 2),  # end of stim\n",
    "            (\"delay_end\",   3),  # end of delay\n",
    "            (\"trial_end\",   0),  # end of resp\n",
    "        ]\n",
    "\n",
    "        period_markers = {\n",
    "            \"Fixation\": 1,   # 'v'\n",
    "            \"Stimulus\": 2,   # '*'\n",
    "            \"Delay\":    3,   # 'x'\n",
    "            \"Response\": 0,   \n",
    "        }\n",
    "        \n",
    "        stim0 = time_stamp_long[\"stimulus_start\"]\n",
    "        trial_end = time_stamp_long[\"trial_end\"]\n",
    "\n",
    "        legend_handles = [\n",
    "            Line2D(\n",
    "                [0], [0],\n",
    "                marker=markers_vals[idx],\n",
    "                linestyle=\"None\",\n",
    "                markersize=10,\n",
    "                markerfacecolor=\"k\",\n",
    "                markeredgecolor=\"k\",\n",
    "                label=label\n",
    "            )\n",
    "            for label, idx in period_markers.items()\n",
    "        ]\n",
    "        \n",
    "        for i in range(batch_num):\n",
    "            task = label_task_comb_longdelay[i, 1]\n",
    "            if task not in (0, 1):\n",
    "                continue\n",
    "        \n",
    "            color = c_vals[label_task_comb_longdelay[i, 0]]\n",
    "            ls = linestyles[task]\n",
    "            data = projected_data[i]\n",
    "            seg = slice(stim0, trial_end)\n",
    "        \n",
    "            for ax, (a, b) in zip(axshs, combination):\n",
    "                # full trajectory\n",
    "                ax.plot(data[seg, a], data[seg, b], c=color, linestyle=ls, alpha=0.01)\n",
    "        \n",
    "                # phase point clouds\n",
    "                for _, t0_key, t1_key, mk_idx in phases:\n",
    "                    sl = slice(time_stamp_long[t0_key], time_stamp_long[t1_key])\n",
    "                    ax.scatter(data[sl, a], data[sl, b],\n",
    "                               c=color, marker=markers_vals[mk_idx], \n",
    "                               alpha=0.01 if sname != \"normal\" else 0.1)\n",
    "        \n",
    "                # highlight transition endpoints (single points)\n",
    "                for t_key, mk_idx in transitions:\n",
    "                    t = time_stamp_long[t_key] - 1  # end index of that period\n",
    "                    ax.scatter([data[t, a]], [data[t, b]],\n",
    "                               c=color,\n",
    "                               marker=markers_vals[mk_idx],\n",
    "                               alpha=0.8,      # much more visible\n",
    "                               s=60,           # larger\n",
    "                               linewidths=0.6,\n",
    "                               zorder=10)\n",
    "        \n",
    "        # Set cosmetics once (not every batch)\n",
    "        for ax, (a, b) in zip(axshs, combination):\n",
    "            ax.set_xlabel(f\"PCA {a+1}\", fontsize=12)\n",
    "            ax.set_ylabel(f\"PCA {b+1}\", fontsize=12)\n",
    "            ax.set_title(f\"name: {name}; sname: {sname}\", fontsize=15)\n",
    "            ax.legend(\n",
    "                handles=legend_handles, \n",
    "                loc=\"upper right\", \n",
    "                frameon=False\n",
    "            )\n",
    "            \n",
    "        fighs.tight_layout()\n",
    "        fighs.savefig(f\"./twotasks/m_pca_{name}_seed{seed}_{hyp_dict['addon_name']}_{sname}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2558abd0-b2d1-439f-9504-61ea70cbbba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(projected_data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b86079-6e62-4093-8c68-dc94c3afcf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "for indd, projected_data in enumerate(projected_data_all): \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for i in range(batch_num):\n",
    "        data_batch = projected_data[i, :, :]\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=data_batch[:,0], y=data_batch[:,1], z=data_batch[:,2],\n",
    "                mode=\"lines\",\n",
    "                line=dict(width=2, color=c_vals[label_task_comb[i,0]]),\n",
    "                opacity=0.5,\n",
    "                showlegend=False           \n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # origin point\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=[zeros_pca[0, 0]], y=[zeros_pca[0, 1]], z=[zeros_pca[0, 2]],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=4, color=\"black\"),\n",
    "            showlegend=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    zero_pt = zeros_pca[0]\n",
    "    \n",
    "    # define the two spanning vesample_non_nanctors (from the origin)\n",
    "    v1 = wout_proj[0,:]\n",
    "    v2 = wout_proj[1,:]\n",
    "    \n",
    "    # pick a side-length that matches your datas overall scale\n",
    "    traj = projected_data[:, :, :].reshape(-1, 3)\n",
    "    plane_half = 0.5 * np.linalg.norm(traj - zero_pt, axis=1).max()           \n",
    "    \n",
    "    # build an (almost) orthonormal basis in the v1v2 plane\n",
    "    u_hat = v1 / np.linalg.norm(v1)\n",
    "    v2_proj = v2 - v2.dot(u_hat) * u_hat      \n",
    "    v_hat = v2_proj / np.linalg.norm(v2_proj)\n",
    "    \n",
    "    # four corners of a square patch centred at the origin\n",
    "    corners = np.array([\n",
    "        -plane_half*u_hat - plane_half*v_hat,\n",
    "         plane_half*u_hat - plane_half*v_hat,\n",
    "         plane_half*u_hat + plane_half*v_hat,\n",
    "        -plane_half*u_hat + plane_half*v_hat\n",
    "    ])\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Mesh3d(\n",
    "            x=corners[:, 0],\n",
    "            y=corners[:, 1],\n",
    "            z=corners[:, 2],\n",
    "            i=[0, 0],\n",
    "            j=[1, 2],\n",
    "            k=[2, 3],\n",
    "            opacity=0.25,\n",
    "            color=\"lightblue\",\n",
    "            name=\"spanning plane\",\n",
    "            showscale=False\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title=\"PCA 1\",\n",
    "            yaxis_title=\"PCA 2\",\n",
    "            zaxis_title=\"PCA 3\"\n",
    "        ),\n",
    "        width=600,          \n",
    "        height=600,\n",
    "        margin=dict(l=0, r=0, t=40, b=0),\n",
    "        showlegend=False     \n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "    response_half = int((time_stamps_usual[\"trial_end\"] - time_stamps_usual[\"delay_end\"])/2) + time_stamps_usual[\"delay_end\"]\n",
    "    endpoints = projected_data[:,response_half + 1, :]\n",
    "\n",
    "    figproj, axproj = plt.subplots(1, 1, figsize=(4, 4))\n",
    "    for ei in range(endpoints.shape[0]):\n",
    "        endpoint = endpoints[ei, :] - zero_pt\n",
    "        u_coord = endpoint.dot(u_hat)\n",
    "        v_coord = endpoint.dot(v_hat)\n",
    "    \n",
    "        color_index = label_task_comb[ei, 0]\n",
    "        task = label_task_comb[ei, 1]\n",
    "        marker = \"o\" if task == 0 else \"x\"\n",
    "    \n",
    "        axproj.scatter(u_coord, v_coord, c=c_vals[color_index], marker=marker, alpha=0.5)\n",
    "    \n",
    "    figproj.show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cf0129-92db-45e7-82d6-4a3f53e164fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import SymLogNorm\n",
    "from scipy.spatial import ConvexHull   \n",
    "\n",
    "def ring_length(pts: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    diffs = np.diff(pts, axis=0, append=pts[:1])   # close the loop\n",
    "    return np.linalg.norm(diffs, axis=1).sum()\n",
    "\n",
    "def ring_volume_nd(pts: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    T, D = pts.shape\n",
    "    if T <= D:\n",
    "        raise ValueError(\n",
    "            f\"Need at least D+1={D+1} non-coplanar points, got {T}.\"\n",
    "        )\n",
    "\n",
    "    hull = ConvexHull(pts)\n",
    "    return hull.volume\n",
    "\n",
    "def ring_volume_3d(pts: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if pts.shape[1] != 3:\n",
    "        raise ValueError(\"ring_volume_3d expects a 3-D point set.\")\n",
    "\n",
    "    hull = ConvexHull(pts)                # triangulated convex surface\n",
    "    return hull.volume                    # signed; take abs if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf73ff-b881-49c6-9c62-03af900a3672",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_stamps_longresponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93f208d-53fa-49c5-ba77-82ddefbb3812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 Red, 1 blue, 2 green, 3 purple, 4 orange, 5 teal, 6 gray, 7 pink, 8 yellow\n",
    "# 2025-12-30: this part effectively mapped to Figure 2f and 2h in Driscoll NN\n",
    "stacked_interpolation_lst = [stacked_interpolation_ld, stacked_interpolation_lr, stacked_interpolation_ls, stacked_interpolation_lf]\n",
    "time_stamps_lst = [time_stamps, time_stamps_longresponse, time_stamps_longstimulus, time_stamps_longfixation]\n",
    "stacked_interpolation_name_lst = [\"longdelay\", \"longresponse\", \"longstimulus\", \"longfixation\"]\n",
    "desire_period = [[time_stamps[\"delay_start\"], time_stamps[\"delay_end\"]], \n",
    "                 [time_stamps_longresponse[\"delay_end\"], time_stamps_longresponse[\"trial_end\"]],\n",
    "                 [time_stamps_longstimulus[\"stimulus_start\"], time_stamps_longstimulus[\"stimulus_end\"]],\n",
    "                 [time_stamps_longfixation[\"fixation_start\"], time_stamps_longfixation[\"fixation_end\"]]]\n",
    "\n",
    "int_input_all = []\n",
    "raw_data_ring_all, raw_data_ring_magnitude_all, projected_data_ring_all = [], [], []\n",
    "\n",
    "for siindex, stacked_interpolation_ in enumerate(stacked_interpolation_lst):\n",
    "    sname = stacked_interpolation_name_lst[siindex]\n",
    "    print(f\"sname: {sname}\")\n",
    "\n",
    "    # hidden, modulation, W \\cdot modulation\n",
    "    names = [\"hidden\", \"modulation\", \"w_modulation\"]\n",
    "    \n",
    "    raw_data_ring = [[], [], []] \n",
    "    raw_data_ring_magnitude = [[], [], []]\n",
    "    projected_data_ring = [[], [], []]\n",
    "    \n",
    "    for nindex, name in enumerate(names):\n",
    "        fighs, axshs = plt.subplots(1,3,figsize=(5*3,5*1))\n",
    "        fighsadd, axshsadd = plt.subplots(1,3,figsize=(5*3,5*1))\n",
    "        \n",
    "        fig3dfix = go.Figure()\n",
    "\n",
    "        fig3dfix.update_traces(\n",
    "            selector=dict(type=\"scatter3d\"),\n",
    "            marker=dict(size=3),\n",
    "            line=dict(width=2)\n",
    "        )\n",
    "        \n",
    "        PCA_downsample = 3\n",
    "        combination = [[0,1],[0,2],[1,2]]\n",
    "        \n",
    "        interpolation_label = [i for i in range(len(stacked_interpolation_[0]))]\n",
    "        print(interpolation_label)\n",
    "        \n",
    "        def numbered_markers(n):\n",
    "            \"\"\"\n",
    "            Return a list ['\\$0\\$', '\\$1\\$', ... '\\$(n-1)\\$'] that Matplotlib accepts\n",
    "            as per-point marker styles.\n",
    "            \"\"\"\n",
    "            return [f'${i}$' for i in range(n)]\n",
    "        \n",
    "        marker_new = numbered_markers(len(stacked_interpolation_))\n",
    "        \n",
    "        projected_data_fix_all = []\n",
    "        \n",
    "        for (int_index, int_input) in enumerate(stacked_interpolation_): \n",
    "            if int_index == 0 and nindex == 0: # sample input (just for marker)\n",
    "                int_input_all.append(int_input)\n",
    "                \n",
    "            stack_output, _, db_intp = net.iterate_sequence_batch(int_input, run_mode='track_states', \n",
    "                                                                  save_to_cpu=True, detach_saved=True)\n",
    "    \n",
    "            Ms, Ms_orig, hs, bs = modulation_extraction(int_input, db_intp, layer_index, cuda=True)\n",
    "            batch_num = Ms_orig.shape[0]\n",
    "        \n",
    "            if name == \"hidden\": \n",
    "                data = hs\n",
    "            elif name == \"modulation\": \n",
    "                data = Ms\n",
    "            elif name == \"w_modulation\":\n",
    "                W = net.mp_layer1.W.data.detach().cpu().numpy()\n",
    "                data = (Ms_orig * W[None,None,:,:]).reshape(Ms.shape[0], Ms.shape[1], -1)\n",
    "\n",
    "            if int_index == 0: \n",
    "                print(data.shape)\n",
    "\n",
    "            n_activity = data.shape[-1]\n",
    "    \n",
    "            # extract the delay period information\n",
    "            as_flat_wantperiod_ = data[:,desire_period[siindex][0]:desire_period[siindex][1],:]\n",
    "            as_flat_wantperiod = as_flat_wantperiod_.reshape((-1, n_activity))\n",
    "    \n",
    "            # fixed point in original dimension\n",
    "            as_flat_fixedpoint_raw = data[:,desire_period[siindex][1],:]\n",
    "    \n",
    "            raw_data_ring[names.index(name)].append(ring_length(as_flat_fixedpoint_raw))\n",
    "            \n",
    "            fixpt_norm = np.linalg.norm(as_flat_fixedpoint_raw, axis=1)\n",
    "            raw_data_ring_magnitude[names.index(name)].append(fixpt_norm.mean())\n",
    "            \n",
    "            as_flat = data.reshape((-1, n_activity))\n",
    "        \n",
    "            if int_index == 0: \n",
    "                print(\"Generate New PCA axes\")\n",
    "                pca_delay = PCA(n_components = PCA_downsample, random_state=42)\n",
    "                activity_zero = np.zeros((1, n_activity))\n",
    "                pca_delay.fit(as_flat_wantperiod) \n",
    "            \n",
    "            as_pca = pca_delay.transform(as_flat)\n",
    "            projected_data = as_pca.reshape((data.shape[0], data.shape[1], -1))\n",
    "        \n",
    "            projected_data_fix = projected_data[:,desire_period[siindex][1],:]\n",
    "    \n",
    "            projected_data_ring[names.index(name)].append(ring_length(projected_data_fix))\n",
    "            \n",
    "            projected_data_fix_all.append(projected_data_fix)\n",
    "            \n",
    "            for i in range(batch_num):\n",
    "                data_batch = projected_data_fix[i,:]\n",
    "                for index, comb in enumerate(combination):\n",
    "                    marker_value = marker_new[int_index] if int_index == 0 or int_index == len(stacked_interpolation_)-1 else \"o\"\n",
    "                    alpha_value = 0.1 if marker_value == \"o\" else 1.0\n",
    "                    \n",
    "                    axshs[index].scatter(data_batch[comb[0]], data_batch[comb[1]], c=c_vals[interpolation_label[i]], \n",
    "                                         marker=marker_value, alpha=alpha_value)\n",
    "                    axshs[index].set_xlabel(f\"PCA {comb[0]+1}; Anti {sname}\", fontsize=15)\n",
    "                    axshs[index].set_ylabel(f\"PCA {comb[1]+1}; Anti {sname}\", fontsize=15)\n",
    "        \n",
    "        for index, comb in enumerate(combination):\n",
    "            select1 = [pa[:,comb[0]] for pa in projected_data_fix_all] \n",
    "            min_select1 = min(arr.min() for arr in select1)\n",
    "            \n",
    "            select2 = [pa[:,comb[1]] for pa in projected_data_fix_all] \n",
    "            min_select2 = min(arr.min() for arr in select2) \n",
    "    \n",
    "            epsilon = 1 if name == \"hidden\" else 10\n",
    "            min_select1 -= epsilon\n",
    "            min_select2 -= epsilon\n",
    "            \n",
    "            indices_lst = [0, 10, -1] \n",
    "            for it_idx, it in enumerate(indices_lst):\n",
    "                xy = projected_data_fix_all[it][:,[comb[0],comb[1]]]  \n",
    "                num_xy = xy.shape[0] \n",
    "    \n",
    "                for xy_index in range(num_xy): \n",
    "                    axshsadd[index].plot([xy[xy_index%num_xy,0], xy[(xy_index+1)%num_xy,0]],\\\n",
    "                                       [xy[xy_index%num_xy,1], xy[(xy_index+1)%num_xy,1]],\\\n",
    "                                       linestyle=\"--\", linewidth=3, color=c_vals_l[it_idx])\n",
    "        \n",
    "            for i in range(len(interpolation_label)): \n",
    "                fixed_points = np.array([projected_data_fix[i,:] for projected_data_fix in projected_data_fix_all])\n",
    "                axshsadd[index].plot(fixed_points[:,comb[0]],\n",
    "                                     fixed_points[:,comb[1]],\n",
    "                                     \"-o\", c=c_vals[interpolation_label[i]])\n",
    "\n",
    "                axshsadd[index].set_xlabel(f\"PCA {comb[0]+1}\", fontsize=15)\n",
    "                axshsadd[index].set_ylabel(f\"PCA {comb[1]+1}\", fontsize=15)\n",
    "    \n",
    "                if index == 0:\n",
    "                    fig3dfix.add_trace(\n",
    "                        go.Scatter3d(\n",
    "                            x=np.array(alpha_lst),\n",
    "                            y=fixed_points[:,0], \n",
    "                            z=fixed_points[:,1],\n",
    "                            mode=\"lines+markers\",\n",
    "                            line=dict(\n",
    "                                width=6,                  \n",
    "                                color=c_vals[interpolation_label[i]],\n",
    "                            ),\n",
    "                            marker=dict(\n",
    "                                size=5,                    \n",
    "                                color=c_vals[interpolation_label[i]],\n",
    "                                symbol=\"circle\",           \n",
    "                            ),\n",
    "                            opacity=0.5,\n",
    "                            name=f\"Stimulus {i}\",\n",
    "                            showlegend=True\n",
    "                        )\n",
    "                    )\n",
    "    \n",
    "        fighs.suptitle(f\"name: {name}; sname: {sname}\", fontsize=20)\n",
    "        fighs.tight_layout()\n",
    "        fighs.savefig(f\"./twotasks/m_pca_attractor_{name}_seed{seed}_{hyp_dict['addon_name']}_{int_index}_{sname}.png\", dpi=300)\n",
    "    \n",
    "        fighsadd.suptitle(f\"name: {name}; sname: {sname}\", fontsize=20)\n",
    "        fighsadd.tight_layout() \n",
    "        fighsadd.savefig(f\"./twotasks/m_pca_attractor_cycle_{name}_seed{seed}_{hyp_dict['addon_name']}_{int_index}_{sname}.png\", dpi=300)\n",
    "\n",
    "        fig3dfix.update_layout(\n",
    "            title=dict(\n",
    "                text=f\"name: {name}; sname: {sname}\",\n",
    "                x=0.5,\n",
    "                xanchor=\"center\",\n",
    "                y=0.95,\n",
    "                font=dict(size=14)              #  smaller title\n",
    "            ),\n",
    "            scene=dict(\n",
    "                domain=dict(\n",
    "                    x=[0.05,0.95],\n",
    "                    y=[0.05,0.95],\n",
    "                ),\n",
    "                xaxis=dict(\n",
    "                    title=\"Alpha\",\n",
    "                    tickfont=dict(size=12),\n",
    "                ),\n",
    "                yaxis=dict(\n",
    "                    title=f\"PCA 1; Anti {sname}\",\n",
    "                    tickfont=dict(size=12),\n",
    "                ),\n",
    "                zaxis=dict(\n",
    "                    title=f\"PCA 2; Anti {sname}\",\n",
    "                    tickfont=dict(size=12),\n",
    "                ),\n",
    "                aspectratio=dict(x=1, y=1, z=0.8)  # slightly flatter cube\n",
    "            ),\n",
    "            width=650,                         #  smaller overall figure\n",
    "            height=650,\n",
    "            margin=dict(l=10, r=10, t=35, b=10),\n",
    "            showlegend=True\n",
    "        )\n",
    "\n",
    "        fig3dfix.show()\n",
    "\n",
    "    raw_data_ring_all.append(raw_data_ring)\n",
    "    raw_data_ring_magnitude_all.append(raw_data_ring_magnitude)\n",
    "    projected_data_ring_all.append(projected_data_ring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e251834-99be-4070-b830-1c4185971af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_lst(lst, value=None):\n",
    "    if value is None: \n",
    "        value = lst[0]\n",
    "    return [val_/value for val_ in lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77720a32-1fd4-4c04-9e49-910d28456e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 2x2 panels: rows = {High-D, 3-D}, cols = {Hidden, Modulation}\n",
    "fig, axs = plt.subplots(2, 2, figsize=(4*2, 4*2), sharex=True)\n",
    "\n",
    "ax_hd_hid = axs[0, 0]\n",
    "ax_hd_mod = axs[0, 1]\n",
    "ax_3d_hid = axs[1, 0]\n",
    "ax_3d_mod = axs[1, 1]\n",
    "\n",
    "for i, sname in enumerate(stacked_interpolation_name_lst):\n",
    "    # High-D ring perimeter\n",
    "    y_hd_hidden = normalize_lst(raw_data_ring_all[i][0])\n",
    "    y_hd_mod    = normalize_lst(raw_data_ring_all[i][1])\n",
    "\n",
    "    # 3-D ring perimeter\n",
    "    y_3d_hidden = normalize_lst(projected_data_ring_all[i][0])\n",
    "    y_3d_mod    = normalize_lst(projected_data_ring_all[i][1])\n",
    "\n",
    "    # Plot (one line per sname)\n",
    "    ax_hd_hid.plot(alpha_lst, y_hd_hidden, \"-o\", color=c_vals[i], alpha=0.9, label=sname)\n",
    "    ax_hd_mod.plot(alpha_lst, y_hd_mod,    \"-o\", color=c_vals[i], alpha=0.9, label=sname)\n",
    "    ax_3d_hid.plot(alpha_lst, y_3d_hidden, \"-o\", color=c_vals[i], alpha=0.9, label=sname)\n",
    "    ax_3d_mod.plot(alpha_lst, y_3d_mod,    \"-o\", color=c_vals[i], alpha=0.9, label=sname)\n",
    "\n",
    "# Titles / labels\n",
    "ax_hd_hid.set_title(\"High-D Ring Perimeter (Hidden)\", fontsize=14)\n",
    "ax_hd_mod.set_title(\"High-D Ring Perimeter (Modulation)\", fontsize=14)\n",
    "ax_3d_hid.set_title(\"3-D Ring Perimeter (Hidden)\", fontsize=14)\n",
    "ax_3d_mod.set_title(\"3-D Ring Perimeter (Modulation)\", fontsize=14)\n",
    "\n",
    "ax_hd_hid.set_ylabel(\"Normalized Ring Perimeter\", fontsize=13)\n",
    "ax_3d_hid.set_ylabel(\"Normalized Ring Perimeter\", fontsize=13)\n",
    "ax_3d_hid.set_xlabel(\"Alpha\", fontsize=13)\n",
    "ax_3d_mod.set_xlabel(\"Alpha\", fontsize=13)\n",
    "\n",
    "# Styling\n",
    "for ax in axs.ravel():\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_ylim([5e-2, 1e0+2e-1])\n",
    "    ax.tick_params(axis=\"y\", labelsize=11)\n",
    "    ax.tick_params(axis=\"x\", labelsize=11)\n",
    "\n",
    "# handles, labels = ax_hd_hid.get_legend_handles_labels()\n",
    "# fig.legend(handles, labels, fontsize=13, frameon=True, loc=\"center left\",\n",
    "#            bbox_to_anchor=(1.01, 0.5))\n",
    "for ax in axs.flatten():\n",
    "    ax.legend(frameon=True, fontsize=12)\n",
    "\n",
    "# fig.suptitle(f\"Ring Perimeter vs Alpha (name={name}, seed={seed})\", fontsize=15)\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(\n",
    "    f\"./twotasks/m_pca_ring_ALL_{name}_seed{seed}_{hyp_dict['addon_name']}_{int_index}.png\",dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67b0e7d-0a53-4369-8171-8685ea1de10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "desire_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f606c4c-3f2c-459b-a215-e2286a5300d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_convex_hull_mesh(\n",
    "    fig,\n",
    "    x, y, z,\n",
    "    row=None, col=None,\n",
    "    name=\"Endpoint hull\",\n",
    "    mesh_opacity=0.15,\n",
    "    mesh_color=\"black\",\n",
    "    showlegend=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Add a 3D convex hull mesh to a Plotly figure.\n",
    "\n",
    "    Works with both:\n",
    "      - go.Figure() (single scene): call without row/col\n",
    "      - make_subplots(..., specs=[[{\"type\":\"scene\"}, ...]]): pass row/col\n",
    "    \"\"\"\n",
    "    pts = np.column_stack([np.asarray(x), np.asarray(y), np.asarray(z)])\n",
    "\n",
    "    # Need at least 4 non-coplanar points for a 3D hull\n",
    "    if pts.shape[0] < 4:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        hull = ConvexHull(pts)\n",
    "    except Exception as e:\n",
    "        # Often fails if points are coplanar\n",
    "        print(f\"ConvexHull failed: {e}\")\n",
    "        return\n",
    "\n",
    "    tri = hull.simplices  # (n_facets, 3)\n",
    "\n",
    "    mesh = go.Mesh3d(\n",
    "        x=pts[:, 0], y=pts[:, 1], z=pts[:, 2],\n",
    "        i=tri[:, 0], j=tri[:, 1], k=tri[:, 2],\n",
    "        opacity=mesh_opacity,\n",
    "        color=mesh_color,\n",
    "        name=name,\n",
    "        showlegend=showlegend,\n",
    "    )\n",
    "\n",
    "    # If used with subplots, attach to a specific scene via row/col\n",
    "    if row is not None and col is not None:\n",
    "        fig.add_trace(mesh, row=row, col=col)\n",
    "    else:\n",
    "        fig.add_trace(mesh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a01002-faea-429d-85b6-906ed7c121dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_fit_plane_normal(pts, center=True, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Return unit normal of best-fit plane to pts (N,3), via SVD.\n",
    "    Normal is the right-singular vector with smallest singular value.\n",
    "    \"\"\"\n",
    "    pts = np.asarray(pts, float)\n",
    "    if pts.shape[0] < 3:\n",
    "        return None\n",
    "\n",
    "    X = pts - pts.mean(axis=0) if center else pts.copy()\n",
    "    # If points are nearly identical / degenerate\n",
    "    if np.linalg.norm(X) < eps:\n",
    "        return None\n",
    "\n",
    "    # SVD of centered coordinates\n",
    "    # Vt[-1] corresponds to smallest singular value => plane normal\n",
    "    _, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "    # If rank < 2, plane not well-defined\n",
    "    if s.size < 3 and np.all(s < eps):\n",
    "        return None\n",
    "\n",
    "    n = Vt[-1]\n",
    "    n_norm = np.linalg.norm(n)\n",
    "    if n_norm < eps:\n",
    "        return None\n",
    "    return n / n_norm\n",
    "\n",
    "def angle_between_unit_vectors(u, v, degrees=True, eps=1e-12):\n",
    "    if u is None or v is None:\n",
    "        return np.nan\n",
    "    c = float(np.clip(np.abs(np.dot(u, v)), -1.0, 1.0))\n",
    "    ang = np.arccos(c)\n",
    "    return np.degrees(ang) if degrees else ang\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd47510-a69e-437c-af08-c965f33462a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "desire_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e496907-4945-48f4-8895-082a0bb79657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traj(name):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    for siindex, stacked_interpolation in enumerate(stacked_interpolation_lst): \n",
    "        sname = stacked_interpolation_name_lst[siindex]\n",
    "        print(sname)\n",
    "    \n",
    "        # single 3D figure (cosine only)\n",
    "        fig3dresponse = go.Figure()\n",
    "        \n",
    "        N = len(stacked_interpolation)\n",
    "        \n",
    "        wout = net.W_output.detach().cpu().numpy()\n",
    "        print(wout.shape)\n",
    "                \n",
    "        # anti, hybrid, go\n",
    "        anti_go = [\n",
    "            stacked_interpolation[0],\n",
    "            stacked_interpolation[int((N + 1) / 2)],\n",
    "            stacked_interpolation[-1]\n",
    "        ]\n",
    "        \n",
    "        _, _, db_intp_anti = net.iterate_sequence_batch(anti_go[0], run_mode=\"track_states\", \n",
    "                                                        save_to_cpu=True, detach_saved=True)\n",
    "        _, _, db_intp_go   = net.iterate_sequence_batch(anti_go[2], run_mode=\"track_states\", \n",
    "                                                        save_to_cpu=True, detach_saved=True)\n",
    "        \n",
    "        Ms_anti, Ms_orig_anti, hs_anti, bs_anti = modulation_extraction(\n",
    "            int_input_all[siindex], db_intp_anti, layer_index\n",
    "        )\n",
    "        Ms_go, Ms_orig_go, hs_go, bs_go = modulation_extraction(\n",
    "            int_input_all[siindex], db_intp_go, layer_index\n",
    "        )\n",
    "        \n",
    "        if name == \"hidden\":\n",
    "            data_anti, data_go = hs_anti, hs_go\n",
    "        elif name == \"modulation\":\n",
    "            data_anti, data_go = Ms_anti, Ms_go\n",
    "        elif name == \"w_modulation\":\n",
    "            W = net.mp_layer1.W.data.detach().cpu().numpy()\n",
    "            data_anti = (Ms_orig_anti * W[None,None,:,:]).reshape(Ms_anti.shape[0], Ms_anti.shape[1], -1)\n",
    "            data_go = (Ms_orig_go * W[None,None,:,:]).reshape(Ms_go.shape[0], Ms_go.shape[1], -1)\n",
    "\n",
    "        print(f\"data_anti: {data_anti.shape}\")\n",
    "        print(f\"data_go: {data_go.shape}\")\n",
    "        \n",
    "        n_activity = data_anti.shape[-1]\n",
    "        \n",
    "        as_flat_stim = data_anti[\n",
    "            :, time_stamps_lst[siindex][\"stimulus_start\"]:\n",
    "               time_stamps_lst[siindex][\"stimulus_end\"], :\n",
    "        ].reshape((-1, n_activity))\n",
    "        \n",
    "        data_anti_ = data_anti[:, desire_period[siindex][0]:desire_period[siindex][1], :]\n",
    "        data_go_   = data_go[:, desire_period[siindex][0]:desire_period[siindex][1], :]\n",
    "        \n",
    "        as_flat_anti = data_anti_.reshape((-1, n_activity))\n",
    "        as_flat_go   = data_go_.reshape((-1, n_activity))\n",
    "        \n",
    "        pca_stim = PCA(n_components=PCA_downsample, random_state=42)\n",
    "        pca_stim.fit(as_flat_stim)\n",
    "        \n",
    "        projected_data_anti = pca_stim.transform(as_flat_anti).reshape(\n",
    "            data_anti_.shape[0], data_anti_.shape[1], -1)\n",
    "        projected_data_go = pca_stim.transform(as_flat_go).reshape(\n",
    "            data_go_.shape[0], data_go_.shape[1], -1)\n",
    "        \n",
    "        projected_data_stim_anti = projected_data_anti[:, :-1, :]\n",
    "        projected_data_stim_go   = projected_data_go[:,:-1, :]\n",
    "        \n",
    "        response_anti = -1 * hs_anti[:,desire_period[siindex][0]:desire_period[siindex][1], :] @ wout.T\n",
    "        response_go   = hs_go[:, desire_period[siindex][0]:desire_period[siindex][1], :] @ wout.T\n",
    "        \n",
    "        resp = 0  # cosine only\n",
    "        \n",
    "        for i in range(projected_data_stim_anti.shape[0]):\n",
    "            # anti trajectory\n",
    "            fig3dresponse.add_trace(\n",
    "                go.Scatter3d(\n",
    "                    x=projected_data_stim_anti[i, :, 0],\n",
    "                    y=projected_data_stim_anti[i, :, 1],\n",
    "                    z=response_anti[i, :, resp + 1],\n",
    "                    mode=\"lines\",\n",
    "                    line=dict(width=8, color=c_vals[i]),\n",
    "                    name=f\"Anti S{i}\",\n",
    "                    showlegend=True,\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # anti start / end\n",
    "            fig3dresponse.add_trace(\n",
    "                go.Scatter3d(\n",
    "                    x=[projected_data_stim_anti[i, 0, 0]],\n",
    "                    y=[projected_data_stim_anti[i, 0, 1]],\n",
    "                    z=[response_anti[i, 0, resp + 1]],\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(size=6, color=c_vals[i], symbol=\"circle-open\"),\n",
    "                    showlegend=False,\n",
    "                )\n",
    "            )\n",
    "            fig3dresponse.add_trace(\n",
    "                go.Scatter3d(\n",
    "                    x=[projected_data_stim_anti[i, -1, 0]],\n",
    "                    y=[projected_data_stim_anti[i, -1, 1]],\n",
    "                    z=[response_anti[i, -1, resp + 1]],\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(size=6, color=c_vals[i], symbol=\"circle\"),\n",
    "                    showlegend=False,\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # go trajectory\n",
    "            fig3dresponse.add_trace(\n",
    "                go.Scatter3d(\n",
    "                    x=projected_data_stim_go[i, :, 0],\n",
    "                    y=projected_data_stim_go[i, :, 1],\n",
    "                    z=response_go[i, :, resp + 1],\n",
    "                    mode=\"lines\",\n",
    "                    line=dict(width=8, color=c_vals[i], dash=\"dash\"),\n",
    "                    name=f\"Go S{i}\",\n",
    "                    showlegend=True,\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # go start / end\n",
    "            fig3dresponse.add_trace(\n",
    "                go.Scatter3d(\n",
    "                    x=[projected_data_stim_go[i, 0, 0]],\n",
    "                    y=[projected_data_stim_go[i, 0, 1]],\n",
    "                    z=[response_go[i, 0, resp + 1]],\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(size=6, color=c_vals[i], symbol=\"diamond-open\"),\n",
    "                    showlegend=False,\n",
    "                )\n",
    "            )\n",
    "            fig3dresponse.add_trace(\n",
    "                go.Scatter3d(\n",
    "                    x=[projected_data_stim_go[i, -1, 0]],\n",
    "                    y=[projected_data_stim_go[i, -1, 1]],\n",
    "                    z=[response_go[i, -1, resp + 1]],\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(size=6, color=c_vals[i], symbol=\"diamond\"),\n",
    "                    showlegend=False,\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # convex hulls (cosine only)\n",
    "        x_anti = projected_data_stim_anti[:, -1, 0]\n",
    "        y_anti = projected_data_stim_anti[:, -1, 1]\n",
    "        z_anti = response_anti[:, -1, resp + 1]\n",
    "        \n",
    "        add_convex_hull_mesh(\n",
    "            fig3dresponse, x_anti, y_anti, z_anti,\n",
    "            name=\"Anti endpoint hull (cos)\",\n",
    "            mesh_opacity=0.18,\n",
    "            mesh_color=\"black\",\n",
    "            showlegend=True,\n",
    "        )\n",
    "        \n",
    "        x_go = projected_data_stim_go[:, -1, 0]\n",
    "        y_go = projected_data_stim_go[:, -1, 1]\n",
    "        z_go = response_go[:, -1, resp + 1]\n",
    "        \n",
    "        add_convex_hull_mesh(\n",
    "            fig3dresponse, x_go, y_go, z_go,\n",
    "            name=\"Go endpoint hull (cos)\",\n",
    "            mesh_opacity=0.18,\n",
    "            mesh_color=\"red\",\n",
    "            showlegend=True,\n",
    "        )\n",
    "        \n",
    "        ang_deg = angle_between_unit_vectors(\n",
    "            best_fit_plane_normal(np.column_stack([x_anti, y_anti, z_anti])),\n",
    "            best_fit_plane_normal(np.column_stack([x_go,   y_go,   z_go])),\n",
    "            degrees=True,\n",
    "        )\n",
    "        \n",
    "        print(f\"[{sname}] cosine: plane-normal angle = {ang_deg:.2f} deg\")\n",
    "        \n",
    "        fig3dresponse.update_layout(\n",
    "            template=\"plotly_white\",\n",
    "            width=1000,\n",
    "            height=800,\n",
    "            scene=dict(\n",
    "                xaxis_title=\"Memoryanti Stimulus PCA 1\",\n",
    "                yaxis_title=\"Memoryanti Stimulus PCA 2\",\n",
    "                zaxis_title=\"cos \",\n",
    "                zaxis=dict(range=[-1.1, 1.1]),\n",
    "                aspectmode=\"cube\",\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        fig3dresponse.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46191c26-49c1-4cf1-a4f6-a36d336c359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj(\"hidden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80f6a87-9e1d-4676-9da6-8311231e1bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj(\"modulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314a4e38-7d75-45ca-898c-5d46d2f9d4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj(\"w_modulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38730cb-93f7-443c-bb65-2bb472a286bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traj_diff_alpha(name):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    for siindex, stacked_interpolation in enumerate(stacked_interpolation_lst):\n",
    "        sname = stacked_interpolation_name_lst[siindex]\n",
    "        print(sname)\n",
    "    \n",
    "        # cosine-only: single 3D scene\n",
    "        fig3dresponse_c = go.Figure()\n",
    "    \n",
    "        N = len(stacked_interpolation)\n",
    "    \n",
    "        wout = net.W_output.detach().cpu().numpy()\n",
    "        print(wout.shape)\n",
    "    \n",
    "        # name = \"hidden\"\n",
    "    \n",
    "        # anti, median (mid), go\n",
    "        anti_go = [\n",
    "            stacked_interpolation[0],\n",
    "            stacked_interpolation[int((N + 1) / 2)],\n",
    "            stacked_interpolation[-1],\n",
    "        ]\n",
    "    \n",
    "        _, _, db_intp_anti   = net.iterate_sequence_batch(anti_go[0], run_mode=\"track_states\", \n",
    "                                                          save_to_cpu=True, detach_saved=True)\n",
    "        _, _, db_intp_median = net.iterate_sequence_batch(anti_go[1], run_mode=\"track_states\", \n",
    "                                                          save_to_cpu=True, detach_saved=True)\n",
    "        _, _, db_intp_go     = net.iterate_sequence_batch(anti_go[2], run_mode=\"track_states\", \n",
    "                                                          save_to_cpu=True, detach_saved=True)\n",
    "    \n",
    "        Ms_anti,   Ms_orig_anti,   hs_anti,   bs_anti   = modulation_extraction(int_input_all[siindex], db_intp_anti,   layer_index)\n",
    "        Ms_median, Ms_orig_median, hs_median, bs_median = modulation_extraction(int_input_all[siindex], db_intp_median, layer_index)\n",
    "        Ms_go,     Ms_orig_go,     hs_go,     bs_go     = modulation_extraction(int_input_all[siindex], db_intp_go,     layer_index)\n",
    "    \n",
    "        if name == \"hidden\":\n",
    "            data_anti, data_go, data_median = hs_anti, hs_go, hs_median\n",
    "        elif name == \"modulation\":\n",
    "            data_anti, data_go, data_median = Ms_anti, Ms_go, Ms_median\n",
    "        elif name == \"w_modulation\":\n",
    "            W = net.mp_layer1.W.data.detach().cpu().numpy()\n",
    "            data_anti = (Ms_orig_anti * W[None,None,:,:]).reshape(Ms_anti.shape[0], Ms_anti.shape[1], -1)\n",
    "            data_go = (Ms_orig_go * W[None,None,:,:]).reshape(Ms_go.shape[0], Ms_go.shape[1], -1)\n",
    "            data_median = (Ms_orig_median * W[None,None,:,:]).reshape(Ms_median.shape[0], Ms_median.shape[1], -1)\n",
    "    \n",
    "        print(f\"data_anti.shape: {data_anti.shape}\")\n",
    "        n_activity = data_anti.shape[-1]\n",
    "    \n",
    "        as_flat_stim = data_anti[\n",
    "            :, time_stamps_lst[siindex][\"stimulus_start\"]:time_stamps_lst[siindex][\"stimulus_end\"], :\n",
    "        ].reshape((-1, n_activity))\n",
    "    \n",
    "        # desired period slicing\n",
    "        data_anti_   = data_anti[:,   desire_period[siindex][0]:desire_period[siindex][1], :]\n",
    "        data_go_     = data_go[:,     desire_period[siindex][0]:desire_period[siindex][1], :]\n",
    "        data_median_ = data_median[:, desire_period[siindex][0]:desire_period[siindex][1], :]\n",
    "    \n",
    "        as_flat_anti   = data_anti_.reshape((-1, n_activity))\n",
    "        as_flat_go     = data_go_.reshape((-1, n_activity))\n",
    "        as_flat_median = data_median_.reshape((-1, n_activity))\n",
    "    \n",
    "        pca_stim = PCA(n_components=PCA_downsample, random_state=42)\n",
    "        pca_stim.fit(as_flat_stim)\n",
    "    \n",
    "        projected_data_anti = pca_stim.transform(as_flat_anti).reshape(\n",
    "            data_anti_.shape[0], data_anti_.shape[1], -1\n",
    "        )\n",
    "        projected_data_go = pca_stim.transform(as_flat_go).reshape(\n",
    "            data_go_.shape[0], data_go_.shape[1], -1\n",
    "        )\n",
    "        projected_data_median = pca_stim.transform(as_flat_median).reshape(\n",
    "            data_median_.shape[0], data_median_.shape[1], -1\n",
    "        )\n",
    "    \n",
    "        projected_data_stim_anti   = projected_data_anti[:,   :-1, :]\n",
    "        projected_data_stim_go     = projected_data_go[:,     :-1, :]\n",
    "        projected_data_stim_median = projected_data_median[:, :-1, :]\n",
    "    \n",
    "        # responses (same as your code)\n",
    "        response_anti   = hs_anti[:,   :desire_period[siindex][1], :] @ wout.T\n",
    "        response_go     = hs_go[:,     :desire_period[siindex][1], :] @ wout.T\n",
    "        response_median = hs_median[:, :desire_period[siindex][1], :] @ wout.T\n",
    "    \n",
    "        resp = 0  # cosine only (resp+1 indexes cos component as in your original code)\n",
    "    \n",
    "        # plot endpoints only (anti / go / median)\n",
    "        for i in range(projected_data_stim_anti.shape[0]):\n",
    "            fig3dresponse_c.add_trace(\n",
    "                go.Scatter3d(\n",
    "                    x=[projected_data_stim_anti[i, -1, 0]],\n",
    "                    y=[projected_data_stim_anti[i, -1, 1]],\n",
    "                    z=[response_anti[i, -1, resp + 1]],\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(size=6, color=c_vals[i], symbol=\"circle\"),\n",
    "                    legendgroup=f\"S{i}\",\n",
    "                    showlegend=False,\n",
    "                )\n",
    "            )\n",
    "    \n",
    "            fig3dresponse_c.add_trace(\n",
    "                go.Scatter3d(\n",
    "                    x=[projected_data_stim_go[i, -1, 0]],\n",
    "                    y=[projected_data_stim_go[i, -1, 1]],\n",
    "                    z=[response_go[i, -1, resp + 1]],\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(size=6, color=c_vals[i], symbol=\"diamond\"),\n",
    "                    legendgroup=f\"S{i}\",\n",
    "                    showlegend=False,\n",
    "                )\n",
    "            )\n",
    "    \n",
    "            fig3dresponse_c.add_trace(\n",
    "                go.Scatter3d(\n",
    "                    x=[projected_data_stim_median[i, -1, 0]],\n",
    "                    y=[projected_data_stim_median[i, -1, 1]],\n",
    "                    z=[response_median[i, -1, resp + 1]],\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(size=6, color=c_vals[i], symbol=\"square\"),\n",
    "                    legendgroup=f\"S{i}\",\n",
    "                    showlegend=False,\n",
    "                )\n",
    "            )\n",
    "    \n",
    "        # convex hulls + plane-normal angles (cosine only)\n",
    "        x_end = projected_data_stim_anti[:, -1, 0]\n",
    "        y_end = projected_data_stim_anti[:, -1, 1]\n",
    "        z_end = response_anti[:, -1, resp + 1]\n",
    "    \n",
    "        add_convex_hull_mesh(\n",
    "            fig3dresponse_c,\n",
    "            x_end, y_end, z_end,\n",
    "            name=\"Anti endpoint hull (cos)\",\n",
    "            mesh_opacity=0.18,\n",
    "            mesh_color=c_vals[0],\n",
    "            showlegend=True,\n",
    "        )\n",
    "    \n",
    "        x_go = projected_data_stim_go[:, -1, 0]\n",
    "        y_go = projected_data_stim_go[:, -1, 1]\n",
    "        z_go = response_go[:, -1, resp + 1]\n",
    "    \n",
    "        add_convex_hull_mesh(\n",
    "            fig3dresponse_c,\n",
    "            x_go, y_go, z_go,\n",
    "            name=\"Go endpoint hull (cos)\",\n",
    "            mesh_opacity=0.18,\n",
    "            mesh_color=c_vals[1],\n",
    "            showlegend=True,\n",
    "        )\n",
    "    \n",
    "        x_median = projected_data_stim_median[:, -1, 0]\n",
    "        y_median = projected_data_stim_median[:, -1, 1]\n",
    "        z_median = response_median[:, -1, resp + 1]\n",
    "    \n",
    "        add_convex_hull_mesh(\n",
    "            fig3dresponse_c,\n",
    "            x_median, y_median, z_median,\n",
    "            name=\"Median endpoint hull (cos)\",\n",
    "            mesh_opacity=0.18,\n",
    "            mesh_color=c_vals[2],\n",
    "            showlegend=True,\n",
    "        )\n",
    "    \n",
    "        pts_anti   = np.column_stack([x_end,    y_end,    z_end])\n",
    "        pts_go     = np.column_stack([x_go,     y_go,     z_go])\n",
    "        pts_median = np.column_stack([x_median, y_median, z_median])\n",
    "    \n",
    "        n_anti   = best_fit_plane_normal(pts_anti)\n",
    "        n_go     = best_fit_plane_normal(pts_go)\n",
    "        n_median = best_fit_plane_normal(pts_median)\n",
    "    \n",
    "        checknames = [\"anti\", \"go\", \"median\"]\n",
    "        ns = [n_anti, n_go, n_median]\n",
    "    \n",
    "        for i in range(len(checknames)):\n",
    "            for j in range(i + 1, len(checknames)):\n",
    "                ang_deg = angle_between_unit_vectors(ns[i], ns[j], degrees=True)\n",
    "                print(f\"[{sname}] cosine: angle between {checknames[i]} - {checknames[j]} = {ang_deg:.2f} deg\")\n",
    "    \n",
    "        fig3dresponse_c.update_layout(\n",
    "            template=\"plotly_white\",\n",
    "            width=1000,\n",
    "            height=800,\n",
    "            scene=dict(\n",
    "                xaxis_title=\"Memoryanti Stimulus PCA 1\",\n",
    "                yaxis_title=\"Memoryanti Stimulus PCA 2\",\n",
    "                zaxis_title=\"cos \",\n",
    "                zaxis=dict(range=[-1.1, 1.1]),\n",
    "                aspectmode=\"cube\",\n",
    "            ),\n",
    "        )\n",
    "    \n",
    "        fig3dresponse_c.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5e8a0b-077e-4835-a1f8-28c1a92965a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_diff_alpha(\"hidden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c74e45a-67fe-4388-b426-1de1d7cb9bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_diff_alpha(\"modulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b77af6-00ec-488d-aa30-f5955a009b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_diff_alpha(\"w_modulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2e7293-228b-4341-bb4b-5c06d1ecf576",
   "metadata": {},
   "outputs": [],
   "source": [
    "for siindex, stacked_interpolation in enumerate(stacked_interpolation_lst):\n",
    "    N = len(stacked_interpolation)\n",
    "    sname = stacked_interpolation_name_lst[siindex]\n",
    "    \n",
    "    for name in names: \n",
    "        anti_go = [stacked_interpolation[0], stacked_interpolation[int((N+1)/2)], stacked_interpolation[-1]]\n",
    "        _, _, db_intp_anti = net.iterate_sequence_batch(anti_go[0], run_mode='track_states', \n",
    "                                                        save_to_cpu=True, detach_saved=True)\n",
    "        _, _, db_inp_middle = net.iterate_sequence_batch(anti_go[1], run_mode='track_states', \n",
    "                                                         save_to_cpu=True, detach_saved=True)\n",
    "        _, _, db_intp_go = net.iterate_sequence_batch(anti_go[2], run_mode='track_states', \n",
    "                                                      save_to_cpu=True, detach_saved=True)\n",
    "        \n",
    "        Ms_anti, Ms_orig_anti, hs_anti, bs_anti = modulation_extraction(int_input_all[siindex], db_intp_anti, layer_index)\n",
    "        Ms_middle, Ms_orig_middle, hs_middle, bs_middle = modulation_extraction(int_input_all[siindex], db_inp_middle, layer_index)\n",
    "        Ms_go, Ms_orig_go, hs_go, bs_go = modulation_extraction(int_input_all[siindex], db_intp_go, layer_index)\n",
    "    \n",
    "        batch_num = Ms_orig_go.shape[0]\n",
    "            \n",
    "        if name == \"hidden\": \n",
    "            data_anti, data_middle, data_go = hs_anti, hs_middle, hs_go\n",
    "        elif name == \"modulation\": \n",
    "            data_anti, data_middle, data_go = Ms_anti, Ms_middle, Ms_go\n",
    "            \n",
    "        n_activity = data_anti.shape[-1]\n",
    "    \n",
    "        as_flat_stim = data_anti[:,time_stamps_lst[siindex][\"stimulus_start\"]:time_stamps_lst[siindex][\"stimulus_end\"],:].reshape((-1, n_activity))\n",
    "    \n",
    "        as_flat_anti = data_anti.reshape((-1, n_activity))\n",
    "        as_flat_middle = data_middle.reshape((-1, n_activity))\n",
    "        as_flat_go = data_go.reshape((-1, n_activity))\n",
    "    \n",
    "        pca_stim = PCA(n_components = PCA_downsample, random_state=42)\n",
    "        pca_stim.fit(as_flat_stim) \n",
    "    \n",
    "        as_pca_anti = pca_stim.transform(as_flat_anti)\n",
    "        projected_data_anti = as_pca_anti.reshape((data_anti.shape[0], data_anti.shape[1], -1))\n",
    "        as_pca_middle = pca_stim.transform(as_flat_middle)\n",
    "        projected_data_middle = as_pca_middle.reshape((data_middle.shape[0], data_middle.shape[1], -1))\n",
    "        as_pca_go = pca_stim.transform(as_flat_go)\n",
    "        projected_data_go = as_pca_go.reshape((data_go.shape[0], data_go.shape[1], -1))\n",
    "    \n",
    "        projected_data_stim_anti = projected_data_anti[:,time_stamps_lst[siindex][\"stimulus_start\"]:time_stamps_lst[siindex][\"stimulus_end\"],:]\n",
    "        projected_data_stim_middle = projected_data_middle[:,time_stamps_lst[siindex][\"stimulus_start\"]:time_stamps_lst[siindex][\"stimulus_end\"],:]\n",
    "        projected_data_stim_go = projected_data_go[:,time_stamps_lst[siindex][\"stimulus_start\"]:time_stamps_lst[siindex][\"stimulus_end\"],:]\n",
    "    \n",
    "        fig, axs = plt.subplots(1,3,figsize=(4*3,4))\n",
    "        combination = [[0,1],[0,2],[1,2]]\n",
    "        for comb_index, comb in enumerate(combination): \n",
    "            for i in range(projected_data_stim_anti.shape[0]): \n",
    "                axs[comb_index].plot(projected_data_stim_anti[i,:,comb[0]], projected_data_stim_anti[i,:,comb[1]], \\\n",
    "                                    color=c_vals[i], linestyle=linestyles[0])\n",
    "                axs[comb_index].plot(projected_data_stim_middle[i,:,comb[0]], projected_data_stim_middle[i,:,comb[1]], \\\n",
    "                                    color=c_vals[i], linestyle=linestyles[1])\n",
    "                axs[comb_index].plot(projected_data_stim_go[i,:,comb[0]], projected_data_stim_go[i,:,comb[1]], \\\n",
    "                                    color=c_vals[i], linestyle=linestyles[2])\n",
    "        for ax in axs: \n",
    "            ax.set_title(f\"name: {name}; sname: {sname}\", fontsize=12)\n",
    "\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f\"./twotasks/m_pca_stimulus_{name}_{sname}_seed{seed}_{hyp_dict['addon_name']}_{int_index}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ec1824-4a6d-4f7c-b34b-97e17b64eca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d19143-4948-467b-9fb1-df6cf8665a74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mpn)",
   "language": "python",
   "name": "mpn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
