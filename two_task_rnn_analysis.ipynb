{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd1556c8-a4b6-434b-a60f-37035980bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "import copy \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# PyTorch Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Data Handling and Image Processing\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Style for Matplotlib\n",
    "import scienceplots\n",
    "plt.style.use('science')\n",
    "plt.style.use(['no-latex'])\n",
    "\n",
    "# Scientific Computing and Machine Learning\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.linalg import subspace_angles\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Custom Modules and Extensions\n",
    "sys.path.append(\"../netrep/\")\n",
    "sys.path.append(\"../svcca/\")\n",
    "\n",
    "import networks as nets  # Contains RNNs\n",
    "import net_helpers\n",
    "import mpn_tasks\n",
    "import helper\n",
    "import mpn\n",
    "\n",
    "import scienceplots\n",
    "plt.style.use('science')\n",
    "plt.style.use(['no-latex'])\n",
    "\n",
    "# Memory Optimization\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f19a5112-da33-4c1a-b6ad-1679afd19ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59c418b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 Red, 1 blue, 2 green, 3 purple, 4 orange, 5 teal, 6 gray, 7 pink, 8 yellow\n",
    "c_vals = ['#e53e3e', '#3182ce', '#38a169', '#805ad5','#dd6b20', '#319795', '#718096', '#d53f8c', '#d69e2e',] * 10\n",
    "c_vals_l = ['#feb2b2', '#90cdf4', '#9ae6b4', '#d6bcfa', '#fbd38d', '#81e6d9', '#e2e8f0', '#fbb6ce', '#faf089',] * 10\n",
    "c_vals_d = ['#9b2c2c', '#2c5282', '#276749', '#553c9a', '#9c4221', '#285e61', '#2d3748', '#97266d', '#975a16',] * 10 \n",
    "l_vals = ['solid', 'dashed', 'dotted', 'dashdot', '-', '--', '-.', ':', (0, (3, 1, 1, 1)), (0, (5, 10))]\n",
    "markers_vals = ['o', 'v', '*', 'x', '>', '1', '2', '3', '4', 's', 'p', '*', 'h', 'H', '+', 'x', 'D', 'd', '|', '_']\n",
    "linestyles = [\"-\", \"--\", \"-.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34978bf6-67b8-41bd-a022-a7b46a320686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set seed 158\n",
      "Fixation_off: True; Task_info: True\n",
      "Rules: ['delaygo', 'delayanti']\n",
      "  Input size 8, Output size 3\n",
      "Using CUDA...\n"
     ]
    }
   ],
   "source": [
    "# Reload modules if changes have been made to them\n",
    "from importlib import reload\n",
    "\n",
    "reload(nets)\n",
    "reload(net_helpers)\n",
    "\n",
    "fixseed = False # randomize setting the seed may lead to not perfectly solved results\n",
    "seed = random.randint(1,1000) if not fixseed else 8 # random set the seed to test robustness by default\n",
    "print(f\"Set seed {seed}\")\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "hyp_dict = {}\n",
    "\n",
    "hyp_dict['task_type'] = 'multitask' # int, NeuroGym, multitask\n",
    "hyp_dict['mode_for_all'] = \"random_batch\"\n",
    "hyp_dict['ruleset'] = 'delaygofamily' # low_dim, all, test\n",
    "\n",
    "accept_rules = ('fdgo', 'fdanti', 'delaygo', 'delayanti', 'reactgo', 'reactanti', \n",
    "                'delaydm1', 'delaydm2', 'dmsgo', 'dmcgo', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm')\n",
    "\n",
    "\n",
    "rules_dict = \\\n",
    "    {'all' : ['fdgo', 'reactgo', 'delaygo', 'fdanti', 'reactanti', 'delayanti',\n",
    "              'dm1', 'dm2', 'contextdm1', 'contextdm2', 'multidm',\n",
    "              'delaydm1', 'delaydm2', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm',\n",
    "              'dmsgo', 'dmsnogo', 'dmcgo', 'dmcnogo'],\n",
    "     'low_dim' : ['fdgo', 'reactgo', 'delaygo', 'fdanti', 'reactanti', 'delayanti',\n",
    "                 'delaydm1', 'delaydm2', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm',\n",
    "                 'dmsgo', 'dmsnogo', 'dmcgo', 'dmcnogo'],\n",
    "     'gofamily': ['fdgo', 'fdanti', 'reactgo', 'reactanti', 'delaygo', 'delayanti'],\n",
    "     'delaygo': ['delaygo'],\n",
    "     'delaygofamily': ['delaygo', 'delayanti'],\n",
    "     'fdgo': ['fdgo'],\n",
    "     'fdfamily': ['fdgo', 'fdanti'],\n",
    "     'reactgo': ['reactgo'],\n",
    "     'reactfamily': ['reactgo', 'reactanti'],\n",
    "     'delaydm1': ['delaydm1'],\n",
    "     'delaydmfamily': ['delaydm1', 'delaydm2'],\n",
    "     'dmsgofamily': ['dmsgo', 'dmsnogo'],\n",
    "     'dmsgo': ['dmsgo'],\n",
    "     'dmcgo': ['dmcgo'],\n",
    "     'contextdelayfamily': ['contextdelaydm1', 'contextdelaydm2'],\n",
    "    }\n",
    "    \n",
    "\n",
    "# This can either be used to set parameters OR set parameters and train\n",
    "train = True # whether or not to train the network\n",
    "verbose = True\n",
    "hyp_dict['run_mode'] = 'minimal' # minimal, debug\n",
    "hyp_dict['chosen_network'] = \"vanilla\"\n",
    "\n",
    "# suffix for saving images\n",
    "# inputadd, Wfix, WL2, hL2\n",
    "# inputrandom, Wtrain\n",
    "# noise001\n",
    "# largeregularization\n",
    "# trainetalambda\n",
    "\n",
    "mpn_depth = 1\n",
    "n_hidden = 200\n",
    "\n",
    "hyp_dict['addon_name'] = \"Wtrain+WL2+hL2+reg1e3\"\n",
    "hyp_dict['addon_name'] += f\"+hidden{n_hidden}\"\n",
    "\n",
    "# for coding \n",
    "if hyp_dict['chosen_network'] in (\"gru\", \"vanilla\"):\n",
    "    mpn_depth = 1\n",
    "\n",
    "def current_basic_params():\n",
    "    task_params = {\n",
    "        'task_type': hyp_dict['task_type'],\n",
    "        'rules': rules_dict[hyp_dict['ruleset']],\n",
    "        'dt': 40, # ms, directly influence sequence lengths,\n",
    "        'ruleset': hyp_dict['ruleset'],\n",
    "        'n_eachring': 8, # Number of distinct possible inputs on each ring\n",
    "        'in_out_mode': 'low_dim',  # high_dim or low_dim or low_dim_pos (Robert vs. Laura's paper, resp)\n",
    "        'sigma_x': 0.00, # Laura raised to 0.1 to prevent overfitting (Robert uses 0.01)\n",
    "        'mask_type': 'cost', # 'cost', None\n",
    "        'fixate_off': True, # Second fixation signal goes on when first is off\n",
    "        'task_info': True, \n",
    "        'randomize_inputs': False,\n",
    "        'n_input': 20, # Only used if inputs are randomized,\n",
    "        'modality_diff': False,\n",
    "        'label_strength': False, \n",
    "        'long_delay': 'normal',\n",
    "        'long_response': 'normal',\n",
    "        'long_stimulus': 'normal', \n",
    "        'long_fixation': 'normal', \n",
    "        'adjust_task_prop': True,\n",
    "        'adjust_task_decay': 0.9, \n",
    "    }\n",
    "\n",
    "    print(f\"Fixation_off: {task_params['fixate_off']}; Task_info: {task_params['task_info']}\")\n",
    "\n",
    "    train_params = {\n",
    "        'lr': 1e-3,\n",
    "        'n_batches': 128,\n",
    "        'batch_size': 128,\n",
    "        'gradient_clip': 10,\n",
    "        'valid_n_batch': 50,\n",
    "        'n_datasets': 10000, # Number of distinct batches\n",
    "        'valid_check': None, \n",
    "        'n_epochs_per_set': 1, # longer/shorter training\n",
    "        'weight_reg': 'L2',\n",
    "        'activity_reg': 'L2', \n",
    "        'reg_lambda': 1e-3,\n",
    "\n",
    "        'scheduler': {\n",
    "            'type': 'ReduceLROnPlateau',  # or 'StepLR'\n",
    "            'mode': 'min',                # for ReduceLROnPlateau\n",
    "            'factor': 0.5,                # factor to reduce LR\n",
    "            'patience': 10,                # epochs to wait before reducing LR\n",
    "            'min_lr': 1e-8,\n",
    "            'step_size': 30,              # for StepLR (step every 30 datasets)\n",
    "            'gamma': 0.1                  # for StepLR (multiply LR by 0.1)\n",
    "        },\n",
    "    }\n",
    "\n",
    "    if not train: # some \n",
    "        assert train_params['n_epochs_per_set'] == 0\n",
    "\n",
    "    net_params = {\n",
    "        'net_type': hyp_dict['chosen_network'], # mpn1, dmpn, vanilla\n",
    "        'n_neurons': [1] + [n_hidden] * mpn_depth + [1],\n",
    "        'linear_embed': 200, \n",
    "        'output_bias': False, # Turn off biases for easier interpretation\n",
    "        'loss_type': 'MSE', # XE, MSE\n",
    "        'activation': 'tanh', # linear, ReLU, sigmoid, tanh, tanh_re, tukey, heaviside\n",
    "        'cuda': True,\n",
    "        'monitor_freq': train_params[\"n_epochs_per_set\"],\n",
    "        'monitor_valid_out': True, # Whether or not to save validation output throughout training\n",
    "        'output_matrix': '',# \"\" (default); \"untrained\", or \"orthogonal\"\n",
    "        'input_layer_add': True, \n",
    "        'input_layer_add_trainable': True, # revise this is effectively to [randomize_inputs], tune this\n",
    "        'input_layer_bias': False, \n",
    "        'input_layer': \"trainable\", # for RNN only\n",
    "        'acc_measure': 'stimulus', \n",
    "        \n",
    "        # for one-layer MPN, GRU or Vanilla\n",
    "        'ml_params': {\n",
    "            'bias': True, # Bias of layer\n",
    "            'mp_type': 'mult',\n",
    "            'm_update_type': 'hebb_assoc', # hebb_assoc, hebb_pre\n",
    "            'eta_type': 'scalar', # scalar, pre_vector, post_vector, matrix\n",
    "            'eta_train': False,\n",
    "            # 'eta_init': 'mirror_gaussian', #0.0,\n",
    "            'lam_type': 'scalar', # scalar, pre_vector, post_vector, matrix\n",
    "            'm_time_scale': 4000, # ms, sets lambda\n",
    "            'lam_train': False,\n",
    "            'W_freeze': False, # different combination with [input_layer_add_trainable]\n",
    "        },\n",
    "\n",
    "        # Vanilla RNN params\n",
    "        'leaky': True,\n",
    "        'alpha': 0.2,\n",
    "    }\n",
    "\n",
    "    # Ensure the two options are *not* activated at the same time\n",
    "    assert not (task_params[\"randomize_inputs\"] and net_params[\"input_layer_add\"]), (\n",
    "        \"task_params['randomize_inputs'] and net_params['input_layer_add'] cannot both be True.\"\n",
    "    )\n",
    "\n",
    "    # for multiple MPN layers, assert \n",
    "    if mpn_depth > 1:\n",
    "        for mpl_idx in range(mpn_depth - 1):\n",
    "            assert f'ml_params{mpl_idx}' in net_params.keys()\n",
    "\n",
    "    # actually I don't think it is needed\n",
    "    # putting here to warn the parameter checking every time \n",
    "    # when switching network\n",
    "    if hyp_dict['chosen_network'] in (\"gru\", \"vanilla\"):\n",
    "        assert f'ml_params' in net_params.keys()\n",
    "\n",
    "    return task_params, train_params, net_params\n",
    "\n",
    "task_params, train_params, net_params = current_basic_params()\n",
    "\n",
    "shift_index = 1 if not task_params['fixate_off'] else 0\n",
    "\n",
    "if hyp_dict['task_type'] in ('multitask',):\n",
    "    task_params, train_params, net_params = mpn_tasks.convert_and_init_multitask_params(\n",
    "        (task_params, train_params, net_params)\n",
    "    )\n",
    "\n",
    "    net_params['prefs'] = mpn_tasks.get_prefs(task_params['hp'])\n",
    "\n",
    "    print('Rules: {}'.format(task_params['rules']))\n",
    "    print('  Input size {}, Output size {}'.format(\n",
    "        task_params['n_input'], task_params['n_output'],\n",
    "    ))\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "if net_params['cuda']:\n",
    "    print('Using CUDA...')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('Using CPU...')\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# how many epoch each dataset will be trained on\n",
    "epoch_multiply = train_params[\"n_epochs_per_set\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a341b36-dc7a-42b0-bffb-cb17d380041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_dict[\"mess_with_training\"] = False\n",
    "\n",
    "if hyp_dict['mess_with_training']:\n",
    "    hyp_dict['addon_name'] += \"messwithtraining\"\n",
    "\n",
    "params = task_params, train_params, net_params\n",
    "\n",
    "if net_params['net_type'] == 'mpn1':\n",
    "    netFunction = mpn.MultiPlasticNet\n",
    "elif net_params['net_type'] == 'dmpn':\n",
    "    netFunction = mpn.DeepMultiPlasticNet\n",
    "elif net_params['net_type'] == 'vanilla':\n",
    "    netFunction = nets.VanillaRNN\n",
    "elif net_params['net_type'] == 'gru':\n",
    "    netFunction = nets.GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07e4fe48-2af6-4741-8d42-01803ba0abf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Align ['delaygo', 'delayanti'] With Same Time\n",
      "rng reset with seed 1494\n",
      "rng reset with seed 1494\n",
      "rng reset with seed 1494\n",
      "rng reset with seed 1494\n",
      "rng reset with seed 1494\n",
      "rng reset with seed 1494\n",
      "rng reset with seed 1494\n",
      "rng reset with seed 1494\n",
      "rng reset with seed 1494\n",
      "rng reset with seed 1494\n",
      "delaygo\n",
      "delayanti\n",
      "test_input_longdelay.shape: torch.Size([100, 295, 8])\n",
      "test_output_longdelay.shape: torch.Size([100, 295, 3])\n",
      "test_input_longresponse.shape: torch.Size([100, 311, 8])\n",
      "test_output_longresponse.shape: torch.Size([100, 311, 3])\n",
      "test_input_longstimulus.shape: torch.Size([100, 295, 8])\n",
      "test_output_longstimulus.shape: torch.Size([100, 295, 3])\n"
     ]
    }
   ],
   "source": [
    "test_n_batch = train_params[\"valid_n_batch\"]\n",
    "color_by = \"stim\" # or \"resp\" \n",
    "\n",
    "task_random_fix = True\n",
    "if task_random_fix:\n",
    "    print(f\"Align {task_params['rules']} With Same Time\")\n",
    "\n",
    "if task_params['task_type'] in ('multitask',): # Test batch consists of all the rules\n",
    "    task_params['hp']['batch_size_train'] = test_n_batch\n",
    "    # using homogeneous cutting off\n",
    "    test_mode_for_all = \"random\"\n",
    "    # ZIHAN\n",
    "    # generate test data using \"random\"\n",
    "    test_data, test_trials_extra = mpn_tasks.generate_trials_wrap(task_params, test_n_batch, \\\n",
    "                rules=task_params['rules'], mode_input=test_mode_for_all, fix=task_random_fix\n",
    "    )\n",
    "    _, test_trials, test_rule_idxs = test_trials_extra\n",
    "\n",
    "    # generate test input data with separate period extension to obtain fixed points information\n",
    "    task_params_longdelay = copy.deepcopy(task_params)\n",
    "    task_params_longdelay[\"long_delay\"] = \"long\"\n",
    "    test_data_longdelay, test_trials_extra_longdelay = mpn_tasks.generate_trials_wrap(task_params_longdelay, test_n_batch, \\\n",
    "                                                                                      rules=task_params_longdelay['rules'], \\\n",
    "                                                                                      mode_input=test_mode_for_all, fix=task_random_fix)\n",
    "    \n",
    "    _, test_trials_longdelay, test_rule_idxs_longdelay = test_trials_extra_longdelay\n",
    "\n",
    "    task_params_longresponse = copy.deepcopy(task_params)\n",
    "    task_params_longresponse[\"long_response\"] = \"long\"\n",
    "    test_data_longresponse, test_trials_extra_longresponse = mpn_tasks.generate_trials_wrap(task_params_longresponse, test_n_batch, \\\n",
    "                                                                                      rules=task_params_longresponse['rules'], \\\n",
    "                                                                                      mode_input=test_mode_for_all, fix=task_random_fix)\n",
    "    \n",
    "    _, test_trials_longresponse, test_rule_idxs_longresponse = test_trials_extra_longresponse\n",
    "\n",
    "    task_params_longstimulus = copy.deepcopy(task_params)\n",
    "    task_params_longstimulus[\"long_stimulus\"] = \"long\"\n",
    "    test_data_longstimulus, test_trials_extra_longstimulus = mpn_tasks.generate_trials_wrap(task_params_longstimulus, test_n_batch, \\\n",
    "                                                                                      rules=task_params_longstimulus['rules'], \\\n",
    "                                                                                      mode_input=test_mode_for_all, fix=task_random_fix)\n",
    "    \n",
    "    _, test_trials_longstimulus, test_rule_idxs_longstimulus = test_trials_extra_longstimulus\n",
    "\n",
    "    task_params_longfixation = copy.deepcopy(task_params)\n",
    "    task_params_longfixation[\"long_fixation\"] = \"long\"\n",
    "    test_data_longfixation, test_trials_extra_longfixation = mpn_tasks.generate_trials_wrap(task_params_longfixation, test_n_batch, \\\n",
    "                                                                                      rules=task_params_longfixation['rules'], \\\n",
    "                                                                                      mode_input=test_mode_for_all, fix=task_random_fix)\n",
    "    \n",
    "    _, test_trials_longfixation, test_rule_idxs_longfixation = test_trials_extra_longfixation\n",
    "    \n",
    "\n",
    "    task_params['dataset_name'] = 'multitask'\n",
    "\n",
    "    if task_params['in_out_mode'] in ('low_dim_pos',):\n",
    "        output_dim_labels = ('Fixate', 'Cos', '-Cos', 'Sin', '-Sin')\n",
    "    elif task_params['in_out_mode'] in ('low_dim',):\n",
    "        output_dim_labels = ('Fixate', 'Cos', 'Sin')\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def generate_response_stimulus(task_params, test_trials): \n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        labels_resp, labels_stim = [], []\n",
    "        for rule_idx, rule in enumerate(task_params['rules']):\n",
    "            print(rule)\n",
    "            if rule in accept_rules:\n",
    "                if hyp_dict['ruleset'] in ('dmsgo', 'dmcgo'):\n",
    "                    labels.append(test_trials[rule_idx].meta['matches'])\n",
    "                else:\n",
    "                    labels_resp.append(test_trials[rule_idx].meta['resp1'])\n",
    "                    labels_stim.append(test_trials[rule_idx].meta['stim1']) \n",
    "    \n",
    "            else:\n",
    "                raise NotImplementedError()\n",
    "                \n",
    "        labels_resp = np.concatenate(labels_resp, axis=0).reshape(-1,1)\n",
    "        labels_stim = np.concatenate(labels_stim, axis=0).reshape(-1,1)\n",
    "\n",
    "        return labels_resp, labels_stim\n",
    "\n",
    "    labels_resp, labels_stim = generate_response_stimulus(task_params, test_trials)\n",
    "\n",
    "labels = labels_stim if color_by == \"stim\" else labels_resp\n",
    "    \n",
    "test_input, test_output, test_mask = test_data\n",
    "test_input_longfixation, test_output_longfixation, test_mask_longfixation = test_data_longfixation\n",
    "test_input_longstimulus, test_output_longstimulus, test_mask_longstimulus = test_data_longstimulus\n",
    "test_input_longdelay, test_output_longdelay, test_mask_longdelay = test_data_longdelay\n",
    "test_input_longresponse, test_output_longresponse, test_mask_longresponse = test_data_longresponse\n",
    "\n",
    "print(f\"test_input_longdelay.shape: {test_input_longdelay.shape}\")\n",
    "print(f\"test_output_longdelay.shape: {test_output_longdelay.shape}\")\n",
    "print(f\"test_input_longresponse.shape: {test_input_longresponse.shape}\")\n",
    "print(f\"test_output_longresponse.shape: {test_output_longresponse.shape}\")\n",
    "print(f\"test_input_longstimulus.shape: {test_input_longstimulus.shape}\")\n",
    "print(f\"test_output_longstimulus.shape: {test_output_longstimulus.shape}\")\n",
    "\n",
    "# permutation = np.random.permutation(test_input.shape[0])\n",
    "# test_input = test_input[permutation]\n",
    "# test_output = test_output[permutation]\n",
    "# test_mask = test_mask[permutation]\n",
    "# labels = labels[permutation]\n",
    "\n",
    "test_input_np = test_input.detach().cpu().numpy()\n",
    "test_output_np = test_output.detach().cpu().numpy()\n",
    "\n",
    "# Total number of batches, might be different than test_n_batch\n",
    "# this should be the same regardless of variety of test_input\n",
    "n_batch_all = test_input_np.shape[0] \n",
    "\n",
    "def find_task(task_params, test_input_np, shift_index):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    test_task = [] # which task\n",
    "    for batch_idx in range(test_input_np.shape[0]):\n",
    "        \n",
    "        if task_params[\"randomize_inputs\"]: \n",
    "            test_input_np_ = test_input_np @ np.linalg.pinv(task_params[\"randomize_matrix\"])\n",
    "        else: \n",
    "            test_input_np_ = test_input_np\n",
    "            \n",
    "        task_label = test_input_np_[batch_idx, 0, 6-shift_index:]\n",
    "        # task_label_index = np.where(task_label == 1)[0][0]\n",
    "        \n",
    "        # tol = 1e-3      \n",
    "        # mask = np.isclose(task_label, 1, atol=tol)\n",
    "        task_label = np.asarray(task_label)       \n",
    "        dist = np.abs(task_label - 1)     \n",
    "        mask = dist == dist.min() \n",
    "        \n",
    "        indices = np.where(mask)[0]\n",
    "        \n",
    "        if indices.size:                \n",
    "            task_label_index = indices[0]   \n",
    "        else:\n",
    "            raise ValueError(\"No entry close enough to 1 found\")\n",
    "            \n",
    "        test_task.append(task_label_index)\n",
    "\n",
    "    return test_task  \n",
    "\n",
    "test_task = find_task(task_params, test_input_np, shift_index)\n",
    "\n",
    "test_task_longfixation = find_task(task_params_longfixation, test_input_longfixation.detach().cpu().numpy(), shift_index)\n",
    "test_task_longstimulus = find_task(task_params_longstimulus, test_input_longstimulus.detach().cpu().numpy(), shift_index)\n",
    "test_task_longdelay = find_task(task_params_longdelay, test_input_longdelay.detach().cpu().numpy(), shift_index)\n",
    "test_task_longresponse = find_task(task_params_longresponse, test_input_longresponse.detach().cpu().numpy(), shift_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78b44e9-b6e3-4c1c-a5c7-5a608a6d090b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla RNN:\n",
      "  n_neurons - input: 8, hidden: 200, output: 3\n",
      "  input layer: trainable // rec layer: trainable // TBPTT: False\n",
      " output layer: trainable\n",
      "  Act: tanh\n",
      "  Leaky updates, timescale 200 ms (old: 0.8 new: 0.2)\n",
      "Trainable parameters: 42,400\n",
      "W_input: (200, 8)\n",
      "W_rec: (200, 200)\n",
      "Parameter containing:\n",
      "tensor([[-0.0977, -0.0795,  0.0063,  ...,  0.0386, -0.0322,  0.0722],\n",
      "        [-0.0407, -0.0809, -0.0435,  ...,  0.0392,  0.0123, -0.0998],\n",
      "        [ 0.1034,  0.0041,  0.0840,  ..., -0.0953, -0.0710,  0.0198],\n",
      "        ...,\n",
      "        [-0.1130, -0.1030,  0.0349,  ..., -0.0054,  0.1145, -0.0544],\n",
      "        [-0.0933,  0.0808,  0.1189,  ...,  0.0848, -0.0728,  0.0321],\n",
      "        [ 0.0603,  0.1072, -0.0363,  ...,  0.0390,  0.0952,  0.0533]],\n",
      "       requires_grad=True)\n",
      "W_output: (3, 200)\n",
      "b_hidden: (200,)\n",
      "task_params['rules_probs']: [0.5 0.5]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 100, max_seq_len 100\n",
      "inputs_all paddled: (128, 100, 8)\n",
      "inputs_all: torch.Size([128, 100, 8])\n",
      "========== Setup Parameters ==========\n",
      "Train parameters:\n",
      "  Loss: MSE // LR: 1.00e-03 // Optim: adam\n",
      "  Grad type: backprop // Gradient clip: 1.0e+01\n",
      "Weight reg: L2, coef: 1.0e-03\n",
      "Activity reg: L2, coef: 1.0e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zihan.zhang/.conda/envs/mpn/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 100, LR: 1.000e-03 - train_loss:2.042e-01, rounded train_acc:0.000, valid_loss:2.357e-01, rounded valid_acc:0.000\n",
      "task_params['rules_probs']: [0.49998362 0.50001638]\n",
      "Rule: delaygo\n",
      "Rule delaygo seq_len 99, max_seq_len 99\n",
      "inputs_all paddled: (128, 99, 8)\n",
      "inputs_all: torch.Size([128, 99, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 200, LR: 1.000e-03 - train_loss:1.908e-01, rounded train_acc:0.153, valid_loss:1.872e-01, rounded valid_acc:0.127\n",
      "task_params['rules_probs']: [0.5196531 0.4803469]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 107, max_seq_len 107\n",
      "inputs_all paddled: (128, 107, 8)\n",
      "inputs_all: torch.Size([128, 107, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 300, LR: 1.000e-03 - train_loss:1.343e-01, rounded train_acc:0.672, valid_loss:1.423e-01, rounded valid_acc:0.602\n",
      "task_params['rules_probs']: [0.48059933 0.51940067]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 96, max_seq_len 96\n",
      "inputs_all paddled: (128, 96, 8)\n",
      "inputs_all: torch.Size([128, 96, 8])\n",
      "valid_acc_history: [None, None, None, None]\n",
      "Iter: 400, LR: 1.000e-03 - train_loss:1.296e-01, rounded train_acc:0.641, valid_loss:1.321e-01, rounded valid_acc:0.620\n",
      "task_params['rules_probs']: [0.48287043 0.51712957]\n",
      "Rule: delayanti\n",
      "Rule delayanti seq_len 104, max_seq_len 104\n",
      "inputs_all paddled: (128, 104, 8)\n",
      "inputs_all: torch.Size([128, 104, 8])\n",
      "valid_acc_history: [None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "# we use net at different training stage on the same test_input\n",
    "start_time = time.time()\n",
    "net, _, (counter_lst, netout_lst, db_lst, Winput_lst, Winputbias_lst,\\\n",
    "         Woutput_lst, Wall_lst, marker_lst, loss_lst, acc_lst), _ = net_helpers.train_network(params, device=device, verbose=verbose,\n",
    "                                                                                              train=train, hyp_dict=hyp_dict,\n",
    "                                                                                              netFunction=netFunction,\n",
    "                                                                                              test_input=[test_input, \n",
    "                                                                                                          test_input_longfixation,\n",
    "                                                                                                          test_input_longstimulus, \n",
    "                                                                                                          test_input_longdelay,\n",
    "                                                                                                          test_input_longresponse],\n",
    "                                                                                              print_frequency=100)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Running Time: {end_time - start_time}\")\n",
    "counter_lst = [x * epoch_multiply + 1 for x in counter_lst] # avoid log plot issue    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4319f0e3-121a-49c4-93eb-97724ef57715",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_dict['addon_name'] += \"rnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c10ab17-bab6-4228-9096-af042e9ac385",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_finalstage = False\n",
    "if use_finalstage:\n",
    "    # plotting output in the validation set\n",
    "    net_out, db = net.iterate_sequence_batch(test_input, run_mode='track_states')\n",
    "    W_output = net.W_output.detach().cpu().numpy()\n",
    "\n",
    "    W_all_ = []\n",
    "    for i in range(len(net.mp_layers)):\n",
    "        W_all_.append(net.mp_layers[i].W.detach().cpu().numpy())\n",
    "    W_ = W_all_[0]\n",
    "    \n",
    "else:\n",
    "    # ind = len(marker_lst)-1 \n",
    "    ind = -1\n",
    "    # network_at_percent = (marker_lst[ind]+1)/train_params['n_datasets']*100\n",
    "    # print(f\"Using network at {network_at_percent}%\")\n",
    "    # by default using the first test_input \n",
    "    net_out = netout_lst[0][ind]\n",
    "    db = db_lst[0][ind]\n",
    "    W_output = Woutput_lst[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8feb8c0-4029-49b1-b6e9-a5fd01ab5efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_input_output(test_input_np, net_out, test_output_np, test_task=None, tag=\"\", batch_num=5, label=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    test_input_np = helper.to_ndarray(test_input_np)\n",
    "    net_out = helper.to_ndarray(net_out)\n",
    "    test_output_np = helper.to_ndarray(test_output_np)\n",
    "    \n",
    "    fig_all, axs_all = plt.subplots(batch_num,2,figsize=(4*2,batch_num*2))\n",
    "    \n",
    "    if test_output_np.shape[-1] == 1:\n",
    "        for batch_idx, ax in enumerate(axs):\n",
    "            ax.plot(net_out[batch_idx, :, 0], color=c_vals[batch_idx])\n",
    "            ax.plot(test_output_np[batch_idx, :, 0], color=c_vals_l[batch_idx])\n",
    "    \n",
    "    else:\n",
    "        for batch_idx in range(batch_num):\n",
    "            for out_idx in range(test_output_np.shape[-1]):\n",
    "                axs_all[batch_idx,0].plot(net_out[batch_idx, :, out_idx], color=c_vals[out_idx], label=out_idx)\n",
    "                axs_all[batch_idx,0].plot(test_output_np[batch_idx, :, out_idx], color=c_vals_l[out_idx], linewidth=5, alpha=0.5)\n",
    "                if test_task is not None: \n",
    "                    outname = f\"{task_params['rules'][test_task[batch_idx]]}; {tag}\"\n",
    "                    axs_all[batch_idx,0].set_title(outname)\n",
    "                axs_all[batch_idx,0].legend()\n",
    "    \n",
    "            input_batch = test_input_np[batch_idx,:,:]\n",
    "            if task_params[\"randomize_inputs\"]: \n",
    "                input_batch = input_batch @ np.linalg.pinv(task_params[\"randomize_matrix\"])\n",
    "            for inp_idx in range(input_batch.shape[-1]):\n",
    "                axs_all[batch_idx,1].plot(input_batch[:,inp_idx], color=c_vals[inp_idx], label=inp_idx)\n",
    "                if test_task is not None: \n",
    "                    axs_all[batch_idx,1].set_title(f\"{task_params['rules'][test_task[batch_idx]]}; {tag}\")\n",
    "                axs_all[batch_idx,1].legend()\n",
    "\n",
    "    for ax in axs_all.flatten(): \n",
    "        ax.set_ylim([-2, 2])\n",
    "    fig_all.tight_layout()\n",
    "    fig_all.savefig(f\"./twotasks/lowD_{hyp_dict['ruleset']}_{hyp_dict['chosen_network']}_seed{seed}_{hyp_dict['addon_name']}_{tag}.png\", dpi=300)\n",
    "\n",
    "    return fig_all, axs_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d2228b-2da9-40aa-8cdc-e624560c4b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_input_output(test_input_np, net_out, test_output_np, test_task, tag=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b78d25-9681-4c92-9fff-0cf68f6d1b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_label_task_comb(task_params, test_trials, test_task, color_by=\"stim\"):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    labels_resp, labels_stim = generate_response_stimulus(task_params, test_trials)\n",
    "    labels_ = labels_stim if color_by == \"stim\" else labels_resp\n",
    "    # pair: [label_first_dim, task_id] for each trial\n",
    "    return np.column_stack((labels_[:, 0], test_task))\n",
    "\n",
    "label_task_comb_longdelay = make_label_task_comb(\n",
    "    task_params_longdelay, test_trials_longdelay, test_task_longdelay, color_by=color_by\n",
    ")\n",
    "\n",
    "label_task_comb_longresponse = make_label_task_comb(\n",
    "    task_params_longresponse, test_trials_longresponse, test_task_longresponse, color_by=color_by\n",
    ")\n",
    "\n",
    "label_task_comb_longstimulus = make_label_task_comb(\n",
    "    task_params_longstimulus, test_trials_longstimulus, test_task_longstimulus, color_by=color_by\n",
    ")\n",
    "\n",
    "label_task_comb_longfixation = make_label_task_comb(\n",
    "    task_params_longfixation, test_trials_longfixation, test_task_longfixation, color_by=color_by\n",
    ")\n",
    "\n",
    "label_task_comb = make_label_task_comb(\n",
    "    task_params, test_trials, test_task, color_by=color_by\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c23ea03-ed43-4b9c-a4ca-b922020fa4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_similarity(Ms_orig, hs, net, net_params, label_task_comb, checktime, compare=\"modulation\", moddim=0): \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # print(f\"compare: {compare}; moddim: {moddim}\")\n",
    "    inverse_modulation_ss_dt = []\n",
    "    inverse_modulation_sr_dt = []\n",
    "    inverse_modulation_st_ds = [[], []]\n",
    "    modulation_save = [[],[]]\n",
    "    modulation_save_time = [[],[]]\n",
    "    hidden_save_time = [[],[]]\n",
    "\n",
    "    if compare == \"w_modulation\":\n",
    "        Ms_orig = Ms_orig * W[None,None,:,:]\n",
    "\n",
    "    # same stimulus (effectively anti-response), different task\n",
    "    for k in range(8):\n",
    "        ind1 = [i for i, lst in enumerate(label_task_comb) if np.array_equal(lst, [k, 0])]\n",
    "        ind2 = [i for i, lst in enumerate(label_task_comb) if np.array_equal(lst, [k, 1])]\n",
    "        ll = min(len(ind1), len(ind2))\n",
    "\n",
    "        # M @ win: effective plastic sensitivity\n",
    "        # How does the current memory (M) distort or amplify specific input channels?\n",
    "        if compare in (\"modulation\", \"w_modulation\"):                 \n",
    "            winadd = False if moddim == None else True\n",
    " \n",
    "            if winadd: \n",
    "                if moddim == \"Win\": \n",
    "                    Ms1_change_stimulus = [((Ms_orig[ind1[i],checktime,:,:]) @ win).flatten() for i in range(ll)]\n",
    "                    Ms2_change_stimulus = [((Ms_orig[ind2[i],checktime,:,:]) @ win).flatten() for i in range(ll)]\n",
    "                else:\n",
    "                    Ms1_change_stimulus = [((Ms_orig[ind1[i],checktime,:,:]) @ win)[:,moddim].flatten() for i in range(ll)]\n",
    "                    Ms2_change_stimulus = [((Ms_orig[ind2[i],checktime,:,:]) @ win)[:,moddim].flatten() for i in range(ll)]\n",
    "            else:\n",
    "                Ms1_change_stimulus = [(Ms_orig[ind1[i],checktime,:,:]).flatten() for i in range(ll)]\n",
    "                Ms2_change_stimulus = [(Ms_orig[ind2[i],checktime,:,:]).flatten() for i in range(ll)]\n",
    "                \n",
    "        elif compare == \"hidden\": \n",
    "            Ms1_change_stimulus = [hs[ind1[i],checktime,:].flatten() for i in range(ll)]\n",
    "            Ms2_change_stimulus = [hs[ind2[i],checktime,:].flatten() for i in range(ll)]\n",
    "\n",
    "        # since we group by the same stimulus at the same task\n",
    "        # the period is aligned in all trials of testing data\n",
    "        # therefore the resulting modulation & hidden should be identical\n",
    "        assert_sums_close(Ms1_change_stimulus, rtol=1e-3, atol=1e-3)\n",
    "        assert_sums_close(Ms2_change_stimulus, rtol=1e-3, atol=1e-3)\n",
    "        \n",
    "        inverse_modulation_ss_dt.append(1 - cosine(Ms1_change_stimulus[0], Ms2_change_stimulus[0]))\n",
    "\n",
    "        modulation_save[0].append(Ms1_change_stimulus[0])\n",
    "        modulation_save[1].append(Ms2_change_stimulus[0])\n",
    "\n",
    "        # modulation for one type of stimulus for two tasks\n",
    "        Ms1_all = Ms_orig[ind1[0],:,:,:]\n",
    "        Ms2_all = Ms_orig[ind2[0],:,:,:]\n",
    "        h1_all = hs[ind1[0],:,:]\n",
    "        h2_all = hs[ind2[0],:,:]\n",
    "        # save the whole modulation trajectory for this stimulus (go and anti)\n",
    "        modulation_save_time[0].append(Ms1_all)\n",
    "        modulation_save_time[1].append(Ms2_all)\n",
    "        hidden_save_time[0].append(h1_all)\n",
    "        hidden_save_time[1].append(h2_all)\n",
    "\n",
    "    # same response, different task \n",
    "    for k in range(8):\n",
    "        ind1 = [i for i, lst in enumerate(label_task_comb) if np.array_equal(lst, [k, 0])]\n",
    "        ind2 = [i for i, lst in enumerate(label_task_comb) if np.array_equal(lst, [(k + 4) % 8, 1])]\n",
    "        ll = min(len(ind1), len(ind2))\n",
    "\n",
    "        if compare in (\"modulation\", \"w_modulation\"): \n",
    "            winadd = False if moddim == None else True\n",
    " \n",
    "            if winadd: \n",
    "                if moddim == \"Win\": \n",
    "                    Ms1_change_stimulus = [((Ms_orig[ind1[i],checktime,:,:]) @ win).flatten() for i in range(ll)]\n",
    "                    Ms2_change_stimulus = [((Ms_orig[ind2[i],checktime,:,:]) @ win).flatten() for i in range(ll)]\n",
    "                else:\n",
    "                    Ms1_change_stimulus = [((Ms_orig[ind1[i],checktime,:,:]) @ win)[:,moddim].flatten() for i in range(ll)]\n",
    "                    Ms2_change_stimulus = [((Ms_orig[ind2[i],checktime,:,:]) @ win)[:,moddim].flatten() for i in range(ll)]\n",
    "            else:\n",
    "                Ms1_change_stimulus = [(Ms_orig[ind1[i],checktime,:,:]).flatten() for i in range(ll)]\n",
    "                Ms2_change_stimulus = [(Ms_orig[ind2[i],checktime,:,:]).flatten() for i in range(ll)]\n",
    "                \n",
    "        elif compare == \"hidden\": \n",
    "            Ms1_change_stimulus = [hs[ind1[i],checktime,:].flatten() for i in range(ll)]\n",
    "            Ms2_change_stimulus = [hs[ind2[i],checktime,:].flatten() for i in range(ll)]\n",
    "\n",
    "        assert_sums_close(Ms1_change_stimulus, rtol=1e-3, atol=1e-3)\n",
    "        assert_sums_close(Ms2_change_stimulus, rtol=1e-3, atol=1e-3)\n",
    "        \n",
    "        inverse_modulation_sr_dt.append(1 - cosine(Ms1_change_stimulus[0], Ms2_change_stimulus[0]))\n",
    "\n",
    "    # same task, different stimulus \n",
    "    repeat = 100\n",
    "    modulation_matrices_all = [] \n",
    "    for _ in range(repeat): \n",
    "        modulation_matrices = [\n",
    "            np.full((len(modulation_save[0]), len(modulation_save[0])), np.nan), \n",
    "            np.full((len(modulation_save[0]), len(modulation_save[0])), np.nan)\n",
    "        ]\n",
    "        \n",
    "        for i in range(len(modulation_save[0])):\n",
    "            for j in range(i+1, len(modulation_save[0])):\n",
    "                modulation_matrices[0][i,j] = 1 - cosine(modulation_save[0][i], modulation_save[0][j])\n",
    "                modulation_matrices[1][i,j] = 1 - cosine(modulation_save[1][i], modulation_save[1][j])\n",
    "\n",
    "        try: \n",
    "            modulation_matrices_all.append([np.nanmean(sample_non_nan(modulation_matrices[0], 8)),\n",
    "                                            np.nanmean(sample_non_nan(modulation_matrices[1], 8))])\n",
    "        except Exception as e:\n",
    "            modulation_matrices_all.append([np.nan, np.nan])\n",
    "\n",
    "    modulation_matrices_all = np.array(modulation_matrices_all)\n",
    "\n",
    "    result = [[np.mean(inverse_modulation_ss_dt), np.std(inverse_modulation_ss_dt)], \n",
    "              [np.mean(inverse_modulation_sr_dt), np.std(inverse_modulation_sr_dt)], \n",
    "              [np.mean(modulation_matrices_all[:,0]), np.std(modulation_matrices_all[:,0])], \n",
    "              [np.mean(modulation_matrices_all[:,1]), np.std(modulation_matrices_all[:,1])]]\n",
    "\n",
    "    return result, modulation_save_time, hidden_save_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aa9796-4f22-4e96-9521-435a9bd290ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_non_nan(arr, k):\n",
    "    \"\"\"\n",
    "    Pick `k` distinct (non-NaN) numbers from a 2-D NumPy array.\n",
    "    \"\"\"\n",
    "    pool = arr[~np.isnan(arr)]            # flatten & keep only real numbers\n",
    "    if k > pool.size:                     # ensure enough unique values\n",
    "        raise ValueError(\"k exceeds number of non-NaN entries.\")\n",
    "    return np.random.choice(pool, k, replace=False).tolist()\n",
    "\n",
    "\n",
    "def assert_sums_close(arr_list, rtol=1e-5, atol=1e-8):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    assert len(arr_list) > 0, \"Empty list.\"\n",
    "    sums = np.array([np.sum(a) for a in arr_list], dtype=float)\n",
    "    ref = sums[0]\n",
    "    ok = np.isclose(sums, ref, rtol=rtol, atol=atol)\n",
    "\n",
    "    if not np.all(ok):\n",
    "        bad = np.where(~ok)[0]\n",
    "        raise AssertionError(\n",
    "            f\"Sum mismatch at indices {bad.tolist()}.\\n\"\n",
    "            f\"ref_sum={ref}, bad_sums={sums[bad].tolist()}, all_sums={sums.tolist()}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1f92a0-c04f-4a7a-8b08-e753efb1b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modulation_extraction_rnn(test_input, db, cuda=False):\n",
    "    \"\"\"\n",
    "    Extracts modulation tensors from `db` and returns:\n",
    "        Ms:      (batch, seq, features) reshaped version of M\n",
    "        Ms_orig: original M (no reshape)\n",
    "        hs:      (batch, seq, features) reshaped version of hidden\n",
    "        bs:      bias vector/matrix as-is (or concatenated if list)\n",
    "    \"\"\"\n",
    "\n",
    "    def _to_numpy(x):\n",
    "        # Convert torch.Tensor -> numpy, otherwise np.asarray\n",
    "        try:\n",
    "            import torch\n",
    "            if isinstance(x, torch.Tensor):\n",
    "                return x.detach().cpu().numpy()\n",
    "        except Exception:\n",
    "            pass\n",
    "        return np.asarray(x)\n",
    "\n",
    "    def _concat_last(x):\n",
    "        # If list/tuple of arrays: concatenate on last axis; else return as-is\n",
    "        return np.concatenate(x, axis=-1) if isinstance(x, (list, tuple)) else x\n",
    "\n",
    "    n_batch, max_seq_len = test_input.shape[0], test_input.shape[1]\n",
    "    \n",
    "    # hidden\n",
    "    H_raw = _concat_last(_to_numpy(db[f\"hidden\"]))\n",
    "    hs = H_raw.reshape(n_batch, max_seq_len, -1)\n",
    "\n",
    "    return hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a350fae9-ae28-4341-a5e9-a6d98bd227f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# across training stage\n",
    "result_attractor_all_h, result_attractor_all_m, result_attractor_all_wmwin = [], [], []\n",
    "modulation_save_time = []\n",
    "pr_all = [] \n",
    "test_input_long_all = [test_input, test_input_longfixation, test_input_longstimulus, \n",
    "                       test_input_longdelay, test_input_longresponse]\n",
    "label_task_comb_long_all = [label_task_comb, label_task_comb_longfixation, label_task_comb_longstimulus, \n",
    "                            label_task_comb_longdelay, label_task_comb_longresponse]\n",
    "# register the timestamps information for different long periods\n",
    "time_stamps_usual, time_stamps_longfixation, time_stamps_longstimulus, time_stamps, time_stamps_longresponse = {}, {}, {}, {}, {}\n",
    "\n",
    "for i in range(5): \n",
    "    for db_attractor in db_lst[i]:\n",
    "        h_long = modulation_extraction_rnn(test_input_long_all[i], db_attractor)\n",
    "        \n",
    "        # to handle noise, find the time when fixation is off\n",
    "        checktime_sample = test_input_long_all[i][0,:,0].detach().cpu()\n",
    "        mask = checktime_sample < 0.5 \n",
    "        idx = torch.nonzero(mask, as_tuple=False) \n",
    "        checktime_attractor = idx[0].item()  \n",
    "\n",
    "        if i == 3: \n",
    "            time_stamps[\"delay_end\"] = checktime_attractor - 1 # a little bit before the fixation off\n",
    "        elif i == 4:\n",
    "            time_stamps_longresponse[\"delay_end\"] = checktime_attractor - 1\n",
    "        elif i == 0: \n",
    "            time_stamps_usual[\"delay_end\"] = checktime_attractor - 1\n",
    "            cc = time_stamps_usual[\"delay_end\"]\n",
    "        elif i == 2: \n",
    "            time_stamps_longstimulus[\"delay_end\"] = checktime_attractor - 1\n",
    "        elif i == 1:\n",
    "            time_stamps_longfixation[\"delay_end\"] = checktime_attractor - 1\n",
    "\n",
    "        if i == 0: \n",
    "            result_attractor_h, _, _ = analyze_similarity(np.zeros((100, 100, 100, 100), dtype=np.float32), h_long, net, net_params, label_task_comb_long_all[i], \n",
    "                                                            checktime=cc, compare=\"hidden\")\n",
    "            # result_attractor_m, m_save, _ = analyze_similarity(M_long, h_long, net, net_params, label_task_comb_long_all[i], \n",
    "            #                                                 checktime=cc, compare=\"modulation\")\n",
    "            # result_attractor_wmwin, _, _ = analyze_similarity(M_long, h_long, net, net_params, label_task_comb_long_all[i], \n",
    "            #                                                 checktime=cc, compare=\"w_modulation\", moddim=\"Win\")\n",
    "            \n",
    "            result_attractor_all_h.append(result_attractor_h)\n",
    "            # result_attractor_all_m.append(result_attractor_m)\n",
    "            # result_attractor_all_wmwin.append(result_attractor_wmwin)\n",
    "            # modulation_save_time.append(m_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397a4952-20cc-4e7d-81da-4513ac52a37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "figattractor, axsattractor = plt.subplots(1,1,figsize=(4,4))\n",
    "break_names = [\n",
    "    \"Same Stim\",\n",
    "    \"Same Resp\",\n",
    "    \"MemoryPro Diff Stim\",\n",
    "    \"MemoryAnti Diff Stim\",\n",
    "]\n",
    "\n",
    "def plot_mean_std(ax, x, mean, std, color, fill_color, label):\n",
    "    ax.plot(x, mean, \"-o\", color=color, label=label)\n",
    "    ax.fill_between(x, np.asarray(mean) - np.asarray(std), np.asarray(mean) + np.asarray(std),\n",
    "                    alpha=0.5, color=fill_color)\n",
    "\n",
    "# each entry: (results, axis_index)\n",
    "panels = [\n",
    "    (result_attractor_all_h, 0),\n",
    "]\n",
    "\n",
    "n_groups = len(result_attractor_all_h[0])  # same as your original len(result_attractor_all[0])\n",
    "\n",
    "for i in range(n_groups):\n",
    "    for results, ax_idx in panels:\n",
    "        mean = [rs[i][0] for rs in results]\n",
    "        std  = [rs[i][1] for rs in results]\n",
    "        plot_mean_std(\n",
    "            axsattractor,\n",
    "            counter_lst,\n",
    "            mean,\n",
    "            std,\n",
    "            color=c_vals[i],\n",
    "            fill_color=c_vals_l[i],\n",
    "            label=break_names[i],\n",
    "        )\n",
    "\n",
    "axsattractor.set_xscale(\"log\")\n",
    "axsattractor.legend()\n",
    "axsattractor.set_ylabel(\"Cosine Similarity\", fontsize=12)\n",
    "axsattractor.set_xlabel(\"Iteration\", fontsize=12)\n",
    "    \n",
    "axsattractor.set_title(\"Hidden\", fontsize=12)\n",
    "\n",
    "figattractor.tight_layout()\n",
    "figattractor.savefig(\n",
    "    f\"./twotasks/attractor_{hyp_dict['ruleset']}_seed{seed}_{hyp_dict['addon_name']}.png\",\n",
    "    dpi=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37d6a4f-57e2-480b-93b9-a005017cf77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "learning_hm_similarity = {\n",
    "    \"break_names\": break_names, \n",
    "    \"counter_lst\": counter_lst, \n",
    "    \"result_attractor_all_h\": result_attractor_all_h, \n",
    "}\n",
    "\n",
    "save_all = {\n",
    "    \"learning_hm_similarity\": learning_hm_similarity, \n",
    "}\n",
    "\n",
    "filename = f\"twotasks_data/seed{seed}_{hyp_dict['addon_name']}.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(save_all, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb580f4-bd30-404b-ab6e-db2039048c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "woutput = net.W_output.detach().cpu().numpy()\n",
    "hiddens = db_lst[0][-1][\"hidden\"]\n",
    "outputs = hiddens @ woutput.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a0f11c-45f2-4a4d-8643-a18f4d8116cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(8): \n",
    "    fig, axs = plt.subplots(1,2,figsize=(4*2,2))\n",
    "    match1 = np.where(np.all(label_task_comb == np.array([k,0]), axis=1))[0]\n",
    "    match2 = np.where(np.all(label_task_comb == np.array([(k+4)%8,1]), axis=1))[0]\n",
    "    for j in range(3):\n",
    "        axs[0].plot(outputs[match1[0],:,j])\n",
    "    for j in range(3):\n",
    "        axs[1].plot(outputs[match2[0],:,j])\n",
    "    for ax in axs:\n",
    "        ax.set_ylim([-1.1,1.1])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    print(1-cosine(hiddens[match1[0],cc,:].flatten(), hiddens[match2[0],cc,:].flatten()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mpn)",
   "language": "python",
   "name": "mpn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
